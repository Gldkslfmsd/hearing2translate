{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Matthias Lendermann e oggi vi fornirò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi utilizzando il tagging multi-insieme e le permutazioni latenti. Questo lavoro è stato realizzato in collaborazione con i miei supervisori Alexander Koller e Ivan Titoff.\n\nLa generalizzazione composizionale può essere intesa come la capacità di un apprenditore di gestire una ricorsione più profonda e composizioni di frasi non viste in precedenza, ma che sono state osservate individualmente durante l'addestramento. Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe svolgersi nel seguente modo. Come al solito, disponiamo di un insieme di addestramento di enunciati, in questo caso, \"la ragazza dormì\" e \"Mary sapeva che la ragazza dormì\". Questi enunciati sono accoppiati con forme logiche che rappresentano gli aspetti fondamentali del loro significato. A differenza della valutazione standard del machine learning, l'insieme di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha osservato una ricorsione più superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda.\n\nI modelli sequenziali-sequenziali ingenui faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output distaccati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate in colore nell'esempio. Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi sono progettati per catturare il processo composizionale che collega gli enunciati alle forme logiche. Questo funziona bene, ma gli alberi non sono solitamente forniti e devono essere ottenuti in qualche modo. Questo può essere un processo complicato e talvolta computazionalmente costoso. Di solito, ciò comporta una pre-elaborazione formalistica considerevole delle forme logiche, ad esempio per gestire i simboli variabili. L'ottenimento degli alberi può anche richiedere procedure specializzate di induzione grammaticale.\n\nNel nostro articolo, non utilizziamo alberi e introduciamo un modello sequenziale-sequenziale neurale che modella direttamente le corrispondenze tra frammenti di input e frammenti di output. Per la prima volta, dimostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due passaggi. Prima, etichettiamo ogni token di input con un multi-insieme non ordinato di token che appariranno nell'output. Dopo il primo passo, abbiamo tutti i token corretti, ma non sono ordinati. Ecco perché, nel secondo passo, utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine corretto. Introduciamo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo.\n\nConcettualmente, il nostro modello di permutazione funziona approssimativamente nel seguente modo. Andiamo da sinistra a destra sull'output e determiniamo quale token del multi-insieme posizionare in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno come evidenziato in rosso. Quindi, saltiamo al prossimo token del multi-insieme per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token del multi-insieme. Continuiamo questo processo fino a quando ogni token dalla prima fase è stato visitato esattamente una volta.\n\nPer darvi un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark Koggs. Il nostro modello supera gli altri con un ampio margine nella generalizzazione alla ricorsione più profonda. Tuttavia, altri tipi di generalizzazione strutturale rimangono molto impegnativi.\n\nNel nostro articolo, affrontiamo diverse interessanti sfide tecniche. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-insieme provenga, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento.\n\nIl nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è NP-difficile. Ciò è dovuto al fatto che è correlato al problema del commesso viaggiatore. Approssimiamo questo con una rilassamento continuo amico della GPU che ci consente anche di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili. Se desiderate saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, vi invitiamo a leggere il nostro articolo o a visitare il nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Myra, e oggi parlerò del nostro articolo, \"Marked Personas: l'uso di prompt di linguaggio naturale per misurare gli stereotipi nei modelli linguistici\". Questo lavoro è stato realizzato in collaborazione con Essendermouch e Dangerowski. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLMs. Tuttavia, queste misurazioni presentano varie limitazioni. Di solito si basano su dataset costruiti a mano, che richiedono molto tempo per essere curati. Inoltre, di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con determinati gruppi. Inoltre, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, ovvero il concetto secondo cui le identità sociali multifaccettate possono amplificare i pregiudizi e diventare luoghi unici di danno.\n\nPer superare queste limitazioni, ci affidiamo alla proprietà che questi più recenti LLMs accordati per istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario utilizzando un prompt come: \"Immagina di essere una donna asiatica, descriviti\". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità desideriamo in questo prompt. Quindi ecco alcuni esempi di generazioni da GPT 4. Immediatamente vediamo che, sebbene i risultati non siano eccessivamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è ritratta come non pretenziosa, la donna mediorientale è definita con parole come esotica e affascinante, e entrambe le persone di colore fanno riferimento alla loro discendenza, mentre la persona dell'uomo bianco non ha nulla di tutto ciò.\n\nPer catturare questi schemi, il nostro metodo ha due parti. La prima è la generazione di queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che, fornendoli a soggetti umani, sono stati in grado di evidenziare anche stereotipi razziali. E questo consente anche un confronto diretto tra le nostre persone generate e le risposte scritte da umani. La seconda parte sono le parole contrassegnate, un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, di cui parlerò tra breve. Il vantaggio è che otteniamo stereotipi e schemi molto specifici senza doverci affidare a alcun lessico specifico. Quindi il metodo delle parole contrassegnate si basa sul concetto sociolinguistico di contrassegno, che afferma che esiste un default non contrassegnato e che qualsiasi gruppo differente da quel default è linguisticamente contrassegnato. Quindi, per esempio, la parola guerriero è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano un guerriero uomo e contrassegnano il termine con donna. E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi marginalizzati sono solitamente contrassegnati.\n\nNel nostro metodo, prima designiamo quali sono i gruppi non contrassegnati e contrassegnati. E poi confrontiamo le persone utilizzando il metodo delle parole combattenti, che essenzialmente utilizza rapporti log-odds ponderati per distinguere le parole principali per ogni gruppo contrassegnato. Quindi, per esempio, per le persone delle donne nere, faremmo parole combattenti e confronteremmo i rapporti log-odds contro entrambe le persone bianche e le persone maschili, perché quelli sono i due gruppi non contrassegnati corrispondenti.\n\nOra alcuni risultati. Prima usiamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi di quelle scritte da umani. Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse. Quindi, mentre le persone generate hanno tassi molto più elevati delle parole del lessico, quelle scritte da umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate presenti nelle persone generate sono davvero solo alte e atletiche. Quindi, davvero solo quelle positive o almeno non negative. E in effetti, questo lessico non cattura affatto molti degli schemi dannosi che abbiamo visto nelle diapositive precedenti.\n\nQuindi, invece, per fare ciò, ci rivolgiamo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzialiste. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Prima, per i gruppi contrassegnati, le parole principali includono cose come cultura, tradizione, orgogliosa ed esotica. E queste parole definiscono questi gruppi solo in base al loro rapporto con la loro identità e li distinguono come diversi dalla norma bianca. Questo contribuisce a una lunga storia di discriminazione e alienazione per questi gruppi. Inoltre, ci sono molti tropi comuni riflessi in queste parole, soprattutto per le donne di colore. Quindi, per esempio, le parole che descrivono le donne latine includono cose come vibrante e formosa, che si collegano a un tropismo di tropicalismo. Per le donne asiatiche, le parole sono cose come piccola, delicata e setosa, che si collegano a una lunga storia di sessualizzazione delle donne asiatiche, viste come molto docili e sottomesse, e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resiliente. Questo si collega ad un archetipo che le persone hanno chiamato l'archetipo della donna nera forte. E sebbene sembri positivo a prima vista, ci sono lavori che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su questi gruppi demografici a essere resilienti e forti contro gli ostacoli sociali. Quindi, invece di lavorare effettivamente per cambiare questi ostacoli, mette pressione su queste persone a superarli, il che porta a risultati molto negativi per la salute di queste persone, tra gli altri danni.\n\nPiù in generale, scopriamo che le parole per ogni gruppo contrassegnato riflettono praticamente solo narrazioni essenzialiste. Quindi, sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Primo, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialiste. Dovremmo anche utilizzare una prospettiva intersezionale per studiare i pregiudizi e i danni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. E infine, dovrebbe davvero esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi perché, per esempio, come questi stereotipi positivi, non sappiamo se è perché c'è qualche tipo di allineamento dei valori strano e eccessivo o forse altri metodi anti-stereotipici che producono questi schemi perniciosi. Non possiamo davvero fare alcuna assunzione o studiare ulteriormente senza una maggiore trasparenza.\n\nGrazie mille per l'ascolto. Buon divertimento all'ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono James Finch. E io sono Sarah Finch. Oggi vi parleremo di ABCEval, un nuovo approccio dimensionale per la valutazione dell'intelligenza artificiale conversazionale. Questo lavoro è stato svolto dal laboratorio NLP dell'Emory, guidato dal professor Gino Choi presso l'Università di Emory, in collaborazione con Amazon Alexa AI.\n\nImmaginiamo che abbiate appena sviluppato un modello di dialogo e vogliate confrontarlo con lo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molteplici aspetti. Pertanto, potreste voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare.\n\nUn approccio consiste semplicemente nel chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi o di scala Likert esistenti. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Annotando esplicitamente se ogni risposta del modello esprime o meno determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi. Chiamiamo questo approccio \"annotazione dei comportamenti nella chat\", o ABCEval in breve.\n\nAbbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che, secondo la letteratura recente, influenzano la qualità della chat. ABCEval è in grado di misurare le frequenze con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABCEval misura il numero di turni in cui un modello di chat ignora il suo interlocutore o fornisce informazioni irrilevanti, si contraddice o contraddice se stesso o il suo interlocutore, inventa fatti errati o viola il senso comune, e quando il modello riesce o fallisce nel mostrare empatia.\n\nPer determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni uomo-bot per modello utilizzando ABCEval. A scopo di confronto, abbiamo valutato queste conversazioni anche con tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti paritari a livello di dialogo.\n\nOltre ai metodi di valutazione, abbiamo raccolto valutazioni su otto aspetti della conversazione più comunemente misurati, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni. Dalle nostre analisi dei risultati della valutazione, abbiamo riscontrato che le etichette di comportamento ABCEval sono complessivamente più affidabili rispetto alle etichette raccolte con i metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni etichettate due volte. Inoltre, le etichette ABCEval sono più predittive della qualità complessiva della conversazione rispetto ai metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare.\n\nAd esempio, potete vedere come la misurazione della proporzione di turni con contraddizioni tra sé e l'interlocutore spieghi rispettivamente il 5% e il 10% della qualità della conversazione, mentre i punteggi di coerenza media su una scala Likert spiegano solo il 4% o meno. Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare passo-passo. Potete vedere come la combinazione di tutte le metriche ABCEval spieghi oltre il 25% della qualità della conversazione, e rimuovendo le metriche una alla volta, la maggior parte di esse comporta una perdita significativa di informazioni sulla qualità. La combinazione delle metriche Likert a livello di dialogo spiega molto meno della qualità, e meno di queste metriche trasportano informazioni uniche.\n\nQueste metriche ABCEval affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione più elevata rispetto ai metodi precedenti. Dai risultati del nostro esperimento, potete vedere che rimangono ancora diverse sfide, che sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni del senso comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il loro interlocutore circa il 10% delle volte.\n\nCon il rapido miglioramento del settore, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli. Ci auguriamo che ABCEval possa essere sfruttato da altri nel settore come un passo significativo in questa direzione, e attendiamo con ansia di vedere come l'intelligenza artificiale conversazionale si evolverà nei prossimi mesi e anni. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Vasudha e sono una dottoranda in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato in ACL 2023 come articolo lungo, \"Transfer Learning per la Rilevazione della Dissonanza\", che affronta la Sfida della Classe Rara. Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In breve, la dissonanza cognitiva si verifica quando due credenze o azioni sono incoerenti, come in questo esempio in cui una persona afferma: \"So che le sigarette potrebbero uccidermi\" e poi dice: \"Ho preso un paio di sigarette dopo la riunione\". Questa credenza e azione sono incoerenti e in uno stato di dissonanza. Menzionare ulteriormente: \"Non penso di poter mantenere il mio lavoro senza di esse\" giustifica la seconda occorrenza e stabilisce una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio tra altri tipi di relazioni discorsive. Perché questo è importante? Lo studio della dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a tracciare tendenze e valori di credenze e cambiamenti di atteggiamento nella popolazione. Una elevata dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Lo studio della dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a capire meglio i processi decisionali.\n\nCon l'obiettivo di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo utilizzato un approccio \"dissonanza prima\" come mostrato nel diagramma di flusso qui. I tweet sono stati elaborati utilizzando un parser PDTB e coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Raccogliendo circa 1.000 esempi di coppie di unità discorsive, abbiamo addestrato un classificatore iniziale, addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore abbia ottenuto prestazioni non migliori del caso. Data la bassa frequenza di occorrenza della dissonanza e l'assenza di qualsiasi precedente set di dati, ci troviamo di fronte al problema della rarità assoluta. Per mitigare questo problema, sperimentiamo combinazioni di apprendimento trasferibile e apprendimento attivo per annotare un maggior numero di esempi di dissonanza in meno round di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non è stato in grado di catturare affatto la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi: classificazione della posizione di dissonanza indipendente dall'argomento, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo indipendentemente dall'argomento, chiamato qui \"dibattito\", e classificazione binaria delle classi di espansione e confronto di PDTB, poiché queste due sono strettamente correlate al concetto di consonanti e dissonanza, che chiamiamo qui \"CE\". Abbiamo riscontrato che, trasferendo i pesi, le prestazioni zero-shot sul set di dati annotato sono già molto migliori del caso, con un AUC massimo di 0,62. Inoltre, affinando iterativamente su entrambi i compiti, scopriamo che l'affinamento dei compiti CE seguito da un ulteriore affinamento sul dibattito produce prestazioni zero-shot molto migliori. Quindi, questo è il modello che utilizziamo per avviare l'apprendimento attivo.\n\nSuccessivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazione. \"Cumulative\" accumula tutti i dati raccolti finora dalle annotazioni attive, mentre \"iterative\" aggiorna il modello addestrandolo sull'ultimo set di dati raccolti. Tra le diverse strategie, abbiamo riscontrato che \"cumulative\" ha prestazioni uguali o migliori di \"iterative\" in tutti i casi. Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara (PRC) per selezionare principalmente esempi che hanno una elevata probabilità di essere dissonanti secondo il modello corrente in qualsiasi round di AL. Confrontiamo questa strategia con altre strategie di apprendimento attivo all'avanguardia comunemente utilizzate nella comunità. Scopriamo che la strategia PRC proposta funziona meglio delle altre strategie all'avanguardia, anche se la differenza è piccola. Si noti che le prestazioni sono significativamente inferiori per \"random\". In ulteriori round di AL con le due migliori strategie, abbiamo migliorato l'AUC della classificazione della distanza a 0,75, che è la migliore prestazione ottenuta finora su questo compito. Abbiamo anche valutato la fattibilità di ciascuna strategia in termini di qualità dell'annotazione e costi per gli annotatori. Scopriamo che PRC ha la percentuale più alta di distanza e funziona al meglio per la classe rara. Tuttavia, gli annotatori trovano anche questi esempi difficili.\n\nIn sintesi, scopriamo che PRC è una semplice strategia di apprendimento attivo per l'acquisizione di classi rare e che l'avvio freddo dell'apprendimento attivo con compiti di apprendimento trasferibile opportunamente progettati può aiutare in modo significativo. Scopriamo anche che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive in-domain traggono vantaggio dall'aggiornamento cumulativo. Questi sono i link al nostro codice, al set di dati e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, sono Aksheta e oggi il mio coautore Martin ed io presentiamo il nostro lavoro \"The Kitmasteff: Valutazione dell'Integrazione della Conoscenza da Fonti Multiple\". Questo studio è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale nazionale sfruttano varie fonti di conoscenza, come le conoscenze contenute nei loro parametri, solitamente acquisite tramite pre-addestramento, e le conoscenze fornite negli input al momento dell'inferenza. Studi recenti su compiti come il question answering dimostrano che i modelli possono utilizzare le conoscenze pre-addestrate per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede conoscenze fornite anche al momento dell'inferenza. Ad esempio, nella frase \"John ha visto il neo-eletto presidente in TV\", i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cos'è una TV, ma non possono conoscere affidabilmente chi sia l'entità specifica John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo il pre-addestramento. Pertanto, i modelli di successo per i compiti di comprensione del linguaggio naturale intensivo richiedono la capacità di integrare e utilizzare sia le conoscenze pre-addestrate che quelle dell'inferenza.\n\nIn questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introdurremo un compito di risoluzione delle coreferenze progettato per valutare la capacità di attingere alle conoscenze disponibili in diverse fonti. Valuteremo il dataset con partecipanti umani e consolidati modelli di risoluzione delle coreferenze. Ecco un esempio dal nostro dataset:\n\n\"Servin è un giudice. Kia è un panettiere. Termin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, lui era felice di rilassarsi.\"\n\nIl compito qui è identificare l'entità corretta a cui si riferisce il pronome \"lui\", che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. Primo, conoscenze specifiche sull'entità, come \"Servin è un giudice\". E secondo, conoscenze di sfondo apprese durante il pre-addestramento dei grandi modelli linguistici, mentre le conoscenze specifiche sulle entità sono solitamente osservate al momento dell'inferenza. Varieamo la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in fonti multiple.\n\nAbbiamo definito tre impostazioni di Kitmos. Prima, abbiamo l'impostazione di argomento, pre-addestramento di sfondo, dove si presume che le conoscenze di sfondo siano disponibili al momento del pre-addestramento. Seconda, c'è l'impostazione di inferenza di sfondo entrambe, dove entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Questa ultima impostazione è particolarmente interessante, poiché simula un caso in cui le conoscenze di sfondo necessarie per risolvere un compito non fanno parte dei dati pre-addestrati dei modelli, ad esempio, perché nuove occupazioni sono emerse dopo il pre-addestramento.\n\nEcco un esempio di come controlliamo la disponibilità dei fatti nelle due fonti. Nell'impostazione di pre-addestramento di sfondo, si presume che la conoscenza di sfondo \"i politici cercano seggi eletti nel governo\" sia contenuta nei parametri pre-addestrati. Nel contesto di sfondo, forniamo la conoscenza antispecifica \"Chichester è un politico\". Nell'impostazione di inferenza di sfondo entrambe, forniamo non solo la conoscenza antispecifica, ma anche la conoscenza di sfondo sui politici nel contesto di inferenza. Nell'impostazione di inferenza di sfondo, forniamo l'occupazione fittizia \"mirror tour\" invece di \"politico\" perché \"mirror tour\" è improbabile che sia contenuto nei parametri pre-addestrati.\n\nValutiamo il dataset sia con partecipanti umani che con consolidati modelli di risoluzione delle coreferenze. In questa figura mostriamo i risultati dei modelli migliori sulla variante più difficile dell'impostazione di pre-addestramento di sfondo. Senza addestramento specifico sul compito su KitMus, entrambi i modelli non si comportano bene. Tuttavia, quando addestrati su KitMus, sia C2F che Berth per Koref performano significativamente meglio della scelta casuale. Questo suggerisce che, quando addestrati su dataset generali di risoluzione delle coreferenze, i modelli imparano a sfruttare indizi superficiali, che non sono utili quando si testa su KitMus dove tali indizi sono stati rimossi.\n\nEsperimenti aggiuntivi con conoscenze fittizie indicano che anche i modelli migliori non riescono a integrare affidabilmente le conoscenze di sfondo fornite solo al momento dell'inferenza.\n\nPer riassumere i punti chiave del nostro articolo, molti modelli di risoluzione delle coreferenze sembrano incapaci di ragionare su conoscenze da diverse fonti senza addestramento specifico sul compito. Tuttavia, con addestramento specifico sul compito, alcuni modelli integrano con successo conoscenze da fonti multiple. Tuttavia, anche i modelli migliori sembrano avere difficoltà a integrare in modo affidabile le conoscenze di sfondo presentate solo al momento dell'inferenza.\n\nPer ulteriori dettagli, si prega di consultare il nostro articolo e il dataset nel codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Sarah Pappy dell'Università di Toronto e della Fondazione Bruno Kessler, e introdurrò brevemente il paper \"Attenzione come guida per la traduzione simultanea del parlato\", un lavoro congiunto con Matteo Negri e Marco Turki.\n\nChe cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o simulata, è il processo di tradurre una lingua parlata in un testo in un'altra lingua in tempo reale, abilitando la comunicazione tra lingue diverse. E quali sono i problemi dei modelli simulati attuali? Architetture specifiche vengono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare, procedure di addestramento lunghe e complesse, ad esempio, addestramento che coinvolge diversi obiettivi di ottimizzazione, e addestramento e manutenzione di più modelli per raggiungere diversi regimi di latenza, come addestrare un modello con una latenza media di un secondo e un altro con due secondi di latenza, e così via.\n\nQual è quindi la nostra soluzione? In primo luogo, utilizzare modelli ST offline già esistenti senza riaddestramento o adottare architetture specifiche per CMLSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici, sfruttando le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, ovvero il meccanismo di attenzione incrociata, come mostrato nell'esempio a destra.\n\nLa nostra soluzione propone un'attenzione a punti o encoder-decoder, che è una strategia in cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, ovvero questa somma è al di sotto di una certa soglia alfa verso le ultime lambda frame del parlato, il che significa che le informazioni ricevute sono sufficientemente stabili. Ad esempio, se riceviamo un frammento di parlato contenente \"Vado a parlare di\" e il nostro modello prevede la traduzione in tedesco, e osserviamo i pesi dell'attenzione incrociata, vedremo che le prime due parole puntano alle prime frame di parlato ricevute, mentre l'ultima parola punta alle ultime frame di parlato ricevute come lambda frame. Ciò significa che le prime due parole verranno emesse, mentre, poiché la somma dell'attenzione incrociata è al di sopra di una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro frammento di parlato.\n\nSe continuiamo e riceviamo un altro frammento di parlato, e il nostro modello prevede altre tre parole, osservando i pesi dell'attenzione incrociata, vedremo che nessuna parola punta alle ultime lambda frame del parlato. Ciò significa che queste tre parole verranno emesse.\n\nSe esaminiamo i risultati principali, tracciamo i risultati della traduzione simultanea del parlato su grafici in cui il blu su un lato misura la qualità della traduzione e la latenza media, ovvero la misura della latenza. Consideriamo anche la latenza media consapevole del calcolo, che tiene conto del tempo di calcolo del modello per prevedere l'output. Quindi, vogliamo che le nostre curve siano il più alte possibile in questo grafico, ma vogliamo anche che siano spostate verso sinistra. Confrontiamo con strategie preparate che vengono applicate anche a modelli offline, ovvero la strategia Weighted Key e Local Agreement, e confrontiamo anche con architetture all'avanguardia specificamente progettate per la traduzione simultanea del parlato.\n\nQuesti sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco, e vediamo che l'attenzione a punti supera tutte le strategie applicate a modelli offline, poiché le curve sono spostate verso sinistra. Vediamo anche che, se consideriamo il tempo effettivo trascorso o il tempo di usura del calcolo, è la strategia più veloce. Per scoprire altri risultati, leggete il nostro paper, e abbiamo anche rilasciato open source il codice, i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Xuheng. Oggi presenterò il nostro articolo intitolato \"I tagger di entità denominate Kernel 2003 funzionano ancora bene nel 2023?\" Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento di entità denominate (NER). Abbiamo osservato che i modelli hanno utilizzato Kernel 2003 per sviluppare NER da quasi 20 anni. E questo solleva naturalmente diversi problemi. Innanzitutto, questi modelli possono generalizzarsi su dati moderni e, quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli?\n\nPer indagare questi problemi, abbiamo sviluppato il dataset Kernel Plus Plus, un insieme di dati raccolti da Reuters News nel 2020 e poi annotati con le stesse linee guida di annotazione di Kernel 2003. Abbiamo quindi affinato oltre 20 modelli su Kernel 2003 e li abbiamo valutati sia sul set di test Conor 3 che sul set di test Conor Plus Plus. Infine, abbiamo calcolato la variazione percentuale dell'F1 per valutare la generalizzazione di ciascun modello.\n\nQuindi, cosa è necessario per una buona generalizzazione? Dalle nostre sperimentazioni, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Abbiamo riscontrato che i modelli transformer generalmente si generalizzano meglio su nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che solitamente i modelli più grandi portano a una migliore generalizzazione. E infine, sappiamo tutti che il numero di esempi di affinamento influisce direttamente sulle prestazioni di un compito a valle. Qui abbiamo anche riscontrato che più esempi di affinamento portano effettivamente a una migliore generalizzazione.\n\nPer la nostra successiva domanda, cosa causa il calo delle prestazioni di alcuni modelli? Abbiamo avuto due ipotesi. La prima è l'overfitting adattivo, causato dal riutilizzo dello stesso set di test più e più volte, e questo si manifesta solitamente come rendimento decrescente su un nuovo set di test. La seconda ipotesi è la deriva temporale, il degrado delle prestazioni causato dall'aumentare del divario temporale tra i dati di allenamento e quelli di test.\n\nPer l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea di regressione migliore (in rosso) ha una pendenza superiore a uno. Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su Color 2003 si traduce in più di un'unità di miglioramento su Kernel Plus Plus, il che significa che non ci sono rendimenti decrescenti e questo ci mostra che in questo caso non si osserva l'overfitting adattivo.\n\nQuindi, cosa riguarda la deriva temporale? Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo riscontrato che le prestazioni degradano con un divario temporale più ampio. Ciò conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale.\n\nLa nostra conclusione è che per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, di una dimensione del modello più grande, nonché di più esempi di affinamento, e questi vanno di pari passo. Non possiamo avere solo un ingrediente e scartare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni è causato dalla deriva temporale e, a sorpresa, non è causato dall'overfitting adattivo, nonostante Kernel 2003 sia stato utilizzato per oltre 20 anni.\n\nTornando alla domanda che abbiamo posto nel titolo del nostro articolo, i tagger Kernel 2003 funzionano ancora nel 2023? Abbiamo scoperto che la risposta è un deciso sì. Ci auguriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli. Infine, vi invitiamo a consultare il nostro articolo e il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Salve, benvenuti alla nostra presentazione di DeepLean, un nuovo corpus per la semplificazione del testo in tedesco a livello di documento e di frase. Mi chiamo Regina Stodden e vi guiderò nella prima parte della presentazione. Iniziamo definendo la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come persone con problemi di lettura o madrelingua non nativi. Per addestrare un modello di semplificazione del testo, richiediamo coppie parallele di testi, ad esempio documenti o frasi. Nell'esempio qui riportato, potete vedere una coppia di frasi parallele allineate di una frase tedesca complessa e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio, quali la sostituzione lessicale, l'eliminazione della clausola, la riordinazione della clausola o l'inserimento di parole.\n\nProponiamo ora il nostro nuovo corpus DPlane, poiché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Alcuni sono troppo piccoli per addestrare un modello di classificazione. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono presentare errori negli allineamenti. Pertanto, proponiamo il nostro nuovo corpus DPlane, suddiviso in due sub-corpora, DPlane APA e DPlane Web. DPlane APA si basa su testi utilizzati. In DPlane APA, abbiamo allineato manualmente 483 documenti, ottenendo circa 30.000, 13.000 coppie di frasi parallele. Per DPlane Web, questo corpus include diversi domini e abbiamo allineato manualmente tutti questi 750 documenti e, d'altra parte, con metodi di allineamento automatico. In totale, abbiamo ottenuto 30.450 coppie di frasi.\n\nAbbiamo analizzato più approfonditamente le nostre coppie di frasi, ad esempio, in base al tipo di semplificazione. Come potete vedere qui, i testi biblici sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per apprendisti linguistici a tutti i livelli, per quanto riguarda, ad esempio, la semplificazione lessicale, la semplificazione strutturale o il livello generale di semplificazione. Inoltre, potete vedere che il nostro corpus D-plane presenta una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus D-plane API, abbiamo molte più riordinazioni e modifiche di parole rispetto a quanto abbiamo nel corpus D-plane Web. D'altra parte, nel corpus Web, abbiamo molte più riformulazioni.\n\nVediamo ora cosa possiamo fare con questo corpus. Salve, mi chiamo Omar e ora parlerò dei casi d'uso del nostro dataset DPlane. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni sono stati sviluppati molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi nei due documenti. Nel nostro caso, invece, stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli, che hanno la stessa lingua, lo stesso contenuto, ma sono a diversi livelli di complessità. Ora che abbiamo il nostro dataset D-plane, che contiene frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo di mass align, e potete trovare anche il codice per eseguire questo metodo sui vostri documenti nel documento.\n\nIl secondo caso d'uso che abbiamo presentato nel nostro articolo è quello della semplificazione automatica del testo attraverso il fine-tuning dei modelli linguistici per produrre testo semplificato da un testo di input complesso. Abbiamo eseguito il fine-tuning di due modelli diversi. Abbiamo eseguito il fine-tuning del modello Long Import per produrre semplificazioni a livello di documento e abbiamo anche eseguito il fine-tuning del modello Base Import normale per produrre semplificazioni a livello di frase. Potete trovare anche tutti i checkpoint e potete esaminare più in dettaglio i punteggi e le metriche di valutazione dei nostri esperimenti nel documento. Abbiamo concluso che questo fine-tuning di base può produrre punteggi migliori rispetto ai punteggi di riferimento e proponiamo questi risultati come benchmark di base per il problema della semplificazione automatica del testo in futuro.\n\nGrazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Xi Yuan dell'Università Fen. Sono qui per presentare il nostro lavoro sulla distinzione tra conoscenze di script specifici e modelli linguistici di grandi dimensioni per la pianificazione linguistica vincolata. Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo dopo passo sotto forma di script garantiti. Studi precedenti hanno esplorato l'utilizzo di modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come preparare una torta, dimostrando che i modelli linguistici di grandi dimensioni possono efficacemente scomporre gli obiettivi in passaggi. Tuttavia, gli studi precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con vincoli specifici, come preparare una torta al cioccolato, rimane ancora poco studiata.\n\nIn questo articolo, definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli agli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli più complessi e multidimensionali. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli.\n\nPrima valutiamo e miglioriamo la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni. Poiché non esiste un set di dati di obiettivi specifici per supportare il nostro studio, dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multidimensionali per l'acquisizione di dati con ciclo umano utilizzando TPT istruito. Campioniamo cento obiettivi specifici e valutiamo gli script generati dai modelli Light Logic. Questa tabella riporta la precisione complessiva dei risultati.\n\nScopriamo che tutti i modelli Light Logic ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici. Quindi, conduciamo un'analisi dettagliata per indagare cosa i modelli Light Logic siano in grado di pianificare. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita.\n\nEsploriamo categorie di argomenti più franchi e graduati dei vincoli a seconda del contesto domestico. La mappa testa nella figura mostra che le prestazioni di pianificazione dei GPD istruiti variano considerevolmente per vincoli di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli Larry presenta una elevata varianza, portando a scarse prestazioni. Pertanto, adottamos l'idea di un filtro over-generato Zen per migliorare la qualità della generazione.\n\nPrima mostriamo i tipi di vincoli con esempi per GPT istruito e otteniamo obiettivi specifici basati sugli obiettivi astratti definiti. Quindi, GPT istruito over-genera script chiave per obiettivi specifici. Successivamente, viene sviluppato un modello filtro per selezionare gli script adatti. Convertiamo gli script e gli obiettivi in GPT istruito in segmenti e calcoliamo la similarità coseno e i punteggi di similarità per misurare la similarità semantica. Inoltre, scriviamo lo script che contiene le parole chiave del vincolo bersaglio. Conserviamo solo lo script se il punteggio del bersaglio è il più alto nel sito dell'obiettivo.\n\nCon il nostro metodo, la città può generare script di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità sia in termini di completezza semantica che di fedeltà al vincolo. Poiché il dispiegamento di modelli linguistici di grandi dimensioni è costoso, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di set di dati è un passaggio essenziale per raggiungere questo obiettivo. Tuttavia, gli studi precedenti non abilitano la pianificazione di obiettivi specifici e l'annotazione manuale dei set di dati è costosa.\n\nPertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare i modelli di pianificazione linguistica vincolata. Applichiamo il nostro metodo per costruire un set di dati di pianificazione linguistica vincolata chiamato CodeScript. In totale, generiamo cinquantacinquemila obiettivi specifici con script per garantire la qualità dei siti di convalida e test. Chiediamo ai lavoratori cloud di trovare e rivedere i campioni errati. Questa figura mostra la distribuzione vincolata di CodeScript.\n\nScopriamo che CodeScript conferma l'ipotesi negli obiettivi specifici generati. Con CodeScript, possiamo addestrare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Riscontriamo che la funzione TFIL su CodeScript genera script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che i modelli più piccoli possono supportare quelli più grandi quando adeguatamente addestrati su set di dati appropriati.\n\nIn sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Abbiamo valutato la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni e sviluppato un metodo di filtro over-generato per questi modelli. Utilizziamo i modelli linguistici di grandi dimensioni per generare un set di dati di script di alta qualità, CodeScript, per la pianificazione linguistica costruttiva. Ci auguriamo che il set di dati CodeScript possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica. Grazie per la vostra attenzione. Per maggiori dettagli su CodeScript, consultate il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Yanis Lavrack e vi presenterò i nostri lavori su Dr. Berth, un robusto modello di pre-addestramento in francese per il dominio biomedico e clinico. In questa presentazione, parleremo innanzitutto di modellazione del linguaggio nel settore sanitario. Successivamente, presenteremo il principale contributo del nostro articolo. Introdurremo il primo modello biomedico in francese, denominato Dr. Berth, basato su Roberta e addestrato su Natchios, un dataset di dati medici estratti dal web. Presenteremo inoltre un confronto tra modelli con diverse impostazioni di pre-addestramento e fonti di dati. Quindi, mostreremo i nostri risultati su 11 compiti a valle biomedici e clinici in francese. Infine, concluderemo con i dettagli sugli esperimenti e su come accedere ai modelli.\n\nDa quando è stato rilasciato nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere i compiti di elaborazione del linguaggio naturale, offrendo un notevole miglioramento delle prestazioni rispetto ai metodi storici statici e contestualizzati come Word2Vec, Fastex o NWO. Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert, e ad altri domini, come il biomedico con PermetteBERT e BioBERT, e il clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano sul pre-addestramento continuo a causa della mancanza di dati in-domain. Tuttavia, fino ad ora, non esisteva alcun modello open source per il francese biomedico. Ci siamo quindi chiesti quali siano le fonti di dati più appropriate per un'ampia gamma di utilizzi. Gli attuali dati, sebbene non siano un perfetto sostituto dei dati clinici, rappresentano una buona alternativa.\n\nPer rispondere a questa domanda, abbiamo confrontato Dr. Bert con il nostro modello Schubert, basato su dati anonimizzati ottenuti dall'ospedale non universitario di cui disponiamo. In seguito, ci siamo chiesti: quanta quantità di dati è necessaria per addestrare un modello specializzato sui dati in francese? 4 GB, 8 GB o più? Per rispondere, abbiamo prima addestrato e confrontato quattro modelli ex novo. Una prima versione di Dr. Bert con 7 GB di Natchios, una seconda versione con 4 GB di Natchios, una prima versione di Schubert, un modello clinico, con 4 GB di frasi estratte da nodi clinici, e una versione finale di Schubert con una combinazione di 4 GB di Natchios e 4 GB di nodi clinici.\n\nOltre a questo confronto, abbiamo introdotto tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sui pesi di Camembert e addestrato su 4 GB di Natchios. Un altro, sempre basato su Camembert, ma addestrato questa volta sui 4 GB di nodi clinici. E infine, uno basato sul modello biomedico inglese Bermud Bert, addestrato su 4 GB di Natchios. In totale, abbiamo sette modelli.\n\nPer valutare i nostri sette modelli, abbiamo raccolto compiti pubblici e privati di corrispondenza, come il riconoscimento di entità denominate e l'altitudine, la classificazione, l'etichettatura del passaggio di modelli e la risposta alle domande. Questi modelli sono stati confrontati con sei modelli di riferimento, ovvero Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBERT, BioBERT e ClinicalBERT. La valutazione ha evidenziato che il modello ha ottenuto i migliori risultati nei compiti con dati della stessa natura di quelli su cui è stato addestrato. Tuttavia, dai dati eterogenei possiamo osservare una maggiore versatilità. Abbiamo anche notato che l'utilizzo di più dati si traduce in prestazioni migliori. In generale, il fine-tuning ex novo sembra ottenere prestazioni più elevate nella maggior parte dei compiti. Tuttavia, il nostro esperimento sul fine-tuning continuo utilizzando i pesi e il tokenizzatore di PumedBeard, addestrato sul sottoinsieme di 4 GB di Natchios, ha mostrato risultati confrontabili con quelli ottenuti con Dr. Beard 4 GB ex novo, il che non è vero per il modello basato sui pesi e il tokenizzatore di Camembert, che presenta problemi di stabilità.\n\nIn conclusione, il nostro sistema proposto offre prestazioni migliori in nove dei undici compiti DOTSTRIMS e supera globalmente il risultato del modello generico, Camembert. Osserviamo anche che i dati specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da Natchios sono disponibili gratuitamente su YuginFace e tutti gli script di addestramento si trovano nel nostro repository GitHub. Grazie quindi per questa presentazione e non vediamo l'ora di scambiare idee nella sessione post-conferenza a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Xiang Bin, sono uno studente di dottorato presso l'Università di Washington. Oggi presenterò il nostro lavoro, che va dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando i sentieri dei pregiudizi politici che portano a modelli NLP ingiusti. I modelli linguistici, quindi, vengono addestrati su dati su larga scala estratti dal web. I media delle notizie politiche sono ben rappresentati nei loro dati di pre-addestramento. Secondo un'indagine sul corpus C quattro, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, e altri, sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di imparare da diverse prospettive, celebrando la democrazia e la pluralità delle idee. D'altro canto, queste diverse opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle. A tale scopo, proponiamo di indagare sulla pipeline di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, chiedendoci specificamente le seguenti domande. Primo, come valutiamo la guida politica dei modelli linguistici e quale ruolo potrebbero avere i dati di pre-addestramento su tali pregiudizi politici? Secondo, come si comportano effettivamente i modelli linguistici con diverse guide politiche nei compiti a valle e se ciò potrebbe portare a problemi di equità nelle applicazioni NLP? Specificamente, proponiamo innanzitutto di sollecitare i modelli linguistici con diversi formati di sollecitazione utilizzando i questionari politici, come il test della bussola politica. Questo ci assicura di poter effettuare una valutazione automatica ben fondata nella letteratura di scienze politiche. Alcuni risultati preliminari dimostrano che, primo, i modelli linguistici hanno significati politici variabili. Occupano tutti e quattro i quadranti sulla bussola politica. Possiamo anche vedere che GPT 4 è il modello linguistico più liberale di tutti, e le teorie GPT sono generalmente più socialmente liberali della teoria BERT e delle sue varianti. In secondo luogo, miriamo a indagare in che misura i pregiudizi politici dei modelli linguistici vengono effettivamente acquisiti dai dati di addestramento. Potremmo condurre un esperimento controllato addestrando ulteriormente i punti di controllo del modello linguistico su sei diversi corpora partigiani separati in notizie e social media ulteriormente suddivisi in base al loro orientamento politico. Addestrando ulteriormente i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per Roberta ulteriormente affinata, addestrata sul corpus Reddit di sinistra, possiamo vedere uno spostamento liberale sostanziale in termini di pregiudizi politici. Per indagare se i modelli linguistici possono acquisire la polarizzazione prevalente nella nostra società moderna. Quindi dividiamo i corpora di pre-addestramento prima e dopo il 45° Presidente degli Stati Uniti. Addestriamo separatamente i modelli linguistici sui due diversi corpora temporali. Possiamo vedere che i modelli linguistici avevano generalmente un orientamento politico più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche acquisire la polarizzazione nella nostra società. Infine, valutiamo i modelli linguistici con diversi orientamenti politici nel rilevamento del discorso d'odio e nel rilevamento delle fake news, applicazioni NLP che spesso coinvolgono i modelli linguistici e che potrebbero avere implicazioni molto significative. Vediamo che, se indagiamo sulle prestazioni per categoria, cioè se separiamo le prestazioni in base a diverse demografie o significati politici dei media, possiamo vedere un modello secondo cui, ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di sinistra sono migliori nel rilevare il discorso d'odio che prende di mira i gruppi socialmente minoritari, ma sono peggiori nel rilevare il discorso d'odio che prende di mira i gruppi più potenti della nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare il discorso d'odio che prende di mira i bianchi e gli uomini, ma sono peggiori nel rilevare il discorso d'odio che prende di mira neri, LGBTQ+ e altre comunità minoritarie. Tendenza simile anche nel rilevamento delle fake news, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare le disinformazioni dal loro significato politico opposto e viceversa. Questo mostrerà ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diversi significati politici danno diverse previsioni sul discorso d'odio e sugli esempi di disinformazione in base alle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente questo punto. Ciò indica che esiste un problema di equità molto urgente per quanto riguarda i pregiudizi politici dei modelli linguistici. Ad esempio, se un modello linguistico di destra venisse affinato sul discorso d'odio o sulla disinformazione o qualsiasi altra cosa e distribuito su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e il discorso d'odio che prende di mira i gruppi minoritari potrebbe diffondersi senza controllo. Quindi, questo suona l'allarme per riconosce e affrontare i problemi di equità derivanti dai pregiudizi politici dei modelli linguistici. Una breve discussione. Vorremmo anche evidenziare che esponiamo il dilemma unico relativo ai pregiudizi politici dei modelli linguistici. È come trovarsi tra Scilla e Cariddi. Se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando alla fine problemi di equità. Se proviamo in qualche modo a sanare, rischieremmo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e debba essere mantenuto nei dati di addestramento dei modelli linguistici. È un po' come il problema di Charlie Elettrico. Bene, credo di aver detto più o meno tutto per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, sono Coast di Sina e sono lieto di darvi il benvenuto alla nostra presentazione del nostro articolo ACL twenty twenty three intitolato \"I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto\". Questo è un lavoro congiunto con John Bakhier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy e Adina Williams. In questo studio rivalutiamo il paradigma dei coppie minime. Il paradigma delle coppie minime, essenzialmente, valuta i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità come BLIMP, sintassi gem o accettabilità in termini di stereotipi come crowd spares. Nel paradigma delle coppie minime, il modo tipico di valutare i modelli linguistici consiste nel mostrare una frase accettabile o grammaticale e poi una frase inaccettabile o ungrammaticale, con la speranza che il modello assegni una probabilità maggiore alla frase accettabile. L'attuale pipeline MPP non ci consente di valutare l'accettazione del modello verso frasi più lunghe. Oggi i grandi modelli linguistici stanno sviluppando finestre di contesto sempre più ampie. È quindi fondamentale valutare l'accettabilità del modello in tutto l'intervallo della finestra di contesto. Ed è ciò che stiamo cercando di fare qui. Stiamo cercando di rivalutare la pipeline NPV chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Questo è l'approccio. Quello che facciamo è rivalutare gli stessi set di dati per simulare queste sequenze più lunghe e ricreare frasi scegliendo frasi accettabili o inaccettabili da quei set di dati. Ad esempio, qui abbiamo scelto una coppia tipica di grammaticalità dal set di dati BLIMP, dal caso dell'isola ausiliare. Per ricreare sequenze più lunghe, che siano accettabili e che abbiano la stessa struttura grammaticale, estraiamo frasi grammaticali dall'isola ausiliare e le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile. Possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso set di dati. E possiamo anche farlo scegliendo frasi da un sottoinsieme diverso o da un set di dati diverso. Questo è ciò che chiamiamo scenario di mismatch. Qui, le frasi provengono ancora da set di dati rilevanti, ma non dallo stesso set di dati che stiamo valutando. Possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia. Questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati dal contesto, se il contesto proviene da un sottoinsieme diverso del set di dati o se è completamente irrilevante per la frase che stiamo esaminando. Come si comporta il modello? Prima guardiamo le frasi di Wikipedia, completamente irrilevanti per la coppia di query corrente, e scopriamo che i giudizi MPP sono per lo più robusti per qualsiasi lunghezza del contesto. Aumentiamo la lunghezza del contesto fino a 1024 per sfruttare al massimo i modelli OPT e GPT2, e come vediamo nella linea puntinata arancione, i giudizi MPP sono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso set di dati? Qui stiamo creando frasi dai domini accettabile e inaccettabile dallo stesso set di dati BLIMP o sintassi gem, e vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando aggiungiamo prefissi accettabili o inaccettabili. Ma quando corrispondiamo la struttura, cioè quando scegliamo frasi dallo stesso fenomeno in BLIMP-based sintassi gem, osserviamo un enorme aumento o diminuzione dei giudizi MPP del modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Questo effetto aumenta con la lunghezza del contesto e potrebbe probabilmente influenzare i nuovi modelli linguistici con ampie finestre di contesto. Perché il prefisso corrispondente influenza così tanto il giudizio del modello linguistico? Abbiamo effettuato una serie di analisi cercando di perturbare la frase di input, preservando la struttura rilevante ma aggiungendo rumore. Dopo diverse perturbazioni, abbiamo scoperto che nessuno di questi rumori fa cambiare il modello in termini di tendenza del giudizio MPP. In sostanza, abbiamo riscontrato che i modelli sono sensibili alle frasi perturbate in modi simili: quando perturbiamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni, e quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile. I punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti condivise tra le frasi. E la valutazione MPP, come la eseguiamo attualmente con input di frasi brevi e singole, potrebbe non catturare completamente la conoscenza astratta del modello linguistico in tutto l'intervallo della finestra di contesto. Leggete il nostro articolo per maggiori dettagli sugli esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Dawe, sono uno studente di dottorato presso l'Università Stalant in Germania. In questo video, vorrei presentare il nostro recente lavoro, \"Più debole di quanto pensiate\", un'analisi critica dell'apprendimento supervisionato settimanale. Questo è un lavoro congiunto con Xiao Yushchen, Maios Musbach, Giaz Steffen e Dietrich Clarkov. Vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale. Nella supervisione settimanale, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura settimanale, come semplici regole euristiche, basi di conoscenza o crowdsourcing locale, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente reti neurali su dati etichettati settimanali, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. Nell'apprendimento supervisionato settimanale, vengono proposti algoritmi di addestramento per addestrare robustamente reti neurali su tale rumore delle etichette in modo che i modelli addestrati generalizzino comunque bene. In recenti lavori sull'WSL, dove WSL sta per apprendimento supervisionato settimanale, una rivendicazione comune è che le persone affermano di addestrare modelli solo sui dati etichettati settimanali e di ottenere elevate prestazioni su set di test puliti. Tecnicamente, questa affermazione non è errata, ma c'è un inghippo, ovvero che le persone assumono che sia disponibile un ulteriore set di convalida pulito per la selezione del modello. Mettiamo in dubbio questa impostazione del problema, poiché implica che siano necessarie annotazioni manuali aggiuntive nell'apprendimento supervisionato settimanale. Ma come un elefante in una stanza, questa necessità è spesso trascurata. Il dubbio menzionato ci porta a porci tre domande di ricerca. Primo, i dati di convalida puliti sono necessari per l'WSL? Oppure possiamo utilizzare un set di convalida rumoroso? Secondo, se i dati puliti sono richiesti o se i dati puliti sono obbligatori per il funzionamento dell'WSL, quanti campioni puliti sono necessari? Infine, dovremmo utilizzare solo i campioni puliti per la convalida o ci sono modi migliori per sfruttarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e le nostre scoperte sono le seguenti. Primo, scopriamo che, interessantemente, i recenti metodi WSL richiedono effettivamente campioni di convalida puliti per funzionare correttamente. In caso contrario, si verifica un notevole calo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di convalida puliti, i modelli addestrati non riescono a generalizzare oltre le originali etichette deboli, il che significa che l'addestramento è inutile. Ciò indica che i metodi WSL richiedono effettivamente dati etichettati puliti per funzionare correttamente e il costo di annotazione per ottenere campioni di convalida puliti non dovrebbe essere trascurato. La nostra seconda scoperta è che l'aumento del numero di campioni di convalida puliti aiuta i metodi WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, abbiamo bisogno solo di venti campioni per classe per ottenere elevate prestazioni. Ma non è tutto, perché se decidiamo comunque di accedere a campioni puliti, l'addestramento diretto su di essi otterrà persino prestazioni migliori. La figura rossa mostra la differenza di prestazioni tra gli approcci di fine-tuning, applicati direttamente sui dati puliti, e i metodi WSL, che utilizzano i dati puliti solo per la convalida. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare i metodi WSL. Infine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare il fine-tuning sui campioni di convalida puliti. Come possiamo vedere dalle figure, il modello Berliner chiamato FTW inizialmente ha prestazioni inferiori rispetto a metodi WSL più complessi come cosine. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, FTW ha prestazioni equivalenti ad altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni manualmente annotati puliti per funzionare correttamente. Il loro miglioramento delle prestazioni e la loro praticità sono fortemente sovrastimati. Le nostre concrete raccomandazioni per i lavori futuri sono le seguenti. Primo, segnalare che la selezione del modello viene eseguita con campioni di convalida puliti. Secondo, i metodi WSL dovrebbero essere confrontati con le linee di base dell'apprendimento futuro, poiché entrambi lavorano su campioni puliti. Terzo, il fine-tuning continuo è una semplice ma solida linea di base che dovrebbe essere presa in considerazione nei lavori futuri sull'WSL. Infine, abbiamo reso open source il nostro codice. Puoi trovarlo tramite il codice QR su questa diapositiva. Sentiti libero di dargli un'occhiata. Grazie e buon congresso."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Ayud Villar e fornirò una breve panoramica del documento \"Prompting Palm: Strategie e Prestazioni nella Valutazione della Traduzione\". Questo è un lavoro congiunto con i miei colleghi di Google Translate. Palm è un modello linguistico di grandi dimensioni da 540 miliardi di parametri, presentato lo scorso anno nel 2022. È addestrato su una vasta raccolta di testi composta da 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP.\n\nIn questo studio, presentiamo il primo studio sistematico sul prompting di grandi modelli linguistici per la traduzione automatica. Valutiamo la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità AMT. Ciò comporta l'utilizzo dei più recenti set di test per evitare la sovrapposizione dei dati di test con i dati di addestramento del modello linguistico, e confrontiamo due sistemi all'avanguardia, i sistemi con le migliori prestazioni delle valutazioni WMT. Utilizziamo metriche neurali all'avanguardia per la traduzione automatica e, inoltre, mostriamo anche i risultati della valutazione umana basata su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt.\n\nIl prompting ha una grande influenza sulle prestazioni dei modelli linguistici per la traduzione, come possiamo vedere in un semplice esperimento in cui utilizziamo un breve prompt e forniamo due prompt diversi per una sola frase. Nella maggior parte delle frasi, 516 su 1000, la differenza osservata è di oltre un punto di sfocatura. In casi estremi, può arrivare fino a 40 punti di sfocatura. Quindi, è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting a cinque colpi, dove contrassegniamo semplicemente ogni frase fornita al sistema con la lingua in cui è scritta. In questo esempio, dove abbiamo eseguito la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine, sono contrassegnate con due punti tedeschi e le traduzioni inglesi con due punti inglesi.\n\nAbbiamo riscontrato che la forma effettiva del prompting non ha una grande influenza nel caso di più prompt brevi. È cruciale per zero e un prompt breve, ma quando si passa, come nel nostro caso, a cinque prompt brevi, non c'è quasi alcuna differenza nella forma effettiva del prompting. Sono gli esempi a portare il peso maggiore. La sintesi dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di origine. Quindi, è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, e i risultati mostrano prestazioni migliori utilizzando i dati di sviluppo.\n\nTuttavia, i sistemi ODR specializzati hanno un vantaggio sostanziale sulle traduzioni di PALM, ma PALM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ottenuto dalla valutazione umana che abbiamo condotto utilizzando il framework MQM sono che la fluidità di Palm è paragonabile ai sistemi all'avanguardia, ma la principale differenza risiede nell'accuratezza. In particolare, gli errori più comuni sono errori di omissione. Quindi, sembra che Palm scelga di produrre una traduzione migliore a volte omettendo parti della frase che vengono omesse nella traduzione. Tuttavia, la categoria di stile per PAM è inferiore rispetto ai sistemi all'avanguardia, che è un segnale aggiuntivo che PAM fornisce un output molto fluido, ma con alcuni problemi di accuratezza.\n\nE questo è tutto per questa breve panoramica. Per maggiori dettagli, vi invitiamo alla presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Jing Wei dall'Università della Scienza e della Tecnologia della Cina. È un piacere per me presentare un breve video promozionale del nostro articolo \"Stai copiando il mio modello? Protezione del copyright dei grandi modelli linguistici per l'embedding e i servizi\". Presentiamo prima lo sfondo sull'embedding e i servizi. Attualmente, grandi modelli linguistici come GPT, Lama, PELM eccellono nella comprensione e generazione del linguaggio naturale. L'embedding e i servizi sono uno dei servizi basati su grandi modelli linguistici per assistere vari compiti di NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, recenti lavori hanno dimostrato che un attaccante potrebbe rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright dell'embedding come servizio.\n\nPer proteggere il copyright dell'embedding come servizio, una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark. Il metodo deve soddisfare le seguenti proprietà. Primo, il metodo dovrebbe essere applicabile all'embedding come servizio. Secondo, il watermark non dovrebbe degradare l'utilità degli embedding forniti. Terzo, il watermark dovrebbe essere sufficientemente nascosto all'attaccante o l'attaccante non dovrebbe poter rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.\n\nI lavori esistenti possono essere classificati in quattro categorie principali. Tuttavia, questi metodi non sono applicabili all'embedding come servizio o mancano di trasferibilità. Pertanto, in questo articolo proponiamo l'embedding marker, che è un metodo di watermark basato su backdoor applicabile all'embedding come servizio.\n\nOra vi presenterò i dettagli del nostro embedding marker. L'embedding marker comprende due passaggi principali: l'iniezione del watermark e la verifica del copyright. Prima di questi passaggi principali, selezioniamo un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderato. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione del watermark, definiamo prima un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embedding fornito è esattamente uguale all'embedding target.\n\nLa verifica del copyright rileva se un modello dietro un altro servizio contiene il watermark. Costruiamo prima un backdoor e un dataset benigno. Il dataset backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger. Poi il fornitore richiede embedding dal servizio del ladro con il dataset. Calcoliamo la similarità tra l'embedding richiesto e l'embedding target, utilizzando la distanza coseno e la distanza L2. Calcoliamo anche la differenza di similarità tra i dati benigni e il dataset backdoor, definita come delta coseno e delta L2. Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica.\n\nAbbiamo condotto esperimenti su quattro dataset: AG News, Mind, SSD2 ed Erospam. Supponiamo che il fornitore utilizzi il dataset Wiki Text per contare la frequenza delle parole. I risultati sui quattro dataset mostrano che il nostro embedding marker può avere un'ottima prestazione di rilevamento mantenendo l'utilità per i compiti a valle. Abbiamo anche validato la nascostezza dell'embedding fornito visualizzando l'embedding delle frasi come BOPCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding backdoor e gli embedding normali.\n\nÈ tutto, grazie. Benvenuti a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Ying e io e il mio collega Jian presenteremo la nostra ricerca su MultiInstruct: miglioramento dell'apprendimento multimodale a zero colpi tramite il tuning delle istruzioni. Quindi, con i progressi nei grandi modelli linguistici, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare i modelli linguistici pre-addestrati per diversi compiti a valle in modo efficiente sia in termini di parametri che di dati. Recentemente, molti studi hanno dimostrato che il tuning delle istruzioni consente ai grandi modelli linguistici di eseguire compiti non visti in modo a zero colpi seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sul tuning delle istruzioni si è concentrata sul miglioramento delle prestazioni a zero colpi su compiti solo linguistici, lasciando da parte la visione artificiale e i compiti multimodali. Pertanto, in questo lavoro, vogliamo indagare se il tuning delle istruzioni su modelli multimodali pre-addestrati può effettivamente migliorare la generalizzazione a compiti multimodali non visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di set di dati di istruzioni tra LP e multimodali. Esistono più di mille seicento compiti di istruzioni solo linguistiche. Tuttavia, non esiste un ampio set di dati di istruzioni multimodali pubblicamente disponibile su larga scala. Pertanto, questo ci ha motivato a costruire un set di dati di tuning delle istruzioni multimodali. Qui presentiamo MultiInstruct, il primo set di dati di benchmark per il tuning delle istruzioni multimodali che consiste in sessantadue diversi compiti multimodali che coprono dieci categorie evidenziate. Questi compiti sono derivati da ventuno set di dati open source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti. Per indagare il tuning delle istruzioni multimodali sul nostro set di dati proposto, prendiamo OFA, il modello unificato multimodale di pattern, come nostro modello di base. OFA utilizza un vocabolario unificato per il linguaggio, i token delle immagini e le coordinate di una casella di delimitazione. Qui mostriamo alcuni esempi dal nostro set di dati MultiInstruct. Per unificare l'elaborazione di vari tipi di dati di input e output, seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-sequenza unificato in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentate nello stesso spazio dei token.\n\nOk, ora parlerò del tuning delle istruzioni multimodali. Quindi, per il set di dati di addestramento, utilizziamo 53 compiti dal gruppo NIG per l'addestramento e campioniamo 10.000 istanze per ogni compito. Per il test, riserviamo l'intero gruppo di ragionamento comune per il test e selezioniamo altri cinque compiti da WQA e dal gruppo miscellaneous. Utilizziamo tutte le istanze nella partizione di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla partizione di test delle istruzioni NIG come compito SIN per l'elaborazione del linguaggio naturale. Quindi, utilizziamo un modello OFA pre-addestrato di grandi dimensioni come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con una delle sue cinque template di istruzioni. Quindi, durante il test per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento. Riportiamo la media e il valore massimo delle prestazioni e la deviazione standard delle prestazioni su tutti e cinque gli esperimenti. Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza. Se si tratta di un compito di generazione multimodale, riportiamo ROOGEL. Per un compito RP, riportiamo anche ROOGEL. Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Quindi, questa misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito indipendentemente da lievi variazioni nella formulazione dell'istruzione.\n\nEcco il nostro risultato principale. Come possiamo vedere, il tuning delle istruzioni può migliorare significativamente le prestazioni di OFA sui compiti multimodali visti. Inoltre, il transfer learning da un set di dati di istruzioni naturali può beneficiare il tuning delle istruzioni. Qui possiamo vedere che all'aumentare del numero di compiti il modello ottiene prestazioni migliori e, allo stesso tempo, una minore sensibilità. Quindi, abbiamo anche condotto un esperimento in cui abbiamo utilizzato una sola istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità, mostrando così l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Come possiamo vedere, il transfer learning da un set di dati di istruzioni naturali consente al modello di raggiungere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che il transfer learning da un set di dati di istruzioni naturali può aiutare OFA a raggiungere prestazioni molto migliori sul set di dati di istruzioni naturali.\n\nQuindi, in sintesi, abbiamo proposto il primo grande set di dati di tuning delle istruzioni multimodali. Abbiamo migliorato significativamente la capacità DAROCHOT di OFA ed esplorato diverse tecniche di transfer learning, dimostrando i loro benefici. Abbiamo progettato una nuova metrica chiamata sensibilità. Una cosa in più: stiamo raccogliendo un set di dati di tuning delle istruzioni multimodali molto più grande con circa 150 compiti aggiuntivi in varianti linguistiche e li rilasceremo. Quindi, questo è un codice QR per i nostri dati e il modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Yusuf Zhang dell'Università di Penn State. Oggi presenterò il nostro lavoro, Exampler, il parsing semantico cross-lingue in più lingue naturali e rappresentazioni del significato. Quindi, il parsing semantico è un'attività volta a costruire rappresentazioni semantiche delle query degli utenti, come SQL e Lambda calcolo. E il parsing semantico cross-lingue è il compito di tradurre le query in più lingue naturali in diverse rappresentazioni del significato. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda, FunQL e altro ancora. I modelli esistenti di parsing semantico cross-lingue sono stati proposti e valutati separatamente su set di dati di compiti e applicazioni limitati. Ad esempio, ci sono lacune nella copertura di alcune lingue naturali. Il cinese manca e ci sono lacune nella copertura di molte rappresentazioni. Manca il calcolo Lambda. Oppure sono valutati solo su determinati modelli più recenti. Per esempio, esiste un solo modello per valutarli. A tal fine, proponiamo Exampler, forniamo un set di dati uniforme, Exampler, per il parsing semantico cross-lingue in più lingue naturali e rappresentazioni del significato. Contiene nove set di dati in vari domini, cinque tassonomie di parsing semantico, otto rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è TranslateTest. Utilizziamo l'API di Google Translate per tradurre la sorgente nella lingua di destinazione, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione. E, ad esempio, addestriamo il modello inglese su... su query inglesi e durante l'inferenza traduciamo la query tedesca utilizzando l'API in inglese e quindi utilizziamo il modello addestrato per prevedere l'output SQL. Testiamo anche il modello monolingue. In questa impostazione, la lingua sorgente è la stessa della lingua di destinazione. Ad esempio, tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione di fusione monolingue addestrando modelli monolingue con solo il 10% dei dati di addestramento. E testiamo un modello multilingue monolingue, che addestriamo un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue. E durante l'inferenza, possiamo utilizzare questo modello per tradurre le query tedesche o cinesi e così via. Consideriamo anche il trasferimento cross-lingue zero-shot e few-shot. Addestriamo su una lingua sorgente e trasferiamo ad un'altra lingua. Quindi, durante l'addestramento, addestriamo su una query inglese o sulla combinazione di query inglesi e tedesche few-shot per addestrare un modello multilingue e prevedere l'output SQL. Abbiamo anche trovato molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingue, valutiamo due gruppi di modelli, tra cui Encoder PDR, che sta per encoder pre-addestrati multilingue con decodificatori basati su puntatore, come XLMR più PDR e BERT più PDR. Valutiamo anche modelli encoder-decoder, che sono modelli encoder-decoder pre-addestrati multilingue, come MBART e MT5. Abbiamo scoperto che l'encoder-decoder ottiene le migliori prestazioni su tutti e nove i set di dati. Valutiamo MT5 e XLMR più PDR nell'impostazione multilingue. Abbiamo scoperto che l'encoder-decoder o l'encoder PDR possono essere migliorati addestrandoli in un mix di varie lingue. E abbiamo scoperto che ciò è dovuto al fatto che la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, ad eccezione del fatto che le prestazioni dell'inglese diminuiscono in sette set di dati e aumentano solo in tre set di dati. Penso che questo sia noto come curve del multilingue. Abbiamo anche confrontato il divario di prestazioni cross-lingue. In questa figura, la linea blu è il trasferimento cross-lingue few-shot. La linea arancione è il trasferimento cross-lingue zero-shot, mentre la linea verde è l'impostazione monolingue. Abbiamo scoperto che confrontando la linea verde e arancione, per l'impostazione zero-shot, il divario di prestazioni del trasferimento cross-lingue è significativo. E confrontando la linea blu e arancione, abbiamo scoperto che per l'impostazione few-shot, il divario di trasferimento si riduce rapidamente. Abbiamo anche trovato altre scoperte interessanti. Ad esempio, l'encoder-decoder supera i lavori precedenti o ottiene risultati confrontabili. L'addestramento sull'inglese come lingua naturale può migliorare significativamente le prestazioni di few-shot sulle lingue naturali di destinazione. E abbiamo scoperto che i modelli di linguaggio multilingue come CodeSwitch e Bloom sono ancora inadeguati per i compiti di parsing semantico cross-lingue. In sintesi, abbiamo costruito Exampler, un benchmark unificato per il parsing semantico cross-lingue con più lingue naturali e molte rappresentazioni. Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli di linguaggio multilingue. E i nostri risultati mostrano molte scoperte interessanti e altro ancora. E siete invitati a visitare il nostro articolo e il codice. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Adam Spirakowski e questo intervento riguarda la struttura di dipendenza della coordinazione. Come sapete, esistono diverse strutture di dipendenza ipotizzate da diverse teorie e approcci basati su corpus. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie è tale che il primo congiunto è il capo dell'intera struttura coordinata, quindi in questo caso Lisa. Un approccio simile è assunto nella teoria del testo del significato di Igor Milchuk, dove ancora l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici, giusto? Ne evidenziano uno dei congiunti.\n\nEsistono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga, l'approccio guidato dalla congiunzione assunto negli alberi di dipendenza del corpus di Praga, dove le strutture coordinate sono guidate dalla congiunzione. Quindi otteniamo dipendenze da e verso tutti i congiunti. E infine c'è anche un approccio a più teste utilizzato, per esempio, nella grammatica delle parole di Cutson, dove, per così dire, tutti i congiunti sono teste della struttura coordinata, quindi otteniamo dipendenze dal governatore qui ride verso tutti i congiunti separatamente, questi sono Bart e Maggie.\n\nOra, lo scopo di questo articolo è produrre un nuovo argomento a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due. L'argomento si basa sul principio della minimizzazione della lunghezza delle dipendenze che spiegherò sulla base di questi esempi. Quindi in inglese, come sapete, gli oggetti diretti preferiscono essere vicini al verbo mentre gli complementi possono essere più lontani, giusto? Quindi \"March l'ha letto ieri\" va bene perché l'oggetto diretto \"lo\" è vicino al verbo, mentre \"March l'ha letto ieri\" è molto peggio, giusto? Perché qui tra il verbo e l'oggetto diretto c'è un complemento \"ieri\". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo il complemento. Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. \"March ha letto questo libro assolutamente affascinante sulle api ieri\" va bene, dove invece di \"lo\" abbiamo questo lungo NP. È anche corretto dire \"March ieri ha letto questo libro assolutamente affascinante sulle api\". Quindi il ragionamento qui è che questo è possibile perché anche se questa frase viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che afferma che sono preferite le dipendenze più corte. Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dipendenza da \"letto\" al complemento di lunghezza 7, misurata in parole, e da \"letto\" a \"libro\" di lunghezza 4, quindi insieme fa 11. Quando si spostano, si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, giusto, quindi invece di 11 diventa sei, molto più corto, ed è per questo che suona piuttosto bene, viola un principio ma ne soddisfa un altro.\n\nQuindi ciò che abbiamo fatto è stato estrarre varie statistiche sulla coordinazione dalla versione potenziata del Penn Tree Bank e vedere nel documento perché non abbiamo utilizzato le dipendenze universali. E queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendono ad essere più brevi. Quindi \"sale e pepe\" e non \"pepe e sale\", misurati in sillabe. E anche l'osservazione fatta incidentalmente che questa tendenza aumenta con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più breve preferisce essere il primo, più forte, giusto? Quindi la proporzione dei congiunti sinistri più brevi è maggiore.\n\nMa ciò che è nuovo in questo documento è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente, giusto? Quindi il governatore è a sinistra in questo esempio. \"Ho visto Bart e Lisa\". Quindi il governatore è a sinistra. È assente nel secondo esempio \"Homer è arrivato e ha starnutito\", qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, quindi in questi casi il congiunto sinistro preferisce essere più breve, tanto più quanto maggiore è la differenza tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, \"sinistra governa la coordinazione ted e net\", questo effetto scompare. Lo dimostriamo misurando la lunghezza in caratteri, che è la prima colonna, in sillabe, la colonna centrale, e in parole, la colonna di destra. Quindi mi concentrerò su quella di destra. Quello che vediamo qui è che quando il governatore è a sinistra, la tendenza del congiunto sinistro ad essere più breve aumenta costantemente con la differenza assoluta in parole e la stessa cosa si osserva quando non c'è governatore, come nella coordinazione di frasi, ma quando il governatore è a destra questa tendenza scompare e nel documento dimostriamo come questo fornisca un argomento contro le strutture asimmetriche di coordinazione come queste due e a favore delle strutture simmetriche come queste due. Quindi vedere il documento per l'accordo completo e gli argomenti e parlarne con noi nella sessione post-intervento. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Kyo Yin e presenterò il nostro lavoro intitolato \"Quando la Traduzione Richiede Contesto? Un'Esplorazione Multilingue Basata sui Dati\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emily Liu, Andre FD Martins e Graham Newbig.\n\nMolte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase? Beh, se la frase precedente fosse \"Le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono\", allora \"mole\" si riferisce a una spia. Ma se la frase precedente fosse \"Potrebbe essere qualcosa di grave, dottore?\", allora \"mole\" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e di conseguenza anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possono tradurre casi del genere è piuttosto difficile. Innanzitutto, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus, come BLEU, incapaci di catturare queste traduzioni. E alcuni hanno suggerito valutazioni mirate sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla cura umana.\n\nIn questo lavoro, cerchiamo di rispondere a due domande. Prima, quando la traduzione richiede contesto? E seconda, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. E questo viene fatto misurando quanto informazione il contesto C fornisce sul bersaglio Y dato il fonte X. Si può pensare a CXMI come all'informazione ottenuta fornendo il contesto al modello. In questo lavoro, estendiamo CXMI al CXMI puntuale, che può misurare l'utilizzo del contesto a livello di frase o di parola. Possiamo considerare le parole con un alto PCXMI come quelle che richiedono contesto per la traduzione.\n\nOra analizziamo le parole con un alto PCXMI per cercare schemi tra queste parole. E svolgiamo la nostra analisi su trascrizioni di TED talk tradotte dall'inglese a quattordici lingue diverse. Effettuiamo la nostra analisi su tre diversi livelli. Prima, esaminiamo le etichette delle parti del discorso che hanno elevati valori medi di PCXMI. E questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un relativamente alto PCXMI. E questo può essere spiegato dal fatto che l'inglese non ha pronomi duali. Quindi, è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. E similmente, scopriamo che alcune lingue richiedono il contesto anche quando si tratta di scegliere la forma verbale appropriata.\n\nPoi, esaminiamo le voci del vocabolario che hanno un alto PCXMI medio su tutte le loro diverse occorrenze. E questo ci aiuta a identificare casi come questo, in cui in cinese è necessario il contesto per tradurre correttamente. E infine, scopriamo che il contesto è importante per tradurre con il giusto livello di formalità.\n\nInfine, esaminiamo diversi token individuali con un alto PCXMI. E questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi.\n\nQuindi, utilizziamo le nostre scoperte dall'analisi per progettare un punto di riferimento per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni discorsivi identificati, creiamo tagger per identificare automaticamente le parole che riguardano il fenomeno, e chiamiamo il nostro tagger Multilingual Discourse Aware (MUDA). Possiamo anche notare che le diverse lingue hanno proporzioni diverse di questi fenomeni discorsivi. Quindi, applichiamo il tagger MUDA sul corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto identificati dal tagger MUDA.\n\nInfine, utilizziamo il nostro punto di riferimento, insieme ad altre metriche, per valutare diversi modelli sulla traduzione a livello di documento. Prima di tutto, quando usiamo metriche a livello di corpus, quindi per BLEU, scopriamo che i modelli ignari del contesto hanno le migliori prestazioni. Ma se usiamo COMET, i modelli consapevoli del contesto si comportano meglio. E se usiamo la misura WordF, i modelli con o senza contesto hanno prestazioni comparabili. Questo dimostra nuovamente che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo metriche a livello di corpus.\n\nOra utilizziamo il punto di riferimento MUDA per valutare i modelli e scopriamo che i modelli a livello di contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per alcuni fenomeni discorsivi, come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli senza contesto per altri fenomeni, come le ellissi, i pronomi e la forma verbale. Quindi, questo suggerisce dove dovremmo vedere più progressi per la traduzione a livello di documento.\n\nAbbiamo anche confrontato diversi sistemi commerciali e il nostro punto di riferimento mostra che DPL è solitamente più accurato di Google Translate per la traduzione a livello di documento.\n\nIn sintesi, eseguiamo un'analisi basata sui dati su quattordici coppie linguistiche per identificare quando le traduzioni richiedono contesto. E poi utilizziamo le nostre scoperte per costruire un punto di riferimento per la traduzione a livello di documento, che può aiutarci a identificare quali fenomeni discorsivi i modelli gestiscono bene o meno, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento.\n\nGrazie mille per la vostra attenzione. Ci vediamo domani."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa del primo anno di dottorato presso la Carnegie Mellon University, e oggi presenterò il vostro lavoro, \"Analisi Posizionale: Caratterizzazione dei Pregiudizi di Progettazione, Insiemi e Modelli Beta\". Questo studio è stato realizzato in collaborazione con alcuni ricercatori dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santi, Ronin Lebras, Katarina Reinicke e Martin Sapp.\n\nIniziamo immaginando di lavorare per un giornale e di dover moderare i commenti sotto un articolo per rimuovere contenuti tossici. Potremmo rivolgerci a un'API popolare come Perspective API per il rilevamento della tossicità, che funziona molto bene se siamo Carl Jones, dove le API di prospettiva riescono a identificare correttamente i casi tossici. Ma non è così per Dithya Sharma, dove le API di prospettiva non sono altrettanto sensibili ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di pregiudizio di progettazione, in cui si osservano differenze sistematiche nelle prestazioni tecnologiche tra diverse popolazioni.\n\nI pregiudizi di progettazione come quello descritto possono verificarsi a causa della posizione dei ricercatori di NLP e degli sviluppatori di modelli. La posizione si riferisce alle prospettive che le persone hanno in base alla loro demografia, identità ed esperienze di vita. È un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. Come ricercatori, la posizione può influenzare il processo di ricerca e i suoi risultati, poiché può modificare le decisioni prese.\n\nUna domanda che ci si potrebbe porre è: i dataset e i modelli hanno una posizione? Non stiamo affermando che i modelli e i dataset stessi abbiano identità demografiche ed esperienze di vita, ma aggregando i giudizi e le opinioni di persone reali, possono rappresentare alcune posizioni più di altre. Studi precedenti hanno fornito alcune prove aneddotiche di posizione, come lacune culturali nei modelli e nei dataset, nonché definizioni teoriche della posizione del modello. Tuttavia, questi studi non confrontano gli utenti finali con i dataset e i modelli stessi.\n\nLo studio della posizione dei modelli e dei dataset è sempre più importante man mano che i compiti di NLP diventano più soggettivi e orientati al sociale. È difficile caratterizzare come queste posizioni siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Per studiare la posizione dei dataset e dei modelli, confrontiamo le annotazioni di utenti reali con dataset e modelli esistenti. Lo facciamo attraverso il nostro framework, NLPositionality.\n\nIl framework funziona in due passaggi principali. Il primo passo è ri-annotare i dataset con annotatori diversi. Preferiamo farlo anziché esaminare le demografie dei dataset originali e degli annotatori, perché di solito solo pochi annotatori annotano ogni istanza e perché i dati demografici vengono raramente raccolti e condivisi. Quindi, optiamo per ri-annotare i dati per ottenere molti annotatori per istanza e un set ricco di dati demografici. Quindi, prendiamo le annotazioni in base ai dati demografici e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Parsons R.\n\nIl nostro framework differisce dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con le previsioni e le etichette dei modelli e dei dataset, piuttosto che limitarsi a esaminare l'accordo degli annotatori o a modellare le distribuzioni degli annotatori. Il nostro framework è reso possibile da Lab in the Wild, una piattaforma di sperimentazione online in cui possiamo reclutare volontari diversi rispetto a piattaforme come MTurk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild fornisce ancora dati di alta qualità.\n\nAbbiamo ospitato due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. Funziona nel seguente modo: i partecipanti leggono una situazione dal dataset Social Chemistry e poi scrivono quanto sia socialmente accettabile. Successivamente, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'AI e con quelle di altri. Abbiamo poi confrontato queste annotazioni con Social Chemistry Delphi in GPT 4. Abbiamo replicato un setup simile per il compito di rilevamento di tossicità e discorsi d'odio, in cui i partecipanti leggono un'istanza da DynaHate e scrivono se pensano che sia un caso di discorso d'odio. Abbiamo quindi confrontato queste annotazioni con DynaHate, Perspective API, Rewire API, Hate Roberta in GPT 4.\n\nIl nostro studio ha accumulato oltre sedicimila annotazioni da oltre mille annotatori di ottantasette paesi. Siamo quindi meglio equipaggiati per rispondere a chi si allineano di più i dataset e i modelli di NLP? Abbiamo scoperto che esiste una posizione nell'NLP. Ad esempio, abbiamo riscontrato che i dataset e i modelli sono più allineati ai paesi anglofoni. Per l'analisi dell'accettabilità sociale di GPD 4, abbiamo scoperto che è più allineata ai paesi confuciani e anglofoni. Abbiamo riscontrato lo stesso per DanaHate, che è anch'esso più allineato ai paesi anglofoni. Abbiamo anche trovato un allineamento aggiuntivo con persone che hanno un'istruzione universitaria.\n\nPer GPD 4 nel compito di accettabilità sociale, abbiamo scoperto che è più allineato a persone con un'istruzione universitaria o post-laurea. E lo stesso vale per DanaHate, che è più allineato a persone con un'istruzione universitaria. Tuttavia, quando i modelli e i dataset sono allineati a popolazioni specifiche, alcuni vengono inevitabilmente lasciati indietro. Un esempio è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro equivalenti maschi e femmine. Abbiamo riscontrato questo nel compito di accettabilità sociale di GPT 4 e nell'analisi del compito DynaHate.\n\nDato che esiste una posizione nell'NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni. La prima è tenere un registro di tutte le scelte di progettazione rilevanti durante l'intero processo di ricerca. La seconda è condurre ricerche NLP attraverso la lente del perspectivismo. La nostra terza raccomandazione è costruire dataset e modelli specializzati all'interno di comunità specifiche. Un buon esempio è l'iniziativa Masakane.\n\nVogliamo sottolineare che l'NLP inclusivo non significa semplicemente far funzionare tutte le tecnologie per tutti. Questo conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di consultare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Salve, parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indirette per la selezione delle entità, in cui introduciamo il corpus altentity. Mi chiamo Javot Hosseini e questo è un lavoro congiunto con Philip Radlinsky, Silvia Pareti e Annie Luis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando desiderano effettuare una scelta. Considerate questa domanda alternativa: intendevi \"easy on me\" o \"I got a feeling\"? Qui un utente vuole selezionare tra questi due segni. La cosa più ovvia è utilizzare un riferimento diretto, ad esempio dicendo il nome della canzone \"easy on me\" o la sua posizione, \"la prima\". Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale; ciò può accadere quando l'utente non riesce a ricordare il nome della canzone o le pronunce sono troppo simili e difficili da disambiguare, o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti: ad esempio, \"la più recente\" o \"la canzone che non è energetica\". Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking dei modelli linguistici di grandi dimensioni (LLM) nella comprensione delle entità. Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per questo compito, quindi ne abbiamo raccolto uno utilizzando l'annotazione da parte di un gruppo di persone. Il nostro set di dati copre tre diversi domini: musica, libri e ricette. La metodologia di raccolta del set di dati enfatizza l'informalità utilizzando un'impostazione di completamento a fumetti. Il fumetto ha tre nuvolette. Nella prima nuvoletta, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\". Con questo, Bob imposta il contesto del dialogo. Nella seconda nuvoletta, Alice dice: \"Intendi 'easy on me' o 'I got a feeling'?\", che è la domanda alternativa. E nella terza nuvoletta, Bob utilizza un riferimento indiretto per selezionare una di queste entità, ad esempio \"Neo-Ervandal\". Forniamo automaticamente le prime due nuvolette, ma la terza è compilata dall'annotatore. La prima nuvoletta è scelta tra alcuni prompt manuali per dominio. La seconda, che è la domanda alternativa, viene generata nel modo seguente: utilizziamo sempre un semplice modello. \"Intendi A o B?\", dove A e B sono campionati da Wikipedia. Ecco i diversi metodi di campionamento utilizzati. Man mano che si sale nella lista, le entità diventano più simili tra loro e di solito è più difficile effettuare la disambiguazione. La prima è uniforme e casuale, la seconda è quando le entità hanno titoli simili, ad esempio due libri con lo stesso nome. La terza è quando hanno descrizioni simili su Wikipedia e, infine, quando hanno infobox o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente le conoscono. Quello che facciamo è mostrare alcune conoscenze di sfondo sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno una parte di ogni canzone e di leggere su ciascuna. Ecco, ad esempio, il risultato della ricerca Google per la canzone \"Easy on Me\". Per i domini ricette e libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo anche le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono. Poi chiediamo agli annotatori di scegliere una di queste entità, ad esempio qui la prima, e di descriverle utilizzando da tre a cinque espressioni di riferimento indirette. Ad esempio, \"quella con la musica per pianoforte\". Ecco alcuni esempi dal nostro set di dati: \"quella senza parole\", \"non quella con il ragazzo di 12 anni\", \"quella fittizia\", \"quella che viene dall'Azerbaigian\" e così via. Il corpus altentities contiene 6.000 domande alternative in tre domini e ha 42.000 espressioni di riferimento indirette. I risultati con il modello T5xLarge sono riassunti qui sotto. Se il modello linguistico ha accesso alle stesse conoscenze di sfondo degli annotatori, allora l'accuratezza è molto alta, intorno al 92-95%. Ma questo non è realistico. Se il modello linguistico ha accesso a conoscenze di sfondo parzialmente sovrapposte, l'accuratezza è tra l'82 e l'87%, che è più realistico, ad esempio quando il modello linguistico recupera le conoscenze di sfondo. Se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 60%. Quindi c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili per dominio. Ecco un link al nostro set di dati. Grazie."}
