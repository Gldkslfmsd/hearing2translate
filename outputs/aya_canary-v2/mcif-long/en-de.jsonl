{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lendermann und heute gebe ich Ihnen eine kurze Einführung in unsere Arbeit über kompositorische Generalisierung ohne Bäume unter Verwendung von Multi-Set-Tagging und latenten Permutationen. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titoff. Kompositorische Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursionen und ungesichtete Kompositionen von Phrasen zu bewältigen, die während des Trainings individuell gesichtet wurden. Im Kontext des semantischen Parsings könnte ein Test für kompositorische Generalisierung wie folgt aussehen. Wie üblich haben wir einen Trainingsdatensatz von Äußerungen, in diesem Fall \"das Mädchen schlief\" und \"Maria wusste, dass das Mädchen schlief\". Diese Äußerungen sind mit logischen Formen gepaart, die wesentliche Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur standardmäßigen maschinellen Lernbewertung stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell ungesichtete logische Formen. In diesem Beispiel hat das Modell während des Trainings eine flachere Rekursion gesehen und wird mit einem Beispiel mit tieferer Rekursion getestet. Naive Sequenz-zu-Sequenz-Modelle haben Schwierigkeiten mit dieser Art von Generalisierung außerhalb der Verteilung und erzeugen oft Ausgaben, die vom Eingang entkoppelt sind. Insbesondere scheitern sie häufig daran, die systematischen Korrespondenzen zwischen Eingang und Ausgang wiederherzustellen, wie die farblich gekennzeichneten in diesem Beispiel. Eine beliebte Methode, dies zu beheben, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der Äußerungen mit logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden normalerweise nicht bereitgestellt und müssen irgendwie beschafft werden. Dies kann kompliziert und manchmal ein rechenintensiver Prozess sein. In der Regel erfordert dies eine erhebliche, auf Formalismen basierende Vorverarbeitung der logischen Formen, um beispielsweise mit Variablensymbolen umzugehen. Die Beschaffung von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren beinhalten. In dieser Arbeit verwenden wir keine Bäume und führen ein neuronales Sequenz-zu-Sequenz-Modell ein, das die Korrespondenzen zwischen Fragmenten des Eingangs und Fragmenten des Ausgangs direkt modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung auf tiefere Rekursionen ohne Verwendung von Bäumen. Unser Ansatz prognostiziert den Ausgang aus dem Eingang in zwei Schritten. Zunächst markieren wir jedes Eingabetoken mit einer ungeordneten Multimenge von Tokens, die im Ausgang erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht geordnet. Aus diesem Grund verwenden wir in einem zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode zur Vorhersage einer Permutation ein, die keine harten Einschränkungen für die möglichen Permutationen vornimmt. Dies macht unseren Ansatz sehr flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell in etwa so. Wir gehen von links nach rechts über den Ausgang und bestimmen, welches Multiset-Token an jeder Position platziert werden soll. Für die erste Ausgangsposition wählen wir einfach eines aus, wie hier rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um das zweite Token im Ausgang zu bestimmen. Auf ähnliche Weise bestimmen wir das dritte Token im Ausgang, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unseren Ansatz mit anderen baulosen Modellen auf dem Koggs-Benchmark. Unser Modell übertrifft die anderen bei der Generalisierung auf tiefere Rekursionen bei weitem. Einige andere Arten struktureller Generalisierung bleiben jedoch sehr herausfordernd. In unserer Arbeit lösen wir mehrere interessante technische Herausforderungen. Erstens ist die Ausrichtung zwischen Eingang und Ausgang in den Trainingsdaten nicht gegeben. Folglich wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die sprachlich korrekte Permutation ist latent. Wir beheben dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass die Suche nach der höchstbewerteten Permutation NP-schwer ist. Dies liegt daran, dass sie mit dem Problem des Handlungsreisenden zusammenhängt. Wir nähern uns diesem Problem mit einer GPU-freundlichen, kontinuierlichen Relaxation an, die es uns auch ermöglicht, durch die Lösung zurückzupropagieren und sprachlich plausiblere Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und die Bewältigung dieser Herausforderungen erfahren möchten, lesen Sie bitte unsere Arbeit oder besuchen Sie unseren Posterbeitrag."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra, und heute werde ich über unsere Arbeit sprechen, die den Titel trägt: „Markierte Personas: Verwendung natürlicher Sprachanreize zur Messung von Stereotypen in Sprachmodellen“. Diese Forschung wurde in Zusammenarbeit mit Essendermouch und Dangerowski durchgeführt. In den letzten Jahren haben viele Studien die Verbreitung sozialer Voreingenommenheit und Stereotypen in großen Sprachmodellen, auch LLMs genannt, dokumentiert. Diese Messungen haben jedoch verschiedene Einschränkungen. Sie stützen sich in der Regel auf von Hand erstellte Datensätze, die sehr zeitaufwändig zu kuratieren sind. Außerdem messen sie meist nur sehr spezifische Stereotypen, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte verallgemeinern lassen oder lediglich sehr allgemeine, breite Assoziationen erfassen, wie negative Verbindungen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die Mehrheit der Arbeiten in diesem Bereich nicht die Schnittstellenproblematik (Intersectionalität), die besagt, dass vielschichtige soziale Identitäten Vorurteile verstärken und einzigartige Quellen von Schaden sein können.\n\nUm diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft der neueren, anweisungsorientiert trainierten LLMs, die sehr gut darauf reagieren. Wir können das Modell also anweisen, eine Persona zu generieren, eine Darstellung einer imaginären Person, indem wir einen Anreiz wie diesen geben: „Stellen Sie sich vor, Sie sind eine asiatische Frau, beschreiben Sie sich selbst.“ Wir können sofort sehen, dass dies auf jede Demografie anwendbar ist, da wir einfach den Identitätsmarker in diesem Anreiz angeben können, den wir wünschen. Hier sind einige Beispielgenerierungen von GPT 4. Sofort erkennen wir, dass die Ausgaben zwar nicht übermäßig negativ oder toxisch im traditionellen Sinne dieser Wörter sind, sich aber interessante Muster abzeichnen. Die asiatische Frau wird als unauffällig dargestellt, die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch beschrieben und auf eine faszinierende Region verwiesen, und beide Personas von Frauen of Color beziehen sich auf ihre Abstammung, während die Persona des weißen Mannes nichts Derartiges enthält.\n\nUm diese Muster zu erfassen, besteht unsere Methode aus zwei Teilen. Der erste Teil ist die Generierung dieser Personas. Unsere Anreize zur Generierung dieser Personas wurden von einer Studie inspiriert, in der diese Anreize menschlichen Probanden gegeben wurden. Dabei stellte sich heraus, dass auch bei Menschen rassistische Stereotypen zum Vorschein kamen. Außerdem ermöglicht dies einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten. Der zweite Teil ist die Methode der markierten Wörter, die dazu dient, die Wörter zu identifizieren, die markierte Gruppen von unmarkierten Gruppen unterscheiden, worauf ich gleich näher eingehen werde. Der Vorteil besteht darin, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne uns auf ein bestimmtes Lexikon stützen zu müssen.\n\nDie Methode der markierten Wörter basiert auf dem soziolinguistischen Konzept der Markiertheit, das besagt, dass es eine unmarkierte Norm gibt und jede Gruppe, die sich von dieser Norm unterscheidet, sprachlich markiert wird. So wird beispielsweise das Wort Kriegerin normalerweise mit Männern assoziiert. Wenn Menschen also eine Kriegerin beschreiben, die eine Frau ist, spezifizieren sie in der Regel einen männlichen Krieger und markieren den Begriff mit Frau. Im Allgemeinen sind dominierende Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode bezeichnen wir zunächst, welche Gruppen unmarkiert und markiert sind. Anschließend vergleichen wir die Personas mit der Methode der „Kampfwörter“, die im Wesentlichen gewichtete Logod-Verhältnisse verwendet, um die wichtigsten Wörter für jede markierte Gruppe zu ermitteln. Für die Personas schwarzer Frauen würden wir beispielsweise „Kampfwörter“ anwenden und die Logod-Verhältnisse mit den weißen Personas und den Personas von Männern vergleichen, da diese die beiden entsprechenden unmarkierten Gruppen sind.\n\nNun zu einigen Ergebnissen. Zunächst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Wenn wir jedoch die Verteilung der Wörter im Lexikon genauer betrachten, erkennen wir Folgendes: Während die generierten Personas eine viel höhere Rate an Lexikonwörtern aufweisen, haben die von Menschen geschriebenen Personas eine viel breitere Verteilung von Wörtern. Die Stereotypwörter in den generierten Personas beschränken sich tatsächlich auf die Wörter groß und athletisch, also nur auf positive oder zumindest nicht-negative Wörter. Dieses Lexikon erfasst tatsächlich viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht.\n\nStattdessen wenden wir uns den Ergebnissen unserer Methode der markierten Wörter zu, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essenzialisierende Erzählungen fördern. In unserer Analyse offenbaren wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zunächst einmal umfassen die Top-Wörter für markierte Gruppen Begriffe wie Kultur, Tradition, stolz und exotisch. Diese Wörter definieren diese Gruppen ausschließlich durch ihre Beziehung zu ihrer Identität und unterscheiden sie vom weißen Normwert. Dies trägt zu einer langen Geschichte der Diskriminierung und Ausgrenzung dieser Gruppen bei. Darüber hinaus spiegeln sich in diesen Wörtern viele gängige Klischees wider, insbesondere für Frauen of Color. So beinhalten beispielsweise die Wörter, die lateinamerikanische Frauen beschreiben, Begriffe wie lebhaft und kurvig, was mit dem Klischee des Tropicalismus verbunden ist. Bei asiatischen Frauen sind es Wörter wie klein, zart und seidig, was auf eine lange Geschichte der Sexualisierung asiatischer Frauen verweist, die als sehr sanft und unterwürfig angesehen werden.\n\nSchließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Begriffe wie stark und widerstandsfähig sind. Dies steht in Verbindung mit dem Archetyp der „starken schwarzen Frau“. Obwohl dies auf den ersten Blick positiv klingt, haben Studien gezeigt, dass dieser Archetyp in Wirklichkeit sehr schädlich ist, da er von diesen Demografien enormen Druck ausübt, widerstandsfähig und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt sich für die Beseitigung dieser Hindernisse einzusetzen, wird der Druck auf die Betroffenen ausgeübt, sie zu überwinden, was zu sehr negativen gesundheitlichen Folgen für diese Menschen führt, neben anderen Schäden.\n\nAllgemeiner gesagt stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur essenzialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer. Erstens sollten wir Forscher positive Stereotypen und essenzialisierende Erzählungen thematisieren. Wir sollten auch eine Schnittstellenperspektive (intersectionale Linse) verwenden, um Vorurteile und Schäden zu untersuchen, da viele Dinge übersehen werden könnten, wenn wir dies nicht tun. Und schließlich sollte es mehr Transparenz über Methoden zur Vorurteilsminderung geben, da wir beispielsweise bei diesen positiven Stereotypen nicht wissen, ob dies auf eine Art übermäßige Wertausrichtung oder vielleicht andere Anti-Stereotypen-Methoden zurückzuführen ist, die zu diesen schädlichen Mustern führen. Ohne mehr Transparenz können wir hier keine Annahmen treffen oder dies weiter untersuchen.\n\nVielen Dank fürs Zuhören. Ich wünsche Ihnen eine gute Zeit bei der ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Heute werden wir Ihnen alles über ABCEval erzählen, einen neuen dimensionalen Ansatz zur Bewertung von konversationsbasierter KI. Diese Arbeit wurde vom Emory NLP Lab durchgeführt, geleitet von Professor Gino Choi an der Emory University, in Zusammenarbeit mit Amazon Alexa AI.\n\nNehmen wir an, Sie haben gerade ein Dialogmodell entwickelt und möchten wissen, wie es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis besteht darin, menschliche Bewertungen zu verwenden, beispielsweise indem man menschliche Bewerter bittet, auszuwählen, welcher von zwei Gesprächen besser ist, oder Gespräche auf einer Likert-Skala zu bewerten. Diese Ansätze eignen sich gut für umfassende Bewertungen der allgemeinen Dialogqualität, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer detaillierteren Ebene zu verstehen.\n\nEin Ansatz besteht darin, menschliche Bewerter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie z. B. die Relevanz der Modellantworten, unter Verwendung bestehender vergleichender oder Likert-Skalen-Methoden. Wir sind jedoch der Meinung, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Indem man explizit annotiert, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z. B. das Bereitstellen irrelevanter Informationen oder das Widersprechen sich selbst. Wir nennen diesen Ansatz \"Annotierung von Verhaltensweisen im Chat\" oder kurz ABCEval.\n\nWir haben diese Methode entwickelt, um die Verhaltensweisen von Chat-Modellen umfassend abzudecken, die in jüngster Literatur als Einflussfaktoren auf die Chat-Qualität vorgeschlagen wurden. ABCEval ist in der Lage, die Häufigkeit zu messen, mit der Chat-Modelle verschiedene thematische Fehler begehen. Beispielsweise misst ABCEval die Anzahl der Dialogzüge, in denen ein Chat-Modell seinen Gesprächspartner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinen Gesprächspartner widerspricht, falsche Fakten halluziniert oder allgemeines Wissen verletzt, und wann das Modell Empathie zeigt oder nicht.\n\nUm zu bestimmen, welche Bewertungsmethode am effektivsten ist, wählten wir vier aktuelle Chat-Modelle aus und bewerteten sie anhand von jeweils hundert menschlichen Bot-Gesprächen mit ABCEval. Zur Vergleichbarkeit bewerteten wir diese Gespräche auch mit drei bestehenden Methoden: Likert-Bewertungen auf Dialogzug-Ebene, Likert-Bewertungen auf Dialog-Ebene und dialogebene Paarvergleiche. Zusätzlich zu den Bewertungsmethoden sammelten wir Bewertungen zu acht der am häufigsten gemessenen Aspekte der Dialogqualität, da dies die Standardpraxis für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist.\n\nAus unserer Analyse dieser Bewertungsergebnisse geht hervor, dass die Verhaltenslabels von ABCEval insgesamt zuverlässiger sind als mit bestehenden Methoden gesammelte Labels, gemessen an der Inter-Annotator-Übereinstimmung bei 100 doppelt annotierten Gesprächen. Darüber hinaus sind die ABCEval-Labels prädiktiver für die allgemeine Gesprächsqualität als Metriken, die mit bestehenden Methoden erzeugt werden, wie diese einfache lineare Regressionsanalyse zeigt. Beispielsweise können Sie sehen, dass die Messung des Anteils der Dialogzüge mit Widersprüchen zwischen sich selbst und dem Gesprächspartner jeweils fünf und zehn Prozent der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenz-Scores nur vier Prozent oder weniger erklären.\n\nSchließlich überprüften wir, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, mithilfe einer schrittweisen linearen Regression. Sie können sehen, dass die Kombination aller ABCEval-Metriken über 25 Prozent der Gesprächsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, führt dies in den meisten Fällen zu einem erheblichen Informationsverlust über die Qualität. Die Kombination der alternativen Likert-Metriken auf Dialogebene erklärt deutlich weniger von der Qualität, und weniger dieser Metriken tragen einzigartige Informationen bei.\n\nDiese zuverlässigen, informativen und eindeutigen ABCEval-Metriken ermöglichen es uns, konversationsbasierte KI mit einer höheren Auflösung zu bewerten als dies mit vorherigen Methoden möglich war. Sie können in den Ergebnissen unseres Experiments sehen, dass noch mehrere Herausforderungen bestehen, die präzise quantifiziert wurden. Beispielsweise weisen die von uns getesteten Bots in etwa zwanzig Prozent ihrer Antworten Verstöße gegen den gesunden Menschenverstand auf. Sie produzieren in etwa fünfzehn Prozent der Antworten irrelevante Informationen, und sie widersprechen sich selbst oder ihrem Gesprächspartner in etwa zehn Prozent der Fälle.\n\nAngesichts des schnellen Fortschritts in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, sinken. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken für den Modellvergleich anzustreben. Wir hoffen, dass ABCEval von anderen in diesem Bereich als sinnvoller Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf zu sehen, wie sich konversationsbasierte KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Vasudha und ich bin Doktorandin im Fach Informatik an der Stony Brook University. Ich möchte unsere Arbeit, die beim ACL 2023 als Langbeitrag angenommen wurde, vorstellen: „Transfer Learning für Dissonanzerkennung“, eine Auseinandersetzung mit der Herausforderung seltener Klassen. Wir beginnen mit der Definition kognitiver Dissonanz und der Erklärung, warum es wichtig ist, dieses Phänomen in der Sprache zu untersuchen. Kurz gesagt, kognitive Dissonanz liegt vor, wenn zwei Überzeugungen oder Handlungen inkonsistent sind, wie in diesem Beispiel: Eine Person sagt „Ich weiß, dass Zigaretten mich töten könnten“ und fügt dann hinzu „Ich habe nach der Besprechung ein paar Zigaretten geholt“. Diese Überzeugung und Handlung sind inkonsistent und stehen in Dissonanz. Die weitere Äußerung „Ich glaube nicht, dass ich meinen Job ohne sie behalten könnte“ rechtfertigt das zweite Ereignis, und es entsteht eine Konsonanzbeziehung. Während Dissonanz ein sehr häufiges Phänomen in unseren täglichen Entscheidungen ist, kommt es in der Sprache und anderen Diskursbeziehungen äußerst selten zum Ausdruck. Warum ist das relevant? Die Untersuchung kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Überzeugungen in der Bevölkerung zu verfolgen und Einstellungsänderungen zu erkennen. Hohe kognitive Dissonanz steht auch in Verbindung mit Angststörungen und kann dazu beitragen, die psychische Gesundheit von Menschen besser zu verstehen. Die Analyse von Dissonanz in der Sprache kann auch hilfreich sein, um Extremismus und Polarisierung anfälliger Gruppen zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die individuellen kognitiven Stile von Personen zu erfassen und uns dabei zu unterstützen, Entscheidungsprozesse besser zu verstehen.\n\nUm eine Ressource für kognitive Dissonanz zu erstellen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir verwendeten einen Dissonanz-First-Ansatz, wie im Flussdiagramm dargestellt. Tweets wurden mit einem PDTB-Parser verarbeitet, und Paare von Diskursseinheiten wurden gemäß den in unserer Arbeit beschriebenen Richtlinien annotiert. Wie hier zu sehen ist, lag die Dissonanzrate bei nur 3,5 % der annotierten Paare. Nach der Sammlung von etwa 1.000 Beispielen von Diskursseinheitspaaren trainierten wir einen anfänglichen Klassifikator, der nur mit 43 Beispielen von Dissonanz trainiert wurde. Kein Wunder, dass der Klassifikator kaum besser als zufällig funktionierte. Angesichts der geringen Häufigkeit von Dissonanz und dem Fehlen vorheriger Datensätze zu diesem Thema stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen aus Transfer Learning und Active Learning, um mehr Dissonanzbeispiele in weniger Annotierungsrunden zu sammeln, die Gesamtkosten zu senken und die Dissonanzerkennung zu verbessern. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, beginnen wir den Active-Learning-Prozess mit dem Transfer von Gewichten aus eng verwandten Aufgaben. Wir transferieren aus zwei verschiedenen Aufgaben: Topic-unabhängige Dissonanz-Stance-Klassifizierung, eine Aufgabe, die bestimmt, ob zwei Debattenäußerungen verschiedener Personen übereinstimmen oder nicht, unabhängig vom Thema, hier als „Debatte“ bezeichnet, und binäre Klassifizierung der Erweiterungs- und Vergleichsklassen des PDTB, da diese beiden eng mit dem Konzept von Konsonanz und Dissonanz verwandt sind und wir sie hier als „CE“ bezeichnen. Wir stellen fest, dass die Zero-Shot-Leistung auf dem annotierten Datensatz nach dem Transfer bereits deutlich besser als zufällig ist, mit einem AUC von 0,62 bei der besten Leistung. Durch iteratives Feinabstimmen beider Aufgaben finden wir heraus, dass das Feinabstimmen der CE-Aufgaben gefolgt von weiterer Feinabstimmung der Debatte eine deutlich bessere Zero-Shot-Leistung ergibt. Dies ist das Modell, das wir verwenden, um den Active Learning zu starten.\n\nAls Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde des Active Learning und der Annotierung zu aktualisieren. „Cumulative“ akkumuliert alle bisher gesammelten Daten aus den Active Annotations, während „Iterative“ das Modell durch Training auf dem neuesten Datensatz aktualisiert. Unter den verschiedenen Strategien stellte sich heraus, dass „Cumulative“ gleich gut oder besser als „Iterative“ funktionierte. Um die Anzahl der Dissonanzbeispiele zu erhöhen, verwenden wir eine Strategie mit Wahrscheinlichkeit seltener Klassen (PRC), um hauptsächlich Beispiele auszuwählen, die nach dem aktuellen Modell in jeder AL-Runde mit hoher Wahrscheinlichkeit dissonant sind. Wir vergleichen dies mit anderen State-of-the-Art-AL-Strategien, die in der Community üblich sind. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-Art-Strategien, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung für „Random“ deutlich niedriger ist. In weiteren Runden des Active Learning mit den beiden besten Strategien verbesserten wir die AUC der Distanzklassifizierung auf 0,75, was die beste Leistung ist, die wir bisher in dieser Aufgabe erreicht haben. Wir untersuchten auch die Machbarkeit jeder Strategie hinsichtlich der Annotierungsqualität und der Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an korrekten Annotationen aufweist und sich am besten für seltene Klassen eignet. Allerdings finden die Annotatoren die Beispiele auch schwieriger.\n\nZusammenfassend lässt sich sagen, dass PRC eine einfache Active-Learning-Strategie für die Akquise seltener Klassen ist und dass das Cold-Starten des Active Learning mit entsprechend gestalteten Transfer-Learning-Aufgaben erheblich helfen kann. Wir stellen auch fest, dass iterative Aktualisierungen für den Transfer von einem anderen Bereich nützlich sind, während in-domain-Active-Annotations von kumulativen Aktualisierungen profitieren. Hier sind die Links zu unserem Code, Datensatz und unserer Arbeit. Zögern Sie nicht, sich mit uns in Verbindung zu setzen, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Aksheta und heute präsentiere ich gemeinsam mit meinem Mitautor Martin unsere Arbeit „Kitmasteff: Evaluierung der Wissensintegration aus mehreren Quellen“. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Nationale Sprachverständnis-Modelle greifen auf verschiedene Wissensquellen zurück, wie beispielsweise das in ihren Parametern enthaltene Wissen, das in der Regel während des Prätrainings erworben wird, und das in den Eingaben bei der Inferenz bereitgestellte Wissen. Jüngste Arbeiten zu Aufgaben wie dem Beantworten von Fragen zeigen, dass Modelle das im Voraus trainierte Wissen nutzen können, um die Aufgabe zu lösen. Doch das natürliche Sprachverständnis erfordert oft Wissen, das auch erst bei der Inferenz bereitgestellt wird. Beispielsweise im Satz „John sah den neu gewählten Präsidenten im Fernsehen“ können die prätrainierten Parameter Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese instanzspezifische Entität John ist oder wer der neue Präsident ist, da der Präsident seit dem Prätraining gewechselt haben könnte. Daher benötigen erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl das im Voraus trainierte als auch das bei der Inferenz verfügbare Wissen zu integrieren und zu nutzen.\n\nIn dieser Arbeit schlagen wir einen Diagnosetest für die Wissensintegration vor. Wir führen eine Kernreferenzauflösungsaufgabe ein, die darauf abzielt, die Fähigkeit zu untersuchen, auf Wissen aus verschiedenen Quellen zurückzugreifen. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablierten Kernreferenzauflösungmodellen. Hier ist ein Beispiel aus unserem Datensatz: „Servin ist Richterin. Kia ist Bäckerin. Termin und Kia trafen sich in einem Park. Nach einem langen Arbeitstag, an dem sie Fälle in einem Gerichtssaal entschied, war sie froh, sich zu entspannen.“ Die Aufgabe besteht darin, die korrekte Entität zu identifizieren, auf die das Pronomen „sie“ verweist, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen: Erstens entitätsspezifisches Wissen, wie z. B. „Servin ist Richterin“. Und zweitens Hintergrundwissen, das während des Prätrainings großer Sprachmodelle erlernt wird, während entitätsspezifisches Wissen typischerweise erst bei der Inferenz beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen, sodass sie entweder in einer einzigen Quelle oder in mehreren Quellen zu finden sein kann.\n\nWir haben drei Einstellungen für Kitmos definiert. Erstens haben wir die Themen-Prätrainings-Einstellung, bei der angenommen wird, dass das Hintergrundwissen zum Zeitpunkt des Prätrainings verfügbar ist. Zweitens gibt es die Hintergrund-Beide-Einstellung, bei der beide Wissensarten erst bei der Inferenz verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie einen Fall simuliert, in dem das für die Lösung einer Aufgabe notwendige Hintergrundwissen nicht Teil der prätrainierten Daten der Modelle ist, beispielsweise weil seit dem Prätraining neue Berufe entstanden sind.\n\nHier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den beiden Quellen steuern. In der Hintergrund-Prätrainings-Einstellung gehen wir davon aus, dass das Hintergrundwissen „Politiker streben gewählte Sitze in der Regierung an“ in den prätrainierten Parametern enthalten ist. Im Hintergrund-Kontext stellen wir die spezifische Information „Chichester ist Politiker“ bereit. In der Hintergrund-Beide-Einstellung stellen wir zusätzlich zum spezifischen Wissen auch Hintergrundwissen über Politiker im Inferenz-Kontext bereit. In der Hintergrund-Inferenz-Einstellung ersetzen wir den Beruf „Politiker“ durch die fiktive Berufsangabe „Spiegeltour“, da „Spiegeltour“ wahrscheinlich nicht in den prätrainierten Parametern enthalten ist.\n\nWir bewerten den Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Kernreferenzauflösungmodellen. In dieser Abbildung zeigen wir die Ergebnisse der besten Modelle in der schwierigsten Variante der Hintergrund-Prätrainings-Einstellung. Ohne taskspezifisches Training auf KitMus schneiden beide Modelle schlecht ab. Wenn sie jedoch auf KitMus trainiert werden, performen sowohl C2F als auch Berth für Koref signifikant besser als die zufällige Auswahl. Dies deutet darauf hin, dass Modelle, wenn sie auf allgemeinen Kernreferenzauflösungsdatensätzen trainiert werden, lernen, Oberflächenmerkmale auszunutzen, die beim Testen auf KitMus, wo solche Merkmale entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die besten Modelle nicht zuverlässig Hintergrundwissen integrieren können, das nur bei der Inferenz bereitgestellt wird.\n\nZusammenfassend lassen sich die wichtigsten Erkenntnisse unserer Arbeit wie folgt darstellen: Viele Kernreferenzauflösungsmodelle scheinen ohne taskspezifisches Training nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu raisonieren. Mit taskspezifischem Training integrieren jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen. Selbst die besten Modelle haben jedoch Schwierigkeiten, Hintergrundwissen, das nur bei der Inferenz präsentiert wird, zuverlässig zu integrieren. Wenn Sie mehr Details erfahren möchten, sehen Sie bitte unsere Veröffentlichung ein und erkunden Sie den Datensatz im Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sarah Pappy von der University of Toronto und der Fondazione Bruno Kessler, und ich werde kurz das Papier „Attention als Leitfaden für die simultane Sprachübersetzung“ vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turki ist. Was ist simultane Sprachübersetzung? Simulierte oder simultane Sprachübersetzung ist der Prozess der Übersetzung gesprochener Sprache in einen Text in einer anderen Sprache in Echtzeit, wodurch eine Kommunikation über Sprachbarrieren hinweg ermöglicht wird. Und welche Probleme haben die aktuellen simulierten Modelle? Spezifische Architekturen werden normalerweise trainiert, indem zusätzliche zu optimierende Module eingeführt werden, lange und komplizierte Trainingsverfahren, beispielsweise Training mit unterschiedlichen Optimierungsziele, und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen, beispielsweise das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden und so weiter. Was ist also unsere Lösung? Erstens die Verwendung bereits vorhandener Offline-ST-Modelle ohne erneutes Training oder die Annahme einer spezifischen Architektur für CMLSD. Verwenden Sie nur ein Modell für jedes Latenzregime und steuern Sie die Latenz über spezifische Parameter und nutzen Sie das bereits durch den Aufmerksamkeitsmechanismus zwischen Audioeingabe und Textausgabe erworbene Wissen, also den Kreuzaufmerksamkeitsmechanismus, wie Sie im Beispiel auf der rechten Seite sehen können. Unsere Lösung besteht darin, einen Punkt- oder Encoder-Decoder-Aufmerksamkeitsmechanismus vorzuschlagen, der eine Strategie ist, bei der entschieden wird, ob eine partielle Übersetzung ausgegeben wird oder nicht, basierend darauf, wo die Aufmerksamkeit hinweist. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d. h. diese Summe unter einem bestimmten Schwellenwert Alpha liegt, in Richtung der letzten Lambda-Sprachrahmen, was bedeutet, dass die empfangenen Informationen stabil genug sind. Wenn wir beispielsweise eine Sprachsequenz mit „Ich werde über“ erhalten und unser Modell die Übersetzung auf Deutsch vorhersagt und wir uns die Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen verweisen, während das letzte Wort auf die letzten empfangenen Sprachrahmen verweist. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, während wir das letzte Wort nicht ausgeben, da die Summe der Kreuzaufmerksamkeit über einem bestimmten Schwellenwert Alpha liegt, und auf eine weitere Sprachsequenz warten. Wenn wir fortfahren und eine weitere Sprachsequenz erhalten und unser Modell drei weitere Wörter vorhersagt, werden wir bei der Betrachtung der Kreuzaufmerksamkeitsgewichte sehen, dass kein Wort auf die letzten Lambda-Sprachrahmen verweist. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptresultate ansehen, werden wir die Ergebnisse der simultanen Sprachübersetzung in Graphen darstellen, in denen auf der einen Seite Blau die Übersetzungsqualität und den durchschnittlichen Verzug, also die Latenz, misst. Wir berücksichtigen auch den berechneten durchschnittlichen Verzug, der die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten also, dass unsere Kurven in diesem Diagramm möglichst hoch sind, aber auch nach links verschoben, und wir vergleichen sie mit vorbereitenden Strategien, die auch auf Offline-Modellen angewendet werden, wie der Weighted-Key-Strategie und der lokalen Übereinstimmung. Wir vergleichen auch mit der aktuellen Architektur, die speziell für die simultane Sprachübersetzung entwickelt wurde. Dies sind alle Ergebnisse der simultanen Sprachübersetzungsstrategie auf Deutsch, und wir sehen, dass der Punkt alle auf Offline-Modelle angewendeten Strategien übertrifft, da ihre Kurven nach links verschoben sind. Wir sehen auch, dass es sich bei Berücksichtigung der tatsächlichen verstrichenen Zeit oder der Rechenzeit um die schnellste Strategie handelt. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unsere Arbeit, und wir haben auch den Code, die Modelle und die simultanen Ausgaben open source veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Xuheng. Heute werde ich unseren Artikel „Funktionieren CoNLL-2003-Named-Entity-Tagger auch 2023 noch gut?“ vorstellen. Los geht's. In unserem Artikel untersuchten wir das Problem der Verallgemeinerung unter Verwendung der Named-Entity-Recognition-Aufgabe (NER). Wir stellten fest, dass seit fast 20 Jahren Modelle auf der Grundlage von CoNLL-2003 für die Entwicklung von NER verwendet werden. Und dies wirft natürlich mehrere Probleme auf. Erstens, können diese Modelle auf moderne Daten verallgemeinern, und wenn wir neue Tagger entwickeln, was ist für eine gute Verallgemeinerung erforderlich? Gleichzeitig, wenn wir eine schlechte Verallgemeinerung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, entwickelten wir den CoNLL-Plus-Plus-Datensatz. Dies ist ein Datensatz, den wir aus Reuters-Nachrichten von 2020 gesammelt und dann mit den gleichen CoNLL-2003-Anmerkungsrichtlinien annotiert haben. Anschließend feinabstimmten wir über 20 Modelle auf CoNLL-2003. Wir bewerteten sie sowohl auf dem CoNLL-2003-Testset als auch auf dem CoNLL-Plus-Plus-Testset. Und zuletzt, aber nicht weniger wichtig, berechneten wir die prozentuale Änderung der F1-Score, um die Verallgemeinerung jedes Modells zu bewerten.\n\nWas ist also für eine gute Verallgemeinerung erforderlich? Durch unsere Experimente fanden wir heraus, dass drei Hauptkomponenten erforderlich sind. Die erste ist die Modellarchitektur. In unseren Experimenten stellten wir fest, dass Transformer-Modelle in der Regel besser auf neue Daten verallgemeinern. Die zweite Komponente ist die Modellgröße. Wir fanden heraus, dass in der Regel größere Modelle zu einer besseren Verallgemeinerung führen. Und zuletzt, aber nicht weniger wichtig, wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele die Leistung einer nachgelagerten Aufgabe direkt beeinflusst. Hier stellten wir auch fest, dass mehr Feinabstimmungsbeispiele tatsächlich ebenfalls zu einer besseren Verallgemeinerung führen.\n\nZu unserer nächsten Frage, was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptives Überanpassen, also Überanpassung, die durch wiederholtes Wiederverwenden desselben Testsets verursacht wird, und dies zeigt sich in der Regel als abnehmende Rendite bei einem neuen Testset. Die zweite Hypothese ist zeitliche Drift, also Leistungsabfall, der durch die zunehmende zeitliche Lücke zwischen Trainings- und Testdaten verursacht wird.\n\nFür das Überanpassungsproblem sahen wir, dass aus dem Diagramm auf der rechten Seite die rote Regressionsgerade einen Gradienten hat, der größer als eins ist. Dies bedeutet, dass jede Verbesserungseinheit, die wir bei CoNLL-2003 erzielten, zu mehr als einer Verbesserungseinheit bei CoNLL-Plus-Plus führt, was bedeutet, dass es keine abnehmende Rendite gibt und dies zeigt, dass in diesem Fall keine adaptive Überanpassung beobachtet wird.\n\nWas also die zeitliche Drift betrifft, führten wir ein Experiment durch, um einige Modelle mit aktuelleren Daten erneut zu trainieren oder weiter vorzutrainingen, und stellten fest, dass die Leistung mit größerer zeitlicher Lücke abnimmt. Dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall die zeitliche Drift ist.\n\nUnser Fazit ist, dass für eine gute Verallgemeinerung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele erforderlich sind, und diese gehen Hand in Hand. Wir können nicht nur eine Komponente haben und die anderen wegwerfen. Gleichzeitig stellten wir auch fest, dass der Leistungsabfall hier durch zeitliche Drift verursacht wird und überraschenderweise nicht durch adaptives Überanpassen, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird.\n\nZurück zur Frage, die wir in der Überschrift unseres Artikels aufgeworfen haben: Funktionieren CoNLL-2003-Tagger auch 2023 noch? Und wir fanden heraus, dass die Antwort ein klares Ja ist. Wir hoffen, dass unser Artikel zu mehr Forschung darüber aufruft, wie die Verallgemeinerung der Modelle verbessert werden kann. Und zuletzt, bitte werfen Sie einen Blick auf unseren Artikel, unseren Datensatz und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, herzlich willkommen zu unserer Präsentation von DeepLean, einem neuen Korpus für die Vereinfachung deutscher Texte auf Dokument- und Satzebene. Mein Name ist Regina Stodden und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Textvereinfachung definieren. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um das Textverständnis für eine bestimmte Zielgruppe zu verbessern, wie beispielsweise Menschen mit Leseproblemen oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, zum Beispiel Dokumente oder Sätze. Im hier gezeigten Beispiel sehen Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache. Zur Vereinfachung des Satzes sind verschiedene Techniken möglich, wie im Beispiel zu sehen ist, wie beispielsweise lexikalische Substitution, Klauselentfernung, Klauselreordnung oder Einfügung von Wörtern.\n\nWir schlagen nun unseren neuen Korpus DPlane vor, da es in den letzten Jahren einige Probleme mit bestehenden Korpora gab. Einige sind zu klein, um ein Klassifikationsmodell zu trainieren. Die anderen drei in den letzten Jahren vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass ihre Ausrichtungen fehleranfällig sein können. Daher schlagen wir unseren neuen Korpus DPlane vor, der in zwei Subkorpora unterteilt ist: DPlane APA und DPlane Web. DPlane APA basiert auf verwendeten Texten. In DPlane APA haben wir 483 Dokumente alle manuell ausgerichtet. Dies ergibt etwa 30.000, 13.000 parallele Satzpaare. Für DPlane Web umfasst dieser Korpus verschiedene Domänen, und wir haben auch alle 750 Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergaben sich 30.450 Satzpaare.\n\nWir haben unsere Satzpaare etwas genauer analysiert, beispielsweise hinsichtlich der Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als beispielsweise die Nachrichtentexte oder die Sprachlernertexte auf allen Ebenen, was beispielsweise lexikalische Vereinfachung, strukturelle Vereinfachung oder den allgemeinen Vereinfachungsgrad betrifft. Außerdem lässt sich erkennen, dass unser D-Plane-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So haben wir beispielsweise im D-Plane-API-Korpus viel mehr Reorderings und Wortänderungen als im D-Plane-Web-Korpus. Im Web-Korpus hingegen finden sich deutlich mehr Umschreibungen.\n\nLassen Sie uns nun sehen, was wir mit diesem Korpus erreichen können. Hallo, ich bin Omar und werde nun über die Anwendungsfälle für unseren Datensatz DPlane sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es zahlreiche Ausrichtungsmethoden, jedoch im Kontext der maschinellen Übersetzung, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in beiden Dokumenten extrahieren möchten. In unserem Anwendungsfall versuchen wir hingegen, Ausrichtungen zwischen Sätzen in zwei parallelen Dokumenten zu extrahieren, die dieselbe Sprache und denselben Inhalt haben, aber unterschiedliche Komplexitätsstufen aufweisen. Da wir nun unseren Datensatz D-Plane mit manuell ausgerichteten Sätzen haben, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen sowie die Codes zum Durchführen unserer Experimente in der Arbeit veröffentlicht. Letztendlich kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für die Vereinfachung deutscher Texte die Methode „Mass Align“ ist, und Sie können den Code zur Ausführung dieser Methode auf Ihren eigenen Dokumenten ebenfalls in der Arbeit finden.\n\nDer zweite Anwendungsfall, den wir in unserer Arbeit vorgestellt haben, ist die automatische Textvereinfachung durch Feinabstimmung von Sprachmodellen, um aus einem komplexen Eingabetext vereinfachte Texte zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben das Modell „Long Import“ feinabgestimmt, um Dokumentenebenen-Vereinfachungen zu erzeugen, und wir haben auch das normale Basis-Import-Modell feinabgestimmt, um Satzebenen-Vereinfachungen zu erzeugen. Auch alle Kontrollpunkte und detaillierte Informationen zu den Bewertungsmetriken unserer Experimente finden Sie in der Arbeit. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Ergebnisse als die Basisscores erzielen kann, und wir schlagen diese Ergebnisse als Benchmark, als Basis-Benchmark, für das Problem der automatischen Textvereinfachung in der Zukunft vor.\n\nVielen Dank für Ihre Aufmerksamkeit, und wir hoffen, Sie alle während der Konferenz zu treffen. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xi Yuan von der Fen-Universität. Ich bin hier, um unsere Arbeit zur Extraktion von Skriptwissen aus großen Sprachmodellen für eingeschränktes Sprachplanen vorzustellen. Im Alltag planen Menschen ihre Handlungen oft anhand schrittweiser Anweisungen in Form von garantierten Skripten. Vorherige Arbeiten haben Sprachmodelle zur Planung abstrakter Ziele stereotypischer Aktivitäten wie das Backen eines Kuchens untersucht und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Bisher konzentrierten sich diese Arbeiten jedoch hauptsächlich auf die Planung abstrakter Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen, eingeschränkten Zielen, wie z. B. das Backen eines Schokoladenkuchens, ist noch wenig erforscht.\n\nIn dieser Arbeit definieren wir das Problem des eingeschränkten Sprachplanens, das verschiedene Einschränkungen für die Planungsziele vorsieht. Ein abstraktes Ziel kann von verschiedenen spezifischen Zielen im realen Leben geerbt werden, die komplexere, vielschichtige Einschränkungen aufweisen. Ein guter Planer sollte Skripte erstellen, die vernünftig sind und den Einschränkungen entsprechen.\n\nZunächst bewerten und verbessern wir die Fähigkeit großer Sprachmodelle zum eingeschränkten Sprachplanen. Da kein Datensatz spezifischer Ziele existiert, um unsere Studie zu unterstützen, müssen wir diese Ziele zunächst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele um vielschichtige Einschränkungen für die Datenerhebung im Human-Loop-Verfahren unter Verwendung von instruiertem TPT. Wir generieren 100 spezifische Ziele und bewerten die Skripte, die von Light-Logic-Modellen erstellt wurden. Diese Tabelle zeigt die allgemeine Genauigkeit der Ergebnisse. Wir stellen fest, dass alle Light-Logic-Modelle bei der Planung spezifischer Ziele unbefriedigende Ergebnisse erzielen.\n\nAnschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, wofür Light-Logic-Modelle geeignet sind. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit der generierten Skripte akzeptabel ist. Die Treue zu den Einschränkungen kann jedoch nicht garantiert werden. Wir untersuchen detaillierter die Kategorien von Einschränkungen, die von spezifischen Zielen abhängen. Die Kopfkarte in der Abbildung zeigt, dass die Planungsleistung von instruierten GPTs für Ziele verschiedener Kategorien stark variiert. Vorherige Studien haben gezeigt, dass die Ausgabegüte von Sprachmodellen eine hohe Varianz aufweist, was zu schlechten Leistungen führt. Daher übernehmen wir die Idee eines übergenerierten Zen-Filters, um die Generierungsqualität zu verbessern.\n\nZunächst zeigen wir die Typen von Einschränkungen mit Beispielen für instruierte GPTs und generieren spezifische Ziele basierend auf den festgelegten abstrakten Zielen. Dann generiert instruierter GPT übermäßig Schlüsselskripte für spezifische Ziele. Als Nächstes entwickeln wir ein Filtermodell, um passende Skripte auszuwählen. Wir konvertieren Skripte und Ziele in instruierte GPT-Eingaben und berechnen die kosinussimile Ähnlichkeit, um die semantische Ähnlichkeit zu messen. Darüber hinaus schreiben wir Skripte, die Schlüsselwörter der Zielbeschränkung enthalten. Wir behalten das Skript nur, wenn das Zielskript die höchste Ähnlichkeit aufweist.\n\nMit unserer Methode kann Instanität Skripte höherer Qualität generieren. Unsere Methode verbessert die Planbarkeit sowohl in Bezug auf die semantische Vollständigkeit als auch die Treue zu den Einschränkungen erheblich. Da große Sprachmodelle kostspielig im Einsatz sind, ist es wichtig, die Sprachplanungsfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung von Datensätzen ist ein wesentlicher Schritt dazu. Bisherige Studien ermöglichen jedoch keine Planung spezifischer Ziele, und die manuelle Datensatzanmerkung ist teuer. Daher verfolgen wir die Idee der symbolischen Wissensdestillation, um Modelle für das eingeschränkte Sprachplanen zu destillieren.\n\nWir wenden unsere Methode an, um einen Datensatz für das eingeschränkte Sprachplanen zu erstellen, den wir CodeScript nennen. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten, um die Qualität der Validierungs- und Testdatensätze zu gewährleisten. Wir bitten Cloud-Arbeiter, falsche Proben zu finden und zu korrigieren. Diese Abbildung zeigt die eingeschränkte Verteilung von CodeScript. Wir stellen fest, dass CodeScript die Hypothese in den generierten spezifischen Zielen bestätigt. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für das eingeschränkte Sprachplanen trainieren. Wir finden heraus, dass TFIL-Funktionen auf Code-Rate Skripte höherer Qualität generieren als die meisten großen Sprachmodelle, was darauf hinweist, dass kleinere Modelle, wenn sie angemessen trainiert werden, größere Modelle unterstützen können.\n\nZusammenfassend haben wir das Problem des eingeschränkten Sprachplanens etabliert. Wir haben die Fähigkeit großer Sprachmodelle zum eingeschränkten Sprachplanen bewertet und eine Methode des übergenerierten Filterns für große Sprachmodelle entwickelt. Wir verwenden große Sprachmodelle, um einen hochwertigen Skriptdatensatz, CodeScript, für das konstruktive Sprachplanen zu generieren. Wir hoffen, dass der CodeScript-Datensatz eine wertvolle Ressource für die Förderung der Forschung im Bereich Sprachplanung sein wird. Vielen Dank für Ihre Aufmerksamkeit. Weitere Details zu CodeScript finden Sie in unserer Arbeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Yanis Lavrack und ich werde Ihnen unsere Arbeiten zu Dr. Berth vorstellen, einem robusten Vortrainingsmodell in französischer Sprache für den biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir den Hauptbeitrag unseres Artikels vor. Wir präsentieren das erste biomedizinische Modell in französischer Sprache, Dr. Berth, das auf Roberta basiert und mit Natchios trainiert wurde, einem Datensatz medizinischer Web-Crawldaten. Außerdem führen wir einen Vergleich von Modellen mit verschiedenen Vortrainings-Einstellungen und Datenquellen durch. Dann stellen wir unsere Ergebnisse auf elf biomedizinischen und klinischen Downstream-Aufgaben in französischer Sprache vor. Abschließend fassen wir die Experimente zusammen und geben Ihnen weitere Details dazu, wie Sie auf die Modelle zugreifen können.\n\nSeit seiner Veröffentlichung im Jahr 2018 hat BERT sich als eine der effektivsten Methoden zur Lösung von Aufgaben der natürlichen Sprachverarbeitung erwiesen und bietet im Vergleich zu historischen statischen und kontextuellen Methoden wie Word2Vec, Fastex oder NWO erhebliche Leistungssteigerungen. Seitdem wurde dieses Modell an viele andere Sprachen angepasst, wie zum Beispiel in französischer Sprache mit Camembert, und an andere Bereiche wie den biomedizinischen mit PermetteBERT und BioBERT sowie den klinischen mit ClinicalBERT, hauptsächlich jedoch in englischer Sprache.\n\nSpezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vor-Training aufgrund des Mangels an domänenspezifischen Daten. Bisher gab es jedoch kein Open-Source-Modell für den biomedizinischen Bereich in französischer Sprache. Daher stellten wir uns die Frage, welche Datenquellen für eine breite Palette von Anwendungen am geeignetsten sind. Die aktuellen Daten sind eine gute Alternative zu klinischen Daten.\n\nUm diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die wir von einem nicht-universitären Krankenhaus erhalten haben. Anschließend fragten wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell mit französischen Daten zu trainieren? Sind es 4 GB, 8 GB oder mehr?\n\nUm diese Frage zu beantworten, trainierten und verglichen wir zunächst vier Modelle von Grund auf neu: eine erste Version von Dr. Bert mit 7 GB Natchios, eine zweite Version mit 4 GB Natchios, eine erste Version von Schubert, einem klinischen Modell, mit 4 GB Sätzen aus klinischen Knoten, und eine endgültige Version von Schubert mit einer Mischung aus 4 GB Natchios und 4 GB klinischen Knoten.\n\nZusätzlich zu diesem Vergleich führten wir drei Modelle ein, die mit kontinuierlichem Vor-Training trainiert wurden, um den Einfluss der Vor-Trainingsstrategie zu analysieren. Eines basiert auf den Gewichten von Camembert und wurde mit 4 GB Natchios trainiert. Ein weiteres basiert ebenfalls auf Camembert, wurde aber diesmal mit 4 GB klinischen Knoten trainiert. Und schließlich eines, das auf einem englischen biomedizinischen Modell, Bermuda Bert, basiert und mit 4 GB Natchios trainiert wurde. Insgesamt haben wir sieben Modelle.\n\nUm unsere sieben Modelle zu bewerten, sammelten wir passende öffentliche und private Aufgaben wie Namenserkennung, Klassifizierung, Musterwechsel-Tagging und Fragebeantwortung. Diese Modelle wurden mit sechs Baseline-Modellen verglichen: Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBERT, BioBERT und ClinicalBERT.\n\nDie Bewertung zeigt, dass das Modell am besten auf Aufgaben mit Daten derselben Art abschneidet, auf denen das Modell trainiert wurde. Allerdings können wir beobachten, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen. Wir stellen auch fest, dass die Verwendung größerer Datenmengen zu besseren Leistungen führt. Insgesamt scheint das Training von Grund auf (from scratch) in den meisten Aufgaben höhere Leistungen zu erzielen. Unsere Experimente zum kontinuierlichen Training mit den Gewichten und dem Tokenizer von PumedBeard, trainiert auf einem 4-GB-Subset von Natchios, zeigten jedoch vergleichbare Ergebnisse zu denen von Dr. Beard 4 GB von Grund auf, was nicht der Fall ist für das auf Camembert-Gewichten und -Tokenizer basierende Modell, das unter Stabilitätsproblemen leidet.\n\nAbschließend bietet unser vorgeschlagenes System auf neun der elf DOTSTRIMS-Aufgaben bessere Leistungen und übertrifft global das Ergebnis des generischen Modells Camembert. Wir stellen auch fest, dass spezialisierte Daten besser sind, aber dies nicht gut skaliert. Alle vorab trainierten Modelle, die aus Natchios stammen, sind frei auf HuggingFace verfügbar und alle Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation, und wir freuen uns auf den Austausch in der Post-Session in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xiang Bin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit, die von der Vorabschulungsdaten bis zu Sprachmodellen und weiter zu nachgelagerten Aufgaben reicht, wobei wir die Spuren politischer Voreingenommenheit verfolgen, die zu ungerechten NLP-Modellen führen. Sprachmodelle werden also auf groß angelegten, im Web gesammelten Daten trainiert. Politische Nachrichtenmedien sind in ihren Vorabschulungsdaten gut abgedeckt. Laut einer Umfrage im C4-Corpus können wir sehen, dass die New York Times, die Los Angeles Times, The Guardian, Huffington Post und andere gut in den Trainingsdaten von Sprachmodellen vertreten sind. Dies hat für Anwendungen von Sprachmodellen sowohl Vor- als auch Nachteile. Einerseits konnten sie aus unterschiedlichen Perspektiven lernen, was die Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen von Natur aus sozial voreingenommen und können zu potenziellen Fairness-Problemen in nachgelagerten Aufgaben führen.\n\nUm dies zu beheben, schlagen wir vor, die Pipeline der politischen Voreingenommenheit zu untersuchen, die von den Vorabschulungsdaten über Sprachmodelle bis zu nachgelagerten Aufgaben reicht, indem wir die folgenden Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielen die Vorabschulungsdaten bei diesen politischen Voreingenommenheiten? Zweitens, wie sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen in nachgelagerten Aufgaben schlagen und ob dies zu Fairness-Problemen in NLP-Anwendungen führen könnte?\n\nKonkret schlagen wir zunächst vor, Sprachmodelle mit verschiedenen Prompt-Formaten zu befragen, indem wir politische Fragebögen wie den politischen Kompass-Test verwenden. Dies ermöglicht uns eine automatische Bewertung, die gut in der politischen Wissenschafts-Literatur verankert ist. Vorläufige Ergebnisse zeigen zunächst, dass Sprachmodelle unterschiedliche politische Bedeutungen haben. Sie belegen alle vier Quadranten des politischen Kompasses. Wir können auch sehen, dass GPT 4 das liberalste Sprachmodell aller ist und GPT-Theorien im Allgemeinen sozial liberaler sind als BERT-Theorien und ihre Varianten.\n\nZweitens wollen wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints weiter auf sechs verschiedenen, nach Parteizugehörigkeit getrennten Korpora vortrainieren, die in Nachrichten und soziale Medien unterteilt sind und jeweils eine politische Ausrichtung haben. Durch das weitere Vortrainieren von Sprachmodellen auf solchen parteiischen Korpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschoben werden. Zum Beispiel verschiebt sich bei Roberta, weiter feinabgestimmt auf den linksgerichteten Reddit-Corpus, die politische Ausrichtung deutlich in Richtung Liberalismus.\n\nUm zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht, teilen wir die Vortrainings-Korpora in einen Teil vor und nach dem 45. Präsidenten der Vereinigten Staaten auf. Wir trainieren Sprachmodelle separat auf diesen beiden zeitlich unterschiedlichen Korpora. Wir stellen fest, dass Sprachmodelle nach 2017 im Allgemeinen eine politische Ausrichtung haben, die weiter vom Zentrum entfernt ist. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können.\n\nAbschließend bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen in den NLP-Anwendungen Hassrede-Erkennung und Falschinformationen-Erkennung, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben können. Wir stellen fest, dass bei einer Untersuchung der pro Kategorien Leistung, also wenn wir die Leistung nach verschiedenen Demografien oder politischen Bedeutungen von Nachrichtenmedien aufschlüsseln, ein Muster erkennbar wird: Zum Beispiel sind bei der Hassrede-Erkennung linksgerichtete Sprachmodelle besser darin, Hassreden gegen sozial benachteiligte Gruppen zu erkennen, aber schlechter darin, Hassreden gegen mächtigere Gruppen in unserer Gesellschaft zu erkennen. Umgekehrt sind rechtsgerichtete Sprachmodelle besser darin, Hassreden gegen Weiße und Männer zu erkennen, aber schlechter darin, Hassreden gegen Schwarze, LGBTQ+ und andere Minderheitengemeinschaften zu erkennen. Ähnliche Trends zeigen sich auch bei der Falschinformationen-Erkennung, wo wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Falschinformationen von gegensätzlicher politischer Bedeutung zu erkennen und umgekehrt.\n\nWir werden viele qualitative Beispiele zeigen, um zu veranschaulichen, dass Sprachmodelle mit unterschiedlichen politischen Bedeutungen unterschiedliche Vorhersagen für Hassreden und Falschinformationen treffen, basierend auf ihren sozialen Kategorien. Im Anhang finden sich weitere Beispiele, die dies unterstreichen. Dies weist auf ein sehr dringendes Fairness-Problem im Zusammenhang mit den politischen Voreingenommenheiten von Sprachmodellen hin. Zum Beispiel, wenn ein rechtsgerichtetes Sprachmodell für Hassrede- oder Falschinformationen-Erkennung feinabgestimmt und auf einer beliebten Social-Media-Plattform eingesetzt würde, könnten Menschen mit gegensätzlichen politischen Ansichten marginalisiert werden und Hassreden gegen Minderheitengruppen könnten unkontrolliert verbreitet werden.\n\nDies schlägt Alarm, damit wir die Fairness-Probleme anerkennen und angehen, die durch politische Voreingenommenheiten von Sprachmodellen entstehen. Um die Diskussion abzuschließen, möchten wir auch das einzigartige Dilemma hervorheben, das mit den politischen Voreingenommenheiten von Sprachmodellen verbunden ist. Es ist, als stünden wir zwischen Scylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten von Sprachmodellen nicht bereinigen, wird die Voreingenommenheit von den Vorabschulungsdaten über die Sprachmodelle bis zu den nachgelagerten Aufgaben weitergegeben und schafft letztendlich Fairness-Probleme. Wenn wir versuchen, auf irgendeine Weise zu bereinigen, riskieren wir auch Zensur oder Ausschluss, und es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten von Sprachmodellen beibehalten werden sollte. Es ist eine Art elektrisches Charlie-Problem.\n\nDas war es für heute, vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Coast of Sina und ich freue mich, Sie zu unserem Vortrag über unseren ACL-Aufsatz 2023 willkommen zu heißen: „Language Model Acceptability Judgments sind nicht immer robust gegenüber Kontext“. Dies ist eine gemeinsame Arbeit mit John Bakhier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy und Adina Williams. In dieser Arbeit überprüfen wir das minimale Paar-Paradigma. Das minimale Paar-Paradigma bewertet im Wesentlichen Sprachmodelle anhand von Akzeptanzurteilen, die auch Grammatikalität umfassen können, wie z. B. BLIMP-Syntax-Gem oder Akzeptanz im Hinblick auf Stereotype wie Crowd-Sourcing-Daten. In diesem minimalen Paar-Paradigma besteht die übliche Methode zur Bewertung von Sprachmodellen darin, dass man entweder einen akzeptablen oder einen grammatikalischen Satz und dann einen inakzeptablen oder ungrammatikalischen Satz präsentiert, wobei die Hoffnung besteht, dass das Modell eine höhere Wahrscheinlichkeit für den akzeptablen Satz zuweist. Die aktuelle MPP-Pipeline ermöglicht es uns jedoch nicht, die Akzeptanz des Modells für längere Sätze zu bewerten. Heutzutage verfügen große Sprachmodelle über immer längere Kontextfenster. Es ist daher von entscheidender Bedeutung, die Akzeptanz des Modells im gesamten Kontextfenster zu bewerten. Genau das versuchen wir hier zu erreichen. Wir versuchen, die NPV-Pipeline neu zu gestalten, indem wir das Modell auffordern, die Akzeptanz für immer längere Sequenzen zu bewerten. Das ist der Ansatz.\n\nUm diese längeren Sequenzen zu simulieren, überprüfen wir die Datensätze selbst und erstellen Sätze, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. Beispielsweise haben wir hier ein typisches Paar von Grammatikalitätsurteilen aus dem BLIMP-Datensatz für den Adjunkt-Island-Fall ausgewählt. Um längere Sequenzen zu erstellen, die akzeptabel sind und die gleiche grammatikalische Struktur aufweisen, extrahieren wir grammatikalische Sätze aus dem Adjunkt-Island-Fall und fügen sie als Präfix sowohl für die akzeptable als auch für die inakzeptable Abfrage hinzu. Dasselbe können wir tun, indem wir inakzeptable Sätze aus demselben Datensatz auswählen, um die Akzeptanz des Modells zu testen. Wir können auch Sätze aus einem anderen Unterbereich oder einem anderen Datensatz auswählen. Das nennen wir das „Mismatch-Szenario“. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem wir bewerten. Dasselbe können wir für den inakzeptablen Fall tun. Schließlich können wir Sätze aus einem völlig unzusammenhängenden Bereich wie Wikipedia auswählen. Dies wird uns zeigen, ob die Akzeptanzurteile des Modells tatsächlich von einem Kontext beeinflusst werden, ob dieser Kontext aus einem anderen Unterbereich des Datensatzes stammt oder ob er für den aktuellen Satz, den wir betrachten, völlig irrelevant ist.\n\nWie schlägt sich das Modell also? Zunächst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Abfragepaar sind, und stellen fest, dass die MPP-Urteile für eine beliebige Kontextlänge weitgehend robust sind. Wir erhöhen die Kontextlänge bis zu 1024, um die OPT- und GPT2-Modelle auszureizen, und wie Sie an der orangefarbenen gestrichelten Linie sehen können, sind die MPP-Urteile relativ stabil. Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier erstellen oder wählen wir Sätze aus akzeptablen und inakzeptablen Domänen aus demselben BLIMP- oder Syntax-Gem-Datensatz aus und stellen fest, dass die MPP-Urteile entweder signifikant ansteigen oder abnehmen, wenn wir entweder akzeptable oder inakzeptable Präfixe hinzufügen. Wenn wir jedoch die Struktur abgleichen, d. h. Sätze aus demselben Phänomen in BLAME-basierten Syntax-Gym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang der MPP-Urteile des Modells, je nachdem, ob das gewählte Präfix akzeptabel oder inakzeptabel ist. Dieser Effekt nimmt mit der Kontextlänge zu und könnte wahrscheinlich neuere Sprachmodelle mit großen Kontextfenstern beeinflussen.\n\nWarum beeinflusst das übereinstimmende Präfix das Urteil des Sprachmodells so stark? Wir führten eine Reihe von Analysen durch, bei denen wir versuchten, den Eingabesatz zu stören, indem wir die relevante Struktur beibehielten, aber Rauschen hinzufügten. Nach mehreren dieser Störungen stellten wir fest, dass keines dieser Rauschen das Modell dazu brachte, seinen Kurs in Bezug auf die MPP-Urteilstrend zu ändern. Im Grunde stellen wir fest, dass die Modelle auf gestörte Sätze in ähnlicher Weise sensibel reagieren: Wenn wir Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen, und wenn wir Sätze im inakzeptablen Bereich stören, sehen wir einen ähnlichen Rückgang der MPP-Urteile.\n\nDie wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die in den Sätzen gemeinsam sind. Die MPP-Bewertung, wie wir sie derzeit mit kurzen und einzelnen Satz-Eingaben durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster. Bitte lesen Sie unseren Aufsatz für weitere Details zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawe, ein Promotionsstudent an der Stalant-Universität in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit vorstellen, „Schwächer als du denkst“, eine kritische Betrachtung des wöchentlich überwachten Lernens. Dies ist eine gemeinsame Arbeit mit Xiao Yushchen, Maios Musbach, Giaz Steffen und Dietrich Clarkov. Ich möchte mit einer kurzen Einführung in die wöchentliche Überwachung und das wöchentlich überwachte Lernen beginnen. Bei der wöchentlichen Überwachung beschriften wir die Daten nicht manuell. Stattdessen verwenden wir wöchentliche Beschriftungsquellen wie einfache heuristische Regeln, Wissensdatenbanken oder lokale Cloud-Beschriftung, wie im rechten Abbild illustriert. Im Vergleich zu menschlichen Anmerkungen sind diese schwachen Anmerkungen viel kostengünstiger, aber auch rauschbehaftet, was bedeutet, dass eine bestimmte Anzahl der Anmerkungen falsch ist. Wenn wir neuronale Netze direkt auf wöchentlichen Beschriftungsdaten trainieren, neigen die neuronalen Netze dazu, das Beschriftungsrauschen zu memorieren und nicht zu verallgemeinern. Beim wöchentlich überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust auf solche Beschriftungsfehler zu trainieren, sodass die trainierten Modelle immer noch gut verallgemeinern können. In jüngsten Arbeiten zum WSL (wöchentlich überwachtes Lernen) wird häufig behauptet, dass Modelle nur auf wöchentlichen Beschriftungsdaten trainiert werden und dennoch hohe Leistungen bei sauberen Testdaten erzielen. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken: Es wird angenommen, dass zusätzlich ein sauberer Validierungsdatensatz für die Modellauswahl verfügbar ist. Wir stellen dieses Problemsetting in Frage, da dies zusätzliche manuelle Beschriftungen im wöchentlich überwachten Lernen impliziert. Dieser Bedarf wird jedoch oft übersehen, wie ein Elefant im Raum.\n\nUnsere oben genannten Zweifel führen uns zu drei Forschungsfragen: Erstens, ist sauberes Validierungsdaten für WSL notwendig? Oder können wir möglicherweise einen rauschbehafteten Validierungsdatensatz verwenden? Zweitens, wenn saubere Daten für das Funktionieren des WSL erforderlich sind, wie viele saubere Beispiele benötigen wir dann? Und schließlich, sollten wir nur die sauberen Beispiele für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen?\n\nWir haben diese Forschungsfragen in unserer Arbeit behandelt und folgende Erkenntnisse gewonnen: Erstens stellen wir fest, dass aktuelle WSL-Methoden tatsächlich saubere Validierungsbeispiele benötigen, um ordnungsgemäß zu funktionieren. Andernfalls kommt es zu einem starken Leistungsabfall. Wie in dieser Abbildung gezeigt, können die trainierten Modelle nicht über die ursprünglichen schwachen Beschriftungen hinaus verallgemeinern, wenn keine sauberen Validierungsbeispiele vorhanden sind, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber beschriftete Daten benötigen, um ordnungsgemäß zu funktionieren, und die Kosten für die Beschriftung sauberer Validierungsbeispiele sollten nicht übersehen werden.\n\nUnsere zweite Erkenntnis ist, dass die Erhöhung der Anzahl sauberer Validierungsbeispiele dazu beiträgt, dass WSL-Ansätze bessere Leistungen erbringen, wie in der linken Abbildung gezeigt. In der Regel benötigen wir nur zwanzig Beispiele pro Klasse, um hohe Leistungen zu erzielen. Aber damit ist die Geschichte noch nicht zu Ende, denn wenn wir uns entscheiden, auf saubere Beispiele zuzugreifen, dann führt das direkte Training auf diesen Beispielen sogar zu besseren Leistungen. Die rote Abbildung zeigt den Leistungsunterschied zwischen Feinabstimmungsansätzen, die direkt auf sauberen Daten angewendet werden, und WSL-Ansätzen, die saubere Daten nur für die Validierung verwenden. Wie wir sehen können, beginnt die direkte Feinabstimmung bei zehn Beispielen pro Klasse, WSL-Ansätze zu übertreffen.\n\nSchließlich kann die in früheren WSL-Ansätzen behauptete Leistungssteigerung leicht erreicht werden, indem die Feinabstimmung auf den sauberen Validierungsbeispielen fortgesetzt wird. Wie wir in den Abbildungen sehen können, unterperformt das sogenannte FTW-Modell anfänglich komplexere WSL-Methoden wie Cosine. Wenn wir jedoch die Feinabstimmung auf den sauberen Beispielen fortsetzen, erreicht FTW eine Leistung, die anderen Methoden ebenbürtig ist. In der Praxis besteht also kein Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Festplattenspeicher erfordern.\n\nZusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze sauber manuell beschriftete Beispiele benötigen, um ordnungsgemäß zu funktionieren. Ihre Leistungssteigerung und Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten lauten wie folgt: Erstens sollte berichtet werden, dass die Modellauswahl mit sauberen Validierungsbeispielen durchgeführt wird. Zweitens sollten WSL-Ansätze mit zukunftsorientierten Lern-Baselines verglichen werden, da beide mit sauberen Daten arbeiten. Drittens ist die kontinuierliche Feinabstimmung eine einfache und dennoch starke Baseline, die in zukünftigen WSL-Arbeiten berücksichtigt werden sollte. Und schließlich haben wir unseren Code offen gelegt. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte zögern Sie nicht, ihn zu überprüfen. Vielen Dank und viel Spaß bei der Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Ayud Villar und ich werde Ihnen einen kurzen Überblick über den Artikel „Prompting Palm: Translation Assessing Strategies and Performance“ geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. Palm ist ein Sprachmodell mit 540 Milliarden Parametern, das im letzten Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Textsammlung mit 780 Milliarden Token trainiert. Zum Zeitpunkt der Veröffentlichung erreichte es in Hunderten von NLP-Aufgaben den Stand der Technik. In dieser Arbeit präsentieren wir die erste systematische Untersuchung der Prompting-Technik für große Sprachmodelle im Bereich der maschinellen Übersetzung. Wir bewerten die Übersetzungsfähigkeit solcher Modelle unter Verwendung der Best Practices der AMT-Community. Dazu gehören die Verwendung der neuesten Testdaten, um eine Überlappung mit den Trainingsdaten des Sprachmodells zu vermeiden, und der Vergleich von zwei State-of-the-Art-Systemen, den besten Systemen der WMT-Bewertung. Wir verwenden State-of-the-Art-neuronale MT-Metriken und zeigen zusätzlich auch Ergebnisse der expertenbasierten menschlichen Bewertung. Abschließend geben wir Empfehlungen für Strategien zur Prompt-Auswahl.\n\nDas Prompting hat einen großen Einfluss auf die Leistung von LLMs für die Übersetzung, wie ein einfaches Experiment zeigt, bei dem wir eine kurze Prompting-Sequenz verwenden und zwei verschiedene Prompts für einen Satz bereitstellen. In der Mehrheit der Sätze (516 von 1000) beträgt der beobachtete Unterschied mehr als einen Blur-Punkt. In extremen Fällen kann er bis zu 40 Blur-Punkte betragen. Daher ist es wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten entschieden wir uns für eine Five-Shot-Prompting-Strategie, bei der wir jeden Satz, den wir dem System bereitstellen, mit der Sprache markieren, in der er vorliegt. In diesem Beispiel hier, wo wir die Übersetzung von Deutsch nach Englisch durchführten, sind die deutschen Sätze, die Quellsätze, mit einem deutschen Doppelpunkt markiert, und die englischen Übersetzungen mit einem englischen Doppelpunkt. Wir stellten fest, dass die tatsächliche Form des Promptings bei mehreren kurzen Promptings keinen großen Einfluss hat. Sie ist für Zero- und One-Shot-Prompting entscheidend, aber wenn wir, wie in unserem Fall, zu Five-Shot-Prompting übergehen, gibt es kaum einen Unterschied in der Form des Promptings. Die Beispiele tragen das meiste Gewicht.\n\nZusammenfassend lässt sich sagen, dass die Beispielqualität wichtiger ist als die Ähnlichkeit mit dem Quellsatz. Es ist daher wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten der WMT-Bewertungen oder den Dev-Daten. Die Dev-Daten sind viel besser kuratiert und von höherer Qualität als die Trainingsdaten, was sich in besseren Ergebnissen bei der Verwendung der Dev-Daten zeigt. Dennoch haben spezialisierte State-of-the-Art-Systeme einen erheblichen Vorteil gegenüber den Palm-Übersetzungen, aber Palm kommt einem kommerziellen System sehr nahe. In unserem Fall haben wir uns für die Bewertung mit Google Translate entschieden.\n\nDie Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, sind, dass die Fließfähigkeit von Palm mit den State-of-the-Art-Systemen vergleichbar ist, aber der Hauptunterschied in der Genauigkeit liegt. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, dass Palm manchmal Teile des Satzes weglässt, die in der Übersetzung weggelassen werden, um eine bessere fließende Übersetzung zu erzeugen. Allerdings ist die Stil-Außen-Kategorie für Palm niedriger als für die State-of-the-Art-Systeme, was ein zusätzliches Signal dafür ist, dass Palm wirklich fließende Ausgaben erzeugt, aber immer noch mit einigen Genauigkeitsproblemen.\n\nDas war es für diesen wirklich kurzen Überblick. Für weitere Details kommen Sie bitte zur vollständigen Präsentation des Artikels. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Jing Wei von der University of Science and Technology of China. Es ist mir eine Freude, ein kurzes Werbevideo für unseren Artikel „Sind Sie mein Modell zu kopieren? – Urheberrechtsschutz für Large Language Models bei Embedding und Diensten“ zu präsentieren. Lassen Sie uns zunächst den Hintergrund von Embedding und Diensten erläutern. Derzeit sind Large Language Models wie GPT, Lama und PELM außergewöhnlich in der natürlichen Sprachverstehens- und -generierung. Embedding und Dienste sind eine der Leistungen, die auf Large Language Models aufbauen, um verschiedene NLP-Aufgaben zu unterstützen. Beispielsweise bietet OpenAI eine GPT-basierte Embedding-API an. Jüngste Arbeiten haben jedoch gezeigt, dass ein Angreifer das Modell durch Lernen aus dem Embedding stehlen und ähnliche Dienste anbieten kann. Daher ist es notwendig, das Urheberrecht von Embedding als Dienst zu schützen.\n\nUm das Urheberrecht von Embedding als Dienst zu schützen, besteht eine der Lösungen darin, ein Wasserzeichen in den Anbieterdienst einzubetten und zu überprüfen, ob ein anderer Dienst das Wasserzeichen enthält. Die Methode muss die folgenden Eigenschaften erfüllen: Erstens sollte sie auf Embedding als Dienst anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Embeddings nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer heimlich genug sein oder der Angreifer sollte es nicht leicht entfernen können. Schließlich muss das Wasserzeichen während des Modellausziehprozesses auf die Dienste des Angreifers übertragbar sein.\n\nBestehende Arbeiten lassen sich grob in vier Kategorien einteilen. Diese Methoden sind jedoch entweder nicht auf Embedding als Dienst anwendbar oder fehlt es ihnen an Übertragbarkeit. Daher schlagen wir in diesem Artikel den Embedding-Marker vor, der eine auf Backdoor basierende Wasserzeichen-Methode ist, die auf Embedding als Dienst anwendbar ist.\n\nLassen Sie mich nun die Details unseres Embedding-Markers erläutern. Der Embedding-Marker besteht aus zwei Hauptstufen: Wasserzeicheneinbettung und Urheberrechtsüberprüfung. Vor diesen Hauptstufen wählen wir zunächst einen Auslösersatz aus. Der Auslösersatz ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Nehmen wir an, der Anbieter kann einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen. Bei der Wasserzeicheneinbettung definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Auslöser im Satz. Das bereitgestellte Embedding ist eine Gewichtssummation des Ziel-Embeddings und des Original-Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als M ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding.\n\nDie Urheberrechtsüberprüfung dient dazu, festzustellen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Wir konstruieren zunächst einen Backdoor- und einen harmlosen Datensatz. Der Backdoor-Datensatz enthält Sätze, bei denen alle Wörter zum Auslösersatz gehören, während alle Wörter in den Sätzen des harmlosen Datensatzes nicht zum Auslösersatz gehören. Dann fordert der Anbieter Embeddings vom Dieb-Dienst mit dem Datensatz an. Die Konsistenz und die L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen den Ähnlichkeitsunterschied zwischen dem Backdoor- und dem harmlosen Datensatz, der als Delta-Kosinus und Delta-L2 definiert ist. Gleichzeitig wenden wir den KS-Test an und verwenden seinen p-Wert als dritte Metrik.\n\nWir führen Experimente auf vier Datensätzen durch: AG News, Mind, SSD2 und Erospam. Wir gehen davon aus, dass der Anbieter den Wiki-Text-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding-Marker eine hervorragende Erkennungsleistung erbringen kann, während er die Nützlichkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Heimlichkeit des bereitgestellten Embeddings, indem wir die Embeddings von Sätzen, die sich in BOPCA entfalten, visualisieren. Die Legende der Figuren gibt die Anzahl der Auslöser in jedem Satz an. Wie in den Figuren gezeigt, ist es schwierig, zwischen Backdoor-Embeddings und normalen Embeddings zu unterscheiden.\n\nDas war’s, vielen Dank. Wir freuen uns auf die Diskussion mit Ihnen."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Ying und mein Kollege Jian und ich werden unsere Forschung zum Thema „Multi-Instruct: Verbesserung des multimodalen Zero-Shot-Lernens durch Instruction Tuning“ präsentieren. Mit den Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erkunden, um vortrainierte Sprachmodelle für verschiedene nachgelagerte Aufgaben auf parameter- und dateneffiziente Weise wiederzuverwenden. Kürzlich haben zahlreiche Studien gezeigt, dass Instruction Tuning große Sprachmodelle dazu befähigt, bisher unbekannte Aufgaben auf Zero-Shot-Basis durch Befolgen natürlicher Anweisungen auszuführen. Allerdings konzentrierten sich die meisten bisherigen Arbeiten zum Instruction Tuning auf die Verbesserung der Zero-Shot-Leistung bei sprachbasierten Aufgaben, während computer Vision- und multimodale Aufgaben vernachlässigt wurden. Daher möchten wir in dieser Arbeit untersuchen, ob Instruction Tuning bei multimodalen vortrainierten Modellen tatsächlich die Generalisierung auf unbekannte multimodale Aufgaben verbessern kann. Darüber hinaus stellten wir bei unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Instruktionsdatensätzen zwischen sprachbasierten und multimodalen Aufgaben fest. Es existieren mehr als sechshundert sprachbasierte Instruktionsaufgaben, aber es gibt keine groß angelegte, öffentlich zugängliche multimodale Instruktionsaufgabe. Dies hat uns dazu motiviert, einen multimodalen Instruction-Tuning-Datensatz zu erstellen. Hier stellen wir Multi-Instruct vor, den ersten multimodalen Instruction-Tuning-Benchmark-Datensatz, der aus 62 vielfältigen multimodalen Aufgaben besteht, die zehn fette Kategorien abdecken. Diese Aufgaben sind aus 21 bestehenden Open-Source-Datensätzen abgeleitet und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet. Zur Untersuchung des multimodalen Instruction Tunings auf unserem vorgeschlagenen Datensatz verwenden wir OFA, ein Modell, das multimodale Muster vereint, als Basis. OFA verwendet ein vereinheitlichtes Vokabular für Sprache, Bildtoken und die Koordinaten eines Bounding Box. Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Instruct-Datensatz. Um die Verarbeitung verschiedener Eingabe- und Ausgabetypen zu vereinheitlichen, folgen wir der Methode von OFA und formulieren alle Aufgaben in einem vereinheitlichten Sequenz-zu-Sequenz-Format, in dem Eingabetext, Bilder, Anweisungen und Bounding Boxes im selben Token-Raum dargestellt werden.\n\nOkay, nun werde ich über das multimodale Instruction Tuning sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus der NIG-Gruppe für das Training und wir sampeln 10.000 Instanzen pro Aufgabe. Für die Testphase reservieren wir die gesamte Common-Sense-Reasoning-Gruppe für Tests und wählen zusätzlich fünf Aufgaben aus WQA und der Miscellaneous-Gruppe aus. Wir verwenden alle Instanzen im Testsplit für jede Aufgabe. Darüber hinaus sampeln wir zufällig 20 Aufgaben aus dem Testsplit der NIG-Anweisungen als SIN-Aufgabe für NLP. Wir verwenden ein vortrainiertes OFA-Großmodell als Basis. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Daher führen wir während des Tests für jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment bewerten. Wir berichten über den Mittelwert und das Maximum der Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente hinweg. Wenn die Aufgabe eine multimodale Klassifizierungsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe handelt, berichten wir über ROOGEL. Für eine RP-Aufgabe berichten wir ebenfalls über ROOGEL. Wir haben auch eine zusätzliche Bewertungsmetrik namens Sensitivität eingeführt. Diese misst die Fähigkeit des Modells, für dieselbe Aufgabe konsistent dieselben Ausgaben zu erzeugen, unabhängig von leichten Variationen in der Formulierung der Anweisung.\n\nHier ist unser Hauptergebnis. Wie wir sehen können, kann Instruction Tuning die Leistung von OFA bei multimodalen Aufgaben erheblich verbessern. Außerdem kann Transferlernen von natürlichen Instruktionsdatensätzen das Instruction Tuning fördern. Wir sehen, dass mit zunehmender Anzahl der Aufgaben das Modell bessere Leistungen erbringt und gleichzeitig eine geringere Sensitivität aufweist. Wir haben auch ein Experiment durchgeführt, bei dem wir eine Anweisung gegenüber fünf Anweisungen verwenden. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt den Einfluss verschiedener Feinabstimmungsstrategien auf die Sensitivität des Modells. Wie wir sehen können, kann Transferlernen von natürlichen Instruktionsdatensätzen dem Modell helfen, eine viel bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell zu erreichen. Wir können auch sehen, dass Transferlernen von natürlichen Instruktionsdatensätzen OFA dabei helfen kann, auf dem natürlichen Instruktionsdatensatz viel bessere Leistungen zu erbringen.\n\nZusammenfassend haben wir den ersten groß angelegten multimodalen Instruction-Tuning-Datensatz vorgeschlagen. Wir verbessern die DAROCHOT-Fähigkeiten von OFA erheblich, erkunden verschiedene Transferlerntechniken und zeigen deren Vorteile. Wir entwerfen eine neue Metrik namens Sensitivität. Eine weitere Sache: Wir sammeln derzeit einen viel größeren multimodalen Instruction-Tuning-Datensatz mit etwa 150 zusätzlichen Varianten sprachbasierter Aufgaben und werden diese veröffentlichen. Hier ist ein QR-Code für unsere Daten und Modelle. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Yusuf Zhang von der Penn State University. Heute werde ich unsere Arbeit, Exampler, vorstellen: Kreuzsprachliche semantische Analyse in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Semantische Analyse ist die Aufgabe, semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda-Kalkül zu erstellen. Und kreuzsprachliche semantische Analyse ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir die Anfrage in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda, FunQL usw. übersetzen. Bestehende kreuzsprachliche semantische Analyse-Modelle wurden separat für begrenzte Aufgaben und Anwendungen vorgeschlagen und bewertet. Beispielsweise gibt es Lücken in der Abdeckung bestimmter natürlicher Sprachen. Chinesisch fehlt, und es gibt Lücken in der Abdeckung bestimmter Bedeutungsrepräsentationen. Lambda-Kalkül fehlt. Oder sie werden nur mit bestimmten neuen Modellen bewertet. Es gibt beispielsweise nur ein einziges Modell zur Bewertung.\n\nAus diesem Grund schlagen wir Exampler vor. Wir stellen einen einheitlichen Datensatz, Exampler, für die kreuzsprachliche semantische Analyse in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Er enthält neun Datensätze aus verschiedenen Domänen, fünf semantische Analyse-Steuern, acht Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unseren Benchmark besser zu bewerten, berücksichtigen wir sechs Einstellungen für Training und Bewertung. Die erste ist TranslateTest. Wir verwenden die Google Translate API, um die Quellsprache in die Zielsprache zu übersetzen, und verwenden dann ein einsprachiges Modell für Training und Bewertung. Beispielsweise trainieren wir das englische Modell mit englischen Anfragen und übersetzen während der Inferenz die deutsche Anfrage mithilfe der API in Englisch, um dann das SQL mit dem trainierten Modell vorherzusagen. Wir testen auch einsprachige Modelle. In dieser Einstellung ist die Quellsprache dieselbe wie die Zielsprache. Beispielsweise Deutsch-Deutsch oder Englisch-Englisch.\n\nWir testen auch die Einstellung einsprachige Fusion, indem wir einsprachige Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen ein einsprachig-mehrsprachiges Modell, bei dem wir für alle Sprachen ein mehrsprachiges Modell trainieren. Beispielsweise fügen wir deutsche, englische und chinesische Anfragen zusammen, um ein mehrsprachiges Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche, chinesische Anfragen usw. zu übersetzen. Wir berücksichtigen auch die kreuzsprachliche Zero-Shot- und Few-Shot-Übertragung. Wir trainieren in einer Quellsprache und übertragen in eine andere Sprache. Während des Trainings trainieren wir also ein mehrsprachiges Modell mit englischen Anfragen oder der Kombination aus englischen und deutschen Few-Shot-Anfragen, um das SQL-Ausgabe vorherzusagen.\n\nWir haben auch viele interessante Ergebnisse erzielt. Im Hinblick auf die Analyse einsprachiger Modelle bewerten wir zwei Gruppen von Modellen, einschließlich Encoder PDR, was für mehrsprachige vorab trainierte Encoder mit zeigerbasierten Decodern steht, wie XLMR plus PDR und BERT plus PDR. Wir bewerten auch Encoder-Decoder-Modelle, die mehrsprachige vorab trainierte Encoder-Decoder-Modelle sind, wie MBART und MT5. Wir stellten fest, dass Encoder-Decoder in allen neun Datensätzen die beste Leistung erbringt. Wir bewerteten MT5 und XLMR plus PDR in der mehrsprachigen Einstellung. Wir stellten fest, dass Encoder-Decoder oder Encoder PDR durch Training in einer Mischung verschiedener Sprachen verbessert werden können. Und wir stellten fest, dass dies daran liegt, dass die meisten wichtigen natürlichen Sprachen eine Leistungssteigerung erzielen, mit Ausnahme der Tatsache, dass die Leistung von Englisch in sieben Datensätzen abfällt und nur in drei Datensätzen zunimmt. Dies wird als Kurven der Mehrsprachigkeit bezeichnet.\n\nWir verglichen auch die kreuzsprachliche Leistungsspanne. In dieser Abbildung stellt die blaue Linie die kreuzsprachliche Few-Shot-Übertragung dar. Die orangefarbene Linie ist die kreuzsprachliche Zero-Shot-Übertragung, während die grüne Linie die einsprachige Einstellung darstellt. Wir stellten fest, dass der Vergleich der grünen und orangefarbenen Linie zeigt, dass die kreuzsprachliche Übertragungsleistungsspanne im Zero-Shot-Modus signifikant ist. Und der Vergleich der blauen und orangefarbenen Linie zeigt, dass die Übertragungsspanne im Few-Shot-Modus schnell verkürzt wird.\n\nWir haben auch weitere interessante Erkenntnisse gewonnen. Beispielsweise übertrifft Encoder-Decoder die bisherige Arbeit oder erzielt vergleichbare Ergebnisse. Die Verwendung des englischen natürlichen Sprachmodells kann die Leistung von Few-Shot in den Zielnatürlichen Sprachen erheblich steigern. Und wir stellten fest, dass mehrsprachige Sprachmodelle wie Codice und Bloom für Aufgaben der kreuzsprachlichen semantischen Analyse immer noch unzureichend sind.\n\nZusammenfassend haben wir Exampler entwickelt, einen einheitlichen Benchmark für die kreuzsprachliche semantische Analyse mit mehreren natürlichen Sprachen und vielen Repräsentationen. Wir haben eine umfassende Benchmark-Studie an drei repräsentativen Arten von mehrsprachigen Sprachmodellen durchgeführt. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse usw. Besuchen Sie gerne unsere Publikation und den Code. Danke fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Spirakowski und dieses Referat handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie wissen mögen, gehen verschiedene Theorien und Korpusansätze von unterschiedlichen Abhängigkeitsstrukturen aus. So beispielsweise in den universellen Abhängigkeiten ist die Struktur der koordinierten Koordination Lisa, Bart und Maggie so, dass das erste Konjunkt Kopf der gesamten koordinierten Struktur ist, in diesem Fall also Lisa. Ein ähnlicher Ansatz wird in Igor Milchuks Bedeutungstexttheorie angenommen, wo ebenfalls die gesamte koordinierte Struktur vom ersten Konjunkt geleitet wird. Diese beiden Ansätze sind also asymmetrisch, richtig? Sie heben eines der Konjunkte hervor.\n\nEs gibt aber auch symmetrische Ansätze für koordinierte Strukturen, wie den Prager Ansatz oder den konjunktionsgeleiteten Ansatz, der in den Prager Abhängigkeitsbäumen verwendet wird, wo koordinierte Strukturen von der Konjunktion geleitet werden. So erhalten wir Abhängigkeiten von und zu allen Konjunkten. Und schließlich gibt es auch einen mehrköpfigen Ansatz, der beispielsweise in der Wortgrammatik von Cutson verwendet wird, wo, salopp gesagt, alle Konjunkte Köpfe der koordinierten Struktur sind, sodass wir separate Abhängigkeiten vom Regenten hier lacht zu jedem Konjunkt erhalten, nämlich Bart und Maggie.\n\nDas Ziel dieses Aufsatzes ist es, ein neues Argument für die symmetrischen Strukturen der Koordination wie diese beiden und gegen die asymmetrischen Strukturen der Koordination wie diese beiden vorzubringen. Das Argument basiert auf dem Prinzip der Abhängigkeitslängenminimierung, das ich anhand dieser Beispiele erklären werde.\n\nIm Englischen, wie Sie wissen mögen, bevorzugen direkte Objekte, in der Nähe des Verbs zu stehen, während Adjunkte weiter entfernt sein können, richtig? So ist „March read it yesterday“ in Ordnung, da das direkte Objekt „it“ in der Nähe des Verbs steht, während „March read yesterday it“ viel schlechter klingt, da hier zwischen Verb und direktem Objekt das Adjunkt „yesterday“ steht. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und lang ist, da es dann an die Position nach dem Adjunkt verschoben werden kann. Dies wird hier veranschaulicht. Beide Sätze sind in Ordnung: „March read this absolutely fascinating book about the bees yesterday“ ist akzeptabel, wo statt „it“ ein langer Nominalphrasen-Ausdruck steht. Es ist auch in Ordnung, zu sagen: „March read yesterday this absolutely fascinating book about bees“.\n\nDie Überlegung hier ist, dass dies möglich ist, weil dieser Satz, obwohl er das allgemeine grammatikalische Prinzip verletzt, dass direkte Objekte neben dem Verb stehen sollten, das Prinzip der Abhängigkeitslängenminimierung erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also derjenigen, die sich nicht in diesen beiden Strukturen ändern. Hier haben wir eine Abhängigkeit von „red“ zum Adjunkt mit einer Länge von 7, gemessen in Wörtern, und von „red“ zu „book“ mit einer Länge von 4, also zusammen 11. Wenn Sie diese beiden Konjunkte austauschen, wird die Summe dieser beiden Abhängigkeiten 6, also statt 11 nur 6, was viel kürzer ist und daher besser klingt. Es verletzt zwar ein Prinzip, erfüllt aber ein anderes.\n\nWas wir getan haben, war die Extraktion verschiedener Statistiken über Koordination aus der erweiterten Version des Penn Tree Bank und sehen Sie im Aufsatz, warum wir keine universellen Abhängigkeiten verwendet haben. Diese Statistiken bestätigen die oft gemachte Beobachtung, dass linke Konjunkte tendenziell kürzer sind. So „Salt and pepper“ und nicht „Pepper and salt“, gemessen in Silben. Und auch die Beobachtung, dass diese Tendenz mit der Längendifferenz zunimmt. Wenn also der Unterschied zwischen den Längen der beiden Konjunkte größer wird, bevorzugt das kürzere Konjunkt, das linke zu sein, stärker, richtig? Der Anteil der linken kurzen Konjunkte ist größer.\n\nWas in diesem Aufsatz neu ist, ist die Beobachtung, dass diese Tendenz nur auftritt, wenn der Regent links steht oder abwesend ist, richtig? Der Regent steht in diesem Beispiel links: „I saw Bart and Lisa“. Hier ist der Regent links. Er ist abwesend im zweiten Beispiel: „Homer came and sneezed“, wo wir eine Koordination von zwei Verben haben und keinen äußeren Regent. In solchen Fällen bevorzugt das linke Konjunkt, kürzer zu sein, je größer der Unterschied zwischen den beiden Konjunkten ist. Wenn jedoch der Regent rechts steht, wie hier, wo „left“ die Koordination „ted and net“ regiert, verschwindet dieser Effekt. Wir haben dies durch Messung der Länge in Zeichen (erste Spalte), Silben (mittlere Spalte) und Wörtern (rechte Spalte) gezeigt. Ich werde mich auf die rechte Spalte konzentrieren.\n\nWas wir hier sehen, ist, dass die Tendenz des linken Konjunktes, kürzer zu sein, mit der absoluten Differenz in Wörtern stetig zunimmt, wenn der Regent links steht, und dasselbe wird beobachtet, wenn kein Regent vorhanden ist, wie bei der Koordination von Sätzen. Aber wenn der Regent rechts steht, verschwindet diese Tendenz, und wir zeigen im Aufsatz, wie dies ein Argument gegen asymmetrische Koordinationsstrukturen wie diese beiden und für symmetrische Strukturen wie diese beiden liefert.\n\nSiehe den Aufsatz für die vollständige Argumentation und sprechen Sie mit uns darüber in der Nachbesprechung. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kyo Yin und ich werde unsere Arbeit mit dem Titel „Wann erfordert Übersetzung Kontext? Eine datengesteuerte mehrsprachige Erkundung“ präsentieren. Diese Arbeit entstand in Zusammenarbeit mit Patrick Fernandes, Emily Liu, Andre FD Martins und Graham Newbig. So viele Übersetzungen hängen vom Kontext ab. Zum Beispiel, wie würden wir in diesem Satz das Wort „mole“ übersetzen? Nun, wenn der vorherige Satz lautete: „Die Dinge könnten gefährlich werden, wenn die Minister es herausfinden“, dann bezieht sich „mole“ auf einen Spion. Aber wenn der vorherige Satz lautete: „Könnte es etwas Ernstes sein, Doktor?“, dann bezieht sich „mole“ auf ein Muttermal. Je nach Kontext ändert sich also die Bedeutung des Wortes und damit auch seine Übersetzung. Die Bewertung, wie gut Modelle solche Fälle übersetzen können, ist jedoch recht schwierig. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was bedeutet, dass korpusbasierte Metriken wie BLEU diese Übersetzungen nicht erfassen können. Und obwohl einige Menschen eine gezielte Bewertung von kontextabhängigen Übersetzungen vorgeschlagen haben, unterstützen diese Ressourcen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachsets, da sie normalerweise auf Fachwissen und menschliche Kuratierung angewiesen sind.\n\nIn dieser Arbeit versuchen wir, zwei Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut bewältigen Modelle solche Fälle? Um die erste Frage zu beantworten, begannen wir damit, zu messen, inwieweit ein Wort während der Übersetzung vom Kontext abhängt. In einer vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungsmodelle eingeführt. Dies geschieht durch die Messung, wie viel Information der Kontext C über das Ziel Y liefert, gegeben die Quelle X. Man kann sich CXMI als die Information vorstellen, die durch die Bereitstellung von Kontext für das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI zu pointwise CXMI, das die Kontextnutzung auf Satz- oder Wortniveau messen kann. Wörter mit hohem PCXMI können als solche angesehen werden, die für ihre Übersetzung Kontext erfordern.\n\nNun analysieren wir Wörter mit hohem PCXMI, um Muster zwischen ihnen zu erkennen. Unsere Analyse führen wir auf Transkripten von TED-Vorträgen durch, die aus dem Englischen in vierzehn verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Erstens betrachten wir Wortart-Tags mit hohen PCXMI-Mittelwerten. Dies ermöglicht es uns, beispielsweise duale Pronomen im Arabischen zu finden, die relativ hohe PCXMI-Werte aufweisen. Dies lässt sich damit erklären, dass Englisch keine dualen Pronomen hat. Daher benötigt man Kontext, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Ähnlich stellen wir fest, dass bestimmte Sprachen auch Kontext erfordern, wenn man die passende Verbform wählen möchte.\n\nDann betrachten wir Vokabeln mit hohem PCXMI, gemittelt über alle ihre verschiedenen Vorkommen. Dies hilft uns, Fälle wie diesen hier zu identifizieren, in denen im Chinesischen Kontext für eine korrekte Übersetzung erforderlich ist. Und schließlich stellen wir fest, dass Kontext wichtig ist, um die richtige Formalität zu übersetzen.\n\nSchließlich betrachten wir verschiedene einzelne Token mit hohem PCXMI. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der Satzstruktur zum Ausdruck kommen, wie beispielsweise Ellipsenauflösung.\n\nNun nutzen wir unsere Analyseergebnisse, um einen Benchmark für die dokumentspezifische Übersetzung zu entwickeln. Für jedes der fünf identifizierten Diskursphänomene erstellen wir Tagger, um automatisch Wörter zu identifizieren, die zu dem Phänomen gehören, und nennen unseren Tagger Multilingual Discourse Aware oder MUDA-Tagger. Wir stellen auch fest, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene aufweisen.\n\nAnschließend wenden wir den MUDA-Tagger auf den parallelen Korpus an, den wir für die Bewertung verwenden möchten, und wenden unsere Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der MUDA-Tagger identifiziert hat. Schließlich verwenden wir unseren Benchmark und andere Metriken, um verschiedene Modelle auf der Ebene der dokumentspezifischen maschinellen Übersetzung zu bewerten.\n\nZunächst einmal stellen wir fest, dass kontextunabhängige Modelle bei der Verwendung korpusbasierter Metriken wie BLEU die beste Leistung erbringen. Wenn wir jedoch COMET verwenden, sind kontextbezogene Modelle am besten. Und wenn wir die WordF-Maßzahl verwenden, haben Modelle mit oder ohne Kontext eine vergleichbare Leistung. Dies zeigt erneut, dass es schwierig ist, das beste System für die dokumentspezifische Übersetzung zu bestimmen, wenn man nur korpusbasierte Metriken verwendet.\n\nMit dem MUDA-Benchmark bewerten wir Modelle und stellen fest, dass kontextbezogene Modelle für bestimmte Diskursphänomene, wie Formalität und lexikalische Kohäsion, deutlich genauer sind als Modelle, die keinen Kontext verwenden. Diese Modelle sind jedoch bei anderen Phänomenen, wie Ellipsen, Pronomen und Verbformen, nicht viel besser als Modelle ohne Kontext. Dies deutet darauf hin, wo wir bei der dokumentspezifischen Übersetzung mehr Fortschritte sehen müssen.\n\nWir haben auch verschiedene kommerzielle Systeme verglichen, und unser Benchmark zeigt, dass DPL für die dokumentspezifische Übersetzung in der Regel genauer ist als Google Translate.\n\nZusammenfassend führen wir eine datengesteuerte Analyse über vierzehn Sprachpaare durch, um zu ermitteln, wann Übersetzungen Kontext erfordern. Und dann nutzen wir unsere Erkenntnisse, um einen Benchmark für die dokumentspezifische maschinelle Übersetzung zu entwickeln, der uns helfen kann, zu identifizieren, welche Diskursphänomene Modelle gut oder weniger gut bewältigen, und welche Übersetzungssysteme für die dokumentspezifische Übersetzung geeignet sind. Vielen Dank für Ihre Aufmerksamkeit. Bis morgen."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich euch unsere Arbeit „Anal Positionale: Charakterisierung von Design-Biases, A Beta Sets und Modelle“ vorstellen. Diese Arbeit entstand in Zusammenarbeit mit einigen Kollegen von der University of Washington und dem Allen Institute for AI, nämlich Sebastian Santi, Ronin Lebras, Katarina Reinicke und Martin Sapp.\n\nBeginnen wir mit einem Szenario: Sie arbeiten für eine Zeitung und durchsuchen die Kommentare unter einem Artikel, um toxische Inhalte zu entfernen. Sie könnten eine beliebte API wie die Perspective API für die Toxizitätserkennung verwenden, die bei Carl Jones sehr gut funktioniert, wo die API-Perspektiven korrekt toxische Inhalte erkennen. Bei Dithya Sharma jedoch sind die Perspektiven-APIs nicht so empfindlich gegenüber beleidigenden Begriffen, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede von Technologien zwischen verschiedenen Bevölkerungsgruppen beobachten. Solche Design-Biases können durch die Positionierung von NLP-Forschern und Modellentwicklern entstehen. Positionierung beschreibt einfach die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen. Dies ist ein weit verbreitetes Konzept in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen. Als Forscher kann die Positionierung den Forschungsprozess und seine Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändert.\n\nEine Frage, die sich stellt, ist: Haben Datensätze und Modelle eine Positionierung? Wir behaupten nicht, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren die Urteile und Meinungen echter Menschen und können somit bestimmte Positionierungen gegenüber anderen repräsentieren. Vorherige Arbeiten haben einige anekdotische Beweise für Positionierung geliefert, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen der Modellpositionierung. Diese Arbeiten vergleichen jedoch nicht die Endnutzer mit den Datensätzen und Modellen selbst. Die Untersuchung der Positionierung von Modellen und Datensätzen ist zunehmend wichtig, da NLP-Aufgaben subjektiver und sozialer orientiert werden. Es ist schwierig, die Verzerrungen dieser Positionierungen zu charakterisieren, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.\n\nUm die Positionierung von Datensätzen und Modellen zu untersuchen, vergleichen wir tatsächlich die Anmerkungen echter Nutzer mit bestehenden Datensätzen und Modellen. Dies geschieht über unseren Rahmen, NLPositionality. Unser Rahmen arbeitet in zwei Hauptstufen. Der erste Schritt besteht darin, Datensätze mit verschiedenen Annotatoren neu zu annotieren. Wir entscheiden uns dafür, anstatt die Demografie der ursprünglichen Datensätze und Annotatoren zu betrachten, da normalerweise nur wenige Annotatoren jede Instanz annotieren und Demografie-Daten selten gesammelt und geteilt werden. Wir entscheiden uns daher, Daten neu zu annotieren, um viele Annotatoren pro Instanz zu erhalten und einen reichhaltigen Demografie-Datensatz zu erstellen. Anschließend vergleichen wir die Anmerkungen nach Demografie-Daten mit Modellen und Datensätzen mithilfe eines Parsons R Korrelationswerts. Unser Rahmen unterscheidet sich somit von der Literatur über Annotator-Diskrepanzen, indem er Endnutzer mit Modell- und Datensatz-Vorhersagen und -Beschriftungen vergleicht, anstatt sich nur auf die Übereinstimmung von Annotatoren oder die Modellierung von Annotator-Verteilungen zu konzentrieren.\n\nUnser Rahmen wird größtenteils durch Lab in the Wild ermöglicht, eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können, im Gegensatz zu Plattformen wie MTurk, die hauptsächlich Teilnehmer aus den USA oder Indien haben. Lab in the Wild liefert außerdem weiterhin hochwertige Daten. Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptanz. Hier lesen die Teilnehmer eine Situation aus dem Social Chemistry Datensatz und schreiben dann, wie sozial akzeptabel die Situation ist. Um im Studium engagiert zu bleiben, können sie ihre Antworten anschließend mit einer KI und anderen vergleichen. Wir verglichen diese Anmerkungen dann mit Social Chemistry Delphi in GPT 4. Wir replizierten ein sehr ähnliches Setup für die Aufgabe der Toxizitäts- und Hasssprachen-Erkennung, wo die Teilnehmer eine Instanz aus DynaHate lesen und angeben, ob sie es für eine Hasssprache-Instanz halten. Diese Anmerkungen verglichen wir dann mit DynaHate, Perspective API, Rewire API, Hate Roberta in GPT 4.\n\nUnsere Studie sammelte am Ende über sechzehntausend Anmerkungen von über tausend Annotatoren aus achtundachtzig Ländern. Nun sind wir besser gerüstet, um die Frage zu beantworten: Mit wem stimmen NLP-Datensätze und -Modelle am meisten überein? Wir stellen fest, dass es Positionierung in NLP gibt. Beispielsweise stellen wir fest, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. Für die GPD 4-Analyse der sozialen Akzeptanz stellen wir fest, dass sie am meisten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. DanaHate stimmt ebenfalls am meisten mit englischsprachigen Ländern überein. Wir stellen auch eine zusätzliche Übereinstimmung mit Menschen fest, die eine Hochschulbildung haben. Für GPD 4 in der Aufgabe der sozialen Akzeptanz stellen wir fest, dass es am meisten mit Menschen mit Hochschul- oder Graduiertenbildung übereinstimmt. Und wir stellen das Gleiche für DanaHate fest, wo es am meisten mit Menschen mit Hochschulbildung übereinstimmt. Wenn Modelle und Datensätze jedoch mit bestimmten Bevölkerungsgruppen übereinstimmen, bleiben einige unvermeidlich zurück. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Menschen übereinstimmen als mit ihren männlichen und weiblichen Gegenstücken. Dies stellen wir in der GPT 4-Aufgabe der sozialen Akzeptanz sowie in der DynaHate-Analyse fest.\n\nAngesichts der Positionierung in NLP, was können wir dagegen tun? Wir haben einige Empfehlungen dafür. Die erste ist, während des gesamten Forschungsprozesses alle relevanten Design-Entscheidungen zu dokumentieren. Die andere ist, NLP-Forschung durch die Linse des Perspektivismus zu betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb spezifischer Gemeinschaften zu erstellen. Ein gutes Beispiel dafür ist die Masakane-Initiative. Wir möchten betonen, dass inklusive NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren.\n\nDas war unsere Präsentation. Wenn Sie mehr erfahren möchten, schauen Sie gerne auf unserem Dashboard nach den neuesten Analyseergebnissen und unserer Publikation. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich werde über unsere Arbeit zur Auflösung indirekter Referenzausdrücke für die Entitätsauswahl sprechen, in der wir den Altentity-Korporus einführen. Mein Name ist Javot Hosseini, und dies ist eine gemeinsame Arbeit mit Philip Radlinsky, Silvia Pareti und Annie Luis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Auswahl treffen möchten. Betrachten Sie diese alternative Frage: Meinten Sie „easy on me“ oder „I got a feeling“? Hier möchte ein Benutzer zwischen diesen beiden Optionen wählen. Das Offensichtlichste ist die Verwendung einer direkten Referenz, beispielsweise durch Nennung des Songnamens „easy on me“ oder seiner Position „der erste“. Manchmal ist jedoch eine indirekte Referenz angemessener, um ein natürlicheres Gespräch zu führen. Dies kann der Fall sein, wenn der Benutzer sich den Songnamen nicht merken kann, die Aussprachen zu ähnlich sind und eine Unterscheidung schwierig ist, oder wenn der Benutzer eine Präferenz angeben möchte. Hier einige Beispiele für indirekte Referenzen: zum Beispiel „der neuere“ oder „der Song, der nicht energiegeladen ist“. Dies ist ein wichtiges Problem in konversationsbasierten Systemen und auch für die Bewertung der Entitätsverständnis-Fähigkeiten von LLMs. Uns ist kein öffentlicher Datensatz, ein großflächiger öffentlicher Datensatz für diese Aufgabe bekannt, daher haben wir einen eigenen mittels Crowdannotation erstellt. Unser Datensatz umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensammlungsmethode betont die Informalität unter Verwendung einer Cartoon-Vervollständigung. Der Cartoon enthält drei Sprechblasen. In der ersten Blase sagt Bob: „Erinnerst du dich an den Song, den wir gestern gehört haben?“ und setzt damit den Dialogkontext. In der zweiten Blase fragt Alice: „Meinst du ‚easy on me‘ oder ‚I got a feeling‘?“, was die alternative Frage ist. Und in der dritten Blase verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, zum Beispiel „der Neo-Ervandal“. Die erste und zweite Blase werden automatisch bereitgestellt, während die dritte vom Annotator ausgefüllt wird. Die erste Blase wird aus einigen manuellen Eingaben pro Domäne ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert: Wir verwenden immer eine einfache Vorlage: „Meinst du A oder B?“, wobei A und B aus Wikipedia-Einträgen gezogen werden. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Je höher man in der Liste steigt, desto ähnlicher werden die Entitäten, und die Unterscheidung wird in der Regel schwieriger. Die erste Methode ist eine gleichmäßige Zufallsauswahl, die zweite wählt Entitäten mit ähnlichen Titeln aus, zum Beispiel zwei Bücher mit demselben Namen. Die dritte Methode wählt Entitäten mit ähnlichen Beschreibungen auf Wikipedia aus, und schließlich diejenigen mit ähnlichen Infoboxen oder Attributen auf Wikipedia, wie zum Beispiel demselben Genre oder demselben Künstler für einen Song. Wenn wir diese alternative Frage den Annotatoren präsentieren, kennen sie die Namen dieser Entitäten, aber unbedingt etwas über die Entitäten selbst. Daher zeigen wir ihnen einige Hintergrundinformationen zu den beiden Entitäten. Für Songs zeigen wir einfach einen Google-Suchlink für jeden Song an und bitten die Annotatoren, sich zumindest Teile jedes Songs anzuhören und über jeden Song zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für den Song „Easy on Me“. Für die Domänen Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich Bilder, ebenfalls aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Referenzausdrücken zu beschreiben. Zum Beispiel „der mit der Pianomusik“, hier einige Beispiele aus unserem Datensatz: „der ohne Worte“, „nicht der mit dem 12-jährigen Jungen“, „der fiktive“ oder „der aus Aserbaidschan“ und so weiter. Der Altentity-Korporus enthält 6.000 alternative Fragen in drei Domänen und 42.000 indirekte Referenzausdrücke. Die Ergebnisse mit dem T5xLarge-Modell sind wie folgt zusammengefasst. Wenn das Sprachmodell auf genau dieselben Hintergrundinformationen wie die Annotatoren zugreifen kann, ist die Genauigkeit sehr hoch, etwa 92-95%. Das ist jedoch nicht realistisch. Wenn das Sprachmodell auf teilweise übereinstimmende Hintergrundinformationen zugreifen kann, liegt die Genauigkeit zwischen 82 und 87 Prozent, was realistischer ist, beispielsweise wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur auf die Namen der Entitäten zugreifen kann, liegt die Genauigkeit bei lediglich 60%. Es besteht also viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank."}
