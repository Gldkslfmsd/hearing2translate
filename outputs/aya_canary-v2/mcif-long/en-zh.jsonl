{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是马蒂亚斯·伦达曼（Matthias Lendermann），今天我将简要介绍我们关于使用多集标记和潜在置换进行树结构之外的组合泛化论文。这是我与导师亚历山大·科勒（Alexander Koller）和伊万·蒂托夫（Ivan Titoff）的合作成果。组合泛化可以理解为学习者处理更深层递归和训练过程中单独见过的短语组合的能力。在语义解析的背景下，测试组合泛化可能如下所示。与往常一样，我们有一个训练语句集，例如“女孩睡着了”和“玛丽知道女孩睡着了”。这些语句与代表其核心意义的逻辑形式配对。与标准机器学习评估不同，测试集不来自同一分布，而是包含结构上未见过的逻辑形式。在这个例子中，模型在训练期间见过更浅的递归，但在测试时遇到更深的递归。天真的序列到序列模型在这个出分布泛化方面遇到困难，经常产生与输入脱节的输出。特别是，它们经常无法再现输入和输出之间的系统对应关系，例如在例子中用颜色标注的部分。一种流行的解决方法是将树结构集成到模型中。这些树旨在捕捉与逻辑形式相关联的语句的组合过程。这效果很好，但树结构通常不给定，需要以某种方式获取。这可能很复杂，有时是计算上昂贵的过程。通常这涉及到对逻辑形式进行相当正式的预处理，例如处理变量符号。获取树结构也可能涉及到专业的语法归纳程序。在本文中，我们不使用树结构，并引入一个神经序列到序列模型，直接建模输入片段和输出片段之间的对应关系。首次，我们展示了不依赖树结构而对更深层递归进行强泛化。我们的方法从输入中预测输出，分为两个步骤。首先，我们为每个输入标记添加一个无序的多集标记，其中包含将出现在输出中的标记。在第一个步骤之后，我们有了所有正确的标记，但它们没有顺序。因此，在第二个步骤中，我们使用另一个模型来预测一个置换，将它们放入正确的顺序。我们引入了一种新的方法来预测置换，对可能的置换不施加任何硬约束。这使我们的方法非常灵活和表达力强。从概念上讲，我们的置换模型大致如下工作。我们从左到右遍历输出，确定每个位置应放置的多集标记。对于第一个输出位置，我们简单地选择一个如红色高亮显示的标记。然后，我们跳到下一个多集标记，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，跳到另一个多集标记。我们继续这个过程，直到访问完第一个阶段中的每个标记。为了给您展示实验结果的预览，我们在这里将我们的方法与其他无树模型在Koggs基准上的比较。我们的模型在对更深层递归的泛化方面以大优势超越其他模型。然而，其他一些结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了一些有趣的技术挑战。首先，输入和输出之间的对齐关系在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个多集，这为训练带来了挑战。此外，有时有多个与数据一致的置换，但语言上正确的置换是潜在的。我们通过在训练过程中归纳对齐关系来解决这个问题。我们的置换方法非常灵活，但带来了找到最高得分置换是NP高的挑战。这是因为它与旅行商问题相关。我们通过一个GPU友好的连续放松来近似这个问题，这还允许我们反向传播解决方案并学习语言上更合理的置换。如果您想了解更多关于我们的实验和我们如何解决这些挑战的信息，请阅读我们的论文或参观我们的展板。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是Myra，今天我将讨论我们的一篇论文《标记人设：使用自然语言提示测量语言模型中的刻板印象》。这项工作是与Essendermouch和Dangerowski合作完成的。近年来，许多研究记录了大型语言模型（LLM）中社会偏见和刻板印象的普遍存在。然而，这些测量方法存在各种局限性。它们通常依赖于手工构建的数据集，需要大量时间来整理。而且通常只测量非常具体的刻板印象，这意味着它们不能很好地推广到其他人口统计或背景，或者它们只是捕捉到非常普遍、广泛的关联，如与特定群体相关的负面关联。此外，这方面的大多数工作都没有考虑到交集性，即多层次社会身份可以加剧偏见，并成为独特的伤害焦点。\n\n为了克服这些局限性，我们利用了这些新指令调优LLM的一个特性，即它们非常擅长响应指令和提示。因此，我们可以要求模型生成一个人设，这是一个通过提示（例如，想象你是一个亚洲女性，描述自己）来描绘想象中个体的描述。我们可以立即看到，这非常适用于任何人口统计，因为我们可以在提示中指定任何我们想要的身份标记。以下是GPT 4的一些生成示例。我们可以立即看到，虽然输出不是传统意义上的过度负面或有毒，但有一些有趣的模式。亚洲女性被描绘为不显眼，中东女性被描述为异国情调，令人着迷的地区，而两种有色人种女性的人设都提到了祖先，而白人男性的人设则没有。\n\n为了捕捉这些模式，我们的方法分为两部分。第一部分是生成这些人设。我们生成这些人设的提示来自一项研究，该研究向人类受试者提供了这些提示，发现通过这种方式也能揭示种族刻板印象。这还使我们能够直接比较生成的人物和人类撰写的响应。第二部分是标记词，这是一种方法，用于识别区分标记组和未标记组的词语，我稍后会详细说明。这种方法的优点是，我们可以获得非常具体的刻板印象和模式，而无需依赖任何特定的词典。因此，标记词方法借鉴了社会语言学中的标记概念，即存在一个未标记的默认值，任何与该默认值不同的群体在语言上都被标记。例如，词语“战士”通常与男性相关联。所以，当人们描述一个女性战士时，他们通常会实际指定一个男性战士，并在术语中标记为“女性”。更广泛地说，社会中的主导群体在语言和社会上都是未标记的，而边缘化群体通常被标记。\n\n在我们的方法中，我们首先指定什么是未标记组和标记组。然后，我们使用战斗词方法比较人设，这基本上是使用加权对数奇数比来区分每个标记组的前列词。例如，对于黑人女性的人设，我们将使用战斗词方法，并将对数奇数比与白人人设和男性人设进行比较，因为它们是相应的未标记组。\n\n现在来看一些结果。首先，我们使用了一个刻板印象词典，发现生成的人物包含的刻板印象词比人类撰写的人物多得多。然而，当我们实际查看词典中词语的分布时，发现情况截然不同。虽然生成的人物中词典词的出现率更高，但人类撰写的人物中词语的分布更广泛，而刻板印象词仅为“高”和“健壮”。所以实际上只有积极的或至少不是负面的词语。事实上，这个词典根本没有很好地捕捉到我们在之前的幻灯片中看到的许多有害模式。因此，相反，我们将转向标记词方法的结果，以展示这些看似积极的词语如何促进刻板印象和本质化叙事。\n\n在我们的分析中，我们揭示了这些看似积极的描述如何反映有害模式。首先，对于标记组，前列词包括文化、传统、自豪和异国情调等词语。这些词语仅根据其与身份的关系来定义这些群体，并将其与白人规范区分开来。这为这些群体带来了长期的歧视和其他化历史。此外，这些词语中反映了许多常见的套路，尤其是对于有色女性。例如，描述拉美女性的词语包括充满活力和曲线玲珑，这与热带主义套路相关。对于亚洲女性，词语是娇小、精致和丝滑，这与亚洲女性被性化、被视为温顺和顺从的历史相关。最后，对于黑人女性，我们看到一些前列词是坚强和坚韧。这与人们所说的黑人女性强人形象相关。虽然乍看上去很积极，但研究表明，这种形象实际上非常有害，因为它给这些人口统计群体带来了巨大的压力，要求他们在面对社会障碍时坚强和坚韧。因此，而不是真正致力于改变这些障碍，它给这些人施加了压力，要求他们克服这些障碍，这导致这些人出现非常负面的健康结果，以及其他伤害。\n\n更广泛地说，我们发现每个标记组的词语几乎完全反映了本质化叙事。基于这些模式，我们为模型所有者提出三点建议。首先，作为研究人员，我们应该解决积极刻板印象和本质化叙事。我们还应该使用交集性视角来研究偏见和伤害，因为如果不这样做，可能会忽略许多事情。最后，应该提高偏见缓解方法的透明度，因为例如，像这些积极刻板印象这样，我们不知道它是因为某种奇怪的、过度价值观对齐正在发生，或者可能是其他反刻板印象方法导致这些恶性的模式。如果没有更多的透明度，我们真的不能做出任何假设或进一步研究。\n\n非常感谢您的聆听。在ACL中玩得开心。"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是詹姆斯·芬奇。我是莎拉·芬奇。今天我们将向您介绍 ABCEval，一种评估对话人工智能的新维度方法。这项工作由埃默里大学自然语言处理实验室完成，由埃默里大学的崔吉诺教授领导，并与亚马逊 Alexa AI 合作。假设您刚开发了一个对话模型，想了解它与当前最先进技术的比较情况。常见的做法是使用人工评估，例如让人工评判员选择两个对话中哪一个更好，或在给定的量表上对对话进行评分。这些方法在提供整体对话质量评估方面效果良好，但对话质量有多个方面。因此，您可能希望评估聊天质量的多个维度，以更细致地了解模型的优缺点。一种方法是简单地让人工评判员评估对话质量的多个维度，例如模型响应的相关性，使用现有的比较或利克特量表方法。然而，我们认为存在一种更精确、更可靠的维度对话评估策略。通过明确标注每个模型响应是否表达某些行为，例如提供无关信息或自相矛盾。我们将这种方法称为聊天行为标注，简称 ABCEval。我们开发了这种方法，以全面涵盖最近文献中建议影响聊天质量聊天模型的行为。\n\nABCEval 能够测量聊天模型犯各种主题错误的比例。例如，ABCEval 测量聊天模型忽略其对话伙伴或说无关话的回合数，自相矛盾或与对话伙伴矛盾，编造不正确的事实或违反常识知识，以及模型表现或未能表现出同理心的情况。\n\n为了确定最有效的评估方法，我们选择了四个最先进的聊天模型，并使用 ABCEval 对每个模型进行一百次人工聊天评估。为了比较，我们还使用三种现有方法对这些对话进行了评估：回合级利克特评分、对话级利克特评分和对话级配对比较。此外，我们还收集了八个最常见对话方面的评估，因为这是评估多维度聊天模型的标准做法。\n\n从我们对这些评估结果的分析中，我们发现 ABCEval 行为标签总体上比现有方法收集的标签更可靠，具体体现在 100 个双重标注对话的评分者间一致性上。此外，ABCEval 标签比现有方法产生的指标更能预测整体对话质量，正如这个简单的线性回归分析所示。例如，您可以看到测量自相矛盾和对话伙伴矛盾的回合比例分别解释了对话质量的 5% 和 10%，而平均利克特一致性得分仅解释了 4% 或更少。\n\n最后，我们检查了每个评估指标是否捕捉了聊天质量的独特方面，使用逐步线性回归。您可以看到，所有 ABCEval 指标的组合解释了超过 25% 的对话质量，当您逐个删除指标时，大多数指标都会导致失去大量关于质量的信息。而替代级别的利克特指标组合解释的质量远少，这些指标中更少的指标携带独特信息。\n\n这些可靠、信息丰富且独特的 ABCEval 指标使我们能够以高于先前方法能达到的分辨率评估对话人工智能。您可以在我们实验的结果中看到，仍然存在几个挑战，并且已被精确量化。例如，我们测试的机器人大约有 20% 的响应违反了常识。它们在大约 15% 的响应中产生无关信息，并且大约 10% 的时间自相矛盾或与对话伙伴矛盾。随着该领域的快速改进，这些错误率在新发布的模型中可能会降低。然而，这更说明了追求可靠和精确的评估指标以进行模型比较的重要性。我们希望 ABCEval 能够被该领域的其他人士作为朝此方向迈出的有意义一步，并期待看到未来几个月和几年对话人工智能的进步。谢谢观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是瓦苏达（Vasudha），来自斯托尼布鲁克大学（Stony Brook University）的计算机科学博士候选人。我想向大家介绍我们被ACL 2023会议录用的长篇论文《迁移学习用于不和谐检测》，这篇论文解决了稀有类别挑战问题。\n\n我们首先定义认知不和谐，并解释为什么它在语言研究中是一个重要问题。简单来说，认知不和谐是指两个不一致的信念或行为，例如这个例子：一个人说“我知道香烟可能会要了我的命”，然后又说“会议结束后我拿了几根烟”。这两个信念和行为是不一致的，处于不和谐状态。进一步说，“我认为没有它们我无法保住工作”这句话解释了第二次行为，使它们处于和谐关系。虽然不和谐是我们在日常决策中经常经历的现象，但在语言中表达的不和谐与其他类型的语篇关系相比非常稀少。\n\n那么，这有什么重要意义吗？研究认知不和谐可以帮助我们理解人们之间不同意见的影响，追踪人口中趋势、信念价值和态度变化。高认知不和谐也与焦虑障碍相关，有助于更好地理解人们的心理健康。研究语言中表达的不和谐也可以有利于理解易受伤害群体的极端主义和两极分化。最后，认知不和谐对于理解个人的认知风格至关重要，有助于我们更好地理解决策过程。\n\n为了创建认知不和谐资源，我们对大量不和谐关系进行了标注。我们采用图中展示的不和谐优先方法。使用PDTB分析器处理推文，并根据论文中描述的指南对语篇单位对进行标注。正如图所示，只有3.5%的标注对发现了不和谐。在收集了大约1000个语篇单位对的样本后，我们对初始分类器进行了训练，仅使用43个不和谐样本进行训练。不令人意外的是，分类器的表现几乎不超过随机水平。\n\n由于不和谐的发生率很低，且缺乏任何先前的类似数据集，我们面临着绝对稀有问题。为了缓解这一问题，我们实验了迁移学习和主动学习的组合，以便在更少的标注轮次中收集更多不和谐样本，降低整体标注成本同时提高不和谐检测能力。由于初始模型完全无法捕捉不和谐类别，我们通过从紧密相关任务转移权重开始主动学习过程。我们从两个不同任务转移：主题独立的不和谐立场分类，这个任务判断两个来自不同人的辩论陈述是否一致，无论主题如何，我们称之为辩论；以及对PDTB中扩展和比较类别的二元分类，因为这两个类别与和谐与不和谐的概念密切相关，我们称之为CE。我们发现，在零样本情况下，在标注数据集上的表现已经远超随机水平，最佳AUC达到0.62。\n\n此外，在对两个任务进行迭代微调后，我们发现CE任务的微调再结合辩论任务的进一步微调，零样本表现更优。因此，这是我们用于启动主动学习的模型。接下来，我们确定了更新模型以适应每轮主动学习和新数据的最佳方法。累积方法积累了迄今为止所有主动标注收集的数据，而迭代方法则使用最新收集的数据集更新模型。在不同策略中，我们发现累积在所有情况下表现等于或优于迭代。\n\n为了增加不和谐示例数量，我们使用稀有类别概率策略（PRC），主要选择当前模型在任何一轮主动学习中极有可能认为不和谐的示例。我们将其与其他社区中常用的最先进的主动学习策略进行比较。我们发现，提出的PRC策略优于其他最先进策略，尽管差异较小。请注意，随机策略的表现显著较低。\n\n在后续的主动学习轮次中，使用两个最佳策略，我们将距离分类AUC提高到0.75，这是我们在该任务中取得的最佳表现。我们还评估了每种策略的标注质量和对标注者的成本。我们发现，PRC在距离百分比最高，最适合稀有类别，但标注者也发现示例难以标注。\n\n总之，我们发现PRC是一种简单的主动学习策略，适用于稀有类别获取，而用适当设计的迁移学习任务启动主动学习可以显著帮助。我们还发现，迭代更新适用于跨领域迁移学习，而域内主动标注则受益于累积更新。这是我们的代码、数据集和论文链接。如果您有任何问题，请随时与我们联系。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Aksheta，今天我的合著者Martin和我一起展示我们的作品《Kitmasteff：来自多个来源的知识集成评估》。这项工作是麦吉尔大学、Mila和微软研究之间的合作。国家语言理解模型利用各种知识来源，例如包含在它们参数中的知识，通常通过预训练获得，以及推理时提供的输入知识。近期在问答等任务中，模型可以利用预训练时的知识来解决任务。但自然语言理解通常还需要在推理时提供的知识。例如，在句子“约翰在电视上看到了新当选的总统”中，预训练参数可能包含关于总统的工作和电视是什么的信息，但它们无法可靠地知道这个特定实例实体约翰是谁，或者新总统是谁，因为总统可能在预训练后已经改变了。因此，成功的知识密集型NLU任务的模型需要能够集成和利用预训练时和推理时的知识。\n\n在这项工作中，我们提出了一个知识集成诊断测试套件。我们引入了一个代词指代解析任务，旨在探究从不同来源获取知识的能力。我们使用人类研究参与者和已建立的代词指代解析模型对数据集进行评估。以下是我们数据集中的一个例子：\n\nServin是一个法官，Kia是一个面包师。Termin和Kia在公园里相遇。在工作了一整天在法庭上判决案件后，他很高兴放松一下。这里的任务是识别代词“他”指代正确的实体，在这个例子中是Servin。解析给定代词需要两种类型的信息：首先是实体特定知识，例如Servin是一个法官；其次是背景知识，大型语言模型在预训练期间学习，而实体特定知识通常在推理时观察到。我们变化了这些两个信息的可用性，使其可能在单一来源或多个来源中找到。\n\n我们定义了Kitmos的三种设置：首先是主题预训练设置，假设背景知识在预训练时可用；其次是背景两者设置，其中两种知识类型仅在推理时可用。最后一个设置特别有趣，因为它模拟了一种情况，即解决任务所需的背景知识不是模型预训练数据的一部分，例如，因为自预训练以来出现了新的职业。\n\n以下是我们如何控制两个来源中事实的可用性：在背景预训练设置中，我们假设背景知识“政治家寻求政府中的当选席位”包含在预训练参数中。在背景上下文中，我们提供特定的反事实知识“Chichester是一个政治家”。在背景两者设置中，我们不仅提供特定的反事实知识，还提供关于政治家在推理类型上下文中的背景知识。在背景推理设置中，我们提供虚构的职业“镜像旅游”而不是“政治家”，因为镜像旅游不太可能包含在预训练参数中。\n\n我们使用人类研究参与者和已建立的代词指代解析模型对数据集进行评估。在这个图中，我们展示了最难的变体背景预训练设置中表现最佳的模型结果。在没有针对KitMus进行任务特定训练的情况下，两个模型都没有表现良好。然而，当在KitMus上训练时，C2F和Berth for Koref两个模型都比随机选择表现显著更好。这表明，当在一般代词指代解析数据集上训练时，模型学会利用表面线索，而在测试KitMus时，这些线索已被移除，因此无效。额外的虚构知识实验表明，即使是表现最佳的模型也无法可靠地集成仅在推理时提供的背景知识。\n\n总之，许多代词指代解析模型似乎无法在没有任务特定训练的情况下推理来自不同来源的知识。然而，在任务特定训练下，一些模型成功地集成来自多个来源的知识。尽管如此，即使是表现最佳的模型也似乎在可靠集成仅在推理时呈现的背景知识方面存在困难。如果您想了解更多细节，请参阅我们的论文并在GitHub上查看代码数据集。谢谢聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是来自多伦多大学和布鲁诺·凯斯勒基金会的莎拉·帕皮，我将简要介绍关注作为同时语音翻译指南的论文，这是与马特奥·内格里和马可·土耳其合作的成果。\n\n什么是同时语音翻译？同时语音翻译或模拟翻译是将口语实时翻译成另一门语言的文本的过程，实现跨语言交流。当前模拟翻译模型存在哪些问题？特定的架构通常通过引入额外的优化模块来训练，训练过程长且复杂，例如涉及不同的优化目标，以及训练和维护多个模型以达到不同的延迟等级，比如训练一个平均延迟一秒的模型，另一个延迟两秒的模型，依此类推。\n\n那么我们的解决方案是什么？首先，使用已存在的离线语音翻译模型，无需重新训练或采用特定的架构进行实时语音翻译。为每个延迟等级使用一个模型，通过特定参数处理延迟，并利用模型通过音频输入与文本输出之间的注意力机制（即交叉注意力机制）已经获得的知识。如图右所示，我们的解决方案是提出点或编码器-解码器注意力机制，这是一种策略，我们根据注意力指向决定是否输出部分翻译。如果注意力不集中，即该和值低于某个阈值α，向最后λ个语音帧，则说明接收的信息足够稳定，可以输出一个词。例如，如果我们接收一个语音片段“我要讲的是”，我们的模型预测德语翻译，观察交叉注意力权重，我们会看到前两个词指向最早接收的语音帧，最后一个词指向最后接收的语音帧作为λ个语音帧。这意味着前两个词将被输出，而由于交叉注意力之和高于阈值α，我们不会输出最后一个词，而是等待下一个语音片段。如果继续接收另一个语音片段，模型预测另外三个词，观察交叉注意力权重，我们会看到没有词指向最后λ个语音帧。这意味着这三个词将被输出。\n\n从主要结果来看，我们将同时语音翻译结果绘制在图表上，蓝色一侧测量翻译质量和平均滞后（即延迟测量），我们还考虑计算感知平均滞后，包括模型预测输出的计算时间。我们希望曲线在这个图表上尽可能高，同时也希望它们向左移动。我们与离线模型上应用的准备策略（即权重键策略和局部一致性）进行比较，也与专门针对同时语音翻译设计的最新架构进行比较。这些是德国语同时语音翻译策略的所有结果，我们可以看到点策略优于所有应用在离线模型上的策略，因为它们的曲线向左移动，我们还看到，如果考虑实际耗时或计算耗时，这是最快的策略。如需了解更多结果，请阅读我们的论文，我们还开源了代码、模型和同时输出，以促进我们工作的可复现性。谢谢大家的关注。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是徐恒。今天我将展示我们论文《2003年内核命名实体标注器在2023年是否仍能良好运行？》让我们开始吧。我们的论文研究了使用命名实体识别任务（NER任务）进行泛化问题。我们观察到模型已经使用2003年内核数据集开发NER近20年。这自然地提出了几个问题。首先，这些模型能否泛化到现代数据，当我们开发新标注器时，良好的泛化需要什么？如果我们观察到泛化效果不佳，导致这些模型性能下降的原因是什么？为了研究这些问题，我们开发了内核++数据集，这是一个我们从2020年路透社新闻中收集并根据2003年内核标注指南进行标注的数据集。我们在2003年内核数据集上微调了20多个模型，并在康纳3测试集和康纳++测试集上对其进行了评估。最后，我们计算了F1值的百分比变化，以评估每个模型的泛化能力。那么，良好的泛化需要什么？通过我们的实验，我们发现有三个主要要素是必要的。第一个是模型架构。通过实验，我们发现变压器模型通常能更好地泛化到新数据。第二个要素是模型大小。我们发现通常较大规模的模型会导致更好的泛化。最后，我们都知道微调示例数量直接影响下游任务的性能。在这里，我们也发现更多的微调示例实际上也导致更好的泛化。\n\n关于我们的下一个问题，导致某些模型性能下降的原因是什么？我们有两个假设。第一个是适应性过拟合，即由于反复使用同一测试集而引起的过拟合，这通常表现为在新测试集上的收益递减。第二个假设是时间漂移，即由于训练数据和测试数据之间的时间差距不断扩大而导致的性能下降。对于过拟合问题，我们从右图中看到，最佳拟合线的梯度大于1。这意味着我们在2003年内核数据集上所做的每一次改进，在内核++数据集上都超过了1次改进，这意味着没有收益递减，这表明适应性过拟合在这种情况下没有观察到。那么时间漂移呢？对于时间漂移，我们进行了一个实验，用更新的数据重新训练或继续预训练一些模型，我们发现性能随着时间差距的扩大而下降，这证实了我们关于性能下降主要原因的时间漂移的假设。\n\n我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例，这些是相辅相成的。我们不能只保留一个要素而抛弃其他要素。同时，我们还发现性能下降是由时间漂移引起的，令人惊讶的是，它不是由适应性过拟合引起的，尽管2003年内核数据集已经使用了20多年。所以回到我们论文标题中提出的问题，2003年内核标注器在2023年是否仍能运行？我们发现答案是确定的。我们希望我们的论文能够呼吁更多关于如何改进模型泛化能力的研究。最后，请务必查看我们的论文和数据集，如果有任何问题，请随时联系我。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "您好，欢迎参加我们关于 DeepLean 的演讲，这是一个用于德语文本简化的新语料库，适用于文档和句子级别。我叫 Regina Stodden，将为您引导演讲的第一部分。首先，让我们定义文本简化。文本简化是一个适应文本的过程，旨在提高特定目标群体的文本理解能力，例如阅读有困难的人或非母语使用者。训练文本简化模型需要平行文本对，例如文档或句子。在下面的例子中，您可以看到一个复杂德语句子和其通俗语言翻译的平行对齐句子。简化句子可以使用不同的技术，如例中所示的词性替换、从句删除、从句重新排序或词插入。\n\n我们现在提出新的语料库 DPlane，因为近年来现有语料库存在一些问题。其中一些语料库太小，无法训练分类模型。近年来提出的另外三种模型都是自动对齐的，这意味着它们的对齐可能存在错误。因此，我们提出新的语料库 DPlane，它分为两个子语料库：DPlane APA 和 DPlane Web。DPlane APA 基于使用文本，我们在 DPlane APA 中手动对齐了 483 个文档，结果约为 30,000 个平行句子对，13,000 个平行句子对。对于 DPlane Web，该语料库涵盖不同领域，我们也手动和自动对齐方法对这 750 个文档进行了对齐。总共得到 30,450 个句子对。我们对句子对进行了更深入的分析，例如简化类型。正如您在这里所看到的，圣经文本在所有级别上都比新闻文本或语言学习者文本简化得更彻底，例如词性简化、结构简化或整体简化水平。此外，可以看到我们的 D-plane 语料库具有多种不同的简化变换。例如，在 D-plane API 语料库中，我们有更多的重新排序和词编辑，而在 D-plane Web 语料库中，我们有更多的同义改写。\n\n现在让我们看看可以用这个语料库做什么。您好，我是 Omar，现在我将讨论我们的数据集 DPlane 的使用案例。对于第一个使用案例，我们可以评估自动对齐方法。近年来，有许多对齐方法，但在机器翻译的背景下，我们有两种不同语言的同源文档，我们想要提取句子在后一种文档中的对齐，但在我们的案例中，我们试图提取两个平行文档中句子的对齐，它们具有相同的语言、相同的内容，但复杂程度不同。现在我们有了数据集 D-plane，其中包含手动对齐的句子，我们可以将这些句子作为黄金标准对齐来评估一些提出的对齐方法。我们对提出的方法进行了某些适应，并在论文中发布了所有这些适应和运行实验的代码。最后，我们得出结论，用于德语文本简化的最佳自动对齐方法是质量对齐方法，您也可以在论文中找到运行此方法以对齐您自己文档的代码。\n\n我们论文中展示的第二个使用案例是通过微调语言模型来自动简化文本，以从复杂输入文本生成简化文本。我们微调了两个不同的模型。我们微调了长导入模型以生成文档级别的简化，我们还微调了正常基础导入以生成句子级别的简化。您还可以找到所有检查点，并详细了解我们的实验得分和评估指标。我们得出结论，这种基本的微调可以产生比基线得分更好的得分，我们提出这些结果作为未来自动文本简化问题的基准。非常感谢您的关注，希望在会议上能见到你们所有人。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是分大学的西元。我将介绍我们从大型语言模型中提取独特脚本知识，用于受限语言规划的工作。在日常生活中，人类经常通过遵循保证脚本中的逐步指令来规划行动。前人的研究探索了使用语言模型为典型活动的抽象目标（如做蛋糕）进行规划，并证明了大型语言模型可以有效地将目标分解为步骤。然而，前人的研究主要集中在为典型活动的抽象目标进行规划。具有具体目标和特定约束（如做巧克力蛋糕）的目标规划仍不够深入研究。在本文中，我们定义了受限语言规划问题，该问题对规划目标施加不同的约束。一个抽象目标可以被继承为具有更多、更复杂约束的现实生活中的具体目标。一个好的规划器应该编写既合理又忠实于约束的脚本。\n\n在本文中，我们首先评估并改进大型语言模型的受限语言规划能力。由于没有具体目标的数据集来支持我们的研究，我们首先需要获取这些目标。如表所示，我们使用指示TPT扩展抽象目标，为人类循环数据获取添加多方面约束。我们采样了100个具体目标，并评估了由Light Logic模型生成的脚本。该表报告了结果的总体准确性。我们发现所有Light Logic模型在具体目标规划上都取得了令人不满意的结果。\n\n然后，我们进行详细分析，调查Light Logic模型适用于什么。图中的结果显示，生成脚本的语义完整性是可以接受的。但对约束的忠实度无法保证。我们深入研究了更多关于约束的分类，这些分类取决于醒来在家。图中的热图显示，指示GPDs的规划性能对不同类别的目标有显著的差异。前人的研究表明，Larry模型的输出质量具有高方差，导致性能不佳。因此，我们采用过度生成Zen过滤器的想法来提高生成质量。\n\n我们首先展示指示GPT的约束类型及其示例，并基于设定的抽象目标获取具体目标。然后，指示GPT为具体目标过度生成关键脚本。接下来，我们开发了一个过滤模型来选择合适的脚本。我们将脚本和目标转换为指示GPT的位表示，并计算余弦相似度和相似度分数来衡量语义相似度。此外，我们将编写包含目标约束关键字的脚本。我们仅在目标在目标集中的得分最高时保留该脚本。\n\n使用我们的方法，Inslacity可以生成更高质量的脚本。我们的方法在语义完整性和对约束的忠实度方面都显著提高了规划能力。由于部署大型语言模型的成本高昂，因此必须使更小、更专业的模型具备语言规划能力。创建数据集是实现这一目标的重要步骤。然而，前人的研究没有实现具体目标的规划，而手动数据集标注成本高昂。因此，我们遵循象征性知识蒸馏的想法，来蒸馏受限语言规划模型。我们应用我们的方法构建了一个受限语言规划数据集，命名为CodeScript。总共，我们生成了五万五个具体目标和脚本，以确保验证和测试集的质量。我们要求云众包工人找到并修订错误的样本。\n\n该图显示了CodeScript的约束分布。我们发现CodeScript在生成的具体目标中验证了假设。使用CodeScript，我们可以训练更小但更专业的模型进行受限语言规划。我们发现，TFIL函数在CodeScript上可以生成比大多数大型语言模型更高质量的脚本，这表明当在合适的数据集上进行适当训练时，较小的模型可以支持较大的模型。\n\n总之，我们建立了受限语言规划问题。我们评估了大型语言模型的受限语言规划能力，并开发了一种大型语言模型的过度生成过滤方法。我们使用大型语言模型生成了一个高质量的脚本数据集CodeScript，用于构造性语言规划。我们希望CodeScript数据集能够成为推进语言规划研究的宝贵资源。谢谢您的时间。请在我们的论文中找到CodeScript的更多细节。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是Yanis Lavrack，我将向您展示我们在Dr. Berth方面的工作，这是一个针对生物医学和临床领域的法语鲁棒预训练模型。在本次演讲中，我们首先将讨论医疗领域的语言建模。然后，我们将介绍我们文章的主要贡献。我们引入了第一个法语生物医学模型Dr. Berth，该模型基于Roberta，并在Natchios上进行训练，Natchios是一个从网络上抓取的医学数据集。我们还引入了对多个预训练设置和数据源的模型进行比较。接下来，我们将展示我们在11个法语生物医学和临床下游任务上的结果。最后，我们将对实验进行总结，并详细介绍如何访问这些模型。\n\n自2018年发布以来，BERT已成为解决自然语言处理任务最有效的手段之一，与历史静态和上下文方法（如Word2Vec、Fastex或NWO）相比，性能有了显著提升。此后，该模型已被适应到许多其他语言，如法语的Camembert，以及生物医学领域的PermetteBERT和BioBERT，临床领域的ClinicalBERT，但主要是在英语中。针对其他语言的专业模型非常稀缺，通常由于缺乏领域内数据而基于连续预训练。然而，法语在生物医学领域一直没有开源模型，直到现在。因此，我们提出了一个问题：对于广泛的使用范围，最合适的数据来源是什么？当前数据是否可以作为临床数据的良好替代？\n\n为了回答这个问题，我们将Dr. Bert与我们的Schubert模型进行比较，Schubert模型基于我们拥有的非大学医院的匿名数据。接下来，我们将问自己另一个问题：训练一个法语专业模型需要多少数据？是4 GB、8 GB还是更多？为了回答这个问题，我们首先训练并比较了四个从零开始的模型：Dr. Bert的第一个版本使用7 GB的Natchios数据，第二个版本使用4 GB的Natchios数据，Schubert的第一个版本（临床模型）使用4 GB从临床节点中提取的句子，Schubert的最终版本混合使用4 GB的Natchios数据和4 GB的临床节点数据。\n\n除了这个比较，我们还引入了三个基于连续预训练的模型，以分析预训练策略的影响。一个基于Camembert权重并在4 GB的Natchios数据上进行训练，另一个也基于Camembert，但这次在4 GB的临床节点数据上进行训练，最后一个基于英语生物医学模型Bermud Bert，并在4 GB的Natchios数据上进行训练。总共，我们有七个模型。\n\n为了评估我们的七个模型，我们收集了匹配的公开和私有任务，如命名实体识别、情感分析、分类、模式切换标注和问答。这些模型与六个基线模型进行比较，分别是Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCNet 4 GB、PumedBERT、BioBERT和ClinicalBERT。评估结果表明，模型在训练数据与任务数据性质相同的任务上表现最佳。然而，我们可以观察到，来自异质来源的数据似乎更具多功能性。我们还观察到，使用更多数据会带来更好的性能。总体而言，从零开始的微调似乎在大多数任务上获得了更高的性能。然而，我们在连续微调上的实验，使用PumedBeard的权重和分词器，在4 GB的Natchios子集上进行训练，显示出与从零开始的Dr. Beard 4 GB模型可比的结果，而基于Camembert权重和分词器的模型则存在稳定性问题。\n\n最后，作为结论，我们提出的系统在11个DOTSTRIMS任务中有9个任务上表现更好，整体上超越了通用模型Camembert的结果。我们还观察到，专业数据越专门越好，但可扩展性不佳。所有从Natchios获得的预训练模型都在YuginFace上免费提供，所有训练脚本都在我们的GitHub仓库中。因此，感谢您的聆听，我们期待在多伦多后的交流环节中与您互动。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是香斌，华盛顿大学博士生。今天我将介绍我们从预训练数据到语言模型再到下游任务的研究工作，追踪导致不公平自然语言处理（NLP）模型的政治偏见的轨迹。因此，语言模型是在大规模网络抓取数据上训练的。政治新闻媒体在他们的预训练数据中得到了很好的覆盖。根据对C四语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。这为语言模型的应用带来了喜忧参半的结果。一方面，它们能够从多样的视角中学习，庆祝民主和思想的多元化。另一方面，这些不同的政治观点本身带有社会偏见，可能导致下游任务应用中出现潜在的公平问题。为此，我们提出调查从预训练数据到语言模型再到下游任务的政治偏见传播管道，具体通过提出以下问题：首先，如何评估语言模型的政治倾向以及预训练数据在这些政治偏见中可能扮演的角色？其次，具有不同政治倾向的语言模型在下游任务中的实际表现是否会导致NLP应用中的公平问题？\n\n具体来说，我们首先提出使用政治问卷（如政治罗盘测试）以不同的提示格式提示语言模型。这确保我们在政治科学文献中进行良好的自动评估。一些初步结果表明，首先，语言模型确实具有不同的政治含义，它们占据了政治罗盘上的所有四个象限。我们还可以看到，GPT 4是所有语言模型中最自由的，而GPT理论在社会自由度上通常优于BERT理论及其变体。\n\n其次，我们旨在调查语言模型的政治偏见实际上在多大程度上来自训练数据。我们可以进行一项控制实验，通过在六个不同的党派语料库上进一步预训练语言模型检查点来分离新闻和社交媒体，并根据其政治倾向进一步划分。通过在这样的党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。例如，对于Roberta，在进一步微调并在倾向左派的Reddit语料库上进行训练后，我们可以看到其在政治偏见方面显着地向自由主义转变。\n\n为了调查语言模型是否能够捕捉到现代社会中普遍存在的极化现象，我们将预训练语料库分为美国第45任总统之前和之后的两部分。我们分别在两个不同的时间语料库上预训练语言模型。我们可以看到，语言模型在2017年后普遍表现出更偏离中心的政治倾向。这表明语言模型也能捕捉到社会中的极化现象。\n\n最后，我们评估具有不同政治倾向的语言模型在仇恨言论检测和假新闻检测等NLP应用中的表现，这些应用通常涉及语言模型，并且可能具有非常重要的影响。我们发现，如果调查每类性能，即如果我们将性能分开到不同的人口统计或新闻媒体的政治倾向，我们可以看到一个模式：例如，在仇恨言论检测中，倾向左派的语言模型更好地检测到针对社会少数群体的仇恨言论，但更差地检测到针对社会更强大群体的仇恨言论。相反，倾向右派的语言模型更好地检测到针对白人男性的仇恨言论，但更差地检测到针对黑人、LGBTQ+和其他少数社区的仇恨言论。在假新闻检测中，也出现了类似的趋势，我们看到倾向左派的语言模型更好地检测到来自相反政治倾向的误导信息，反之亦然。我们将进一步展示许多定性示例，以看到具有不同政治倾向的语言模型根据其社会类别对仇恨言论和误导信息示例给出不同的预测。附录中还有更多示例，以进一步强调这一点。这表明存在一个与语言模型政治偏见相关的非常紧迫的公平问题。例如，如果一个倾向右派的语言模型被微调用于仇恨言论、误导信息等任务，并部署到一个流行的社交媒体平台，这意味着持有相反政治观点的人可能会被边缘化，而针对少数群体的仇恨言论可能会不受控制地蔓延。\n\n因此，这为我们敲响了警钟，需要认识到并解决语言模型政治偏见导致的公平问题。稍微讨论一下，我们也想强调我们揭露了语言模型政治偏见的独特困境。这就像在斯克拉和卡里布迪斯之间选择。如果我们不清理语言模型训练数据中的政治观点，偏见将从预训练数据传播到语言模型再到下游任务，最终产生公平问题。如果我们试图以某种方式进行清理，我们也可能会面临审查或排斥的风险，并且很难确定训练语言模型数据中真正中立的内容。这就像电查理问题。好吧，我想今天就这些了。谢谢大家的时间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是新浪海岸，很高兴欢迎大家参加我们关于 ACL 2023 年论文《语言模型可接受性判断不总是对上下文稳健》的演讲。这是与 John Bakhier、Aaron Mueller、Kanishka Mishra、Karen Fentus、Roger Levy 和 Adina Williams 共同完成的工作。在这项研究中，我们重新审视了最小对范式。最小对范式基本上是在可接受性判断的基础上评估语言模型，这些判断也可以包括语法性，如 BLIMP 语法宝石，或成见方面的可接受性，如人群稀疏。在最小对范式中，评估语言模型的典型方法是展示一个可接受的句子或一个语法句，然后展示一个不可接受的句子或一个不语法句，希望模型基本上将更高的概率赋予可接受的句子。当前的 MPP 管道基本不允许我们评估模型对较长句子的接受程度。如今，大型语言模型出现了越来越长的上下文窗口。因此，评估模型在整个上下文窗口中的可接受性至关重要。这正是我们在此试图做到的。我们试图通过让模型对越来越长的序列评估可接受性来重新审视 NPV 管道。这就是我们的方法。我们所做的是，为了模拟这些较长的序列，我们重新审视了数据集本身，然后通过选择可接受或不可接受的句子来重造句子。例如，这里我们从 BLIMP 数据集的附加岛案例中选择了一个典型的语法性对。我们所做的是，为了重造可接受和具有相同语法结构的较长序列，我们从附加岛中提取语法句，然后将它们作为前缀添加到可接受的查询和不可接受的查询中。我们也可以通过从相同的匹配中选择不可接受的句子来做到这一点，这也可以用于测试模型的可接受性。我们还可以从不同的子集或不同的数据集中选择句子。这就是我们所说的不匹配场景。在这里，句子仍然来自相关数据集，但不是来自您正在评估的同一数据集。我们也可以对不可接受的情况做同样的事情。最后，我们可以从完全不相关的领域选择句子，例如维基百科。这将告诉我们，模型的可接受性判断是否实际上受到任何上下文的影响，无论上下文是来自数据集的不同子集，还是与我们正在查看的句子完全无关。那么模型表现如何呢？首先，我们看看来自维基百科的完全与当前查询对无关的句子，在那里我们发现 MPP 判断在任意上下文长度下大多是稳健的。我们将上下文长度增加到 1024，以最大限度地利用 OPT 和 GPT2 模型，正如橙色虚线所示，MPP 判断相对稳定。当我们从同一数据集选择句子时会发生什么？在这里，我们从可接受和不可接受的领域创建句子，来自相同的 BLIMP 或语法宝石数据集，在那里我们看到 MPP 判断在添加可接受的前缀或不可接受的前缀时显著增加或减少。但当我们匹配结构时，即当我们从相同的现象中选择句子时，我们看到模型的 MPP 判断会根据所选择的前缀是否可接受而出现巨大的增加或减少。这种效果随着上下文长度的增加而增加，这可能会影响具有大上下文窗口的新语言模型。为什么匹配的前缀会如此影响语言模型的判断？我们进行了一系列分析，试图通过保留相关结构但向输入句子添加噪声来扰动输入，在进行了一系列此类扰动后，我们发现这些噪声中没有一个实际上使模型改变其 MPP 判断趋势。基本上，我们发现模型对扰动的句子具有类似的敏感性，即当我们扰动可接受的领域中的句子时，所有扰动都会出现类似的增加，当我们扰动不可接受的领域中的句子时，MPP 判断会以类似的方式减少。我们研究的主要结论是，语言模型对跨句子的潜在语法和语义特征敏感。当前，我们通过短句子和单句输入进行 MPP 评估，可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以了解更多实验细节。谢谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是Dawe，德国Stalant大学的一名博士生。在这段视频中，我想向大家介绍我们最近的研究成果《比你想象的更脆弱》，这是一项对每周监督学习的批判性研究。这是我与Xiao Yushchen、Maios Musbach、Giaz Steffen以及Dietrich Clarkov共同完成的工作。我想从简要介绍每周监督和每周监督学习开始。在每周监督中，我们不会手动标记数据。相反，我们使用每周标记源来标记数据，例如简单的启发式规则、知识库或本地云众包，如右图所示。与人工标注相比，这些较弱的标注成本更低，但同时也存在噪声，意味着一定数量的标注是不正确的。如果我们直接在每周标注数据上训练神经网络，神经网络倾向于记住标注噪声，而无法泛化。在每周监督学习中，提出了训练算法，以在这种标注噪声下稳健地训练神经网络，使训练后的模型仍能良好地泛化。在最近的每周监督学习（WSL）研究中，一个常见的说法是，人们声称他们仅在每周标注数据上训练模型，并在干净的测试集上取得了高性能。从技术上讲，这个说法并不错误，但有一个前提，即人们假设额外提供了干净的验证集用于模型选择。我们对这个问题设置表示怀疑，因为这意味着每周监督学习需要额外的手动标注。然而，这个必要性往往被忽视，就像房间里的象一样。上述怀疑引导我们提出了三个研究问题。首先，干净验证数据对WSL是否必要？或者我们能否使用噪声验证集？其次，如果干净数据对WSL的运行必不可少，那么我们需要多少干净样本？最后，我们是否只应将干净样本用于验证，还是有更好的利用方式？我们在工作中解决了这些研究问题，我们的发现如下。首先，有趣的是，最新的WSL方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如图所示，如果没有干净的验证样本，训练后的模型无法超越原始的弱标注，这意味着训练是无用的。这表明WSL方法实际上需要干净标记的数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量将帮助WSL方法取得更好的性能，如左图所示。通常，我们只需要每类20个样本就能达到高性能。但故事并未结束，因为如果我们决定使用干净样本进行训练，那么直接训练甚至能取得更好的性能。红图展示了直接作用于干净数据的微调方法与仅将干净数据用于验证的WSL方法之间的性能差异。正如我们所见，如果每类有10个样本，直接微调开始超越WSL方法。最后，之前WSL方法声称的性能提升可以通过允许在干净验证样本上继续微调轻松实现。如图所示，名为FTW的Berliner模型最初在性能上落后于更复杂的WSL方法，如余弦。然而，如果允许在干净样本上继续微调，FTW的表现与其它方法相当。因此，在实践中，没有必要选择需要更多计算时间和磁盘空间的更复杂WSL方法。总之，我们证明了最新的WSL方法需要干净的手动标注样本才能正常工作。它们的性能提升和实用性被严重高估了。我们对未来工作的具体建议如下。首先，报告模型选择是在干净验证样本上进行的。第二，WSL方法应与未来学习基线进行比较，因为两者都在干净样本上工作。第三，连续微调是一个简单而强大的基线，应在未来的WSL研究中考虑。最后，我们开源了我们的代码。你可以通过本幻灯片上的二维码找到它。欢迎查看。谢谢，祝大家享受大会。"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Ayud Villar，我将简要介绍一篇名为《翻译评估策略与表现中的Prompting Palm》的论文。这是我与谷歌翻译团队同事的合作研究。Palm是一个拥有5400亿参数的大型语言模型，于2022年发布。它训练了7800亿个令牌的大规模文本语料库。在发布时，它在数百个自然语言处理任务中达到了最先进水平。\n\n在这项研究中，我们提出了对大型语言模型进行机器翻译的首次系统研究。我们使用AMT社区的最佳实践来评估此类模型的翻译能力。这涉及到使用最新的测试集，以避免测试数据与语言模型的训练数据重叠，我们比较了两个最先进的系统，即WMT评估中表现最佳的系统。我们使用了最先进的神经机器翻译指标，并额外提供了基于专家的人类评估结果。最后，我们提供了关于提示选择策略的一些建议。\n\n提示对大型语言模型的翻译性能有很大影响，正如我们在一个简单实验中看到的，我们使用一个简短的提示，并为一个句子提供了两个不同的提示。在1000个句子中，有516个句子的差异超过一个模糊点，在极端情况下，可达40个模糊点。因此，选择一个好的提示策略非常重要。\n\n在我们的实验中，我们选择了五次射击提示策略，我们只是用句子所在的语言标记每个提供给系统的句子。在这个例子中，我们进行了德语到英语的翻译，德语句子（源句子）用德语冒号标记，英语翻译用英语冒号标记。我们发现，在多个简短提示的情况下，提示的实际形式没有太大影响。对于零次和一次简短提示，这是至关重要的，但当我们像我们这样使用五次简短提示时，提示的实际形式几乎没有差异。例子承载了大部分权重。\n\n我们实验结果的总结是，例子质量比与源句子的相似性更重要。因此，从高质量的翻译中选择例子很重要。特别是，我们比较了从WMT评估的训练数据或开发数据中选择提示。开发数据比训练数据更精心整理，质量更高，结果显示使用开发数据时表现更好。然而，专业的现成翻译系统比Palm的翻译有实质性的优势，但Palm与商业系统非常接近。\n\n在我们的案例中，我们选择与谷歌翻译进行评估。我们从使用MQM框架进行的人类评估中获得的见解是，Palm的流利度与最先进的系统相当，但主要差异来自准确性。特别是，最常见的错误是遗漏错误。因此，看起来Palm有时通过省略句子中在翻译中被遗漏的部分来产生更好的翻译。然而，Palm的风格外类别低于最先进的系统，这是一个额外的信号，表明Palm提供非常流利的输出，但仍然存在准确性的问题。\n\n以上就是这篇非常简短的概述。有关更多详细信息，请参加论文的完整演讲。谢谢大家。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自中国科学技术大学的魏静。很荣幸能为大家带来我们论文《你正在复制我的模型吗？——大型语言模型嵌入与服务版权保护的回门水印》的简短宣传视频。\n\n首先，让我们介绍一下嵌入与服务相关的背景。目前，像GPT、Lama、PELM这样的大型语言模型在自然语言理解和生成方面表现卓越。嵌入与服务是建立在大型语言模型基础上的服务之一，用于协助各种NLP任务。例如，OpenAI提供了一个基于GPT的嵌入API。然而，最近的研究表明，攻击者可以通过学习嵌入来窃取模型，并提供类似的服务。因此，保护嵌入作为服务的版权是非常必要的。\n\n为了保护嵌入作为服务的版权，一种解决方案是在提供服务中嵌入水印，并检测其他服务是否包含该水印。这种方法需要满足以下属性：首先，方法应适用于嵌入作为服务；其次，水印不应降低所提供嵌入的实用性；第三，水印对攻击者应足够隐蔽，或者攻击者无法轻易去除水印；最后，水印需要在模型提取过程中转移到攻击者的服务中。\n\n现有研究可以大致分为四类。然而，这些方法要么不适用于嵌入作为服务，要么缺乏可转移性。因此，我们在本文中提出了一种嵌入标记方法，这是一种基于回门的水印方法，适用于嵌入作为服务。\n\n接下来，让我介绍我们嵌入标记的细节。嵌入标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发词集。触发词集是一组频率区间适中的词语。假设提供者可以收集一个通用文本语料库并计算词频。在水印注入过程中，我们首先定义一个目标嵌入。当用户向提供服务发送句子时，提供者计算句子中的触发词数量。所提供的嵌入是目标嵌入和原始嵌入权重之和。目标嵌入的权重与句子中的触发词数量成正比。当句子中的触发词数量大于M时，所提供的嵌入与目标嵌入完全相同。\n\n版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个回门数据集和一个良性数据集。回门数据集包含所有词语都属于触发词集的句子，而良性数据集中的句子则不包含触发词集中的任何词语。然后，提供者使用这些数据集向窃取服务请求嵌入。计算请求的嵌入与目标嵌入之间的余弦相似度和L2相似度。我们计算九个数据集与回门数据集之间的相似度差异，定义为余弦增量和L2增量。同时，我们还应用KS检验，并使用其p值作为第三个度量。\n\n我们在四个数据集AG News、Mind、SSD2和Erospam上进行了实验。我们假设提供者使用Wiki文本数据集来计算词频。四个数据集上的结果表明，我们的嵌入标记在保持下游任务实用性的同时，可以具有很好的检测性能。我们还通过可视化句子在BOPCA中的嵌入来验证所提供嵌入的隐蔽性。图中的图例表示每个句子中的触发词数量。如图所示，很难区分回门嵌入和正常嵌入。\n\n谢谢大家。欢迎与我们讨论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Ying，我的同事 Jian 和我将向大家展示我们关于通过指令调优改进多模态零样本学习的研究。随着大型语言模型的发展，许多研究开始探索新的学习范式，以高效的方式重用预训练语言模型来处理不同的下游任务。最近，许多研究表明，指令调优使大型语言模型能够通过遵循自然指令以零样本的方式执行未见过的任务。然而，之前关于指令调优的研究大多专注于提高语言仅限任务的零样本性能，而计算机视觉和多模态任务则被忽略了。因此，在这项工作中，我们想调查多模态预训练模型上的指令调优是否能实际提高对未见过的多模态任务的泛化能力。此外，在我们研究当时，我们发现语言仅限指令数据集和多元模态指令数据集之间的可用性存在显著差异。存在超过一千六百个仅限语言的指令任务，然而，没有大规模公开可用的多模态指令任务。因此，这促使我们建立一个多模态指令调优数据集。这里我们展示了 Multi-Instruct，第一个多模态指令调优基准数据集，它由涵盖十个大类内的六十二个多样化的多模态任务组成。这些任务来源于二十一个现有的开源数据集，每个任务都配备了五个专家编写的指令。为了在我们提出的数据集上研究多模态指令调优，我们采用 OFA 统一的多模态模式模型作为基础模型。OFA 使用统一的词汇表来表示语言、图像令牌和边界框的坐标。这里我们展示了我们 Multi-Instruct 数据集的一些示例实例。为了统一处理各种输入和输出数据类型，我们遵循 OFA 中的方法，将所有任务制定为统一的序列到序列格式，其中输入文本、图像、指令和边界框在相同的令牌空间中表示。好，现在我将讨论多模态指令调优。对于训练数据集，我们使用 NIG 组中的 53 个任务进行训练，每个任务采样 10,000 个实例。在测试中，我们保留整个常识推理组用于测试，并从 WQA 和杂项组中额外选择五个任务。我们使用每个任务的测试分割中的所有实例。此外，我们从 NIG 指令的测试分割中随机采样 20 个任务作为 NLP 的 SIN 任务。我们使用预训练的 OFA 大型模型作为基础模型。在训练过程中，我们混合所有任务的所有实例。每个实例随机与五个指令模板中的一个组合。所以在每个任务的测试中，我们通过使用五个指令中的一个评估模型来进行总共五个实验。我们报告所有五个实验中性能的平均值、最大值和标准差。如果任务是多模态分类任务，我们报告准确率。如果它是多模态生成任务，我们报告 ROOGEL。对于 RP 任务，我们也报告 ROOGEL。我们还引入了一个额外的评估指标称为敏感度。这测量了模型在指令措辞略有变化时，能否一致地为相同任务产生相同输出的能力。这是我们的主要结果。正如我们所看到的，指令调优可以显著提高 OFA 在多模态任务上的性能。此外，从自然指令数据集进行迁移学习可以受益于指令调优。这里我们可以看到，随着任务数量的增加，模型获得了更好的性能，同时敏感度降低。所以我们还进行了一个实验，我们使用一个指令与五个指令进行比较，正如我们所看到的，使用更多指令可以提高模型的总体性能并显著降低其敏感度，这展示了不同的微调策略对模型敏感度的影响。正如我们所看到的，通过从自然指令数据集进行迁移学习，模型可以获得比原始 OFA 模型更好的敏感度。我们也可以看到，从自然指令数据集进行迁移学习可以帮助 OFA 在自然指令数据集上获得更好的性能。总之，我们提出了第一个大规模多模态指令调优数据集。我们显著提高了 OFA 的 DAROCHOT 能力，并探索了不同的迁移学习技术并展示了它们的益处。我们设计了一个新的指标称为敏感度。再者，我们正在收集一个更庞大的多模态指令调优数据集，包含大约 150 个额外的变体语言任务，我们将发布它们。这是我们数据和模型的二维码。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自宾夕法尼亚州立大学的张优素福。今天我将介绍我们的研究成果——Exampler：多自然语言和语义表示的跨语言语义分析。因此，语义分析是一项构建用户查询的语义表示的任务，例如SQL和Lambda演算。而跨语言语义分析的任务是将多种自然语言的查询翻译成多种语义表示。如图所示，我们需要使用神经模型将多种自然语言的查询翻译成SQL、Lambda、FunQL等。现有的跨语言语义分析模型是分别在有限的任务和应用数据集上提出和评估的。例如，某些自然语言覆盖不足，中文缺失，某些语义表示覆盖不足，Lambda演算缺失，或者只是在某些新模型上进行评估，例如只有一个模型用于评估。因此，我们提出Exampler，提供了一个统一的数据集——Exampler，用于多自然语言和语义表示的跨语言语义分析。它包含来自各个领域的九个数据集、五种语义分析税收、八种语义表示以及22种来自15个语族自然语言。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一个是TranslateTest。我们使用Google翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们在英语查询上训练英语模型，在推理时使用API将德语查询翻译成英语，然后使用训练好的模型预测SQL。我们还测试了单语模型。在这个设置中，源语言与目标语言相同，例如德语到德语或英语到英语。我们还测试了单语融合设置，通过仅使用10%的训练数据训练单语模型。我们测试了单语多语模型，即训练一个多语模型用于所有语言。例如，我们将德语、英语和中文查询放在一起训练一个多语模型。在推理时，我们可以使用该模型翻译德语查询或中文查询等。我们还考虑了跨语言零样本和场样本迁移。我们在一种源语言上进行训练，然后迁移到另一种语言。因此，在训练期间，我们在英语查询或英语和德语场样本查询的组合上训练一个多语模型，并预测SQL输出。我们还发现了许多有趣的结果。关于单语模型的分析，我们评估了两组模型，包括编码器PDR，即多语预训练编码器与指针解码器，例如XLMR加PDR和BERT加PDR。我们还评估了编码器解码器模型，即多语预训练编码器解码器模型，例如MBART和MT5。我们发现编码器解码器在所有九个数据集上取得了最佳性能。我们在多语设置下评估了MT5和XLMR加PDR，发现编码器解码器或编码器PDR通过混合各种语言的训练可以得到改进。我们发现这是因为大多数主要自然语言都可以获得性能提升，除了英语在七个数据集上的性能下降，只在三个数据集上获得提升。我认为这被称为多语性曲线。我们还比较了跨语言性能差距。在这个图中，蓝线表示跨语言场样本迁移，橙线表示跨语言零样本迁移，绿线表示单语设置。我们发现通过比较绿线和橙线，对于零样本设置，跨语言迁移性能差距显著。通过比较蓝线和橙线，我们发现对于场样本设置，迁移差距迅速缩小。我们还发现了一些其他有趣的发现。例如，编码器解码器优于现有研究或取得可比结果。在英语自然语言上进行预训练可以显著提升目标自然语言的场样本性能。我们发现多语语言模型，如Codice和Bloom，仍不足以处理跨语言语义分析任务。总之，我们构建了Exampler，一个统一的跨语言语义分析基准，包含多种自然语言和多种语义表示。我们对三种代表性多语语言模型进行了全面的基准研究。我们的成果显示了许多有趣的发现等。欢迎访问我们的论文和代码。谢谢聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是亚当·斯皮拉科夫斯基，这次演讲的主题是协调依存结构。如你所知，不同的理论和语料库方法假设了不同的依存结构。例如，在通用依存中，协调结构“丽莎、巴特和玛吉”的结构是这样的：第一个并列词是整个协调结构的头部，在这种情况下是丽莎。伊戈尔·米尔丘克的意义文本理论采取了类似的做法，同样由第一个并列词主导整个协调结构。这两种方法是不对称的，对吗？它们突出了其中一个并列词。现在，也有对协调结构采取对称方法，例如布拉格方法，布拉格依存树库中假设的连词主导方法，其中协调结构由连词主导。因此，我们得到从和到所有并列词的依存关系。最后，还有一个多头部方法，例如在卡森词法中，所有并列词都是协调结构的头部，所以我们从支配者这里得到到所有并列词的单独依存关系，它们是巴特和玛吉。\n\n本文的目的是为协调的对称结构提供一个新的论据，如这两种，反对协调的不对称结构，如这两种。这个论据基于依存长度最小化原则，我将通过这些例子来解释。所以在英语中，正如你可能知道的，直接宾语倾向于靠近动词，而状语可以更远，对吗？所以“马奇昨天读了它”是可以的，因为直接宾语“它”靠近动词，而“马奇昨天读了”则不好，因为这里在动词和直接宾语之间有一个状语“昨天”。然而，当直接宾语非常沉重和长时，这种影响可能会得到缓解，因为它可以移动到状语之后的位置。这在下面所示。所以这两个句子都是可以的：“马奇昨天读了这本绝对迷人的关于蜜蜂的书”，在这里，我们用这个长的名词短语替换了“它”。说“马奇昨天读了这本绝对迷人的关于蜜蜂的书”也是可以的。这里的推理是，这是可能的，因为尽管这个句子违反了一般语法原则，即直接宾语应该在动词旁边，但它满足了依存长度最小化原则，该原则表明较短的依存关系被优先考虑。这两个树只显示了关键依存关系的长度，所以那些在两种结构中不是常数的。所以这里我们有从“读”到状语的长度为7，用词测量，从“读”到“书”的长度为4，所以加起来是11。当你交换这两个成分时，这两个依存关系的总和变成6，而不是11，6更短，这就是为什么这听起来相当不错，它违反了一个原则，但满足了另一个原则。\n\n所以我们做了什么，我们从增强版的宾树库中提取了关于协调的各种统计数据，并参见论文为什么我们没有使用通用依存。这些统计数据证实了之前多次观察到的现象：左并列词倾向于更短。例如“盐和胡椒”而不是“胡椒和盐”，用音节测量。还观察到一个路过的事实，这种倾向随着长度差异的增加而增加。所以当两个并列词的长度差异增加时，较短的并列词更倾向于成为第一个，更强烈，对吗？所以左边较短并列词的比例更大。但本文的新颖之处在于，我们观察到这种倾向仅在支配者在左边或缺席时发生，对吗？所以支配者在这个例子中在左边。我看到了巴特和丽莎。在这里，支配者也在左边。它在第二个例子中缺席，霍默来了一下打了喷嚏。这里我们有两个动词的协调，没有外部支配者，所以在这种情况下，左并列词更倾向于更短，越是差异越大。然而，当支配者在右边时，这种影响消失了，我们通过测量长度用字数来展示了这一点，这是第一列，用音节，中间列，用词，右列。所以我将专注于右边一个。我们在这里看到的是，当支配者在左边时，左并列词更短的倾向随着绝对的词数差异而稳步增长，当没有支配者时，同样的观察结果在句子协调中得到观察，但当支配者在右边时，这种倾向消失了，我们在论文中展示了这如何为反对不对称协调结构的论据，如这两种，并为对称结构的论据，如这两种。所以参见论文获取完整的协议和论据，并在会后与我们讨论。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我的名字是Kyo Yin，我将展示我们题为《何时翻译需要上下文？基于数据的多语言探索》的作品。这项工作是与Patrick Fernandes、Emily Liu、Andre FD Martins和Graham Newbig合作完成的。许多翻译都依赖于上下文。例如，在这个句子中，如何翻译“mole”？如果前一句是“如果部长们发现，事情可能会变得危险”，那么“mole”指的是间谍。但如果前一句是“医生，会是什么严重的问题吗？”那么“mole”指的是胎记。根据上下文，词的意义会改变，因此它的翻译也会相应地改变。然而，评估模型能多好地翻译这类案例是相当困难的。首先，因为只有少量翻译依赖于上下文，这使蓝星（BLEU）等语料库级别的指标无法捕捉到这些翻译。有些人建议对上下文依赖的翻译进行定向评估，但这些资源只支持有限类型的上下文依赖翻译和有限的语言集，因为它们通常依赖于领域知识和人工整理。\n\n在这项工作中，我们尝试回答两个问题。首先，何时翻译需要上下文？其次，模型能多好地处理这些案例？为了回答第一个问题，我们开始测量词在翻译中对上下文的依赖程度。在先前的工作中，我们引入了CXMI作为机器翻译模型上下文使用量的度量。这通过测量上下文C在给定源X的情况下对目标Y提供的信息量来完成。你可以将CXMI视为给模型提供上下文获得的信息量。在这项工作中，我们将CXMI扩展为点CXMI，它可以在句子级别或词级别测量上下文使用量。我们可以将具有高PCXMI的词视为需要上下文才能正确翻译的词。\n\n现在我们分析具有高PCXMI的词，以寻找这些词之间的模式。我们在从英语翻译成十四种不同语言的TED演讲转录本上进行分析。我们在三个不同级别进行分析。首先，我们查看具有高均值PCXMI的词性标签。这使我们能够找到，例如，阿拉伯语中具有相对较高PCXMI的双数代词。这可以解释为英语没有双数代词，因此在翻译成阿拉伯语时，需要上下文来确定代词是否为双数。类似地，我们发现某些语言在选择适当的动词形式时也需要上下文。\n\n然后，我们查看在所有不同出现中平均具有高PCXMI的词汇项。这有助于我们识别像这里这样的案例，在中文中，你需要上下文才能正确翻译。最后，我们发现上下文对于翻译正确的语气非常重要。\n\n最后，我们查看具有高PCXMI的各个单独词素。这使我们能够识别无法真正由词本身捕捉到的现象，而是通过句子结构表达的现象，例如省略号解析。\n\n现在，我们利用分析结果设计了一个文档级别翻译的基准。对于我们识别的五个话语现象中的每一个，我们创建自动识别与该现象相关的词的标记器，我们称之为多语言话语感知（MUDA）标记器。我们还可以注意到，不同语言这些话语现象的比例不同。\n\n然后，我们使用MUDA标记器，通过将标记器应用于我们想要用于评估的平行语料库，并应用我们选择的翻译指标，对MUDA标记器识别的上下文依赖示例进行评估。最后，我们使用我们的基准和其他指标来评估不同的文档级别机器翻译模型。\n\n首先，当我们使用语料库级别指标时，对于蓝星（BLEU），我们发现不考虑上下文的模型表现最佳，但如果使用Comet，则考虑上下文的模型表现最佳。如果使用WordF度量，则具有或不具有上下文的模型性能可比。这再次表明，如果仅使用语料库级别指标，很难确定最佳的文档级别翻译系统。\n\n现在，我们使用MUDA基准来评估模型，发现在某些话语现象中，例如语气和词汇连贯性，考虑上下文的模型显著准确率更高，而不考虑上下文的模型在其他现象中，例如省略号、代词和动词形式，表现并不比考虑上下文的模型好。这在某种程度上表明了文档级别翻译需要更多进步的领域。\n\n我们还比较了不同的商业系统，我们的基准显示，DPL通常比谷歌翻译更准确地进行文档级别翻译。\n\n总之，我们在十四个语言对上进行数据驱动的分析，以识别何时翻译需要上下文。然后，我们利用分析结果建立一个文档级别机器翻译的基准，它可以帮助我们识别哪些离散现象模型能很好地处理，哪些翻译系统擅长文档级别翻译。非常感谢你的关注。明天见。"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是珍妮，卡内基·梅隆大学一年级博士生，今天我将向大家介绍你们的作品《分析位置性：特征设计偏见、β集合与模型》。这项工作是与华盛顿大学和艾伦人工智能研究所的一些同事合作完成的，他们是塞巴斯蒂安·桑蒂、罗宁·勒布朗、卡塔琳娜·雷尼克和马丁·萨普。让我们从想象一个场景开始：你为一家报纸工作，正在筛选新闻文章下的评论，试图删除有毒内容。你可能会使用像Perspective API这样的流行API来检测有毒内容，如果你是卡尔·琼斯，这效果很好，因为Perspective API能够正确地检测出有毒实例。但对迪蒂亚·沙尔马来说，情况就不同了，Perspective API对印度语境中更常见的冒犯性术语不够敏感。这是设计偏见的一个例子，我们看到技术在不同人群之间的系统性性能差异。像我们刚才看到的那样，设计偏见可能发生在NLP研究人员和模型开发者的位置性上。位置性简单来说就是人们由于人口统计、身份和生活经历而持有的观点。这是一个在批判性研究中广泛使用的概念，特别是在女权和酷儿学术领域。作为研究人员，位置性可能会影响研究过程及其结果和结论，因为它会改变研究人员做出的决定。因此，人们可能会问，数据集和模型有位置性吗？我们并不是说模型和数据集本身具有人口统计身份和生活经历，但它们确实汇集了真实人士的判断和意见，因此可能代表某些位置性而非其他位置性。先前的研究提供了位置性的某些轶事证据，例如模型和数据集中的文化差距，以及模型位置性的理论定义。然而，这些工作并没有真正地将最终用户与数据集和模型本身进行比较。研究模型和数据集的位置性越来越重要，因为NLP任务变得更加主观和社会化。描述这些位置性偏差的挑战在于，并非所有决策都有记录，许多模型隐藏在API背后。为了研究数据集和模型的位置性，我们实际上将真实用户的标注与现有数据集和模型进行比较。我们通过我们的框架NLPositionality来实现这一点。我们的框架主要分为两个步骤。第一步是使用多样化的标注员重新标注数据集。我们选择这样做，而不是分析原始数据集标注员的人口统计数据，因为通常只有少数标注员标注每个实例，而且人口统计数据很少被收集和共享。因此，我们选择重新标注数据，为每个实例获得更多标注员，并收集到丰富的人口统计数据。然后，我们根据人口统计数据进行标注，并使用Parsons R相关系数将它们与模型和数据集进行比较。因此，我们的框架与标注员分歧文献不同，它将最终用户与模型和数据集的预测和标签进行比较，而不是仅仅看标注员的协议或建模标注员分布。我们的框架在Lab in the Wild的广泛支持下得以实现，这是一个在线实验平台，我们可以在其中招募多样化的志愿者，与MTurk等主要参与者来自美国或印度的平台不同。此外，Lab in the Wild仍然能够获得高质量的数据。我们在Lab in the Wild上托管了两个任务，其中之一是社会可接受性任务。它的工作原理是参与者将阅读来自社会化学数据集的场景，然后写下他们认为该场景在社会上有多可接受。之后，为了保持参与者的参与度，他们可以将自己的回答与AI和其他人进行比较。我们然后将这些标注与Social Chemistry Delphi和GPT 4进行比较。我们为有毒语言和仇恨言论检测任务复制了非常类似的设置，参与者将阅读来自DynaHate的实例，并写下他们是否认为它是仇恨言论的实例。我们然后将这些标注与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT 4进行比较。我们的研究最终积累了来自87个国家的1000多名标注员的超过16000个标注。所以现在我们更好地装备来回答NLP数据集和模型最符合谁的问题？我们发现NLP中有位置性。例如，我们发现数据集和模型最符合英语国家。在GPT 4社会可接受性分析中，我们发现它最符合儒家思想和英语国家。我们还发现DanaHate最符合英语国家。我们还发现与具有大学教育的人有更多的额外符合性。在GPT 4的社会可接受性任务中，我们发现它最符合具有大学教育或研究生教育的人。我们在DanaHate中发现了同样的情况，它最符合具有大学教育的人。然而，当模型和数据集符合特定人群时，有些人不可避免地被落在后面。例如，数据集和模型对非二元性别的人比他们的男性和女性同行有更低的符合度。我们在GPT 4的社会可接受性任务以及DynaHate任务分析中都发现了这一点。既然NLP中有位置性，我们能做些什么呢？我们对此有几个建议。第一个是记录整个研究过程中所有相关的设计选择。另一个是通过观点主义镜头进行NLP研究。我们的第三个建议是在四个特定社区内构建专业的数据集和模型。一个很好的例子是Masakane倡议。我们想强调，包容性NLP不仅仅是让所有技术为每个人服务。这就结束了我们的演讲。但如果您想了解更多，请随时查看我们的仪表板以获取最新的分析结果，并阅读我们的论文。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我将讨论我们在解决实体选择中的间接指代表达方面的工作，其中我们引入了altentity语料库。我叫Javot Hosseini，这是我与Philip Radlinsky、Silvia Pareti和Annie Luis的合作项目。我们的目标是理解用户在做出选择时使用的语言。请考虑这个替代问题：你是说“对我温柔点”还是“我有一种感觉”？在这里，用户想要在这两个标志中选择一个。最明显的方法是使用直接引用，例如说出歌曲“对我温柔点”的名字或其位置（第一个），但有时间接引用更合适，可以让对话更自然。这可能发生在用户无法记住歌曲的名字、发音太相似难以区分，或用户想表达偏好时。以下是间接引用的示例，例如，“较新的那个”或“不是充满活力的歌曲”。这在对话系统中是一个重要问题，也是评估大型语言模型（LLM）实体理解能力的重要基准。据我们所知，目前没有一个公开的大型数据集来解决这个问题，所以我们使用众包标注方式收集了一个。我们的数据集涵盖了三个不同领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，使用卡通完成设置。卡通中有三个对话气泡。在第一个气泡中，鲍勃说：“记得我们昨天听的那首歌吗？”从而设定了对话背景。在第二个气泡中，爱丽丝说：“你是说‘对我温柔点’还是‘我有一种感觉’？”这是替代问题。在第三个气泡中，鲍勃使用间接引用来选择这两个实体中的一个，例如“Neo-Ervandal”。我们自动提供第一个和第二个气泡，而第三个由标注者填写。第一个气泡从每个领域的几个手动提示中选择。第二个气泡（替代问题）的生成方法如下：我们始终使用一个简单的模板：“你是说A还是B？”其中A和B从维基百科中采样。以下是我们使用的不同采样方法。当我们在列表中向上移动时，实体之间变得越来越相似，通常更难进行区分。第一个是均匀随机采样，第二个是当实体有相似标题时，例如两本书同名，第三个是当它们在维基百科上有相似描述，最后是当它们在维基百科上有相似信息框或属性，例如歌曲的同一流派或同一艺术家。当我们向标注者展示这个替代问题时，他们知道这些实体的名字，但并不一定了解这些实体。所以我们展示了一些关于两个实体的背景知识。对于歌曲，我们简单地显示每个歌曲的Google搜索链接，然后要求标注者至少听一些每首歌，并阅读关于每首歌的内容。例如，这是歌曲“对我温柔点”的Google搜索结果。对于食谱和书籍领域，我们显示维基百科上的部分背景文本。对于食谱，我们还显示它们的图片，同样来自维基百科，以便标注者知道它们的样子。然后我们要求标注者选择这些实体中的一个，例如这里的第一个，并使用三个到五个间接指代表达来描述它们。例如，“有钢琴音乐的那个”，以下是我们数据集中的一些示例：\"没有字的那个，不是有12岁男孩的，不是虚构的，来自阿塞拜疆\"等等。altentities语料库包含三个领域的6,000个替代问题，以及42,000个间接指代表达。以下是使用T5xLarge模型的结果摘要。如果语言模型拥有与标注者相同的背景知识，准确率非常高，约为92-95%。但这不现实。如果语言模型拥有部分重叠的背景知识，准确率在82%到87%之间，这更现实，例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，准确率仅为60%。因此，改进的空间很大。我们还证明了模型具有领域泛化能力。这是我们数据集的链接。谢谢。"}
