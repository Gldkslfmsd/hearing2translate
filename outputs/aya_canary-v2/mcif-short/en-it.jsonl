{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Salve! Benvenuti alla presentazione di DeepLean, un nuovo corpus per l'identificazione del testo in tedesco a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Iniziamo definendo la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "L'amplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione dello stesso da parte di un gruppo target specifico, come le persone con problemi di lettura o i non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di amplificazione del testo, richiediamo coppie parallele di testi, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "E nell'esempio qui riportato, è possibile osservare una coppia di frasi allineate in parallelo, costituita da una complessa frase tedesca e la sua traduzione in un linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come puoi vedere nell'esempio, quali la sostituzione lessicale, la cancellazione di clausole, la riorganizzazione delle clausole o l'inserimento di punti elenco."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo ora il nostro nuovo corpus dplane. Poiché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Quindi, per esempio, questi corpora qui sono troppo piccoli per addestrare un modello di tassonomizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli che ho proposto negli ultimi anni sono tutti allineati automaticamente, il che significa che possono presentare errori negli allineamenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, proponiamo il nostro nuovo Corpus dPlane, suddiviso in due sottocorpora: dPlane APA e dPlane web. DPlane APA è basato su testi di notizie."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "In DPlane APA, abbiamo allineato manualmente 483 documenti. Questo ha prodotto approssimativamente 30.000 13.000 coppie di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Per dplane web, questo corpus include diversi domini e allineiamo tutti questi 750 documenti da un lato manualmente e dall'altro con metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, otteniamo 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato un po' di più i nostri coppie di frasi, ad esempio sul tipo di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come puoi vedere qui, i testi biblici sono molto più semplici rispetto, per esempio, ai testi di attualità o a quelli per apprendisti della lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "a tutti i livelli, riguardante, per esempio, la semplificazione lessicale, la semplificazione strutturale, inoltre su tutti i livelli di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, è evidente che il nostro corpus DPlane presenta una vasta gamma di diverse trasformazioni di semplificazione. Ad esempio, nel corpus DPlane API, abbiamo molte più operazioni di riordinamento e aggiunte di parole rispetto a quanto osservato nel corpus DPlane web."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, nel corpus web abbiamo molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo ora cosa possiamo fare con questo corpus. Salve, sono Omar, e ora parlerò degli casi d'uso per il nostro dataset dplane. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni sono stati sviluppati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove disponiamo di due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi nei documenti successivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, che hanno la stessa lingua, hanno lo stesso contenuto, ma si trovano su un diverso livello di complessità."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora che disponiamo del nostro dataset dplane, che contiene frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo apportato alcune adattamenti ai metodi proposti e abbiamo pubblicato tutte queste adattamenti e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo dell'allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "E puoi trovare anche il codice per eseguire questo metodo sui tuoi documenti nel paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è quello della semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "attraverso il perfezionamento dei modelli linguistici per generare un testo semplificato dal testo di input complesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo ottimizzato due modelli diversi. Abbiamo ottimizzato il modello di importazione lunga per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche ottimizzato l'importazione di base normale per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "Puoi trovare anche tutti i punti di controllo e puoi esaminare più dettagliatamente i punteggi e le metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questo semplice aggiustamento fine potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo quei risultati come un punto di riferimento, un punto di riferimento di base per il problema della semplificazione automatica dei testi nel futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "La ringraziamo moltissimo per la sua attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Adam Skurkovsky e questo discorso riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come saprai, esistono diverse strutture di dipendenza ipotizzate da diverse teorie e approcci corpus. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione di Lisa, Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "È tale che il primo congiunto è il nucleo dell'intera struttura coordinata, quindi in questo caso, Lisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio simile è assunto nella teoria del testo del significato di Igor Milchuk, dove ancora una volta l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici, giusto? Essi individuano uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Ora esistono anche approcci simmetrici alle strutture coordinate, come l'approccio PRUG, l'approccio basato sulla congiunzione assunto nelle banche di alberi di dipendenza PRUG, in cui le strutture coordinate sono guidate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo le dipendenze dall'estremità a tutti i connettivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esiste anche un approccio a più teste utilizzato, per esempio, nella grammatica delle parole di Dick Cutzman."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove, per così dire, tutti i congiunti sono teste della struttura coordinata. Quindi otteniamo dipendenze dal governatore, qui ride, verso tutti i congiunti separatamente. Questi sono Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo di questo articolo è presentare un nuovo argomento a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Bene, l'argomentazione si basa sul principio della minimizzazione della lunghezza delle dipendenze, che spiegherò sulla base di questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "In italiano, come probabilmente sai, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli complementi circostanziali possono essere più distanti, giusto? Quindi \"ho letto ieri il libro\" è corretto perché l'oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Mentre marzo ha letto ieri è molto peggio, vero? Perché qui tra il verbo e l'oggetto diretto c'è un complemento ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo, poiché in tal caso può essere spostato nella posizione successiva al bordo."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. March ha letto ieri questo libro assolutamente affascinante sulla BC, io va bene, dove invece di esso abbiamo questo lungo NP."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma va bene anche dire: Ieri Marge ha letto questo libro assolutamente affascinante sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il ragionamento qui è che ciò è possibile perché, sebbene questa frase violi il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere adiacenti al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio della minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più brevi sono preferibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, ovvero quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui abbiamo una dipendenza dalla lettura all'aggettivo di lunghezza 7 misurata in parole e dalla lettura al libro di lunghezza 4. Quindi insieme fa 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando sposti, quando scambi questi due costituti, la somma di queste due dipendenze diventa sei, giusto? Quindi invece di undici, sei molto più breve. Ecco perché questo suona piuttosto bene, giusto? Viola un principio, ma ne soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Va bene, quindi ciò che abbiamo fatto è stato estrarre varie statistiche sulla coordinazione dalla versione potenziata del Pentry Bank e vedere nel documento perché non abbiamo utilizzato le dipendenze universali."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "E queste statistiche confermano l'osservazione già fatta in precedenza che le congiunzioni a sinistra tendono ad essere più brevi, quindi \"sale e pepe\" e non \"pepe e sale\" misurate in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione fatta incidentalmente che questa tendenza aumenta con la differenza di lunghezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo più forte, giusto? Quindi, la proporzione è maggiore del congiunto sinistro più corto."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando i governatori a sinistra sono assenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il governatore è a sinistra in questo esempio. Ho visto Bart e Lisa, quindi è il governatore, è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "È assente nel secondo esempio, Omero è arrivato e ha starnutito. Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, giusto? Quindi in questi casi, il congiunto sinistro tende a essere più breve. Tanto più che la differenza tra i due congiunti è maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance di destra, come qui, lascia che la coordinazione Telenet sia governata dalla sinistra, questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo dimostrato che, misurando la lunghezza in caratteri, la prima colonna rappresenta le sillabe, la colonna centrale i caratteri e la colonna di destra le parole. Mi concentrerò su quest'ultima."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Quello che vediamo qui è che quando il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza del congiunto sinistro ad essere più breve aumenta gradualmente con la differenza assoluta nelle parole, e lo stesso si osserva quando non c'è un governatore, come nella coordinazione delle frasi, ma quando il governatore è a destra, questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "E dimostriamo nel documento come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche come queste due e a favore delle strutture simmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Per leggere l'intero accordo e gli argomenti, si prega di consultare il documento, scusate. Discutiamo ulteriormente nel post-sessione. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Xiang Bin, sono uno studente di dottorato presso l'Università di Washington. Oggi presenterò il nostro lavoro, che va dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando i sentieri dei pregiudizi politici che portano a modelli NLP ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici vengono addestrati su dati di scansione web su larga scala."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media di informazione politica sono ben rappresentati nei loro dati di pre-addestramento. Secondo un'indagine sul Corpus C quattro, possiamo notare che il New York Times, il Los Angeles Times, The Guardian, Huffington Post e altri sono ben inclusi nei dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una benedizione a metà per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Da un lato, hanno avuto la possibilità di apprendere da prospettive diverse, celebrando così la democrazia e la pluralità delle idee. D'altro canto, queste differenti opinioni politiche sono intrinsecamente viziate da pregiudizi sociali e potrebbero condurre a potenziali problemi di equità nelle applicazioni dei compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A tale scopo, proponiamo di indagare il pipeline di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, specificamente ponendoci le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo la leadership politica dei modelli linguistici e quale ruolo potrebbero avere i dati di pre-addestramento su tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano i modelli linguistici con diverse unità politiche nelle attività a valle e se ciò possa portare a problemi di equità nelle applicazioni di NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo quindi, in particolare, di stimolare i modelli linguistici con diversi formati di prompt utilizzando i questionari politici, come il test della bussola politica. Ciò ci consente di effettuare una valutazione automatica ben fondata nella letteratura delle scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che i modelli di lingua madre presentano significati politici variabili. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche notare che GPT 4 è il modello linguistico più liberale tra tutti, e la serie GPT è generalmente più socialmente liberale rispetto alla serie BERT e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, ci proponiamo di indagare in che misura i pregiudizi politici dei modelli linguistici siano effettivamente acquisiti dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Quindi potremmo condurre un esperimento controllato pre-allenando ulteriormente i checkpoint del modello linguistico su sei diversi corpora di parti, separati in notizie e social media, ulteriormente suddivisi in base ai loro significati politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Attraverso un ulteriore pre-addestramento dei modelli linguistici su tali parti dei corpora, possiamo osservare uno spostamento corrispondente delle coordinate ideologiche del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per Roberta, ulteriormente raffinata e addestrata sul corpus di Reddit orientato a sinistra, possiamo osservare uno spostamento liberale sostanziale in termini di..."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "in termini di pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare se i modelli linguistici possono rilevare la polarizzazione che è prevalente nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi dividiamo i corpora di pre-addestramento in due: prima del 45° Presidente degli Stati Uniti e dopo il 45° Presidente degli Stati Uniti. Addestriamo quindi separatamente i modelli linguistici sui due diversi corpora temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo osservare che i modelli linguistici hanno generalmente mostrato una tendenza politica più estrema, allontanandosi dal centro dopo il 2017. Ciò indica che anche i modelli linguistici possono riflettere la polarizzazione presente nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Infine, valutiamo i modelli linguistici con diverse implicazioni politiche nella rilevazione di discorsi d'odio e nella rilevazione di fake news, applicazioni di NLP che spesso coinvolgono modelli linguistici e che potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vediamo che, se indagiamo sulle prestazioni per categoria, ovvero se suddividiamo le prestazioni in..."}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "Osservando le diverse demografie o il significato politico dei media, possiamo individuare un modello secondo cui, ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di sinistra sono più efficaci."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "nel rilevare discorsi d'odio rivolti a gruppi minoritari sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i nostri sforzi si concentrano sul rilevamento di discorsi d'odio rivolti a gruppi più potenti nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "E viceversa, i modelli linguistici orientati a destra sono migliori nel rilevare discorsi d'odio mirati ai bianchi e agli uomini, tuttavia, peggiori nel rilevare discorsi d'odio mirati alle comunità nere LGBTQ+ e ad altre minoranze."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Anche per il rilevamento delle fake news si osservano tendenze simili, dove i modelli linguistici di sinistra sono più abili nel rilevare disinformazione proveniente da fonti di destra e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "Questo mostrerà ulteriormente molti esempi qualitativi per osservare come i modelli linguistici possiedono significati politici diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "le previsioni differiscono effettivamente per i casi di discorso d'odio e disinformazione in base alle loro categorie sociali. Nell'appendice sono presenti ulteriori esempi per evidenziare ulteriormente questo punto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che esiste un problema di equità molto pressante relativo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se un modello linguistico a linea retta venisse raffinato su discorsi d'odio, disinformazione o qualsiasi altra cosa e implementato su una popolare piattaforma di social media."}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e che i discorsi d'odio rivolti ai gruppi minoritari potrebbero diffondersi impunemente senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ha suonato l'allarme per riconoscerci e affrontare le questioni di equità derivanti dal nitrire politico dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, una breve discussione. Vorremmo anche evidenziare che esponiamo il dilemma unico riguardante i pregiudizi politici dei modelli linguistici. È come tra Sila e Kryptidis."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento del modello linguistico, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando alla fine problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se provassimo in qualche modo a sanificare, rischieremmo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e debba essere mantenuto nei dati di addestramento del modello linguistico. È un po' come il problema di Charlie elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Bene, ottimo. Penso che questo sia più o meno tutto per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jenny, sono una studentessa del primo anno di dottorato presso la Carnegie Mellon University, e oggi presenterò il vostro lavoro, \"Enol Posizionale: Caratterizzazione dei Bias di Progettazione nei Set Beta di Modelli\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato realizzato in collaborazione con alcuni membri dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santi, Ronin Lebras, Katarina Reinicke e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo quindi immaginando di lavorare per un giornale e di setacciare i commenti sotto un articolo di attualità, cercando di rimuovere i contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potresti orientarti verso un'API popolare come Perspective API per il rilevamento della tossicità. E questo funziona davvero bene se sei Carl Jones, dove Perspective API è in grado di rilevare correttamente gli esempi tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma questo non è realmente il caso per Dithyasharma, dove l'API prospettica non è davvero così sensibile ai termini offensivi che sono più comuni nei contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio nel design in cui si osservano differenze sistematiche nelle prestazioni tecnologiche tra diverse popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "Preconcetti di progettazione come quello che abbiamo appena visto possono verificarsi a causa della posizione degli studiosi di NLP e degli sviluppatori di modelli. La posizione è semplicemente la prospettiva che le persone hanno in virtù delle loro caratteristiche demografiche, identità ed esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E in quanto ricercatore, la posizione può influenzare il processo di ricerca e i suoi esiti e risultati, poiché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno una posizione o un punto di vista?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "E non stiamo cercando di affermare che i modelli stessi e i set di dati stessi abbiano identità demografiche ed esperienze di vita, ma essi aggrega giudizi e opinioni di persone reali e possono quindi rappresentare certe posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, lavori precedenti hanno suggerito alcune evidenze aneddotiche della presenza di positionalità, come ad esempio lacune culturali nei modelli e nei dataset, nonché definizioni teoriche della positionalità dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non si concentrano effettivamente sul confronto tra gli utenti finali con i dataset e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio della posizionalità dei modelli e dei set di dati diventa sempre più importante man mano che i compiti di NLP diventano più soggettivi e orientati al sociale."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "E risulta impegnativo caratterizzare in che modo queste posizioni sono distorte, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Per studiare la posizione del dataset e del modello, confrontiamo effettivamente le annotazioni con gli utenti reali con i dataset e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo facciamo attraverso il nostro framework, NL Positionality."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework funziona in due passaggi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo è quello di ri-annotare i dataset con annotatori diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E optiamo per fare ciò esaminando la demografia dei dataset originali, ehm, degli annotatori, poiché di solito solo pochi annotatori annotano ogni istanza e perché i dati demografici vengono raramente raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "E così optiamo per ri-annotare i dati per ottenere molte annotazioni per istanza e per avere un insieme ricco di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Prendiamo quindi le annotazioni demografiche e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione Parsons R."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E così, il nostro framework differisce effettivamente dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con i modelli e i dataset, le previsioni e le etichette, piuttosto che limitarsi a esaminare solo l'accordo degli annotatori o la modellazione delle distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è in gran parte reso possibile grazie a Lab in the Wild, una piattaforma online di crowdsourcing per i nostri collaboratori nell'HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversificati rispetto a piattaforme come MTurk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Ospitiamo due compiti su Lab in the Wild, uno dei quali riguarda l'accettabilità sociale. Il funzionamento è il seguente: i partecipanti leggeranno una situazione dal dataset sulla chimica sociale e poi scriveranno quanto una situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'IA e con quelle di altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi confrontato queste annotazioni con la chimica sociale, il metodo Delphi e GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi replicato un'impostazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, in cui leggeranno un'istanza da Dana Hate e scriveranno se ritengono che si tratti di un esempio di discorso d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi confrontato queste annotazioni con DynaHate, Perspective API, Rewire API, HateRoberta e GPT quattro. Il nostro studio ha infine accumulato oltre sedicimila annotazioni da oltre mille annotatori provenienti da ottantasette paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Ora siamo meglio attrezzati per rispondere a chi si allinea di più con i set di dati e i modelli di NLP. Scopriamo che esiste una posizione nel NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, riscontriamo che i dataset e i modelli sono più allineati ai paesi di lingua inglese. Pertanto, per l'analisi dell'accettabilità sociale del GPD 4, constatiamo che è più allineata ai paesi confuciani e di lingua inglese. Riscontriamo che anche Dynamite Hate è più allineato ai paesi di lingua inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo anche la maggior parte delle affinità con le persone che hanno un'istruzione universitaria. Quindi, per GPT 4 nel compito di accettabilità sociale, constatiamo che è più allineato con le persone con un'istruzione universitaria o post-laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E lo stesso vale per Dani Hate, dove è più allineato alle persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i dataset sono allineati a popolazioni specifiche, alcuni inevitabilmente rimangono esclusi."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro equivalenti maschi e femmine. Riscontriamo questo sia nel compito di accettabilità sociale di GPT 4 che nell'analisi del compito Dynahate."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dato che esiste una posizione analidina LP, cosa possiamo farci?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo alcune raccomandazioni a riguardo. La prima è di mantenere un registro di tutte le scelte di progettazione rilevanti durante l'intero processo di ricerca. L'altra è di condurre ricerche di NLP con una prospettiva perspectivista."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di costruire dataset e modelli specializzati all'interno di quattro comunità specifiche. Un buon esempio di ciò è l'iniziativa Masakane. E vogliamo sottolineare che l'NLP inclusivo non consiste solo nel far funzionare tutte le tecnologie per chiunque."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "E questo conclude la nostra presentazione, ma se desiderate saperne di più, sentitevi liberi di consultare il nostro cruscotto per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Xi Yuan dell'Università di Fenai. Sono qui per presentare il nostro lavoro sulla distinzione tra conoscenze di script e modelli linguistici di linea per la pianificazione del linguaggio vincolato."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo dopo passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno esplorato l'utilizzo di modelli linguistici per la pianificazione di obiettivi astratti di attività stereotipiche, come preparare una torta, dimostrando che i grandi modelli linguistici possono efficacemente scomporre gli obiettivi in passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione degli obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con obiettivi specifici, vincoli specifici, come ad esempio preparare una torta al cioccolato, rimane ancora poco studiata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "Che impongono vincoli diversi agli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli moltiplicati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Poiché non esiste un insieme di dati di obiettivi specifici per individuare il nostro punto di partenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo prima raggiungere questo obiettivo. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli modificati per l'acquisizione di dati in loop umano utilizzando TPT strutturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Campioniamo 100 obiettivi specifici e valutiamo gli script generati da modelli di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Riscontriamo che tutti i modelli lineari ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi conduciamo un'analisi dettagliata per indagare a cosa servono i moduli di apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati illustrati nella figura dimostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà alle restrizioni non può essere garantita."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Scaviamo più in profondità nelle categorie di argomenti graduati di vincoli a seconda del risveglio a casa. La mappa principale nella figura mostra che le prestazioni di pianificazione dei DPD istruttivi variano considerevolmente per le ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli leggeri presenta una elevata varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di un filtro zen sovra-generato per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo inizialmente i tipi di vincoli con esempi per istruire il CPT e ottenere obiettivi specifici basati sui suddetti obiettivi astratti."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Quindi istruisci il GPT a generare script di casi per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, viene sviluppato un modello di filtro per selezionare gli script adatti."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo script e obiettivi in istruzioni per GPT in frammenti e calcoliamo la similarità coseno e i punteggi di similarità per misurare la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, scriveremo la sceneggiatura che contiene le parole chiave della restrizione target. Conserviamo la sceneggiatura solo se il target go ottiene il punteggio più alto rispetto al sito dell'obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, l'insufficienza può generare viti di qualità dei capelli. Il nostro metodo migliora notevolmente la pianificabilità sia per completezza semantica che per fedeltà al vincolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché i grandi modelli linguistici sono costosi da implementare, è fondamentale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di un dataset è un passaggio essenziale per il raggiungimento di questo obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, gli studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale dei set di dati è costosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per estrarre siti di pianificazione linguistica vincolata da grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Apriremo il nostro metodo per la costruzione di un dataset di pianificazione linguistica congiunta, denominato script di codice."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generiamo cinquantacinquemila obiettivi specifici con script per garantire la qualità dei siti di validazione e test. Chiediamo ai lavoratori di fonti cloud di individuare e revisionare i campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione delle restrizioni della script di codice. Riscontriamo che la script di codice presenta iperplodismo negli obiettivi specifici generati. Con la script di codice, possiamo tracciare modelli più piccoli ma specializzati per la pianificazione del linguaggio delle restrizioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Con AntSights, TFILF e il cursore tarato, è possibile generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che modelli più piccoli possono supportare modelli più grandi quando addestrati correttamente su siti di dati adatti."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo stabilito il problema di pianificazione del linguaggio con vincoli. Abbiamo valutato la capacità di pianificazione del linguaggio con vincoli dei grandi modelli linguistici e sviluppato un metodo di filtro generato per i grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo grandi modelli linguistici per generare un set di dati di script di alta qualità per la pianificazione del linguaggio vincolato. Ci auguriamo che il set di dati del codice possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione del linguaggio."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il suo tempo. Per ulteriori dettagli sullo script del codice, si prega di consultare il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Shu Heng. Oggi presenterò il nostro articolo \"I tagger di entità denominate Do Kernel 2003 funzionano ancora bene nel 2023?\" Iniziamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità denominate, o compito NER."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli hanno utilizzato Kono duemila tre per sviluppare il NER da quasi vent'anni. E questo naturalmente solleva diversi problemi. In primo luogo, questi modelli possono generalizzare a dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, se osserviamo una scarsa generalizzazione, quali sono le cause del calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare questi problemi, abbiamo sviluppato il dataset Kono plus plus. Si tratta di un dataset che abbiamo raccolto da Reuters News a partire dal 2020 e poi annotato utilizzando le stesse linee guida di annotazione di Kono del 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi ottimizzato oltre venti modelli su Kono duemila e tre. Li abbiamo valutati sia sul set di test Kono tre che sul set di test Kono plus."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti abbiamo scoperto che sono necessari tre ingredienti principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "La prima è l'architettura del modello. Dai nostri esperimenti, abbiamo riscontrato che i modelli basati su trasformatori generalmente generalizzano meglio su nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo riscontrato che solitamente i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E infine, ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione fine influenza direttamente le prestazioni di un'attività a valle. Qui abbiamo scoperto anche che un maggior numero di esempi di ottimizzazione fine porta effettivamente a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Alla nostra prossima domanda, quali sono le cause del calo delle prestazioni di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Avevamo due ipotesi. La prima è l'overfitting adattivo, che è un overfitting causato dal riutilizzo dello stesso set di test ripetutamente, e questo si manifesta solitamente come un rendimento decrescente su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e quelli di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, come abbiamo visto dal grafico a destra, la linea di miglior adattamento rossa ha una pendenza superiore a uno."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su Carl nel duemila e tre si traduce in più di un'unità di miglioramento su Carl++, il che implica che non ci sono rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che in questo caso non si osserva il sovradattamento adattivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, che ne è della deriva temporanea?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti, e abbiamo riscontrato che le prestazioni diminuiscono con divari temporali più ampi."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, una dimensione del modello più ampia, nonché di più esempi di ottimizzazione fine, e questi vanno di pari passo. Non possiamo avere solo un ingrediente e scartare gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, abbiamo scoperto anche che il calo delle prestazioni qui è causato da una deriva temporale e, sorprendentemente, non è dovuto a un adattamento eccessivo, nonostante il metodo Kono del 2003 sia stato utilizzato per oltre vent'anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Ritornando alla domanda che abbiamo posto nel titolo del nostro articolo, i tagger di Kono del 2003 funzionano ancora nel 2023? Abbiamo scoperto che la risposta è in realtà un sonoro sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare le generalizzazioni dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "Infine, assicurati di consultare il nostro articolo, il nostro set di dati e, se hai domande, non esitare a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Salve, parlerò del nostro lavoro sulla risoluzione di espressioni riferenziali indirette per la selezione di entità, in cui abbiamo introdotto il corpus altentity."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Il mio nome è Javod Hosseini e questo è un lavoro congiunto con Philip Radinsky, Silvia Paretti e Annie Luis."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è comprendere il linguaggio degli utenti quando desiderano effettuare una scelta. Considera questa domanda alternativa. Intendevi \"facile per me\" o \"ho una sensazione\"? Qui, un utente vuole selezionare uno di questi due segnali."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è utilizzare un riferimento diretto, per esempio dicendo il nome della canzone \"Easy on Me\" o la sua posizione, la prima."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della fonte."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "Tutte le pronunce sono troppo simili tra loro e difficili da distinguere."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "oppure quando l'utente desidera specificare una preferenza. Ecco alcuni esempi in riferimenti diretti, ad esempio quello più recente o la canzone che non è energica."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un problema importante nei sistemi conversazionali e anche per la valutazione della comprensione delle entità da parte dei LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per questo compito, quindi ne raccogliamo uno utilizzando l'annotazione collettiva. Il nostro set di dati copre tre diversi domini: musica, libri e ricerca."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La metodologia di raccolta del nostro insieme di dati enfatizza l'informalità utilizzando un insieme di completamento di cartoni animati."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il fumetto ha tre nuvolette. Nella prima, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\" E con ciò, Bob stabilisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nel secondo fumetto, Alice dice: \"Intendi dire che è facile per me o che mi sono liberata di un peso?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "qual è la domanda alternativa. E nel terzo fumetto, Bob utilizza un riferimento indiretto per selezionare una di queste entità, ad esempio la nuova RF."}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniamo automaticamente i primi e secondi balloon del discorso, ma il terzo viene compilato dall'annotatore. Il primo balloon viene scelto da alcuni prompt manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo, che è la domanda alternativa, viene generato come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo sempre un semplice modello. Ti riferisci al modello A o B? Dove A e B sono esempi tratti da Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Man mano che si sale nella lista, le entità diventano più simili tra loro e di solito è più difficile effettuare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "La prima è l'attrazione uniforme."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, per esempio, due libri con il nome \"il commercio al dettaglio\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso si verifica quando hanno descrizioni simili su Wikipedia. Infine, quando presentano info box o attributi simili su Wikipedia, come ad esempio lo stesso genere o la stessa voce dell'artista."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando presentiamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente ne sanno qualcosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Quello che facciamo è mostrare alcune conoscenze di base sulle due entità. Per le canzoni, forniamo semplicemente un link di ricerca Google per ciascuna canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "e poi chiedere agli annotatori di ascoltare almeno una parte di ogni canzone e di leggere le informazioni su ciascuna canzone. Ecco, per esempio, il risultato della ricerca Google per la canzone \"Easy\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo alcuni testi di sfondo tratti da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Chiediamo quindi agli annotatori di scegliere una di queste entità, per esempio la prima qui, e descriverla utilizzando tre o cinque espressioni di riferimento indiretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica al pianoforte. Ecco alcuni esempi dal nostro insieme di dati. Ad esempio, quello senza parole, non quello con il ragazzo di 12 anni o quello fittizio o che proviene dall'Azerbaigian."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus delle entità alternative contiene 6.000 domande alternative distribuite in tre domini e 42.000 espressioni di riferimento indirette. I risultati ottenuti con il modello T5xLarge sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso allo stesso identico bagaglio di conoscenze degli annotatori, allora la precisione è davvero elevata. Si aggira tra il 92 e il 95 percento. Ma questa è una situazione non realistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso a una conoscenza di base parzialmente sovrapposta, allora l'accuratezza si attesta tra l'82 e l'87 percento, il che è più realistico, ad esempio, quando il modello linguistico recupera la conoscenza di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso solo ai nomi di entità, allora l'accuratezza è solo del 60%. Quindi c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili in diversi domini. Ecco un link al nostro set di dati. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Sarah Pappy dell'Università di Trento e della Fondazione Bruno Kessler, e introdurrò brevemente il documento \"L'attenzione come guida per la traduzione simultanea del parlato\", un lavoro svolto in collaborazione con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Che cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o simul SD, è il processo di tradurre una lingua parlata in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli SimulST attuali? Architetture specifiche vengono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedure di addestramento lunghe e complesse, per esempio l'addestramento che coinvolge diversi obiettivi di ottimizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza, ad esempio addestrare un modello con una latenza media di un secondo e un altro con una latenza di due secondi e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Qual è quindi la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "Per primi, utilizzare modelli SD offline già esistenti senza ritraining o adozione di architetture specifiche per CLSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E sfruttare le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, che è il meccanismo di attenzione incrociata. E puoi vedere un esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre un decodificatore di attenzione a punto o encoder, ed è una strategia in cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Viene emesso un segnale se la tensione non è concentrata, ovvero la sua somma è al di sotto di una certa soglia alfa, nelle ultime lambda finestre di discorso, il che significa che le informazioni ricevute sono sufficientemente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se riceviamo una porzione di discorso contenente \"I'm going to talk about\" e il nostro modello prevede la traduzione in tedesco, noi..."}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i primi schemi di linguaggio ricevuti, mentre l'ultima parola si riferisce agli ultimi schemi di linguaggio ricevuti, gli ultimi schemi di linguaggio lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che le prime due parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "Finché la somma della tensione incrociata supera una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro frammento di discorso."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro discorso sommerso e il nostro modello prevede altre tre parole, esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che nessuna parola fa riferimento agli ultimi frame del discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che queste tre parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se esaminiamo il risultato principale di ciò, osserviamo"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Rappresenteremo graficamente i risultati della traduzione simultanea del discorso su grafici in cui abbiamo il blu su un lato che misura la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Quella è la misura della latenza. Consideriamo anche il ritardo medio consapevole del calcolo che tiene conto del tempo di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vogliamo che le nostre curve siano il più in alto possibile in questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo anche che vengano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo con strategie PROPERA che si applicano anche a modelli offline, come la strategia WitKey e l'accordo locale. Inoltre, confrontiamo con l'architettura allo stato dell'arte specificamente progettata per la pre-traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea del discorso in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E osserviamo che ADUT supera tutte le strategie applicate ai modelli offline, poiché le loro curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, se consideriamo il tempo effettivo di scorrimento o il tempo consapevole del calcolo, quella è la strategia più rapida."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desideri scoprire ulteriori risultati, leggi il nostro articolo. Abbiamo inoltre reso aperto il codice e i modelli, fornendo output simultanei per agevolare la riproducibilità del nostro lavoro. Grazie per la tua attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Ying e il mio collega Jiang ed io presenteremo la nostra ricerca sul miglioramento dell'apprendimento seriale multimodale tramite il tuning delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Con i progressi nei modelli di linguaggio di grandi dimensioni, molti lavori hanno iniziato ad esplorare nuovi paradigmi di apprendimento che riutilizzano i modelli di linguaggio pre-addestrati per diversi compiti a valle in modo efficiente sia in termini di parametri che di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che il tuning delle istruzioni consente ai grandi modelli linguistici di eseguire compiti non visti in precedenza in modo zero-shot seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione delle istruzioni si è concentrata sul miglioramento delle prestazioni sequenziali su compiti solo linguistici, trascurando la visione artificiale e i compiti multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, desideriamo indagare se il tuning delle istruzioni su modelli pre-addestrati multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali non visti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo riscontrato una notevole discrepanza nella disponibilità del set di dati di istruzione tra RLP e multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono più di mille seicento compiti di istruzione basati su un'unica lingua. Tuttavia, non esiste un compito di istruzione multimodale su larga scala pubblicamente accessibile. Pertanto, questo ci motiva a costruire un dataset di accordatura delle istruzioni multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi Instruct, il primo set di dati di benchmark per il tuning delle istruzioni multimodali che consiste in 62 compiti multimodali diversificati che coprono 10 ampie categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi compiti sono derivati da ventuno dataset open source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare il tuning dell'istruzione multimodale sul nostro dataset proposto, utilizziamo OFA, un modello unificato di pattern multimodale, come modello di base. OFA impiega un vocabolario unificato per il linguaggio, i token delle immagini e le coordinate di una casella delimitatrice."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui mostriamo alcuni esempi tratti dal nostro dataset multi-strato."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "Per unificare l'elaborazione di vari tipi di dati in ingresso e in uscita."}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo OFA e formuliamo tutti i compiti in un formato sequenziale unificato in cui il testo in ingresso, le immagini, le istruzioni e le bounding boxes sono rappresentati nello stesso spazio token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Va bene, ora parlerò del tuning dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Per il set di dati di addestramento, utilizziamo 53 compiti dal gruppo NIG per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo Common Sense Reason per il test e selezioniamo ulteriori cinque compiti dai gruppi WQA e Miscellaneous."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutte le istanze nella suddivisione di test per ogni compito. Inoltre, campioniamo casualmente venti compiti dalla suddivisione di test di istruzioni naturali come in Sintassi per l'Elaborazione del Linguaggio Naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo quindi un modello OFA di grandi dimensioni pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata casualmente con una delle sue cinque template di istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante i test per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Riportiamo la media e le prestazioni massime, nonché la deviazione standard delle prestazioni attraverso tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è una classificazione multimodale, riportiamo l'accuratezza. Se si tratta di un compito di generazione multimodale, riportiamo il RougeL. Per un compito RP, riportiamo anch'esso il RougeL."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Questa misura la capacità del modello di produrre in modo coerente gli stessi output per lo stesso compito, indipendentemente da lievi variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il nostro principale risultato. Come possiamo vedere, il tuning delle istruzioni può migliorare significativamente le prestazioni dell'OFE nei compiti multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, il transfer learning da insiemi di dati di istruzioni naturali può giovare all'accordatura delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo osservare come, con l'aumento della quantità di compito, il modello ha raggiunto migliori prestazioni e nel contempo una minore sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento, utilizzando un'istruzione rispetto a cinque istruzioni. Come possiamo osservare, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo mostra l'effetto di diverse strategie di ottimizzazione fine sul grado di sensibilità del modello. Come possiamo vedere attraverso il transfer learning da insiemi di dati di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto al modello IFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche osservare che il transfer learning da un insieme di dati di istruzioni naturali può aiutare l'OFA a ottenere prestazioni molto migliori sull'insieme di dati di istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo proposto il primo set di dati di tuning istruzionale multimodale su larga scala, che migliora significativamente la capacità derivativa di OFA, e abbiamo esplorato diverse tecniche di apprendimento trasferibile, dimostrando i loro vantaggi attraverso la progettazione di una nuova metrica chiamata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Un'ultima cosa, stiamo raccogliendo un set di dati di tuning delle istruzioni multimodali molto più ampio, con circa 150 compiti aggiuntivi in varianti linguistiche, e li renderemo disponibili. Questo è un codice QR per i nostri dati e modelli. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, sono Coast di Sena e sono lieto di darvi il benvenuto alla nostra presentazione del lavoro ACL 2023, \"I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con John Bokier, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina William."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, rivalutiamo il paradigma dei minimi coppie."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il paradigma delle coppie minime valuta essenzialmente i modelli linguistici in base a giudizi di accettabilità, che possono includere anche la grammaticalità, come nel caso di BLIMP, Syntax Gem, o l'accettabilità in termini di stereotipi, come nelle coppie di Krauss."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma di coppia minima, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticalmente corretta e poi presentare una frase inaccettabile o grammaticalmente scorretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi si spera che il modello attribuisca essenzialmente una probabilità maggiore alla frase accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP non ci consente di valutare l'accettazione del modello per le frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Al giorno d'oggi i grandi modelli linguistici stanno sviluppando finestre contestuali sempre più ampie. Pertanto, è cruciale valutare l'accettabilità del modello in tutta la finestra contestuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere il pipeline NPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo è l'approccio. Ciò che facciamo è che, per simulare queste sequenze più lunghe, rivalutiamo gli stessi set di dati e poi ricreiamo le frasi scegliendo tra frasi accettabili o non accettabili da quei set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, qui abbiamo scelto una coppia tipica di grammaticalità dal set di dati BLIMP, dal caso dell'isola degli aggettivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è ricreare sequenze più lunghe e accettabili, con la stessa corrispondenza della struttura grammaticale, estraendo frasi grammaticalmente corrette da un corpus di testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che a quella non accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento, e questo potrebbe essere utilizzato anche per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un diverso insieme di dati. Questo è ciò che definiamo uno scenario di mismatch."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui le frasi provengono ancora da dataset pertinenti, ma non dallo stesso dataset che stai valutando. E possiamo fare lo stesso per il caso di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente estraneo, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "come ad esempio se il contesto deriva da un sottoinsieme diverso del set di dati o se è completamente irrilevante rispetto alla frase che stiamo esaminando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Allora, come si comporta il modello? Innanzitutto, esaminiamo le frasi di Wikipedia che sono completamente irrilevanti per la coppia di query corrente, e qui scopriamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino al 2024 per sfruttare al massimo i modelli OPT e GPT, e qui, come mostra la linea punteggiata arancione, i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Ora, cosa accade quando scegliamo frasi dallo stesso insieme di dati?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ecco che scegliamo o creiamo frasi da domini accettabili e non accettabili dallo stesso dataset BLIMP o SYNTAX GIMP."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E qui vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando allineiamo la struttura, ovvero quando selezioniamo le frasi dallo stesso fenomeno sintattico, Jim."}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "osserviamo un notevole aumento o una notevole diminuzione nel giudizio MPP del modello a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora questo e questo è molto ampio, come questo effetto aumenta lungo la lunghezza del contesto e questo probabilmente influenzerebbe modelli linguistici più recenti che hanno grandi finestre di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Perché quindi il prefisso \"match\" influenza così tanto il giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo condotto una serie di analisi in cui abbiamo cercato di perturbare la frase in ingresso, tentando di preservare la struttura rilevante ma aggiungendo del rumore all'input. E dopo aver eseguito diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "constatiamo che nessuno di questi rumori sta effettivamente inducendo il modello a modificare il suo andamento per quanto riguarda la rappresentazione della tendenza del giudizio MPP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "In sostanza, riscontriamo che i modelli rispondono alle frasi perturbate in modi simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "È quando perturbiamo le frasi nel dominio accettabile che osserviamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo analogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Quindi i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, nel modo in cui la effettuiamo attualmente con input brevi e a singola frase, potrebbe non catturare completamente la conoscenza astratta del modello linguistico attraverso la finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Si prega di leggere il nostro articolo per maggiori dettagli sui nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Yusin Zhang dell'Università di Penn State. Oggi presenterò il nostro lavoro, il parsing semantico crosslingue in più lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Il parsing semantico è un'attività volta a costruire rappresentazioni semantiche delle query degli utenti, come SQL e il calcolo lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "E il parsing semantico crosslinguistico è il compito di tradurre le query in più lingue naturali in diverse rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda, FunQL e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di parsing semantico cross-linguistico esistenti vengono proposti e valutati separatamente su dataset di compiti e applicazioni limitate, per esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "Esistono molte coperture su alcune lingue naturali. Manca il cinese."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "Copertura dei laghi su alcune mini-rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il calcolo lambda è assente."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "O vengono valutati solo su alcuni modelli più recenti. Per esempio, c'è solo un modello singolo per valutarli."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo un esempio, forniamo un dataset uniforme come esempio per il parsing semantico cross-lingue in più lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene nove insiemi di dati in vari domini, cinque parti semantiche e tassonomie, otto rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro punto di riferimento, consideriamo le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è TranslateTest. Utilizziamo l'API di Google Translate per tradurre la fonte nella lingua di destinazione, quindi utilizziamo il MonolingoModel per addestrare una valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, addestriamo un modello in inglese su query in inglese e, durante l'inferenza, traduciamo la query in tedesco utilizzando un'API in inglese. Successivamente, utilizziamo il modello addestrato per prevedere l'SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E testeremo anche il modulo monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco in tedesco o inglese in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testiamo anche l'impostazione di fusione monolingue addestrando modelli monolingue con solo il 10% dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "E testiamo un modello multilingue, che addestriamo come un unico modello multilingue per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo riunito le query in tedesco, inglese e cinese per addestrare un modello multilingue. E durante l'inferenza, possiamo utilizzare questo modello per."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "per tradurre richieste in tedesco o in cinese o altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "E consideriamo anche il trasferimento crosslingue zero shot e field shot, che funziona su una lingua sorgente e viene trasferito a un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Quindi durante l'addestramento, lo allenerò su query in inglese o sulla combinazione di query in inglese e tedesco per addestrare un modello multilingue in grado di prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo anche molti risultati interessanti. Quindi, per quanto riguarda l'analisi dei modelli monolinguali, valutiamo su due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "inclusi gli encoder PDR, che stanno per encoder pre-addestrati multilingue con decodificatori basati su puntatori, come XLMR più PDR e BERT più PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo anche modelli encoder-decoder, come quelli multilingue pre-addestrati, ad esempio MBART e MT5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che l'encoder-decoder ottiene le prestazioni migliori su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo su MT cinque e XLMR più PDR in contesti multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che l'encoder-decoder o l'encoder PDR può essere migliorato addestrandolo su una miscela di varie lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo riscontrato che ciò è dovuto al fatto che la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, ad eccezione dell'inglese, le cui prestazioni diminuiscono in sette set di dati e aumentano solo in tre."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Credo che questo sia noto come la maledizione della plurilinguismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche il divario nelle prestazioni tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento di carburante crosslinguale con sparo, la linea arancione è il trasferimento crosslinguale a zero sparo, mentre la linea verde è l'impostazione monolinguale."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che confrontando la linea verde e quella arancione, abbiamo scoperto che per un'impostazione breve pari a zero, il divario di prestazioni nel trasferimento cross-linguistico è significativo. E confrontando la linea blu con quella arancione, abbiamo rilevato che per poche impostazioni brevi, il divario di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato anche altri risultati interessanti. Ad esempio, il modello encoder-decoder ha superato i lavori precedenti o ha ottenuto risultati comparabili. L'acquisto di dati in lingua naturale inglese può migliorare significativamente le prestazioni del modello su lingue naturali target."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo riscontrato che i modelli linguistici multilingue come Codice e Bloom sono ancora inadeguati per i compiti di analisi semantica cross-lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo sviluppato Exempler, un benchmark unificato per il parsing semantico cross-lingue con più lingue naturali e mini-rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue, e i nostri risultati mostrano molte scoperte interessanti e altro ancora. Vi invitiamo a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Aid Vilar e fornirò una breve panoramica del documento \"Promuovere la traduzione di PowerPoint, valutare strategie e prestazioni\". Questo è un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "Parm è un modello linguistico di apprendimento con 540 miliardi di parametri, presentato lo scorso anno nel 2022. È addestrato su una vasta raccolta di tag composta da 780 miliardi di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, presentiamo il primo studio sistematico sull'utilizzo del modello linguistico Latch per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità AMT. Ciò comporta l'utilizzo dei più recenti set di test per evitare un sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo due sistemi all'avanguardia, i sistemi con le migliori prestazioni dell'evaluazione WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche LMT all'avanguardia e nuove e, inoltre, presentiamo anche risultati di valutazioni basate su esperti umani. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "L'indicazione ha una grande influenza sulle prestazioni dei LLMs per la traduzione, come possiamo osservare in un semplice esperimento in cui utilizziamo una breve indicazione e forniamo due diversi suggerimenti per una sola frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "La maggioranza delle frasi, cinquecentosedici su mille, la differenza osservata è di più di un punto sfocato."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "E questo può arrivare, nei casi estremi, fino a 40 punti di sfocatura. Quindi è importante scegliere una buona strategia di prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nei nostri esperimenti, abbiamo optato per una strategia di prompt a cinque riprese, contrassegnando semplicemente ogni frase che forniamo al sistema con la lingua in cui è scritta."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "Quindi in questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine, sono contrassegnate con il due punti tedesco e la traduzione inglese con il due punti inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo osservato che la forma effettiva della sollecitazione non ha una grande influenza nel caso di diverse sollecitazioni brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È cruciale per il prompt a zero e un colpo, ma quando passiamo, come nel nostro caso, al prompt a cinque colpi, la differenza con la forma effettiva del prompt è quasi nulla."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a portare il peso maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "Una sintesi dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di origine."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "È quindi importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo gli input di selezione dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di profondità sono molto più curati e di qualità superiore rispetto ai dati di addestramento, tanto che, direi, i risultati mostrano una prestazione migliore quando si utilizzano i dati di profondità."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi all'avanguardia specializzati hanno un vantaggio sostanziale rispetto alle traduzioni PALM, ma PALM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo optato per l'integrazione con Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le intuizioni che abbiamo acquisito dall'innovazione umana che abbiamo realizzato utilizzando il framework MQM sono che la fluidità di PALM è paragonabile ai sistemi all'avanguardia, ma la differenza principale risiede nella precisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Quindi sembra che Palm scelga di produrre una traduzione che suona meglio a volte omettendo parti della frase di origine che vengono escluse nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, lo stile della categoria esterna per PAN è inferiore rispetto ai sistemi all'avanguardia, il che rappresenta un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "Quella parte fornisce un output davvero fluido, ma ancora con alcuni problemi di accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa breve panoramica. Per maggiori dettagli, vi prego di partecipare alla presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Dawe, sono uno studente di dottorato presso l'Università di Zalant in Germania. In questo video vorrei presentarvi il nostro recente lavoro, \"Più fragile di quanto pensiate\", un'analisi critica dell'apprendimento basato su forniture settimanali."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Xiao Yushche, Marios Musbach e Gas Steffen e Dietrich Clarkov."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o sourcing di codici di località, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Quando confrontate con le annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se addestriamo direttamente reti neurali su dati di etichette settimanali, queste tendono a memorizzare il rumore delle etichette e non generalizzano."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto dell'apprendimento supervisionato settimanale, vengono proposti algoritmi di addestramento per allenare in modo robusto reti neurali in presenza di tale livello di rumore, in modo che i modelli addestrati generalizzino comunque in modo efficace."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In recenti lavori in WSL, dove WSL sta per Apprendimento Supervisionato Settimanale, una affermazione comune è che le persone dichiarano di addestrare i modelli solo sui dati etichettati settimanalmente e di ottenere elevate prestazioni su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è errata, ma c'è un inghippo."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "Il fatto è che le persone danno per scontato che esista un ulteriore insieme di dati di validazione puliti disponibile per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Mettiamo in discussione questa formulazione del problema, poiché implica che siano necessarie annotazioni manuali aggiuntive nel contesto dell'apprendimento supervisionato settimanale, ma come un elefante in una stanza, questa necessità viene spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il dubbio sopra menzionato ci porta a porci tre domande di ricerca. Innanzitutto, è necessario disporre di dati di validazione puliti per WSL? Oppure possiamo eventualmente utilizzare un insieme di validazione rumoroso?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, se sono necessari dati puliti o se i dati puliti sono obbligatori per il funzionamento di WSL, quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono modi migliori per sfruttarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro e le nostre scoperte sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, scopriamo che, interessantemente, i recenti metodi WSL richiedono effettivamente campioni di bianco pulito per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "In caso contrario, si verifica un notevole calo delle prestazioni, come illustrato in questa figura. Se non sono disponibili campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre le originali etichette deboli."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "significando che l'addestramento è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che gli approcci WSL richiedono in realtà dati etichettati in modo pulito per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere sottovalutato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "La nostra seconda scoperta è che l'aumento del numero di campioni di validazione puliti aiuterà gli approcci WSL a raggiungere prestazioni migliori, come illustrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "In genere, abbiamo bisogno solo di venti campioni per categoria per ottenere un'elevata prestazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questo non è il termine della storia, perché se in ogni caso decidiamo di accedere a campioni puliti, allora l'addestramento diretto su di essi otterrà addirittura prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di prestazioni tra gli approcci di ottimizzazione fine, applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la convalida."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo osservare, se abbiamo dieci campioni per classe, il tuning fine diretto inizia a superare gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare il raffinamento sui campioni di convalida puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo osservare dai dati, il modello Marlina denominato FTW inizialmente presenta prestazioni inferiori rispetto a metodi WSL più complessi come il coseno."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se permettiamo al fine tuning di continuare sui campioni puliti, allora FTW si comporta altrettanto bene quanto altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo dimostrato che gli approcci recenti del WSL richiedono campioni puliti e annotati manualmente per funzionare correttamente. Il loro miglioramento delle prestazioni e la praticità sono fortemente sovrastimati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre concrete raccomandazioni per il lavoro futuro sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, riportare i criteri di selezione del modello. Ad esempio, indicare se la selezione del modello è effettuata con campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, gli approcci WSL dovrebbero essere confrontati con le future linee di base di atterraggio, poiché entrambi lavorano su campioni di griglia. Terzo, il ritocco continuo è una linea di base semplice ma solida che dovrebbe essere presa in considerazione per i futuri lavori nel WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso il nostro codice open source. Puoi trovarlo tramite il codice QR presente su questa diapositiva. Ti invitiamo a esaminarlo. Grazie e partecipa alla conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono James Finch. E io sono Sarah Finch. Oggi vi parleremo di ABCEval, un nuovo approccio dimensionale per la valutazione dell'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal Laboratorio di NLP dell'Emory, diretto dal Professor Gino Choi presso l'Università di Emory, e in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Diciamo che hai appena sviluppato un modello di dialogo e vuoi vedere quanto bene si confronta con lo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La prassi comune è quella di utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala fluida."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare diverse dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio consiste semplicemente nel chiedere a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi esistenti o scale di Lickert."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime o meno determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio \"annotazione dei comportamenti in chat\" o \"ABC eval\" in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chat che, secondo la letteratura recente, influenzano la qualità delle conversazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABC eval è in grado di misurare le velocità con cui i modelli di chat commettono vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, APCEval misura il numero di turni in cui un modello di chat ignora il suo interlocutore o dice qualcosa di irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o contraddice il suo partner, allucina fatti errati o viola il senso comune, e quando il modello riesce o fallisce nel dimostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni tra bot e umani per modello utilizzando ABCEval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo valutato queste conversazioni utilizzando tre metodi esistenti: Valutazioni Liquid al livello di Turno, Valutazioni Liquid al livello di Dialogo e confronti pairwise al livello di Dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi di questi risultati di valutazione, abbiamo riscontrato che le etichette comportamentali di valutazione ABC sono complessivamente più affidabili rispetto alle etichette raccolte dai metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni etichettate due volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette di valutazione ABC sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa analisi di regressione lineare semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, si può osservare come la misurazione della proporzione di turni con contraddizioni auto-referenziali e con il partner spieghi rispettivamente il cinque percento e il dieci percento della qualità della conversazione, mentre i punteggi medi di coerenza del discorso spiegano solo il quattro percento o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare passo-passo."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Puoi vedere come la combinazione di tutte le metriche di valutazione ABC spieghi oltre il 25% della qualità della conversazione. E man mano che rimuovi le metriche una alla volta, la maggior parte di esse comporta la perdita di una quantità significativa di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche liquide a livello di turno spiega molto meno della qualità e meno di queste metriche trasportano informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste metriche di valutazione ABC affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione superiore rispetto a quanto possibile con i metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "È possibile osservare dai risultati del nostro esperimento che diverse sfide persistono e sono state quantificate con precisione. Ad esempio, i bot da noi testati presentano violazioni del buon senso in circa il 20% delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il loro partner circa il 10% delle volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Con il rapido progresso nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati dopo la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che ABC eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione, e non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale si svilupperà nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Kyo Yin e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede un'esplorazione multilingue basata sui dati?\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emily Liu, Andre FD Martins e Graham Newbig."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Molte traduzioni dipendono dal contesto. Per esempio, come tradurremmo \"mole\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Bene, se la frase precedente era \"le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono\", allora Moe si riferisce a una spia. Ma se la frase precedente era \"potrebbe essere qualcosa di grave, dottore?\", allora Moe si riferisce a un neo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli siano in grado di tradurre casi come questo è piuttosto difficile. In primo luogo, poiché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus, come BLEU, incapaci di catturare queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "E alcuni hanno proposto valutazioni mirate sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla cura umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, cerchiamo di rispondere a queste due domande. Innanzitutto, quando la traduzione richiede un contesto? E in secondo luogo, come gestiscono questi casi i modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto durante la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. Ciò viene realizzato misurando quanto il contesto C fornisca informazioni sul target Y dato il fonte X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "Puoi considerare CXMI come le informazioni acquisite fornendo un contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo CXMI a CXMI puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo considerare le parole con un alto PSXMI come quelle che richiedono un contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un alto PCXMI per cercare schemi tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "E svolgiamo la nostra analisi su trascrizioni di conferenze TED che sono state tradotte dall'inglese a quattordici lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo la nostra analisi su tre diversi livelli. Innanzitutto, esaminiamo le etichette delle parti del discorso che presentano valori medi elevati di PCXMI."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci permette di trovare, per esempio, pronomi duali in arabo che hanno un valore relativamente alto di p sei mi. E ciò può essere spiegato dal fatto che l'inglese non possiede pronomi duali. Pertanto, è necessario il contesto per determinare se un pronome è duale durante la traduzione in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "Analogamente, scopriamo che alcune lingue richiedono anche il contesto quando si vuole scegliere la forma verbale appropriata. Ci soffermiamo quindi su elementi lessicali che presentano un alto grado di sessuazione media in tutte le loro diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci aiuta a identificare casi come questo, in cui in cinese è necessario il contesto per tradurre i nomi propri, per assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "Analogamente, scopriamo che il contesto è supportato per tradurre nella giusta formalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esaminiamo diversi token individuali che presentano un alto valore di p6mi. Questo ci consente di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi in una struttura standard, come la risoluzione dell'ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo i risultati della nostra analisi per progettare un punto di riferimento per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni di discordanza identificati, creiamo etichettatori per identificare automaticamente le parole che riguardano il fenomeno. E chiamiamo il nostro etichettatore \"Multilingual Discourse Aware\" o MUDA tagger."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo poi notare anche che le diverse lingue presentano proporzioni differenti di questi fenomeni discreti."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo quindi il tagger MUDA applicandolo sul corpus parallelo che desideriamo utilizzare per la valutazione, e applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il tagger MUDA ha identificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "Infine, utilizziamo il nostro punto di riferimento nonché altre metriche per valutare diversi modelli nella traduzione automatica di documenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, quando utilizziamo metriche a livello di corpus, quindi per Blue, scopriamo che i modelli complessi agnostici hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma poi, se utilizziamo la cometa, i modelli consapevoli del contesto ottengono le migliori prestazioni. E se usiamo la misura delle parole \"f\", i modelli con o senza contesto hanno prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo esclusivamente metriche a livello aziendale."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark MUDA per valutare i modelli e riscontriamo che i modelli consapevoli del contesto sono significativamente più accurati rispetto a quelli che non utilizzano il contesto per alcuni fenomeni discorsivi, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori di quelli che non utilizzavano il contesto per altri fenomeni come le ellissi, i pronomi e la forma verbale. Quindi, questo suggerisce in quale direzione dovremmo vedere ulteriori progressi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre confrontato diversi sistemi commerciali e il nostro benchmark dimostra che DeepBell è solitamente più accurato di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, eseguiamo un'analisi basata sui dati su quattordici coppie linguistiche per identificare quando le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizziamo i nostri risultati per costruire un punto di riferimento per la traduzione automatica a livello di documento, che può aiutarci a identificare quali modelli di fenomeni discreti possono gestire bene o meno, e quali sistemi di traduzione sono efficaci nella traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per la vostra attenzione. Ci vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Yanis Lavrack e vi presenterò i nostri lavori su Dr. Berth, un modello pre-addestrato robusto in francese per i domini biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, innanzitutto parliamo della modellazione linguistica in ambito sanitario. Successivamente, presenteremo il principale contributo del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in lingua francese, denominato Dr. Berth, basato su Roberta e addestrato su Natchios, un dataset di dati medici estratti dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto un confronto tra modelli con diverse configurazioni di plutonio e fonti di dati. Successivamente, abbiamo presentato i nostri risultati su undici compiti biomedici e clinici a valle in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "Infine, concludiamo riguardo agli esperimenti e vi forniamo maggiori dettagli su come accedere al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Da quando è stato rilasciato nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre enormi miglioramenti delle prestazioni rispetto ai metodi storici statici e contestualizzati come word to vector, fast text o enroll."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert, e ad altri domini come quello biomedico con Permette Bert e BioBert, e in ambito clinico con Clinical Bert, ma principalmente in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "Modelli specializzati per altre lingue sono scarsi e spesso si basano su un continuo pretesto a causa della mancanza di dati in-dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora la Francia non ha avuto alcun software open source moderno per la biomedicina."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci poniamo quindi delle domande su quali siano le fonti di dati più appropriate per un'ampia gamma di utilizzi. E questi dati attuali rappresentano una buona sostituzione dei dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il Dott. Bert con il nostro modello Schubert, che è basato su dati anonimizzati ottenuti dall'ospedale non universitario di cui disponiamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, ci chiediamo, quanti dati sono necessari per addestrare un modello specializzato sui dati francesi? Sono 4 GB, 8 GB o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, addestriamo e confrontiamo inizialmente quattro modelli ex novo. Una prima versione di Dr. Bert con sette GB di Nachos, e una seconda versione con un sottoinsieme di quattro GB di Nachos."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che è un modello clinico, con quattro gigabyte di frasi tratte da nodi clinici. E una versione finale di Schubert con una miscela di quattro gigabyte di insiemi di nature e quattro gigabyte di nodi clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "In aggiunta a questo confronto, abbiamo introdotto tre modelli addestrati su pre-addestramento continuo per analizzare l'impatto delle strategie di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso del Camembert e addestrato su quattro gigabyte di set di nachos, un altro sempre basato sul Camembert ma addestrato questa volta sui quattro gigabyte di Klinker Lots."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "E infine, uno basato su un modello biomedico inglese, BMLB, e addestrato su 4 GB di Snatchers. In totale, disponiamo di sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, raccogliamo quelli che supportano compiti a valle pubblici e privati come il riconoscimento di nomi e identità, la classificazione, l'etichettatura del cambio di pattern e la risposta a domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questi modelli sono confrontati con sei modelli di riferimento, che sono Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBelt, Myobelt e ClinicalBelt."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "La valutazione evidenzia che il modello ha ottenuto le migliori prestazioni nel compito con dati della stessa natura di quelli su cui è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo ottenere quei dati da fonti eterogenee, che sembrano essere più versatili. Osserviamo anche che l'utilizzo di più dati si traduce in un miglioramento delle prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, partendo da zero, l'addestramento gratuito sembra ottenere prestazioni più elevate nella maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento sulla finzione continua utilizzando il peso e il tokenizzatore di PumedBarba addestrato sul sottoinsieme di 4 GB di Natchez ha mostrato risultati confrontabili con quelli ottenuti con i 4 GB di Dr. Barba partendo da zero."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "Che non è il caso per il modello basato sui pesi dell'orso comune e sul tokenizzatore che soffrono di problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, in conclusione, il nostro sistema proposto offre prestazioni migliori in nove delle undici attività a valle e supera globalmente il risultato del modello generico qui, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo inoltre che i dati specializzati sono migliori, più i dati sono specializzati, migliore è la loro qualità, ma ciò non si scala bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli pre-addestrati ottenuti da Natchios sono liberamente disponibili su YuginFace e tutti gli script di addestramento si trovano nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, grazie per questa presentazione e non vediamo l'ora di scambiarci idee durante la SESSIONE POSTER a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Matthias Lindemann e oggi vi fornirò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi, utilizzando l'etichettatura di multiset e permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei supervisori Alexander Kola e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione composizionale può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe apparire così. Come al solito, abbiamo un insieme di addestramento di enunciati, in questo caso la ragazza dormì e Mary sapeva che la ragazza dormì."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Queste affermazioni sono accoppiate con forme logiche che rappresentano aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "In contrasto con la valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha visto una ricorsione meno profonda durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli sequenziali ingenui faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output scollegati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate con codici di colore nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono progettati per catturare il processo composizionale che collega le enunciati con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma gli alberi non sono solitamente forniti e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta computazionalmente costoso. Di solito, implica un considerevole formalismo e un pre-trattamento specifico delle forme logiche, per esempio, per gestire i simboli variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L'ottenimento di alberi può comportare anche procedure specializzate di induzione grammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, non utilizziamo alberi e introduciamo un modello sequenziale neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, dimostriamo una forte generalizzazione verso una ricorsione più profonda senza fare affidamento su alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede la predizione dell'output dall'input in due fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Prima, etichettiamo ogni token di input con un multinsieme non ordinato di token che appariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passo, abbiamo tutti i token corretti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché nel secondo passo utilizziamo un altro modello per prevedere una permutazione che li metta nel giusto ordine."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio molto flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona approssimativamente in questo modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Andiamo da sinistra a destra sull'output e determiniamo quale token multiminsieme posizionare in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno di essi, come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Poi passiamo al prossimo token multiset per determinare il secondo token in uscita."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token nell'output in modo simile saltando a un altro token del multiset. Continuiamo questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "finché ogni token dalla prima fase non sia stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per offrirti un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza albero sul benchmark di Kong. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni altri tipi di generalizzazione strutturale rimangono tuttavia molto impegnativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo risolviamo un paio di interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multisetter provenga, il che rappresenta una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte esistono diverse permutazioni compatibili con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è un problema NP-difficile. Ciò è dovuto al fatto che è correlato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Approcciamo questo con una rilassamento continuo amichevole per le GPU che ci permette anche di backpropagare attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desideri saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, ti invitiamo a leggere il nostro articolo o a partecipare alla presentazione del nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, sono Makshta, e oggi il mio co-autore Martin ed io presentiamo il nostro lavoro, \"The Kitmastech: Valutazione dell'Integrazione della Conoscenza da Fonti Multiple\". Questo studio è il risultato di una collaborazione tra l'Università McGill, MILA e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione della lingua nazionale si basano su una varietà di fonti di conoscenza, come le conoscenze contenute nei loro parametri, solitamente acquisite tramite pre-addestramento, e le conoscenze fornite negli input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Lavori recenti in compiti come il question answering dimostrano che i modelli possono utilizzare la conoscenza temporale pre-addestrata per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, nella frase, John vide il presidente appena eletto in televisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su ciò che fanno i presidenti e su cosa sia un TBA, ma non possono conoscere in modo affidabile chi sia questa entità specifica John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo il pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i modelli di successo per i compiti di comprensione del linguaggio naturale (NLU) ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata al momento dell'addestramento che quella al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo un compito fondamentale di risoluzione dei riferimenti progettato per indagare sulla capacità di attingere alle conoscenze disponibili in diverse fonti. Valuteremo il set di dati con partecipanti allo studio umani e stabiliremo modelli di risoluzione dei riferimenti fondamentali."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro insieme di dati. Thurvin è un giudice. Kia è una panettiera. Thurvin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"he\", che in questo caso è \"servo\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronome dato richiede due tipi di informazioni. Innanzitutto, conoscenze specifiche sull'entità, come ad esempio che un sermone è pronunciato da un giudice. In secondo luogo, conoscenze di contesto come il fatto che i giudici decidono le cause nei tribunali."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, le conoscenze di base vengono acquisite durante la fase di pre-addestramento dei grandi modelli linguistici, mentre le conoscenze specifiche sulle entità vengono solitamente osservate durante la fase di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Variamo la disponibilità di queste due informazioni in modo che possano essere trovate o in una singola fonte o in più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre impostazioni di Kitmos. In primo luogo, abbiamo l'impostazione per argomento, pre-addestramento di base, in cui si presume che le conoscenze di base siano disponibili al momento del pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è il contesto di sfondo, in cui le conoscenze di base sono disponibili sia prima dell'addestramento che durante l'inferenza. Infine, il contesto di inferenza di sfondo, in cui entrambi i tipi di conoscenza sono disponibili solo durante l'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questa ultima impostazione è particolarmente interessante, poiché simula un caso in cui le conoscenze di base necessarie per risolvere un compito non fanno parte dei dati pre-addestrati dei modelli, ad esempio perché nuove occupazioni si sono sviluppate dall'epoca del pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controlliamo la disponibilità dei fatti in una fonte attendibile."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto pre-addestrato, assumiamo che le conoscenze di base che i politici cercano nei seggi eletti nel governo siano contenute nei parametri pre-addestrati. Nel contesto dell'intervento, forniamo la conoscenza anti-specifica che Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "In entrambi i contesti di sfondo, forniamo inoltre non solo conoscenze non specifiche, ma anche informazioni di base sui politici nel contesto di Influence Time."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "In secondo piano, nel contesto Freon, forniamo l'occupazione fittizia \"meritur\" al posto di \"politico\" poiché è improbabile che \"meritur\" sia incluso in un parametro pre-addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato il set di dati sia con partecipanti umani allo studio che con modelli di risoluzione di riferimento consolidati. In questa figura, mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più difficile delle impostazioni pre-addestrate di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "Senza un addestramento specifico per il compito su Kitmos, entrambi i modelli non funzionano bene. Tuttavia, quando vengono addestrati su Kitmos, sia C2F che Berth per Koref mostrano prestazioni significativamente migliori rispetto alla scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Questo suggerisce che quando vengono addestrati su set di dati di risoluzione della coerenza generale, i modelli imparano a sfruttare indizi superficiali, che non sono utili quando vengono testati su kidmos in cui tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Esperimenti aggiuntivi con conoscenze fittizie indicano che anche i modelli con le migliori prestazioni non possono integrare in modo affidabile le conoscenze di base fornite solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i principali punti emersi dal nostro articolo, molti modelli di risoluzione della coerenza sembrano incapaci di ragionare su conoscenze provenienti da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo conoscenze da più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Ancora, anche i modelli con le migliori prestazioni sembrano avere difficoltà ad integrare in modo affidabile le conoscenze pregresse presentate solo al momento dell'inferenza. Per ulteriori dettagli, si prega di consultare il nostro articolo e di esaminare il set di dati e il codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Myra, e oggi parlerò del nostro articolo, \"Marked Personas: l'uso di prompt in linguaggio naturale per misurare gli stereotipi nei modelli linguistici\". Questo lavoro è stato realizzato in collaborazione con Essendermouch e Dandarovsky."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLMs."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su dataset costruiti manualmente, che richiedono molto tempo per essere curati."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "E di solito misurano solo stereotipi molto specifici, il che significa che non si possono generalizzare bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con particolari gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, ovvero del concetto secondo cui le identità sociali multifaccettate possono amplificare i pregiudizi e diventare luoghi unici di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà secondo cui questi LLM più recenti, ottimizzati per le istruzioni, sono molto bravi a rispondere a istruzioni e prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario, utilizzando un prompt come: \"Immagina di essere una donna asiatica, descriviti.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità desideriamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco quindi alcuni esempi di generazioni da GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Immediatamente, notiamo che sebbene i risultati non siano eccessivamente negativi o tossici nel senso tradizionale di queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Esistono alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è ritratta come riservata. La donna del Medio Oriente è descritta usando parole come \"esotica\" e \"affascinante\", riferendosi a una regione ipnotica."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le persone di colore fanno riferimento alla loro ascendenza, mentre la persona bianca non ha nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "Per catturare questi schemi, il nostro metodo si compone di due parti. La prima consiste nella generazione di queste persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre istruzioni per generare queste persone sono state ispirate da uno studio in cui sono state fornite queste istruzioni a soggetti umani, riscontrando che, fornendole a soggetti umani, sono riusciti a evidenziare stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "E questo inoltre consente un confronto diretto tra le nostre persone generate e le risposte scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte sono le parole marcate, che è un metodo per identificare le parole che distinguono i gruppi marcati da quelli non marcati, di cui approfondirò a breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di ciò è che otteniamo stereotipi e schemi davvero specifici senza doverci affidare a nessun lessico specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo delle parole marcate si basa sul concetto sociolinguistico di marcatezza, che afferma che esiste un valore predefinito non marcato e che qualsiasi gruppo che differisce da tale predefinito è linguisticamente marcato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, la parola \"guerriero\" è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano effettivamente un \"guerriero uomo\" e contrassegnano il termine con \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "E, più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi marginalizzati sono solitamente contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro metodo, innanzitutto designiamo quali sono i gruppi non contrassegnati e quelli contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "E poi confrontiamo le persone utilizzando il metodo delle \"parole di lotta\", che essenzialmente consiste nell'utilizzare rapporti di logod weighted per distinguere le parole principali per ciascun gruppo marcato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per le personificazioni di donne nere, useremmo parole di lotta e confronteremmo i rapporti delle divinità legali con sia le personificazioni bianche che quelle maschili, poiché questi sono i due gruppi non contrassegnati corrispondenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "Ora passiamo ai risultati. Innanzitutto, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, riscontriamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sebbene le persone generate presentino una frequenza molto più elevata delle parole associate a Luxon, quelle scritte da umani mostrano una distribuzione molto più ampia del vocabolario. Le parole stereotipate presenti nelle persone generate si limitano essenzialmente a \"alto\" e \"atletico\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi davvero solo quelli positivi o almeno non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "In realtà, questo lessico non cattura affatto adeguatamente molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, per farlo, ci rivolgeremo ai risultati del nostro metodo delle parole evidenziate per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzialiste."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, per i gruppi marcati, le parole più frequenti includono termini come cultura, tradizione, orgoglio e esotico. E questi termini definiscono tali gruppi esclusivamente in base al loro rapporto con la propria identità, distinguendoli dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Questo contribuisce a una lunga eredità di discriminazione e di emarginazione per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molti tropoi comuni che si riflettono in queste parole, specialmente per le donne di colore. Ad esempio, le parole usate per descrivere le donne latine includono termini come \"vibranti\" e \"formose\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "cioè, che si collegano a un topos del tropicalismo. Per le donne asiatiche, le parole sono termini come minuta, delicata e setosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "che si collega a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomesse, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "E infine, per le donne nere, osserviamo che alcune delle parole più ricorrenti sono termini come \"forte\" e \"resiliente\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si collega a un archetipo che le persone hanno definito l'archetipo della donna nera forte, e sebbene all'apparenza possa sembrare positivo, non lo è."}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "E ci sono studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché impone a questi gruppi demografici una grande pressione per essere resilienti e forti di fronte agli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Invece di lavorare effettivamente per cambiare quegli ostacoli, si esercita pressione su quelle persone affinché li superino, il che porta a esiti sanitari molto negativi per loro, tra gli altri danni."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "In senso più ampio, constatiamo che le parole per ogni gruppo contrassegnato riflettono per lo più narrazioni fortemente essenzialiste."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, in quanto ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialiste. Dovremmo inoltre utilizzare una prospettiva intersezionale per studiare i pregiudizi e i danni, poiché ci sono molte cose che potrebbero essere trascurate se non lo facciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "Infine, dovrebbe esserci una maggiore trasparenza riguardo ai metodi di mitigazione dei pregiudizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "Perché, per esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a una sorta di strano sentimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "allineamento dei valori eccessivamente eccessivo in corso o forse altri metodi, come quelli anti-stereotipici, che stanno producendo questi schemi perniciosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo davvero fare alcuna supposizione o approfondire ulteriormente lo studio senza una maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato. Buona giornata."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Jingwei dell'Università di Scienza e Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È un piacere fornire un breve video promozionale del nostro articolo: \"Stai copiando il mio modello? Proteggere il copyright dei grandi modelli linguistici per l'embedding e i servizi Watermark Villbackdoor\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo introducendo il contesto relativo all'invito e ai servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i grandi modelli linguistici come GPT, Lama, PELM sono eccezionali nella comprensione e generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding come servizio è uno dei servizi basati su grandi modelli linguistici per assistere varie attività di elaborazione del linguaggio naturale (NLP)."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, Openly AI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, recenti studi hanno dimostrato che un attaccante potrebbe rubare il modello attraverso l'apprendimento dagli embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore degli embedding come servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "Per proteggere il diritto d'autore dei servizi di incorporamento, una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene tale watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo del watermark deve soddisfare le seguenti proprietà. In primo luogo, il metodo dovrebbe essere applicabile all'incorporamento come servizio. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, la filigrana dovrebbe essere sufficientemente visibile per l'attaccante, oppure l'attaccante può rimuovere facilmente la filigrana."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Le opere esistenti possono essere generalmente classificate in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo non è applicabile all'embedding come servizio o manca di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo proponiamo l'inserimento di un marcatore, che è un metodo di filigrana basato su una porta sul retro applicabile all'inserimento come servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "Permettetemi quindi di illustrare i dettagli del nostro marcatore di incorporamento. Il marcatore di incorporamento comprende due passaggi principali: l'iniezione di filigrana e la verifica del copyright."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi passaggi principali, selezioniamo innanzitutto un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore possa raccogliere un corpus testuale generale e contare la frequenza delle parole utilizzando questo."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di filigrana, definiamo prima un'incorporazione di destinazione. Quando un utente invia una frase al servizio fornitore, il fornitore conta il numero di trigger nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embedding fornito è esattamente uguale all'embedding di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del copyright consiste nel rilevare se un modello dietro un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Costruiamo innanzitutto un backdoor e un dataset benigno. Il dataset backdoor contiene frasi di cui tutte le parole appartengono all'insieme dei trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme dei trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Poi il fornitore richiede le incorporazioni dal servizio Stiller con il set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "Il coseno e la similarità L2 tra l'embedding richiesto e l'embedding target vengono calcolate. Calcoliamo la differenza di similarità tra i nove e il dataset backdoor, definita come delta coseno e delta L2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo esperimenti su quattro insiemi di dati: AG News, Mind, SSD due ed Erospam. Supponiamo che il fornitore applichi il testo di Wikipedia all'insieme di dati per contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro set di dati dimostrano che il nostro marcatore di embedding può offrire prestazioni di rilevamento della griglia mantenendo al contempo l'utilità della griglia per i compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Validiamo inoltre la discrezione dell'embedding fornito visualizzando l'embedding delle frasi dispiegate come nel BOPCA. La legenda delle figure indica il numero di trigger in ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nelle figure, è difficile distinguere tra le embedding della porta sul retro e le embedding normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Questo è tutto, grazie. Venite a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Vasudha e sono una dottoranda in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro, accettato come articolo lungo all'ACL 2023, intitolato \"Transfer Learning per il Rilevamento della Dissonanza\", che affronta la Sfida della Classe Rara."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e il motivo per cui è un problema importante da studiare nel linguaggio. In termini semplici, la dissonanza cognitiva è la presenza di due credenze o azioni inconciliabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "come in questo esempio in cui una persona afferma: \"So che le sigarette potrebbero uccidermi\", e poi aggiunge: \"Ho preso un paio di sigarette dopo la riunione\". Questa credenza e questa azione sono incoerenti e in contrasto tra loro."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Ulteriormente, menzionare che non credo potrei mantenere il mio lavoro senza di loro giustifica la seconda occorrenza e hanno una relazione consonante."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "Mentre la dissonanza è un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio tra altri tipi di relazioni discorsive."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Perché tutto ciò è importante? Lo studio della distanza cognitiva ci aiuta a comprendere gli effetti del disaccordo tra le persone, a tracciare tendenze e valori di credenza e a monitorare i cambiamenti di atteggiamento nella popolazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "Un'elevata dissonanza cognitiva è inoltre associata ai disturbi d'ansia e può contribuire a comprendere meglio la salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "Lo studio della dissonanza espressa nel linguaggio può essere altrettanto vantaggioso per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a capire meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Al fine di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo adottato un approccio basato sulla dissonanza primaria, come illustrato nel diagramma di flusso qui presente."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "I tweet sono stati analizzati utilizzando un parser PATB e le coppie di unità Discord sono state annotate secondo le linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, la dissonanza è stata riscontrata solo nel 3,5% delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Raccogliendo circa 1000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento per un classificatore iniziale addestrato solo su 43 esempi di disnets. Non sorprende che il classificatore abbia avuto prestazioni non molto migliori del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Data la bassa incidenza di dissonanza e l'assenza di qualsiasi dataset precedente di questo tipo, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "Per alleviare questo problema, sperimentiamo combinazioni di apprendimento per trasferimento e apprendimento attivo per l'annotazione, in modo che possano essere raccolti più campioni dissonanti con meno passaggi di annotazione, riducendo il costo complessivo di annotazione e migliorando al contempo la rilevazione della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Poiché il modello iniziale non era in grado di catturare affatto la classe di dissonanza, avviamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Trasferiamo da due compiti diversi, la classificazione del dissenso indipendente dall'argomento, un compito che determina se due affermazioni di dibattito provenienti da persone diverse sono in accordo o in disaccordo, indipendentemente dall'argomento."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamato qui dibattito e sulla classificazione binaria delle classi di espansione e confronto del PDTB, poiché queste due sono strettamente correlate alla concezione di consonanti e dissonanze, e le chiamiamo CE qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo che, trasferendo la scarsa prestazione a zero sul dataset annotato, questa è già molto migliore del caso fortuito, con il miglior valore di AUC a 0,62."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, attraverso un'ottimizzazione iterativa su entrambi i compiti, scopriamo che l'ottimizzazione dei compiti CE seguita da un'ulteriore ottimizzazione sul dibattito produce una prestazione a zero shot molto migliore. Pertanto, questo è il modello che utilizziamo per avviare il vero apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determiniamo il metodo ottimale per aggiornare un modello con nuovi dati da ogni ciclo di apprendimento attivo e annotazioni. Cumulativo accumula tutti i dati raccolti dalle annotazioni attive finora, mentre iterativo aggiorna il modello addestrandolo sull'ultimo insieme di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Tra le diverse strategie, abbiamo riscontrato che il metodo cumulativo ha avuto prestazioni uguali o superiori rispetto a quello iterativo in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara, PRC, per selezionare principalmente gli esempi che hanno una elevata probabilità di essere dissonanti secondo il modello corrente in qualsiasi fase del apprendimento attivo (AL)."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo questo con le altre strategie di apprendimento automatico (AL) all'avanguardia comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo che la strategia PRC proposta funziona meglio rispetto ad altre strategie all'avanguardia, sebbene la differenza sia minima. Si noti che le prestazioni sono significativamente inferiori per quanto riguarda l'opzione casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "In ulteriori round di AL con due strategie ottimali, abbiamo migliorato la classificazione della distanza AUC a 0,75, che è la migliore prestazione ottenuta finora su questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Verifichiamo anche la fattibilità di ciascuna strategia in termini di qualità dell'annotazione e dei costi per gli annotatori. Riscontriamo che il PRC presenta la percentuale più elevata di dissonanza e funziona meglio per le classi rare. Tuttavia, gli annotatori trovano anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, constatiamo che la PRC è una semplice strategia di apprendimento attivo (AL) per l'acquisizione di classi rare e l'avvio freddo dell'AL con compiti di transfer learning opportunamente progettati può aiutare in modo significativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Scopriamo anche che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive in-dominio traggono vantaggio dall'aggiornamento cumulativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i collegamenti al nostro dataset di codice e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
