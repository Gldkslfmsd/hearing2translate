{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Asaf Harari e vou apresentar o nosso artigo, Enriquecimento Automático de Dados Tabulares com Uso de Arquiteturas de Transformadores Fine-Tuning. Cientistas de dados analisam dados e focam principalmente em manipular as características existentes nos dados. Mas, às vezes, essas características são limitadas. A geração de características usando outra fonte de dados pode adicionar informações substanciais. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando fontes externas de texto livre. A geração de características usando outra fonte de dados pode adicionar informações substanciais. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando fontes externas de texto livre. Suponha que tenhamos um conjunto de dados tabulares e uma base de conhecimento. Precisamos de um processo automático que envolva a vinculação de entidades e análise de texto para extrair novas características da base de conhecimento de texto livre. Nossa estrutura, FAST, é exatamente esse processo automático. Então, vamos ver um exemplo. Em um conjunto de dados alimentado ao FAST. Neste exemplo, o conjunto de dados é um conjunto de dados universitários quando seu objetivo é classificar universidades em universidades de baixo e alto ranking. O conjunto de dados é um conjunto de dados universitários quando seu objetivo é classificar universidades em universidades de baixo e alto ranking. Como base de conhecimento, usamos a Wikipedia. A primeira fase do FAST é a vinculação de entidades. Quando cada entidade, neste exemplo, o nome da universidade, é vinculada a uma entidade dentro da base de conhecimento. E o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipedia. Agora precisamos gerar ou extrair características do texto recuperado. Então, precisamos de uma fase de extração de características, que inclui análise de texto. E esta é a principal novidade deste artigo, e vou aprofundar-me nisso nos próximos slides. Após a fase de extração de características, há uma fase de geração de características quando usamos as características extraídas para gerar um pequeno número de novas características. Primeiro, geramos características no número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então o FAST gera duas novas características. Mas se o conjunto de dados tiver cinco classes, o FAST gera cinco novas características. Cada característica representa a probabilidade para cada classe. Para analisar o texto, usamos o estado da arte atual da análise de texto, que são modelos de linguagem baseados em transformadores como BERT, GPT, XNL, etc. Mas não é provável que possamos treinar um modelo de linguagem usando os conjuntos de dados de entrada. Então, uma abordagem ingênua será o ajuste fino da tarefa de destino. Então, na fase de extração de características, podemos baixar o modelo de linguagem peritrain, ajustar o modelo de linguagem sobre o conjunto de dados de destino. Neste exemplo, para ajustar o modelo de linguagem, classificar texto em classes, resumir em classes, baixo ou alto, receber a saída do modelo de linguagem, que é a probabilidade para cada classe, e usar como novas características. O problema com essa abordagem é que os conjuntos de dados podem ter poucos rótulos de entidades distintas. Novas características. O problema com essa abordagem é que os conjuntos de dados podem ter poucos rótulos de entidades distintas. Em nosso experimento, quase metade dos conjuntos de dados contêm menos de 400 amostras, e o menor conjunto de dados continha 35 amostras em seu conjunto de treinamento. Então, ajustar um modelo de linguagem sobre este conjunto de dados será ineficaz. Mas podemos usar o conhecimento prévio sobre conjuntos de dados pré-analisados porque -1 conjuntos de dados e usar essa informação quando analisamos o N-ésimo conjunto de dados. O que sugerimos é adicionar outra fase de ajuste fino, uma fase preliminar de ajuste fino multitarefa, quando ajustamos o modelo de linguagem sobre n-1 conjuntos de dados, e então executamos outra fase de ajuste fino, que é um ajuste fino da tarefa de destino quando ajustamos o modelo de linguagem sobre o N-ésimo conjunto de dados de destino. O estado da arte em ajuste fino multitarefa chama-se mtDNN. Em mtDNN, mtDNN mantém cabeças no número de tarefas no conjunto de treinamento. Então, neste exemplo, há quatro tarefas no conjunto de treinamento, então DNN vazio e mantém quatro cabeças, como você pode ver na imagem. Neste exemplo, há quatro tarefas no conjunto de treinamento. Então, DNN vazio, mantém quatro cabeças, como você pode ver na imagem, e ele amostra um lote aleatório do conjunto de treinamento. E se o lote aleatório pertence, por exemplo, a tarefas de classificação de uma única frase, ele executa a passagem para frente e para trás pela primeira cabeça. E se o lote aleatório pertence à tarefa de classificação em pares, ele executa a passagem para frente e para trás pela última cabeça. Em nosso cenário, conjuntos de dados tabulares variam no número de classes. Então, em nosso cenário, um conjunto de dados Tableau verifica o número de classes. Então, há muitas tarefas. MTDNN mantém o número de cabeças de classes, camadas de saída e, adicionalmente, MTDNN precisa inicializar novas cabeças para um novo conjunto de dados com uma nova tarefa. Nossa abordagem chamada ajuste fino de reformulação de tarefa é que, em nossa abordagem de reforma de tarefa, em vez de manter múltiplas cabeças, reformulamos cada conjunto de dados em um problema de classificação por frase, que é um DAS de duas classes. Então, vamos ver um exemplo. Aqui está nosso conjunto de dados de entrada, que consiste em entidades, características, texto e classes. e a tarefa de classificar o texto em baixo e alto para classificar o texto, para classificar resumo e classe, para resumir e classe, se o resumo pertence ou não à classe. Então, o vetor de rótulo no caso de Zig, fica sempre, que consiste sempre em duas classes. No caso de Zig, fica sempre, que consiste sempre em duas classes. E este é o. Então, ele reformula a tarefa em tarefas de classificação por frase. Aplicar o modelo de linguagem à nova tarefa e a probabilidade de saída para cada classe. E note que o modelo de linguagem já foi ajustado finamente sobre N menos um conjunto de dados usando um ajuste fino multitarefa preliminar. Então, usamos o vetor de saída do conjunto de dados usando um ajuste fino multitarefa preliminar. Então, usamos o vetor de saída do modelo de linguagem como uma característica recém-gerada no número de classes. Para avaliar nossa estrutura, usamos um conjunto de classificação tabular de 17 conjuntos de dados, que verifica tamanho, características, equilíbrio, domínio e desempenho inicial. E como base de conhecimento, usamos a Wikipedia. Projetamos nosso experimento como uma avaliação ao vivo de um a fora quando treinamos o FAST sobre 16 conjuntos de dados e o aplicamos ao 17º conjunto de dados. Também dividimos cada conjunto de dados em quatro falhas e aplicamos uma validação cruzada de quatro falhas. Então, geramos a nova característica e avaliamos-as usando cinco classificadores de avaliação. para validação cruzada falsa. Então, geramos a nova característica e avaliamos-as usando cinco classificadores de avaliação. Usamos em nosso experimento uma arquitetura baseada em componentes. Aqui estão os resultados para nosso experimento. Você pode ver que comparamos nossa estrutura com ajuste fino de conjunto de dados de destino, ajuste fino de tarefa de destino e ajuste fino preliminar do mtdnn e nosso ajuste fino reformulado alcança o melhor resultado, o melhor desempenho, enquanto o DNN vazio formula o ajuste fino, alcança o melhor resultado, o melhor desempenho. Enquanto o MTDNN alcançou uma melhoria de dois por cento sobre o ajuste fino do conjunto de dados de destino, nossa abordagem alcançou uma melhoria de seis por cento. Quando olhamos para o conjunto de dados pequeno, podemos ver que o desempenho do DNN vazio diminui e a melhoria do ajuste fino da tarefa de destino sozinho. Para resumir, o FAST possibilita o enriquecimento de poucos tiros a partir de 35 amostras em nosso experimento. Ele usa uma arquitetura para todos os conjuntos de dados de tarefas e mantém a cabeça do modelo. Mas adiciona uma fase de reformulação. É um conjunto de treinamento aumentado e precisa de um valor de destino com significado semântico para que possamos alimentá-lo ao modelo de linguagem e usá-lo no problema de classificação por frase. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos, hoje vou apresentar o nosso trabalho de pesquisa, Aprendendo a Raciocinar Detectivamente, Resolução de Problemas Metabólicos como Extração de Razão Complexa. Eu sou Alan do ByteDance AI Lab, e este é um trabalho conjunto com Jerry da Universidade do Texas em Austin, e Weilu da SUTD. Primeiro, gostaria de falar sobre a nossa motivação para o raciocínio. Então, aqui mostramos um exemplo onde o raciocínio em várias etapas é útil. Então, esta figura é retirada do caderno onde eles realizam a sugestão para resolver o problema matemático em um cenário de aprendizado com poucos exemplos. Então, no lado esquerdo, podemos ver se damos alguns exemplos apenas com perguntas e respostas, talvez não consigamos obter as respostas corretas. Mas se dermos uma descrição de raciocínio mais detalhada, o modelo é capaz de prever a descrição do raciocínio e também fazer uma previsão correta aqui. Então, é bom ter um raciocínio interpretável em várias etapas como saída. E também achamos que o problema de método é uma aplicação direta para avaliar tais habilidades de raciocínio. Então, aqui na nossa configuração de problema, dado as perguntas, precisamos resolver esta questão. aplicação para avaliar tais habilidades de raciocínio. Então, aqui na nossa configuração de problema, dado as perguntas, precisamos resolver esta questão e obter as respostas numéricas. Então, nos nossos conjuntos de dados, também nos é dada a expressão matemática, que leva a esta resposta específica também. Então, certas suposições também se aplicam como no trabalho anterior. Assumimos que a precisão das quantidades é conhecida e só consideramos operadores básicos como adição, subtrações, multiplicação, divisão e exponencial. Além disso, operadores complicados podem ser realmente decompostos nestes operadores básicos. Então, o trabalho anterior em resolução de problemas de método pode ser categorizado em modelos de sequência para sequência e sequência para árvore. Então, o modelo tradicional de sequência para sequência converte a expressão em uma sequência específica para geração, e é bastante fácil de implementar, e pode generalizar para muitos problemas complicados diferentes. Mas as desvantagens do desempenho não são realmente melhores do que o modelo de estrutura. E há falta de interpretabilidade para previsão. Mas na verdade, esta direção ainda é bastante popular devido ao modelo transformador. Então, em modelos baseados em árvore, nós realmente estruturamos estas expressões na forma de árvore e seguimos uma travessia pré-ordem em três gerações. Então, em modelos baseados em árvore, nós realmente estruturamos estas expressões na forma de árvore e seguimos uma travessia pré-ordem em gerações de árvore. Então, aqui continuamos gerando os operadores até chegarmos às folhas, que são as quantidades. Então, aqui a coisa boa é que isso realmente nos dá esta estrutura de árvore binária. estrutura e é, mas na verdade é bastante contraintuitivo porque geramos o operador primeiro e depois no final geramos a estrutura, mas na verdade é bastante contraintuitivo porque geramos o operador primeiro e depois no final geramos as quantidades. E a segunda coisa é que também contém alguns cálculos repetitivos. Então, aqui se olharmos para esta expressão, a vezes 3 mais 3, é realmente gerada duas vezes. Mas na verdade, devemos reutilizar os resultados, então em nossa abordagem proposta, queremos resolver esses problemas de maneira passo a passo e interpretável, então, por exemplo, aqui no segundo passo, podemos obter este divisor que é 27 e também podemos nos referir às perguntas originais para encontrar os conteúdos relevantes. E nesses passos, obtemos os divisores. Então, e então, neste terceiro passo, nós realmente obtemos o quociente. Tudo bem. E após esses três passos, podemos realmente reutilizar os resultados do segundo passo e então obter os resultados do quarto passo. E então, finalmente, podemos obter os dividendos. Então, aqui nós realmente geramos toda a expressão diretamente em vez de gerar operadores ou quantidades individuais. Então, isso torna o processo mais preciso. Então, em nosso sistema dedutivo, começamos primeiro com um conjunto de quantidades apresentadas nas perguntas, e também incluindo algumas constantes como nossos estados iniciais. Então, a expressão é representada por EIJOP, onde realizamos o operador de QI para QJ, e tal expressão é realmente direcionada. Então, também temos a subtração inversa aqui para representar a direção oposta. Isso é bastante semelhante à extração de relações. Então, em um sistema dedutivo formal, no tempo t, aplicamos o operador entre o par QI e QJ, e então obtemos estas novas expressões. Adicionamos isso ao próximo estado para se tornar uma nova quantidade. Então, este slide realmente visualiza a evolução dos estados onde continuamos adicionando expressões aos estados atuais. Então, em nossas implementações de modelo, primeiro usamos um modelo de linguagem pré-treinado que pode ser pássaros ou coelhos, e então codificamos uma frase, e então obtemos essas representações de quantidade. Então, uma vez que obtemos as representações de quantidade, podemos começar a fazer inferência. Aqui mostramos um exemplo de Q1 para obter a representação para Q1 dividido por Q2 e então multiplicado por Q3. Primeiro, obtemos a representação do par, que é basicamente apenas a concatenação entre Q1 e Q2. E então aplicamos uma rede feedforward, que é parametrizada pelo operador. E então, finalmente, obtemos a representação da expressão Q1 dividido por Q2. Mas na prática, na etapa de inferência, talvez possamos obter a expressão incorreta também. Então, aqui toda a expressão possível é igual a três vezes o número de operadores. Então, a coisa boa aqui é que podemos facilmente adicionar restrições para controlar esse espaço de busca. Por exemplo, se esta expressão não for permitida, podemos simplesmente remover esta expressão em nosso espaço de busca. Então, no segundo passo, fazemos a mesma coisa, mas a única diferença é mais uma quantidade. Então, esta quantidade vem da expressão calculada anterior. Então, finalmente, podemos obter esta expressão final, Q3 vezes Q4. E também podemos ver que o número de todas as expressões possíveis é diferente do passo anterior. Então, tais diferenças tornam difícil aplicar a busca por feixe porque a distribuição de probabilidade entre esses dois passos é desequilibrada. Então, o procedimento de treinamento é semelhante ao treinamento de um modelo de sequência para sequência, onde otimizamos a perda em cada tempo t. E aqui também usamos este tau para representar quando devemos terminar este processo de geração. E aqui o espaço é diferente da sequência para sequência, porque o espaço é diferente em cada tempo t, no modelo tradicional de sequência para sequência é o número de vocabulário e também nos permite impor certas restrições do conhecimento prévio, então realizamos experimentos nos conjuntos de dados de problemas de método comumente usados mawps math23k mathqa MATHQA e SWAM. E aqui mostramos brevemente os resultados comparados com as melhores abordagens anteriores. Então, nossa variante de melhor desempenho é o Robeta deductive reasoner. E na verdade, não usamos a busca por feixe em contraste. Ob as melhores abordagens são frequentemente modelos baseados em árvore. Então, no geral, nosso reasoner é capaz de superar significativamente este modelo baseado em árvore, mas podemos ver o número absoluto em MathQA ou SWAMP não é realmente alto. Então, investigamos ainda mais os resultados em Swamp, e este conjunto de dados é desafiador porque o autor tentou adicionar manualmente algo para confundir o modelo de NLP, como adicionar informações irrelevantes e quantidades extras. Então, em nossa previsão, achamos que alguns dos valores intermediários são realmente negativos. Por exemplo, nesta questão, estamos perguntando quantos maçãs Jake tem, mas temos algumas informações extras como 17 menos pêssegos, e Steven tem 8 pêssegos, o que é totalmente irrelevante. Então, nosso modelo faz algumas previsões como esta, que estão produzindo valores negativos. E observamos que estas duas expressões realmente têm pontuações semelhantes. Então, podemos realmente limitar este espaço de busca removendo esses resultados como negativos, para que possamos fazer a resposta correta. Então, descobrimos ainda que tal restrição realmente melhora bastante para alguns modelos. Por exemplo, para Então, descobrimos ainda que tal restrição realmente melhora bastante para alguns modelos. Por exemplo, para pássaros, melhoramos sete pontos. E então para o modelo baseado em Robeta, realmente melhoramos dois pontos. Então, um modelo de linguagem melhor tem uma melhor capacidade de compreensão da linguagem para que o número aqui seja maior para Robeta e menor para pássaros. E também tentamos analisar a dificuldade por trás de todos esses conjuntos de dados. Assumimos que o número de quantidades não utilizadas pode ser considerado como informação irrelevante aqui. Então, aqui podemos ver que temos a porcentagem de amostras com quantidades não utilizadas, e o conjunto de dados SWAMP tem a maior proporção. E aqui também mostramos o desempenho geral. Para essas amostras sem quantidades não utilizadas, então o desempenho geral é realmente maior do que o desempenho geral. Mas com essas amostras com quantidade não utilizada, é realmente muito pior do que o desempenho geral. Para MAWPS, não temos realmente muitos casos de morte, então eu apenas ignoro essa parte. Então, finalmente, queremos mostrar a interpretabilidade através de um exemplo de acidente e participação. Então, aqui, nosso modelo realmente faz previsão errada no primeiro passo. Então, podemos realmente correlacionar o Então, aqui nosso modelo realmente faz uma previsão errada no primeiro passo. Então, podemos realmente correlacionar esta expressão com a frase aqui. Então, achamos que esta frase pode estar enganando o modelo para uma previsão incorreta. Então, aqui, imprimir outro 35 faz o modelo pensar que deveria ser um operador de adição. Então, tentamos revisar a frase para ser algo como o número de árvores de pereira são 55 a menos do que as árvores de maçã. Então, fazemos para transmitir uma semântica mais precisa para que o modelo seja capaz de fazer a previsão correta. Então, este estudo mostra como as previsões interpretáveis nos ajudam a entender o comportamento do modelo. Então, para concluir nosso trabalho, então primeiro nosso modelo é realmente bastante eficiente, e somos capazes de fornecer um procedimento de resolução interpretável. E podemos facilmente incorporar algum conhecimento prévio como restrição, o que pode ajudar a melhorar o desempenho. E a última coisa é que o mecanismo subjacente não se aplica apenas a tarefas de resolução de problemas de rede, mas também outras tarefas que envolvem raciocínio em várias etapas. Mas também temos certas limitações. Se tivermos um grande número de operadores ou constantes, o consumo de memória pode ser bastante alto. E a segunda coisa é que, como mencionado, porque a distribuição de probabilidade é desequilibrada em diferentes tempos, também é bastante desafiador aplicar a estratégia de busca por feixe. Então, este é o fim da palestra, e perguntas são bem-vindas. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine e sou da Universidade de Maastricht. Vou apresentar o meu trabalho conjunto com Jerry, que trata de um novo conjunto de dados para recuperação de artigos legais. Questões jurídicas são uma parte integrante da vida de muitas pessoas, mas a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre os seus direitos e processos legais fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem pagar a assistência dispendiosa de um especialista legal ficam desprotegidos ou, pior, explorados. O nosso trabalho visa colmatar a lacuna entre as pessoas e a lei, desenvolvendo sistemas eficazes de recuperação de artigos legais. Um sistema desse tipo poderia fornecer um serviço gratuito de assistência jurídica para humanos não qualificados. Antes de mergulharmos na principal contribuição deste trabalho, vamos primeiro descrever o problema da recuperação de artigos legais. Dada uma pergunta simples sobre uma questão legal, como o que arrisco se violar a confidencialidade profissional, é necessário um modelo para recuperar todos os artigos legais relevantes de um grande corpo de legislação. Esta tarefa de recuperação de informações vem com o seu próprio conjunto de desafios. Primeiro, lida com dois tipos de linguagem, a linguagem natural comum para as perguntas e a linguagem complexa e jurídica para os estatutos. Esta diferença nas distribuições de linguagem torna mais difícil para um sistema recuperar candidatos relevantes, pois exige indiretamente um sistema de interpretação inerente que pode traduzir uma pergunta natural para uma pergunta legal que corresponda à terminologia dos estatutos. Além disso, o direito estatutário não é um conjunto de artigos independentes que podem ser tratados como uma fonte completa de informação por si só, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção de estruturas de disposições legais que têm um significado completo apenas quando consideradas no seu contexto geral, ou seja, juntamente com as informações suplementares dos seus artigos vizinhos, os campos e subcampos aos quais pertencem e o seu lugar na estrutura da lei. Por último, os artigos legais estão em pequenos parágrafos, que geralmente são a unidade típica de recuperação na maioria dos trabalhos de recuperação. Aqui, há documentos longos que podem ter até 6.000 palavras. Os recentes avanços em PLN despertaram um grande interesse em muitas tarefas legais, como previsão de julgamentos legais ou revisão automatizada de contratos, mas a recuperação de artigos legais permaneceu principalmente intocada devido à falta de conjuntos de dados de grande porte e de alta qualidade. Neste trabalho, apresentamos um novo conjunto de dados centrado no cidadão de língua francesa para verificar se o modelo de recuperação pode aproximar a eficiência e a fiabilidade dos especialistas legais para a tarefa de recuperação de artigos legais. O nosso conjunto de dados de recuperação de artigos legais belgas, PSART, consiste em mais de 1100 perguntas legais feitas por cidadãos belgas. Estas perguntas cobrem uma vasta gama de temas, desde família, habitação, dinheiro, até trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos relevantes de um corpus de mais de 22.600 artigos legais de códigos de leis belgas. Vamos agora falar sobre como recolhemos este conjunto de dados. Primeiro, começámos por compilar um grande corpus de artigos legais. Considerámos 32 códigos belgas publicamente disponíveis e extraímos todos os seus artigos, bem como as respetivas rubricas de secção. Em seguida, reunimos perguntas legais com referências a estatutos relevantes. Para isso, associámo-nos a um escritório de advocacia belga que recebe anualmente cerca de 4.000 e-mails de cidadãos belgas que pedem conselhos sobre uma questão legal pessoal. Tivemos a sorte de aceder aos seus sites, onde a sua equipa de juristas experientes aborda as questões legais mais comuns dos belgas. Recolhemos milhares de perguntas, anotadas com categorias, subcategorias e referências legais a estatutos relevantes. Por último, analisámos as referências legais e filtrámos as perguntas cujas referências não eram artigos de um dos códigos de leis que consideramos. As referências restantes foram correspondidas e convertidas para os respetivos IDs de artigo do nosso corpus. Acabámos por ter 1108 perguntas, cada uma cuidadosamente rotulada com os IDs dos artigos relevantes do nosso grande corpus de 22.633 artigos legais. Além disso, cada pergunta vem com uma categoria principal e uma concatenação de subcategorias, e cada artigo vem com uma concatenação dos seus títulos subsequentes na estrutura da lei. Esta informação adicional não é utilizada no presente trabalho, mas pode ser interessante para futuras pesquisas em recuperação de informações legais ou classificação de texto legal. Vamos analisar algumas características do nosso conjunto de dados. As perguntas têm entre 5 e 44 palavras, com uma mediana de 40 palavras. Os artigos são muito mais longos, com uma mediana de 77 palavras, sendo que 142 deles excedem 1000 palavras, sendo o mais longo de até 5790 palavras. Como mencionado anteriormente, as perguntas cobrem uma vasta gama de temas, com cerca de 85% delas sendo sobre família, habitação, dinheiro ou justiça, enquanto os restantes 15% dizem respeito a segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois provêm de 32 códigos belgas diferentes que cobrem um grande número de temas legais. Aqui está o número total de artigos recolhidos de cada um destes códigos belgas. Dos 22.633 artigos, apenas 1.612 são referenciados como relevantes para pelo menos uma pergunta no conjunto de dados. E cerca de 80% destes artigos citados provêm do código civil, código judicial, código de investigação criminal ou códigos penais. Enquanto isso, 18 dos 32 códigos têm menos de 5 artigos mencionados como relevantes para pelo menos uma pergunta, o que pode ser explicado pelo facto de esses códigos focarem menos nos indivíduos e nas suas preocupações. No geral, o número médio de citações para estes artigos citados é 2, e menos de 25% deles são citados mais de 5 vezes. Usando os nossos conjuntos de dados, avaliámos várias abordagens de recuperação, incluindo arquitetura lexical e densa. Dada uma consulta e um artigo, um modelo lexical atribui uma pontuação ao par consulta-artigo calculando a soma dos termos da consulta dos pesos de cada um desses termos nesse artigo. Experimentamos com as funções de classificação padrão TF-IDF e BM25. O principal problema com estas abordagens é que só conseguem recuperar artigos que contêm palavras-chave presentes na consulta. Para superar esta limitação, experimentamos com uma arquitetura baseada em redes neurais que pode capturar relações semânticas entre consultas e artigos. Usamos um modelo b-encoder que mapeia consultas e artigos para representações vetoriais densas e calcula uma pontuação relevante entre um par consulta-artigo pela semelhança das suas incorporações. Estas incorporações resultam tipicamente de uma operação de pooling na saída de um modelo de incorporação de palavras. Primeiro, estudámos a eficácia dos b-encoders de Siamese em uma configuração de avaliação sem instrução prévia, o que significa que os modelos de incorporação de palavras pré-treinados são aplicados diretamente sem qualquer ajuste adicional. Experimentamos com codificadores de texto independentes do contexto, nomeadamente Word2Vec e FastText, e modelos de incorporação dependentes do contexto, nomeadamente Robota e, mais especificamente, Camembert, que é um modelo Robota francês. Além disso, treinámos os nossos próprios b-encoders baseados em Camembert nos nossos conjuntos de dados. Note que, para o treino, experimentamos com os dois sabores da arquitetura b-encoder. Siamese, que usa um modelo de incorporação de palavras único que mapeia a consulta e o artigo juntos num espaço vetorial denso partilhado, e toTower, que usa dois modelos de incorporação de palavras independentes que codificam a consulta e o artigo separadamente em espaços de incorporação diferentes. Experimentamos com pooling de média, máximo e CLS, bem como produto escalar e coseno para calcular semelhanças. Aqui estão os resultados da nossa linha de base no conjunto com os métodos lexicais acima, os b-encoders de Siamese avaliados em uma configuração sem instrução prévia no meio, e os b-encoders ajustados abaixo. No geral, os b-encoders ajustados superam significativamente todas as outras linhas de base. O modelo de duas torres melhora a sua variante Siamese em recall em 100, mas apresenta desempenho semelhante nas outras métricas. Embora o BM25 tenha um desempenho inferior ao B-Encoder treinado, o seu desempenho indica que ainda é uma linha de base forte para recuperação específica do domínio. No que diz respeito à avaliação sem instrução prévia do B-Encoder de Siamese, descobrimos que o uso direto das incorporações de um modelo Camembert pré-treinado sem otimização para a tarefa de recuperação de informações dá resultados pobres, o que é consistente com descobertas anteriores. Além disso, observámos que o biancoder baseado em Word2Vec superou significativamente o modelo baseado em FastText e BERT, sugerindo que talvez as incorporações de nível de palavra pré-treinadas sejam mais apropriadas para a tarefa do que as de nível de caractere ou subpalavra quando usadas diretamente. Embora promissores, estes resultados sugerem uma ampla oportunidade de melhoria em comparação com um especialista legal qualificado que pode eventualmente recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas. Vamos concluir discutindo duas limitações de todos os conjuntos de dados. Primeiro, o corpus de artigos é limitado aos recolhidos dos 32 códigos belgas considerados, o que não cobre toda a lei belga, pois faltam artigos de decretos, diretivas e ordenanças. Durante a construção do conjunto de dados, todas as referências a estes artigos não recolhidos são ignoradas, o que faz com que algumas perguntas acabem com apenas uma fração do número inicial de artigos relevantes. Esta perda de informação implica que a resposta contida nos artigos relevantes restantes pode ser incompleta, embora ainda seja completamente apropriada. Segundo, devemos notar que nem todas as perguntas legais podem ser respondidas apenas com estatutos. Por exemplo, a pergunta, posso despejar os meus inquilinos se eles fazem demasiado barulho, pode não ter uma resposta detalhada dentro do direito estatutário que quantifique um limite específico de ruído a partir do qual o despejo é permitido. Em vez disso, o senhorio deve provavelmente confiar mais no direito consuetudinário e encontrar precedentes semelhantes à sua situação atual. Por exemplo, o inquilino faz festas até às 2h da manhã. Portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos legais, e o domínio dos menos adequados ainda está por determinar. Esperamos que o nosso trabalho desperte interesse no desenvolvimento de modelos práticos e fiáveis de recuperação de artigos legais que possam ajudar a melhorar o acesso à justiça para todos. Podem consultar o nosso artigo, conjunto de dados e código nos seguintes links. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá! Temos o prazer de apresentar nosso trabalho sobre o VALS, um benchmark independente de tarefas destinado a testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que nos demos ao trabalho de estabelecer este benchmark? Bem, nos últimos anos, assistimos a uma explosão de modelos de visão e linguagem baseados em transformadores, pré-treinados em grandes quantidades de pares de texto e imagem. Cada um desses modelos alcança o estado da arte em tarefas de visão e linguagem, como resposta a perguntas visuais, raciocínio de senso comum visual, recuperação de imagens, fundamentação de frases. Então, recebemos uma mensagem: as acurácias nesses benchmarks específicos de tarefas estão aumentando constantemente. Mas sabemos o que os modelos realmente aprenderam? O que é que um transformador de visão e linguagem entendeu ao atribuir uma pontuação alta para esta imagem e esta frase para combinar, e uma pontuação baixa para esta? Os modelos de visão e linguagem focam na coisa certa, ou focam em vieses, como demonstrado por trabalhos anteriores? Para lançar mais luz sobre este aspecto, propomos uma direção mais agnóstica em relação às tarefas e introduzimos válvulas que testam a sensibilidade dos modelos de visão e linguagem a fenômenos linguísticos específicos que afetam tanto as modalidades linguísticas quanto as visuais. Nossas metas são existência, pluralidade, contagem, relações espaciais, ações, núcleo referencial de entidades. Mas como testamos se os modelos de visão e linguagem capturaram esses fenômenos? Por meio do FOILing, um método anteriormente aplicado para modelos de visão e linguagem, apenas para frases nominais por Ravi Shekhar e colaboradores, na contagem por nós em trabalhos anteriores. O FOILing basicamente significa que pegamos a legenda de uma imagem e produzimos um foil alterando a legenda de tal forma que ela não descreve mais a imagem. E fazemos essas alterações de frases focando em seis peças específicas, como existência, pluralidade, contagem, relações espaciais, ações e núcleo referencial de entidades, onde cada peça pode consistir em um ou mais instrumentos, caso encontremos mais de uma maneira interessante de criar instâncias de FOIL. Por exemplo, no caso da peça de ações, temos dois instrumentos, um em que o verbo da ação é alterado por uma ação diferente, e outro em que os agentes são trocados. Contagem e núcleo referencial também são peças que têm mais de um instrumento. E criamos esses FOILs garantindo que eles falhem em descrever a imagem, que sejam gramaticalmente e de outra forma frases válidas. Isso não é fácil de fazer, porque uma legenda de FOIL pode ser menos provável do que a legenda original. Por exemplo, embora não seja impossível, é estatisticamente menos provável que plantas cortem um homem do que um homem cortar plantas, e grandes modelos de visão e linguagem poderiam captar isso. Portanto, para obter FOILs válidos, devemos tomar medidas. Primeiro, fazemos uso de modelos de linguagem fortes para propor FOILs. Segundo, usamos inferência de linguagem natural, ou NLI, para filtrar FOILs que ainda poderiam estar descrevendo a imagem, já que ao construir FOILs, precisamos garantir que eles falhem em descrever a imagem. Para testar isso automaticamente, aplicamos inferência de linguagem natural com a seguinte justificativa. Consideramos uma imagem como a premissa e sua legenda como o que é inferido. Além disso, consideramos a legenda como a premissa e o FOIL como sua hipótese. Se um modelo de NLI prever que o FOIL contradiz ou é neutro em relação à legenda, consideramos isso como um indicador de um FOIL válido. Se um NLI prever que o FOIL é inferido pela legenda, ele não pode ser um bom FOIL, já que, por transitividade, ele dará uma descrição verdadeira da imagem e filtramos esses FOILs. Mas este procedimento não é perfeito. É apenas um indicador para FOILs válidos, portanto, como uma terceira medida para gerar FOILs válidos, empregamos anotadores humanos para validar os dados usados no VALS. Então, após a filtragem e a avaliação humana, temos tantas instâncias de teste quanto descritas nesta tabela. Note que o VALS não fornece dados de treinamento, mas apenas dados de teste, já que é um benchmark de teste sem treinamento. Ele é projetado para aproveitar as capacidades existentes de modelos de visão e linguagem após o pré-treinamento. O ajuste fino só permitiria que os modelos explorassem artefatos ou vieses estatísticos nos dados. E todos sabemos que esses modelos gostam de trapacear e tomar atalhos. E como dissemos, estamos interessados em avaliar quais capacidades os modelos de visão e linguagem têm após o pré-treinamento. Experimentamos com cinco modelos de visão e linguagem no VALS, nomeadamente com CLIP, LXMERT, Wil BERT, VILBERT 12 em 1 e VISUALBERT. Duas de nossas métricas de avaliação mais importantes são a precisão dos modelos em classificar pares de imagem-frase em legendas e FOILs. Talvez mais relevante para este vídeo, mostraremos nossa métrica mais permissiva, a precisão em pares, que mede se a pontuação de alinhamento de frase de imagem é maior para o par de texto de imagem correto do que para seu par de FOIL. Para mais métricas e resultados sobre elas, consulte nosso artigo. Os resultados com precisão em pares são mostrados aqui e são consistentes com os resultados que obtivemos das outras métricas. é que o melhor desempenho sem treinamento é alcançado pelo Wilbert 12 em 1, seguido pelo Wilbert e são consistentes com os resultados que obtivemos das outras métricas, é que o melhor desempenho sem treinamento é alcançado pelo Wilbert 12 em 1, seguido pelo Wilbert, Alex Mert, Clip e, finalmente, Visual Bird. É notável como instrumentos centrados em objetos individuais, como existência e frases nominais, são quase resolvidos pelo Wilbert 12 em 1, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhuma das peças restantes pode ser confiável resolvida em nossas configurações de FOILing adversarial. Vemos dos instrumentos de pluralidade e contagem que os modelos de visão e linguagem têm dificuldade em distinguir referências a objetos únicos versus múltiplos ou contá-los em uma imagem. A peça de relação mostra que eles têm dificuldades em classificar corretamente uma relação espacial nomeada entre objetos em uma imagem. Eles também têm dificuldade em distinguir ações e identificar seus participantes, mesmo se apoiados por vieses de plausibilidade, como vemos na peça de ações. A partir da peça de núcleo referencial, descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronomes também é difícil para modelos de visão e linguagem. Como uma verificação de sanidade, e porque é um experimento interessante, também fazemos um benchmark de dois modelos apenas de texto, GPT-1 e GPT-2, para avaliar se o VALS é solucionável por esses modelos unimodais, computando a perplexidade da legenda correta e do FOIL e prevendo a entrada com a perplexidade mais baixa. Se a perplexidade for maior para o FOIL, consideramos isso como uma indicação de que a legenda de FOIL pode sofrer de viés de plausibilidade ou outros vieses linguísticos. E é interessante ver que, em alguns casos, os modelos de texto apenas GPT capturaram a plausibilidade do mundo melhor do que os modelos de visão e linguagem. Então, para resumir. O VALS é um benchmark que usa a lente de construções linguísticas para ajudar a comunidade a melhorar modelos de visão e linguagem, testando rigorosamente suas capacidades de fundamentação visual. Nossos experimentos mostram que os modelos de visão e linguagem identificam objetos nomeados em sua presença em imagens bem, como mostrado pela peça de existência, mas lutam para fundamentar sua interdependência e relações em cenas visuais quando forçados a respeitar indicadores linguísticos. Gostaríamos muito de incentivar a comunidade a usar o VALS para medir o progresso em direção à fundamentação linguística com modelos de visão e linguagem. E ainda mais, o VALS poderia ser usado como uma avaliação indireta de conjuntos de dados, pois os modelos poderiam ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados pelo VALS. Se estiver interessado, consulte os dados do VALS no GitHub, e se tiver alguma dúvida, não hesite em nos contatar."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamizawa, da Universidade de Tóquio. Vou apresentar um artigo intitulado RNSUN, um conjunto de dados em grande escala para geração automática de notas de lançamento via sumarização de log de commit. Vou explicar na seguinte ordem. Primeiro, vou apresentar a geração automática de notas de lançamento em que estamos trabalhando nesta pesquisa. A nota de lançamento é um documento técnico que resume as alterações distribuídas com cada lançamento de um produto de software. A imagem mostra as notas de lançamento para a versão 2.6.4 da biblioteca Vue.js. As notas de lançamento desempenham um papel importante no desenvolvimento de código aberto, mas são demoradas para serem preparadas manualmente. Portanto, seria muito útil poder gerar automaticamente notas de lançamento de alta qualidade. Vou me referir a duas pesquisas anteriores sobre geração automática de notas de lançamento. A primeira é um sistema chamado Arena, lançado em 2014. Ele adota uma abordagem baseada em regras, por exemplo, usando o extrator de mudanças para extrair diferenças principais, mudanças de biblioteca e mudanças de documentos das diferenças entre lançamentos, e finalmente combiná-las. A característica mais notável deste sistema é o extrator de problemas no canto superior direito, que deve ser vinculado ao Jira, o sistema de rastreamento de problemas, e só pode ser aplicado a projetos que usam Jira. Em outras palavras, não pode ser usado para muitos projetos no github, o segundo Glyph, anunciado recentemente em 2020. Está disponível na internet e pode ser armazenado via PIP. Este sistema tem um modelo de classificação de texto baseado em aprendizado simples e produz um dos cinco níveis, como recursos ou correções de bugs, para cada mensagem de commit de entrada. A imagem é um exemplo de uso que retorna um rótulo corretivo ou correções de bugs. Os dados de treinamento do Glyph são bastante pequenos, cerca de 5000, e na revisão dos experimentos descritos abaixo, o desempenho do modelo de classificação de texto não é alto. Apresento duas pesquisas relacionadas, mas há problemas de aplicabilidade limitada e recursos de dados escassos. Nosso artigo resolve esses dois problemas e gera automaticamente notas de lançamento de alta qualidade. Para o problema de aplicabilidade limitada, propomos um método de sumarização de classificador de alta qualidade usando apenas a mensagem de commit como entrada. Este método proposto pode ser usado para todos os repositórios em inglês. Para o segundo problema de recursos de dados escassos, construímos um conjunto de dados R e sum composto por cerca de 82.000 peças de dados, coletando dados de repositórios públicos do GitHub usando RR e alguns conjuntos de dados. Nosso conjunto de dados Rnsum composto por cerca de 82.000 peças de dados, coletando dados de repositórios públicos do GitHub usando a API do GitHub. Em seguida, descrevo nosso conjunto de dados. Aqui está um exemplo de dados. O lado esquerdo é a mensagem de commit e o lado direito são as notas de lançamento. As notas de lançamento são rotuladas como melhorias de faces, etc. Temos um conjunto de dados de mensagem de commit e o lado direito são as notas de lançamento. As notas de lançamento são rotuladas como melhorias, correções de bugs, etc. Configuramos uma tarefa que leva as mensagens de commit como entrada e produz as notas de lançamento rotuladas. Isso pode ser considerado uma tarefa de sumarização. Definimos quatro rótulos, recursos, melhorias, correções de bugs, deprecações, remoções e mudanças de ruptura. Estes foram definidos com base em pesquisas anteriores e outros fatores. Os menos nós no canto inferior direito são extraídos dos menos nós mostrados no canto inferior esquerdo. Neste momento, é necessário detectar os quatro rótulos que foram definidos com antecedência. Mas os rótulos nem sempre são consistentes com cada repositório. Por exemplo, o rótulo de melhorias inclui melhorias, aprimoramentos, otimizações e assim por diante. Preparamos uma lista de vocabulário de nossos rótulos de estudo para cada uma dessas variações notacionais, usamos para detectar a classe de nota de lançamento e corrigir o texto da lista que segue como a frase de nota de lançamento para a classe que precisa identificar a versão de lançamento anterior 2.5 a 18 e obtê-la diff. Isso é um pouco tedioso e não é suficiente apenas obter uma lista de lançamentos e olhar o antes e depois. Criamos uma regra de correspondência heurística para obter as versões anteriores e posteriores. Análise do conjunto de dados. No final, 7.200 repositórios e 82.000 peças de dados foram coletados. Além disso, o número médio de tokens de notas de lançamento é 63, o que é bastante alto para uma tarefa de sumarização. Além disso, o número de tokens únicos é bastante grande, em 8.830.000. Isso se deve ao grande número de nomes de classes e métodos únicos encontrados no repositório. Em seguida, explicarei o método proposto. Para o grande número de nomes de classes e métodos únicos encontrados no repositório. Em seguida, explicarei o método proposto. O modelo de sumarização extractiva-então-abstractiva por classe consiste em duas redes neurais que usam um classificador para classificar cada mensagem de commit em cinco classes de notas de lançamento. Escolhemos implementa, correções de bugs, deprecações mais e outros. As mensagens de commit classificadas como outros são descartadas. Em seguida, o CEAS aplica o gerador aos quatro documentos de rótulos independentemente e gera notas de lançamento para cada classe. Nesta tarefa, as correspondências diretas entre mensagens de commit e notas de lançamento não são conhecidas. Portanto, para treinar o classificador, atribuímos rótulos pseudo a cada mensagem de commit de entrada usando os primeiros 10 caracteres de cada mensagem de commit. Modelamos a sumarização abstractiva por classe através da nossa abordagem por dois métodos diferentes. O primeiro modelo, que chamamos de cssingle, consiste em uma única rede de conjunto para conjunto e gera um único texto longo de nó, dando uma concatenação das mensagens de commit de entrada. As redes de texto de saída, cada uma das quais corresponde a uma das classes menos conhecidas. Tudo bem, deixe-me explicar a experiência. Cinco métodos foram comparados, CAS, CASSingle, CASMatch, PlusSelling e estudo anterior, GRIF. Em relação à aberração, em alguns casos, CSMatch, Blustering e Estudo Anterior Glyph. Em relação à avaliação, em alguns casos, os menos-notas são produzidos em várias frases. Como é difícil calcular o número de frases como estão, elas são combinadas com espaços e tratadas como uma única frase longa. O azul é penalizado quando o sistema produz uma frase curta. Esta penalidade resulta em um valor azul mais baixo nos resultados da experiência descritos a seguir. Finalmente, também calculamos a especificidade porque o rouge e o azul não podem ser calculados se as notas de lançamento estiverem vazias. Uma especificidade alta significa que o modelo produz corretamente um texto vazio nos casos em que as notas de lançamento assumem vazio. Aqui estão os resultados. Como o conjunto de dados contém endereços de e-mail, valores de hash, etc., também avaliamos o conjunto de dados limpo, que os exclui. CES e CAS alcançaram escores L de rouge mais de 10 pontos acima dos valores de referência. Em particular, no conjunto de teste limpo, a lacuna de pontuação entre o método proposto e os valores de referência saltou para mais de 20 pontos. Estes resultados indicam que CAS e CAS são significativamente eficazes. CAS obteve uma pontuação de root-A melhor do que CAS, sugerindo que combinar um classificador e um gerador é eficaz no treinamento do classificador usando dois duplos. Uma cobertura alta de CAS pode ser alcançada, provavelmente, porque o classificador pode se concentrar em selecionar mensagens de commit relevantes para cada classe. CAS match tendeu a produzir log L mais alto do que CAS single, sugerindo que também é eficaz desenvolver independentemente modelos de sumarização abstractiva diferentes para cada classe de nota de lançamento. Aqui estão análises de erros. Os métodos CAS tendem a produzir frases mais curtas do que as frases de referência humanas. Na figura à direita, a frase de referência tem três ou quatro frases, enquanto CAS tem apenas uma. A razão para a relutância deste modelo é que, nos dados de treinamento, apenas 33% das frases estão presentes no rótulo de recursos e 40% no rótulo de melhorias. Além disso, os métodos CES não podem gerar notas de lançamento precisas sem informações adicionais. O exemplo superior à direita é um exemplo de um compromisso muito bagunçado que os métodos CS não podem gerar uma nota de lançamento precisa sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito bagunçada, e a frase completa não pode ser gerada sem referência ao pedido ou problema correspondente. O exemplo abaixo mostra que as duas mensagens de commit na entrada estão relacionadas e devem ser combinadas em uma única frase, mas falha em fazê-lo. Finalmente, uma conclusão. Construímos um novo conjunto de dados para notação automática de lista. Também formamos a tarefa de inserir mensagens de commit e resumí-las de modo que seja aplicável a todos os projetos escritos em inglês. Nosso experimento mostra que o método proposto gera notas de lançamento menos ruidosas com cobertura mais alta do que os valores de referência. Por favor, confira nossa guia apenas na aba Descendente. Obrigado."}
