{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Asaf Harari und ich werde unseren Artikel „Few-Shot Tabular Data Enrichment Using Fine-Tuning Transformers Architectures“ vorstellen. Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der vorhandenen Merkmale der Daten. Manchmal sind diese Merkmale jedoch begrenzt. Die Generierung von Merkmalen mithilfe einer anderen Datenquelle kann erhebliche Informationen hinzufügen. Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mithilfe freitextbasierter externer Quellen. Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mithilfe freitextbasierter externer Quellen. Nehmen wir an, wir haben einen tabellarischen Datensatz und eine Wissensdatenbank. Wir benötigen einen automatischen Prozess, der Entität-Verknüpfung und Textanalyse umfasst, um neue Merkmale aus dem freitextbasierten Wissen der Wissensdatenbank zu extrahieren. Unser Framework, FAST, ist genau dieser automatische Prozess. Lassen Sie uns also ein Beispiel betrachten. In einen Datensatz, der in FAST eingespeist wird. In diesem Beispiel ist der Datensatz ein Universitätsdatensatz, wenn das Ziel darin besteht, Universitäten in niedrig- und hochrangige Universitäten zu klassifizieren. Der Datensatz ist ein Universitätsdatensatz, wenn das Ziel darin besteht, Universitäten in niedrig- und hochrangige Universitäten zu klassifizieren. Als Wissensdatenbank verwenden wir Wikipedia. Die erste Phase von FAST ist die Entität-Verknüpfung. Wenn jede Entität, in diesem Beispiel der Universitätsname, mit einer Entität innerhalb der Wissensdatenbank verknüpft wird. Der Text der Entitäten der Wissensdatenbank wird extrahiert und dem Datensatz hinzugefügt. In diesem Beispiel ist der Text das Abstract der Wikipedia-Seite. Jetzt müssen wir Merkmale aus dem abgerufenen Text generieren oder extrahieren. Daher benötigen wir eine Merkmalsextraktionsphase, die Textanalyse umfasst. Und dies ist die Hauptneuheit dieses Artikels, und ich werde in den nächsten Folien darauf eingehen. Nach der Merkmalsextraktionsphase folgt eine Merkmalgenerierungsphase, in der wir die extrahierten Merkmale verwenden, um eine kleine Anzahl neuer Merkmale zu generieren. Zuerst werden Merkmale in der Anzahl der Klassen des ursprünglichen Datensatzes generiert. In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen, daher generiert FAST zwei neue Merkmale. Wenn der Datensatz jedoch fünf Klassen hat, generiert FAST fünf neue Merkmale. Jedes Merkmal repräsentiert die Wahrscheinlichkeit für jede Klasse. Zur Analyse des Textes verwenden wir den aktuellen Stand der Technik in der Textanalyse, d.h. auf Transformer basierende Sprachmodelle wie BERT, GPT, XNL usw. Es ist jedoch unwahrscheinlich, dass wir ein Sprachmodell mit den Eingabe-Datensätzen trainieren können. Ein naiver Ansatz wäre daher das Feinabstimmen einer Zielaufgabe. Daher können wir in der Merkmalsextraktionsphase ein vortrainiertes Sprachmodell herunterladen, das Sprachmodell über den Zieldatensatz feinabstimmen. In diesem Beispiel, um das Sprachmodell zu feinabstimmen, den Text in Klassen zu klassifizieren, das Abstract in Klassen zu klassifizieren, niedrig oder hoch, die Ausgabe des Sprachmodells zu erhalten, die die Wahrscheinlichkeit für jede Klasse darstellt, und dies als neue Merkmale zu verwenden. Das Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige eindeutige Entitätsz tags haben. neue Merkmale. Das Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige eindeutige Entitätsz tags haben. In unserem Experiment enthielten fast die Hälfte der Datensätze weniger als 400 Proben, und der kleinste Datensatz enthielt 35 Proben in seinem Trainingsdatensatz. Daher wäre das Feinabstimmen eines Sprachmodells über diesen Datensatz unwirksam. Aber wir können vorheriges Wissen über bereits analysierte Datensätze nutzen, weil-1 Datensätze und diese Informationen verwenden, wenn wir den N-ten Datensatz analysieren. Was wir vorschlagen, ist, eine weitere Feinabstimmungs-Phase hinzuzufügen, eine vorläufige mehraufgabenorientierte Feinabstimmungs-Phase, wenn Sie das Sprachmodell über n-1 Datensätze feinabstimmen, und dann führen wir eine weitere Feinabstimmungs-Phase durch, die eine zielaufgabenorientierte Feinabstimmung ist, wenn wir das Sprachmodell über den n-ten Zieldatensatz feinabstimmen. Der Stand der Technik in der mehraufgabenorientierten Feinabstimmung heißt mtDNN. In mtDNN werden mtDNN-Köpfe in der Anzahl der Aufgaben im Trainingsdatensatz beibehalten. Daher gibt es in diesem Beispiel vier Aufgaben im Trainingsdatensatz, also leerer DNN und vier Köpfe werden beibehalten, wie Sie es in diesem Beispiel sehen können. In diesem Beispiel gibt es vier Aufgaben im Trainingsdatensatz. Also leerer DNN, vier Köpfe werden beibehalten, wie Sie es in dem Bild sehen können, und es wird eine zufällige Batch-Größe aus dem Trainingsdatensatz ausgewählt. Und wenn die zufällige Batch-Größe beispielsweise zu Einzel-Satzz-Klassifizierungsaufgaben gehört, führt es einen Vorwärts- und Rückwärtsdurchlauf durch den ersten Kopf aus. Und wenn die zufällige Batch-Größe zu einer paarweisen Rangordnungsaufgabe gehört, führt es einen Vorwärts- und Rückwärtsdurchlauf durch den letzten Kopf aus. In unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen. Daher verifiziert ein Tableau-Datensatz in unserem Szenario die Anzahl der Klassen. Daher gibt es viele Aufgaben. MTDNN behält die Anzahl der Klassenköpfe, Ausgabe-Schichten bei, und zusätzlich muss MTDNN neue Köpfe für einen neuen Datensatz mit einer neuen Aufgabe initialisieren. Unser Ansatz, den wir als feinabstimmende Aufgabenreformulierung bezeichnen, reformuliert jeden Datensatz in ein Satz pro Klassifizierungsproblem, was ein Zwei-Klassen-DAS ist, anstatt mehrere Köpfe zu beibehalten, reformulieren wir jeden Datensatz in ein Satz pro Klassifizierungsproblem, was Zwei-Klassen-Aufgaben sind. Lassen Sie uns also ein Beispiel betrachten. Hier ist unser Eingabe-Datensatz, der Entitäten, Merkmale, Text und Klassen enthält. und wir beginnen damit, den Text in niedrig und hoch zu klassifizieren, um den Text zu klassifizieren, um Abstract und Klasse zu klassifizieren, um Abstract und Klasse, wenn der Abstract zur Klasse gehört oder nicht. Daher besteht der Label-Vektor im Fall von Zig immer aus zwei Klassen. Im Fall von Zig besteht er immer aus zwei Klassen. Und das ist es. Dann reformuliert die Aufgabe in ein Satz pro Klassifizierung-Aufgaben. Wenden Sie das Sprachmodell auf die neue Aufgabe an und erhalten Sie die Wahrscheinlichkeit für jede Klasse als Ausgabe. Und beachten Sie, dass das Sprachmodell bereits über N minus einem Datensatz mit einer vorläufigen mehraufgabenorientierten Feinabstimmung feinabstimmt wurde. Dann verwenden wir den Ausgabe-Vektor des Datensatzes mit einer vorläufigen mehraufgabenorientierten Feinabstimmung. Dann verwenden wir den Ausgabe-Vektor des Sprachmodells als ein neu generiertes Merkmal in der Anzahl der Klassen. Um unser Framework zu bewerten, verwenden wir einen 17-tabellarischen Klassifikationsdatensatz, der Größe, Merkmale, Ausgewogenheit, Domäne und anfängliche Leistung überprüft. Und als Wissensdatenbank verwenden wir Wikipedia. Wir gestalten unser Experiment als Live-One-Out-Bewertung, wenn wir FAST über 16 Datensätzen trainieren und auf den 17. Datensatz anwenden. Wir teilen jeden Datensatz auch in vier Fehler auf und wenden eine Vier-Fehler-Kreuzvalidierung an. Dann generieren wir das neue Merkmal und bewerten sie mit fünf Bewertungs-Klassifikatoren. für eine falsche Kreuzvalidierung. Dann generieren wir das neue Merkmal und bewerten sie mit fünf Bewertungs-Klassifikatoren. Wir verwenden in unserem Experiment eine auf Bausteinen basierende Architektur. Hier sind die Ergebnisse für unser Experiment. Sie können sehen, dass wir unseren Rahmen mit dem Feinabstimmen eines Datensatzes, dem Feinabstimmen einer Zielaufgabe und dem vorläufigen Feinabstimmen von mtdnn und unserem reformulierten Feinabstimmen vergleichen, das beste Ergebnis und die beste Leistung erzielt, während leerer DNN, der formulierte Feinabstimmungsansatz, das beste Ergebnis und die beste Leistung erzielt. Während MTDNN eine Zwei-Prozent-Verbesserung gegenüber dem Feinabstimmen eines Zieldatensatzes erzielt hat, hat unser Ansatz eine Sechs-Prozent-Verbesserung erzielt. Wenn wir uns den kleinen Datensatz ansehen, können wir sehen, dass die Leistung des leeren DNN abnimmt und die Verbesserung des alleinigen Feinabstimmungsansatzes der Zielaufgabe. Zusammenfassend ermöglicht FAST eine Few-Shot-Anreicherung ab 35 Proben in unserem Experiment. Es verwendet eine Architektur für alle Aufgaben-Datensätze und behält den Kopf des Modells bei. Aber es fügt eine Reformulierungs-Phase hinzu. Es wird ein erweiterter Trainingsdatensatz und ein Zielwert mit semantischer Bedeutung benötigt, damit wir ihn in das Sprachmodell einfüttern und in das Satz pro Klassifizierungsproblem verwenden können. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, heute werde ich unsere Forschungsarbeit „Learning to Reason Detectively, Metabolic Problem Solving as Complex Reason Extraction“ vorstellen. Ich bin Alan vom ByteDance AI Lab, und dies ist eine gemeinsame Arbeit mit Jerry von der University of Texas at Austin und Weilu von der SUTD. Zunächst möchte ich über unsere Motivation für das Schließen von Argumenten sprechen. Hier zeigen wir ein Beispiel, in dem mehrstufiges Schließen hilfreich ist. Diese Abbildung stammt aus dem Bereich „Pen on Paper“, in dem sie zur Lösung des Mathematikproblems in einem Few-Shot-Learning-Szenario anregen. Auf der linken Seite können wir sehen, dass wir, wenn wir nur Fragen und Antworten geben, möglicherweise nicht die richtigen Antworten erhalten. Wenn wir jedoch eine etwas ausführlichere Beschreibung des Schließens geben, kann das Modell die Beschreibung des Schließens vorhersagen und auch hier eine korrekte Vorhersage treffen. Es ist also gut, interpretierbares mehrstufiges Schließen als Ausgabe zu haben. Wir denken auch, dass das Methode-Problem eine einfache Anwendung zur Bewertung solcher Schließfähigkeiten ist. Hier in unserem Problemstellung müssen wir diese Frage beantworten und die numerischen Antworten erhalten. In unseren Datensätzen wird uns auch der mathematische Ausdruck gegeben, der zu dieser bestimmten Antwort führt. Bestimmte Annahmen gelten auch wie in früheren Arbeiten. Wir nehmen an, dass die Genauigkeit der Größenordnungen bekannt ist und wir nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponential betrachten. Darüber hinaus können komplizierte Operatoren tatsächlich in diese grundlegenden Operatoren zerlegt werden. Frühere Arbeiten zum Lösen von Methode-Problemen lassen sich tatsächlich in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modelle einteilen. Traditionelle Sequenz-zu-Sequenz-Modelle wandeln den Ausdruck in eine bestimmte Sequenz zur Generierung um, und es ist ziemlich einfach zu implementieren und kann auf viele verschiedene komplexe Probleme verallgemeinert werden. Die Nachteile der Leistung sind jedoch im Allgemeinen nicht besser als die des Strukturmodells. Und es fehlt an Interpretierbarkeit für Vorhersagen. Tatsächlich ist diese Richtung jedoch aufgrund des Transformer-Modells immer noch ziemlich beliebt. In baumgestützten Modellen strukturieren wir diese Ausdrücke tatsächlich in Baumform und folgen einer vorrangigen Durchquerung in drei Generationen. Hier erzeugen wir die Operatoren fortlaufend, bis wir die Blätter erreichen, die die Größenordnungen sind. Das Gute daran ist, dass es uns tatsächlich diese binäre Baumstruktur gibt. Struktur und es ist, aber tatsächlich ist es ziemlich kontraintuitiv, weil wir zuerst den Operator erzeugen und dann am Ende die Struktur erzeugen, aber tatsächlich ist es ziemlich kontraintuitiv, weil wir zuerst den Operator erzeugen und dann am Ende die Größenordnungen erzeugen. Die zweite Sache ist, dass es auch einige repetitive Berechnungen enthält. Wenn wir uns diesen Ausdruck ansehen, wird „a mal 3 plus 3“ tatsächlich zweimal erzeugt. In Wirklichkeit sollten wir die Ergebnisse wiederverwenden. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme schrittweise und interpretierbar lösen. Zum Beispiel können wir in Schritt zwei diesen Divisor erhalten, der 27 ist, und wir können uns auch auf die ursprünglichen Fragen beziehen, um den relevanten Inhalt zu finden. In diesen Schritten erhalten wir die Divisor. Und dann erhalten wir in diesem dritten Schritt tatsächlich den Quotienten. Gut. Und nach diesen drei Schritten können wir die Ergebnisse aus dem zweiten Schritt tatsächlich wiederverwenden und dann die Ergebnisse des vierten Schritts erhalten. Und dann können wir schließlich die Dividenden erhalten. Hier erzeugen wir tatsächlich den gesamten Ausdruck direkt, anstatt einzelne Operatoren oder Größenordnungen zu erzeugen. Das macht den Prozess genauer. In unserem deduktiven System beginnen wir zunächst mit einer Reihe von Größenordnungen, die in den Fragen präsentiert werden, und schließen auch einige Konstanten als unsere Anfangszustände ein. Der Ausdruck wird durch EIJOP dargestellt, wobei wir den Operator von QI zu QJ durchführen, und ein solcher Ausdruck ist tatsächlich gerichtet. Wir haben hier auch eine Subtraktion, um die entgegengesetzte Richtung darzustellen. Das ist ziemlich ähnlich zur Relationsextraktion. In einem formalen deduktiven System wenden wir im Zeitschritt t den Operator auf das Paar QI und QJ an und erhalten dann diese neuen Ausdrücke. Wir fügen sie dem nächsten Zustand hinzu, um eine neue Größe zu werden. Diese Folie visualisiert tatsächlich die Entwicklung der Zustände, in denen wir weiterhin Ausdrücke zu den aktuellen Zuständen hinzufügen. In unseren Modellimplementierungen verwenden wir zunächst ein vorgefertigtes Sprachmodell, das Vögel oder Kaninchen sein kann, und kodieren dann einen Satz, und dann erhalten wir diese Größenordnungsdarstellungen. Sobald wir die Größenordnungsdarstellungen erhalten haben, können wir mit der Inferenz beginnen. Hier zeigen wir ein Beispiel von Q1, um die Darstellung für Q1 geteilt durch Q2 und dann mal Q3 zu erhalten. Zuerst erhalten wir die Paardarstellung, die im Grunde nur die Verkettung zwischen Q1 und Q2 ist. Und dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist. Und dann erhalten wir schließlich die Ausdruckdarstellung Q1 geteilt durch Q2. In der Praxis können wir in der Inferenzphase jedoch auch den falschen Ausdruck erhalten. Hier ist der gesamte mögliche Ausdruck gleich drei mal die Anzahl der Operatoren. Das Gute daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu steuern. Wenn dieser Ausdruck beispielsweise nicht erlaubt ist, können wir diesen Ausdruck einfach aus unserem Suchraum entfernen. Im zweiten Schritt tun wir dasselbe, aber der einzige Unterschied ist eine weitere Größe. Diese Größe stammt aus dem vorher berechneten Ausdruck. Schließlich können wir diesen endgültigen Ausdruck erhalten, Q3 mal Q4. Und wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke sich von dem vorherigen Schritt unterscheidet. Solche Unterschiede machen es schwierig, die Balken-Suche anzuwenden, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist. Das Trainingsprogramm ist ähnlich wie das Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust in jedem Zeitschritt optimieren. Und hier verwenden wir auch dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten. Und hier ist der Raum anders als bei Sequenz-zu-Sequenz, da der Raum in jedem Zeitschritt anders ist. Im traditionellen Sequenz-zu-Sequenz-Modell ist es die Anzahl des Vokabulars und es ermöglicht uns auch, bestimmte Einschränkungen aus vorherigem Wissen aufzuerlegen. Wir führen Experimente an den häufig verwendeten Methode-Problem-Datensätzen MAWPS, math23k, mathqa, MATHQA und SWAM durch. Und hier zeigen wir kurz die Ergebnisse im Vergleich zu den vorherigen besten Ansätzen. Unsere beständigste Variante ist der Robeta-Deduktionsbeweiser. Tatsächlich verwenden wir im Gegensatz dazu keine Balken-Suche. Die besten Ansätze sind oft baumgestützte Modelle. Insgesamt kann unser Deduktionsbeweiser dieses baumgestützte Modell erheblich übertreffen, aber wir können sehen, dass die absoluten Zahlen bei MathQA oder SWAMP nicht wirklich hoch sind. Wir untersuchen die Ergebnisse auf SWAMP weiter, und dieser Datensatz ist herausfordernd, weil der Autor versucht hat, dem NLP-Modell etwas hinzuzufügen, um es zu verwirren, wie das Hinzufügen von irrelevanten Informationen und zusätzlichen Größenordnungen. In unserer Vorhersage finden wir, dass einige der Zwischenergebnisse tatsächlich negativ sind. Zum Beispiel fragen wir in dieser Frage, wie viele Äpfel Jake hat, aber wir haben einige zusätzliche Informationen wie 17 weniger Pfirsiche und Steven hat 8 Pfirsiche, was völlig irrelevant ist. Unser Modell macht eine Vorhersage wie diese, die negative Werte produziert. Und wir beobachten, dass diese beiden Ausdrücke tatsächlich ähnliche Punktzahlen haben. Wir können diesen Suchraum tatsächlich einschränken, indem wir diese Ergebnisse als negativ entfernen, sodass wir die Antwort richtig machen können. Wir finden außerdem, dass eine solche Einschränkung für einige Modelle ziemlich viel verbessert. Zum Beispiel haben wir bei Vögeln sieben Punkte verbessert. Und dann haben wir für das Robeta-basierte Modell tatsächlich zwei Punkte verbessert. Ein besseres Sprachmodell hat eine bessere Sprachverständnisfähigkeit, sodass die Zahl hier für Robeta höher und für Vögel niedriger ist. Wir versuchen auch, die Schwierigkeit hinter all diesen Datensätzen zu analysieren. Wir nehmen an, dass die Anzahl der ungenutzten Größenordnungen als irrelevante Informationen angesehen werden kann. Hier können wir sehen, dass wir den Prozentsatz der Proben mit ungenutzten Größenordnungen haben, und der SWAMP-Datensatz hat den größten Anteil. Und hier zeigen wir auch die Gesamtleistung. Für diese Proben ohne ungenutzte Größenordnungen ist die Gesamtleistung tatsächlich höher als die Gesamtleistung. Aber mit diesen Proben mit ungenutzter Größe ist sie tatsächlich viel schlechter als die Gesamtleistung. Bei MAWPS haben wir nicht wirklich viele Todesfälle, also ignoriere ich diesen Teil. Schließlich möchten wir die Interpretierbarkeit durch ein Absturz- und Teilnahmebeispiel zeigen. Hier macht unser Modell tatsächlich im ersten Schritt eine falsche Vorhersage. Wir können tatsächlich die Korrelation zwischen diesem Ausdruck und dem Satz hier sehen. Wir denken, dass dieser Satz das Modell möglicherweise zu einer falschen Vorhersage verleitet. Hier führt das Drucken von weiteren 35 dazu, dass das Modell denkt, es sollte ein Additionsoperator sein. Wir versuchen, den Satz in etwas wie die Anzahl der Birnbäume sind 55 weniger als die Apfelbäume zu überarbeiten. Wir machen es, um genauere Semantik zu vermitteln, sodass das Modell die Vorhersage richtig machen kann. Diese Studie zeigt, wie die interpretierbaren Vorhersagen uns helfen, das Verhalten des Modells zu verstehen. Um unsere Arbeit abzuschließen, ist unser Modell tatsächlich ziemlich effizient, und wir sind in der Lage, ein interpretierbares Lösungsverfahren bereitzustellen. Und wir können leicht einige vorherige Kenntnisse als Einschränkung einbeziehen, was die Leistung verbessern kann. Und das letzte ist, dass der zugrunde liegende Mechanismus nicht nur auf Netzwerkproblem-Lösungsaufgaben anwendbar ist, sondern auch auf andere Aufgaben, die mehrstufiges Schließen beinhalten. Aber wir haben auch bestimmte Einschränkungen. Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein. Und die zweite Sache ist, wie erwähnt, weil die Wahrscheinlichkeitsverteilung in verschiedenen Zeitschritten unausgewogen ist, ist es auch ziemlich herausfordernd, die Balken-Suchstrategie anzuwenden. Das ist das Ende des Vortrags, und Fragen sind willkommen. Danke."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine und ich komme von der Universität Maastricht. Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, die sich auf einen neuen Datensatz für die Artikelrettung im Gesetzbuch bezieht. Rechtliche Fragen sind ein integraler Bestandteil des Lebens vieler Menschen, aber die Mehrheit der Bürger hat wenig bis kein Wissen über ihre Rechte und grundlegende rechtliche Prozesse. Infolgedessen bleiben viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsanwalts nicht leisten können, ungeschützt oder, schlimmer noch, werden ausgebeutet. Unsere Arbeit zielt darauf ab, die Kluft zwischen den Menschen und dem Gesetz zu überbrücken, indem wir effektive Retransystem für gesetzliche Artikel entwickeln. Ein solches System könnte einen kostenlosen Rechtshilfedienst für ungelernte Menschen bereitstellen. Bevor wir uns auf den Hauptbeitrag dieser Arbeit einlassen, lassen Sie uns zunächst das Problem der Artikelrettung im Gesetzbuch beschreiben. Bei einer einfachen Frage zu einer rechtlichen Angelegenheit, wie zum Beispiel, was riskiert man, wenn man die berufliche Vertraulichkeit verletzt, ist ein Modell erforderlich, um alle relevanten Artikel aus einem großen Gesetzeswerk zu finden. Diese Informationsbeschaffung hat ihre eigenen Herausforderungen. Erstens befasst sie sich mit zwei Arten von Sprache, der üblichen natürlichen Sprache für die Fragen und der komplexen juristischen Sprache für die Statuten. Dieser Unterschied in den Sprachverteilungen macht es für ein System schwieriger, relevante Kandidaten zu finden, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die mit der Terminologie der Statuten übereinstimmt. Außerdem ist das Gesetzbuch kein Stapel unabhängiger Artikel, die als vollständige Informationsquelle für sich allein betrachtet werden können, wie zum Beispiel Nachrichten oder Rezepte. Stattdessen ist es eine Strukturkollektion von Rechtsvorschriften, die erst in ihrem Gesamtzusammenhang eine vollständige Bedeutung haben, das heißt zusammen mit den ergänzenden Informationen aus ihren benachbarten Artikeln, den Bereichen und Teilbereichen, zu denen sie gehören, und ihrem Platz in der Struktur des Gesetzes. Zuletzt sind die Artikel im Gesetzbuch in kleinen Absätzen, die normalerweise die typische Retreivaleinheit in den meisten Retrivalarbeiten darstellen. Hier gibt es lange Dokumente, die bis zu 6.000 Wörter umfassen können. Die jüngsten Fortschritte in der NLP haben großes Interesse an vielen rechtlichen Aufgaben geweckt, wie der Vorhersage von Urteilen oder der automatisierten Vertragsüberprüfung, aber die Artikelrettung im Gesetzbuch ist aufgrund des Mangels an großen und hochwertigen Datensätzen weitgehend unberührt geblieben. In dieser Arbeit präsentieren wir einen neuen, französischsprachigen, bürgerzentrierten Datensatz, um zu überprüfen, ob ein Retrivalmodell die Effizienz und Zuverlässigkeit von Rechtsexperten für die Aufgabe der Artikelrettung im Gesetzbuch annähernd erreichen kann. Unser belgischer Datensatz für die Artikelrettung im Gesetzbuch, PSART, besteht aus mehr als 1100 rechtlichen Fragen, die von belgischen Bürgern gestellt wurden. Diese Fragen decken eine breite Palette von Themen ab, von Familie, Wohnen, Geld bis hin zu Arbeit und Sozialversicherung. Jeder von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 rechtlichen Artikeln aus belgischen Gesetzbüchern gekennzeichnet. Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben. Zuerst begannen wir damit, einen großen Korpus von rechtlichen Artikeln zusammenzustellen. Wir betrachteten 32 öffentlich zugängliche belgische Gesetzbücher und extrahierten alle ihre Artikel sowie die entsprechenden Abschnittsüberschriften. Dann sammelten wir rechtliche Fragen mit Verweisen auf relevante Statuten. Dazu arbeiteten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr rund 4.000 E-Mails von belgischen Bürgern erhält, die um Rat zu einem persönlichen rechtlichen Problem bitten. Wir hatten das Glück, Zugang zu ihren Websites zu erhalten, wo ihr Team erfahrener Juristen die häufigsten rechtlichen Probleme der Belgier anspricht. Wir sammelten Tausende von Fragen, annotiert mit Kategorien, Unterkategorien und rechtlichen Verweisen auf relevante Statuten. Zuletzt analysierten wir die rechtlichen Verweise und filterten die Fragen heraus, deren Verweise keine Artikel aus einem der von uns betrachteten Gesetzbücher waren. Die verbleibenden Verweise wurden mit den entsprechenden Artikel-IDs aus unserem Korpus abgeglichen und umgewandelt. Am Ende hatten wir 1108 Fragen, jede sorgfältig gekennzeichnet mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.633 Artikeln im Gesetzbuch. Zusätzlich ist jede Frage mit einer Hauptkategorie und einer Verkettung von Unterkategorien versehen, und jeder Artikel ist mit einer Verkettung seiner nachfolgenden Überschrift in der Struktur des Gesetzes versehen. Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für zukünftige Forschungen zum rechtlichen Informationsbeschaffungs- oder rechtlichen Textklassifizierungsaufgaben von Interesse sein. Werfen wir einen Blick auf einige Merkmale unseres Datensatzes. Die Fragen sind zwischen 5 und 44 Wörter lang, mit einem Median von 40 Wörtern. Die Artikel sind viel länger, mit einem Median von 77 Wörtern, wobei 142 von ihnen länger als 1000 Wörter sind, wobei der längste bis zu 5790 Wörter umfasst. Wie bereits erwähnt, decken die Fragen eine breite Palette von Themen ab, wobei rund 85% von ihnen entweder über Familie, Wohnen, Geld oder Justiz handeln, während die restlichen 15% sich entweder auf Sozialversicherung, Ausländer oder Arbeit beziehen. Die Artikel sind ebenfalls sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Anzahl von rechtlichen Themen abdecken. Hier ist die Gesamtzahl der Artikel, die aus jedem dieser belgischen Gesetzbücher gesammelt wurden. Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage im Datensatz bezeichnet. Und rund 80% dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Gerichtsgesetzbuch, dem Strafprozessgesetzbuch oder den Strafgesetzbüchern. In der Zwischenzeit haben 18 von 32 Gesetzbüchern weniger als 5 Artikel, die als relevant für mindestens eine Frage genannt werden, was darauf zurückzuführen ist, dass sich diese Gesetzbücher weniger auf Einzelpersonen und ihre Anliegen konzentrieren. Insgesamt beträgt die durchschnittliche Anzahl der Zitate für diese zitierten Artikel 2, und weniger als 25% von ihnen werden mehr als 5 Mal zitiert. Mit unseren Datensätzen vergleichen wir mehrere Retrivalansätze, einschließlich lexikalischer und dichter Architekturen. Bei einer Abfrage und einem Artikel weist ein lexikalisches Modell dem Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe über die Abfrageterms der Gewichte jedes dieser Terme in dem Artikel berechnet. Wir experimentieren mit den Standard-TF-IDF- und BM25-Rankingfunktionen. Das Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel abrufen können, die Schlüsselwörter enthalten, die in der Abfrage vorhanden sind. Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronengestützten Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann. Wir verwenden ein b-Encoder-Modell, das Abfragen und Artikel in dichte Vektorrepräsentationen abbildet und eine relevante Punktzahl zwischen einem Abfrage-Artikel-Paar durch die Ähnlichkeit ihrer Einbettungen berechnet. Diese Einbettungen resultieren typischerweise aus einer Pooling-Operation auf dem Ausgang eines Wort-Einbettungsmodells. Zuerst untersuchen wir die Wirksamkeit von Siamese b-Encodern in einem Null-Shot-Evaluation-Setup, was bedeutet, dass vorab trainierte Wort-Einbettungsmodelle sofort ohne zusätzliche Feinabstimmung angewendet werden. Wir experimentieren mit kontextunabhängigen Texteincodierern, nämlich Word2Vec und FastText, und kontextabhängigen Einbettungsmodellen, nämlich Robota und speziell Camembert, was ein französisches Robota-Modell ist. Zusätzlich trainieren wir unsere eigenen Camembert-basierten b-Encoder auf unseren Datensätzen. Beachten Sie, dass wir für das Training mit den beiden Varianten der b-Encoder-Architektur experimentieren. Siamese, die ein einzigartiges Wort-Einbettungsmodell verwendet, das die Abfrage und den Artikel gemeinsam in einem geteilten dichten Vektorraum abbildet, und toTower, die zwei unabhängige Wort-Einbettungsmodelle verwendet, die die Abfrage und den Artikel separat in verschiedene Einbettungsräume kodieren. Wir experimentieren mit Mittelwert-, Max- und CLS-Pooling sowie Punktprodukt und Kosinus für die Berechnung von Ähnlichkeiten. Hier sind die Ergebnisse unserer Basislinie auf dem Satz mit den lexikalischen Methoden oben, die Siamese b-Encoder in einem Null-Shot-Setup in der Mitte bewertet und die fein abgestimmten b-Encoder unten. Insgesamt übertreffen die fein abgestimmten b-Encoder alle anderen Basislinien erheblich. Das Zwei-Turm-Modell verbessert sich gegenüber seiner Siamese-Variante bei der Erinnerung bei 100, zeigt jedoch ähnliche Ergebnisse bei den anderen Metriken. Obwohl BM25 das trainierte B-Encoder-Modell erheblich unterlegen war, deutet seine Leistung darauf hin, dass es immer noch eine starke Basislinie für domänenspezifische Retrival ist. In Bezug auf die Null-Shot-Evaluation von Siamese B-Encoder stellen wir fest, dass die direkte Verwendung der Einbettungen eines vorab trainierten Camembert-Modells ohne Optimierung für die Informationsbeschaffungsaufgabe schlechte Ergebnisse liefert, was mit früheren Befunden übereinstimmt. Darüber hinaus beobachteten wir, dass der auf Word2Vec basierende biancoder das FastText- und BERT-basierte Modell erheblich übertraf, was darauf hindeutet, dass möglicherweise vorab trainierte Wort-Ebeneinbettungen für die Aufgabe geeigneter sind als Zeichenebeneinbettungen oder Einbettungen auf Subwortniveau, wenn sie sofort verwendet werden. Obwohl vielversprechend, deuten diese Ergebnisse auf reichlich Raum für Verbesserungen im Vergleich zu einem erfahrenen Rechtsexperten hin, der schließlich alle relevanten Artikel zu jeder Frage abrufen und somit perfekte Punktzahlen erzielen kann. Lassen Sie uns abschließend zwei Einschränkungen aller Datensätze diskutieren. Erstens ist der Korpus der Artikel auf die aus den 32 betrachteten belgischen Gesetzbüchern beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Richtlinien und Verordnungen fehlen. Während des Datensatzaufbaus werden alle Verweise auf diese nicht gesammelten Artikel ignoriert, was dazu führt, dass einige Fragen mit nur einem Bruchteil der anfänglichen Anzahl relevanter Artikel enden. Dieser Informationsverlust impliziert, dass die in den verbleibenden relevanten Artikeln enthaltene Antwort unvollständig sein könnte, obwohl sie immer noch völlig angemessen ist. Zweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Statuten beantwortet werden können. Zum Beispiel könnte die Frage, ob man seine Mieter des Hauses werfen kann, wenn sie zu viel Lärm machen, keine detaillierte Antwort innerhalb des Gesetzesrechts haben, die einen spezifischen Lärmschwellenwert quantifiziert, bei dem eine Räumung erlaubt ist. Stattdessen sollte der Vermieter wahrscheinlich eher auf Fallrecht zurückgreifen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind. Zum Beispiel mietet der Mieter zwei Parteien pro Woche bis 2 Uhr morgens. Daher eignen sich einige Fragen besser als andere für die Aufgabe der Artikelrettung im Gesetzbuch, und der Bereich der weniger geeigneten bleibt noch zu ermitteln. Wir hoffen, dass unsere Arbeit Interesse an der Entwicklung praktischer und zuverlässiger Modelle zur Artikelrettung im Gesetzbuch weckt, die dazu beitragen können, den Zugang zur Justiz für alle zu verbessern. Sie können unseren Artikel, Datensatz und Code unter den folgenden Links einsehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo! Wir freuen uns, unsere Arbeit an VALS vorstellen zu können, einem aufgabenunabhängigen Benchmark, der für die Testung von Vision- und Sprachmodellen mit spezifischen sprachlichen Phänomenen konzipiert ist. Warum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten? Nun, in den letzten Jahren haben wir eine Explosion von auf Transformer basierenden Vision- und Sprachmodellen erlebt, die auf großen Mengen von Bild-Text-Paaren vortrainiert wurden. Jedes dieser Modelle setzt den aktuellen Stand der Technik bei Vision- und Sprachaufgaben wie der visuellen Fragebeantwortung, dem visuellen Common-Sense-Reasoning, der Bilderkennung und der Phrase-Verankerung voran. Wir haben also eine Botschaft erhalten – die Genauigkeiten bei diesen aufgabenbezogenen Benchmarks steigen stetig. Aber wissen wir, was die Modelle tatsächlich gelernt haben? Was hat ein Vision- und Sprachtransformer verstanden, als er eine hohe Punktzahl für dieses Bild und diesen Satz zu einer Übereinstimmung zuordnen und eine niedrige Punktzahl für diesen zu vergeben hatte? Konzentrieren sich Vision- und Sprachmodelle auf das Richtige, oder konzentrieren sie sich auf Verzerrungen, wie frühere Arbeiten gezeigt haben? Um diesen Aspekt genauer zu beleuchten, schlagen wir eine aufgabenagnostischere Richtung vor und führen Ventile ein, die die Empfindlichkeit von Vision- und Sprachmodellen auf spezifische sprachliche Phänomene testen, die sowohl die sprachliche als auch die visuelle Modalität beeinflussen. Wir zielen auf Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entität-Referenzen ab. Aber wie testen wir, ob die Vision- und Sprachmodelle diese Phänomene erfasst haben? Durch FOILing, eine Methode, die zuvor für Vision- und Sprachmodelle angewendet wurde, nur für Nominalphrasen von Ravi Shekhar und Mitarbeitern, bei uns in früheren Arbeiten für das Zählen. FOILing bedeutet im Grunde, dass wir die Bildunterschrift nehmen und eine Folie erstellen, indem wir die Bildunterschrift so verändern, dass sie das Bild nicht mehr beschreibt. Und wir führen diese Phrase-Änderungen durch, indem wir uns auf sechs spezifische Teile konzentrieren, wie Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entität-Referenzen, wobei jeder Teil aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als eine interessante Möglichkeit gefunden haben, FOIL-Instanzen zu erstellen. Zum Beispiel haben wir im Fall des Handlungsteils zwei Instrumente, eines, bei dem das Handlungsverbum durch eine andere Handlung ersetzt wird, und eines, bei dem die Akteure ausgetauscht werden. Zählen und Referenz sind ebenfalls Teile, die mehr als ein Instrument haben. Und wir erstellen diese FOILs, indem wir sicherstellen, dass sie das Bild nicht mehr beschreiben, dass sie grammatische und ansonsten gültige Sätze sind. Das ist nicht einfach, denn eine gefälschte Bildunterschrift ist möglicherweise weniger wahrscheinlich als die ursprüngliche Bildunterschrift. Zum Beispiel ist es statistisch gesehen weniger wahrscheinlich, dass Pflanzen einen Mann schneiden als dass ein Mann Pflanzen schneidet, und große Vision- und Sprachmodelle könnten dies aufgreifen. Daher müssen wir Maßnahmen ergreifen, um gültige FOILs zu erhalten. Erstens nutzen wir starke Sprachmodelle, um FOILs vorzuschlagen. Zweitens verwenden wir natürliche Sprachinferenz, oder kurz NLI, um FOILs herauszufiltern, die das Bild immer noch beschreiben könnten, da wir beim Erstellen von FOILs sicherstellen müssen, dass sie das Bild nicht mehr beschreiben. Um dies automatisch zu testen, wenden wir natürliche Sprachinferenz mit der folgenden Begründung an. Wir betrachten ein Bild als Prämisse und seine Bildunterschrift als implizit. Zusätzlich betrachten wir die Bildunterschrift als Prämisse und das FOIL als Hypothese. Wenn ein NLI-Modell vorhersagt, dass das FOIL im Widerspruch zur Bildunterschrift steht oder neutral ist, nehmen wir dies als Indikator für ein gültiges FOIL. Wenn ein NLI vorhersagt, dass das FOIL implizit durch die Bildunterschrift ist, kann es kein gutes FOIL sein, da es durch Transitivität eine wahrheitsgetreue Beschreibung des Bildes liefern würde, und wir filtern diese FOILs heraus. Aber dieses Verfahren ist nicht perfekt. Es ist nur ein Indikator für gültige FOILs, daher verwenden wir als dritte Maßnahme zur Generierung gültiger FOILs menschliche Annotatoren, um die in VALS verwendeten Daten zu validieren. Also, nach der Filterung und menschlichen Bewertung haben wir so viele Testinstanzen wie in dieser Tabelle beschrieben. Beachten Sie, dass VALS keine Trainingsdaten liefert, sondern nur Testdaten, da es ein Zero-Shot-Testbenchmark ist. Es ist darauf ausgelegt, die bestehenden Fähigkeiten von Vision- und Sprachmodellen nach dem Vortraining zu nutzen. Feinabstimmung würde den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen. Und wir wissen alle, dass diese Modelle gerne schummeln und Abkürzungen nehmen. Und wie wir sagten, sind wir daran interessiert, zu bewerten, welche Fähigkeiten die Vision- und Sprachmodelle nach dem Vortraining haben. Wir experimentieren mit fünf Vision- und Sprachmodellen auf VALS, nämlich mit CLIP, LXMERT, WilBERT, VILBERT 12 in 1 und VISUALBERT. Zwei unserer wichtigsten Bewertungsmetriken sind die Genauigkeit der Modelle bei der Klassifizierung von Bild-Satz-Paaren in Bildunterschriften und FOILs. Vielleicht relevanter für dieses Video, werden wir unsere permissivere Metrik, die paarweise Genauigkeit, vorstellen, die misst, ob der Bild-Satz-Ausrichtungswert für das korrekte Bild-Text-Paar größer ist als für sein gefälschtes Paar. Für weitere Metriken und Ergebnisse dazu, lesen Sie bitte unseren Artikel. Die Ergebnisse mit der paarweisen Genauigkeit sind hier gezeigt und sie sind konsistent mit den Ergebnissen, die wir von den anderen Metriken erhalten haben. ist, dass die beste Zero-Shot-Leistung von Wilbert 12 in 1 erreicht wird, gefolgt von Wilbert und sie sind konsistent mit den Ergebnissen, die wir von den anderen Metriken erhalten haben, ist, dass die beste Zero-Shot-Leistung von Wilbert 12 in 1 erreicht wird, gefolgt von Wilbert, Alex Mert, Clip und schließlich Visual Bird. Es ist bemerkenswert, wie Instrumente, die auf die einzelnen Objekte wie Existenz und Nominalphrasen konzentriert sind, von Wilbert 12 in 1 fast gelöst werden, was darauf hinweist, dass Modelle in der Lage sind, benannte Objekte und ihre Anwesenheit in Bildern zu identifizieren. Allerdings können keine der verbleibenden Teile in unseren adversariellen FOILing-Einstellungen zuverlässig gelöst werden. Wir sehen aus den Instrumenten für Pluralität und Zählen, dass Vision- und Sprachmodelle Schwierigkeiten haben, Verweise auf einzelne versus mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen. Das Teil für Beziehungen zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Beziehung zwischen Objekten in einem Bild korrekt zu klassifizieren. Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsverzerrungen unterstützt werden, wie wir im Handlungsteil sehen. Aus dem Teil für Referenz erfahren wir, dass das Verfolgen mehrerer Verweise auf dasselbe Objekt in einem Bild durch Pronomen auch für Vision- und Sprachmodelle schwierig ist. Als Sanity-Check und weil es ein interessantes Experiment ist, bennen wir auch zwei textbasierte Modelle, GPT-1 und GPT-2, um zu bewerten, ob VALS von diesen unimodalen Modellen lösbar ist, indem wir die Perplexität der korrekten und der gefälschten Bildunterschrift berechnen und den Eintrag mit der niedrigsten Perplexität vorhersagen. Wenn die Perplexität für das FOIL höher ist, nehmen wir dies als Indikation dafür, dass das gefälschte FOIL unter Plausibilitätsverzerrung oder anderen sprachlichen Verzerrungen leiden könnte. Und es ist interessant zu sehen, dass in einigen Fällen die textbasierten GPT-Modelle die Plausibilität der Welt besser erfasst haben als die Vision- und Sprachmodelle. Also, um zusammenzufassen. VALS ist ein Benchmark, der die Linse sprachlicher Konstrukte nutzt, um der Gemeinschaft zu helfen, Vision- und Sprachmodelle zu verbessern, indem er ihre visuellen Verankerungsfähigkeiten hart testet. Unsere Experimente zeigen, dass Vision- und Sprachmodelle benannte Objekte in ihrer Anwesenheit in Bildern gut identifizieren, wie der Existenzteil zeigt, aber Schwierigkeiten haben, ihre Interdependenz und Beziehungen in visuellen Szenen zu verankern, wenn sie gezwungen sind, sprachliche Indikatoren zu respektieren. Wir würden die Gemeinschaft wirklich ermutigen, VALS zur Messung des Fortschritts bei der sprachlichen Verankerung mit Vision- und Sprachmodellen zu verwenden. Und noch mehr, VALS könnte als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten, um zu sehen, ob ein Datensatz den Modellen hilft, sich bei einem der von VALS getesteten Aspekte zu verbessern. Wenn Sie interessiert sind, schauen Sie sich die VALS-Daten auf GitHub an, und wenn Sie Fragen haben, zögern Sie nicht, uns zu kontaktieren."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamizawa von der Universität Tokio. Ich werde einen Vortrag mit dem Titel RNSUN, ein umfangreicher Datensatz für die automatische Generierung von Versionshinweisen über die Zusammenfassung von Commit-Logs, halten. Ich werde dies in der folgenden Reihenfolge erklären. Zunächst werde ich die automatische Generierung von Versionshinweisen vorstellen, an der wir in dieser Forschung arbeiten. Versionshinweise sind technische Dokumente, die die Änderungen zusammenfassen, die mit jeder Veröffentlichung eines Softwareprodukts verteilt werden. Das Bild zeigt die Versionshinweise für Version 2.6.4 der Vue.js-Bibliothek. Versionshinweise spielen eine wichtige Rolle in der Open-Source-Entwicklung, aber sie sind zeitaufwendig, wenn sie manuell erstellt werden. Daher wäre es sehr nützlich, hochwertige Versionshinweise automatisch generieren zu können. Ich werde mich auf zwei frühere Forschungen zur automatischen Generierung von Versionshinweisen beziehen. Die erste ist ein System namens Arena, das 2014 veröffentlicht wurde. Es verwendet einen regelbasierten Ansatz, z. B. den Änderungs-Extractor, um Kernunterschiede, Bibliotheksänderungen und Dokumentänderungen aus den Unterschieden zwischen den Versionen zu extrahieren und sie schließlich zu kombinieren. Das bemerkenswerteste Merkmal dieses Systems ist der Issue-Extractor in der oberen rechten Ecke, der mit Jira, dem Issue-Tracking-System, verknüpft sein muss und nur für Projekte angewendet werden kann, die Jira verwenden. Mit anderen Worten, es kann nicht für viele Projekte auf GitHub verwendet werden. Das zweite Glyph, das kürzlich 2020 angekündigt wurde. Es ist im Internet verfügbar und kann über PIP gespeichert werden. Dieses System verfügt über ein einfaches lernbasiertes Textklassifizierungsmodell und gibt für jede Eingabeaussage eines Commits eines von fünf Niveaus aus, wie z. B. Funktionen oder Fehlerbehebungen. Das Bild ist ein Beispiel für die Verwendung, das ein korrektives oder Fehlerbehebungslabel zurückgibt. Die Trainingsdaten von Glyph sind ziemlich klein, etwa 5000, und bei den unten beschriebenen Experimenten ist die Leistung des Textklassifizierungsmodells nicht hoch. Ich präsentiere zwei verwandte Forschungen, aber es gibt Probleme der begrenzten Anwendbarkeit und knappen Datenressourcen. Unser Papier löst diese beiden Probleme und generiert automatisch hochwertige Versionshinweise. Für das Problem der begrenzten Anwendbarkeit schlagen wir eine Methode zur hochwertigen Klassifizierungssummierung vor, die nur Commit-Nachrichten als Eingabe verwendet. Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden. Für das zweite Problem der knappen Datenressourcen haben wir einen R- und Sum-Datensatz erstellt, der aus etwa 82.000 Datensätzen besteht, indem wir Daten aus öffentlichen GitHub-Repositories gesammelt haben, unter Verwendung von RR und einigen Datensätzen. Unser Rnsum-Datensatz besteht aus etwa 82.000 Datensätzen, indem wir Daten aus öffentlichen GitHub-Repositories gesammelt haben, unter Verwendung der GitHub-API. Als Nächstes beschreibe ich unseren Datensatz. Hier ist ein Beispiel für Daten. Die linke Seite ist die Commit-Nachricht und die rechte Seite sind die Versionshinweise. Die Versionshinweise sind als Verbesserungen von Gesichtern usw. gekennzeichnet. Wir haben die Commit-Nachricht und die rechte Seite sind die Versionshinweise. Die Versionshinweise sind als Verbesserungen, Fehlerbehebungen usw. gekennzeichnet. Wir haben eine Aufgabe eingerichtet, die die Commit-Nachrichten als Eingabe nimmt und die markierten Versionshinweise als Ausgabe liefert. Dies kann als Zusammenfassung betrachtet werden. Wir haben vier vordefinierte Labels festgelegt: Funktionen, Verbesserungen, Fehlerbehebungen, Veraltungen, Entfernungen und Bruchänderungen. Diese wurden basierend auf früheren Forschungen und anderen Faktoren festgelegt. Die am wenigsten Knoten unten links werden aus den am wenigsten Knoten unten rechts extrahiert. Zu diesem Zeitpunkt ist es notwendig, die vier vorab festgelegten Labels zu erkennen. Aber die Labels sind nicht immer konsistent mit jedem Repository. Zum Beispiel umfasst das Label Verbesserungen Verbesserungen, Erweiterungen, Optimierungen usw. Wir haben eine Vokabelliste unserer Studienergebnisse für jede dieser Notationsschwankungen erstellt, verwendet sie, um die Versionshinweis-Klasse zu erkennen, und korrigiert den Text der Liste, die als Versionshinweis-Satz für die Klasse folgt, um die vorherige Versionshinweisversion 2.5 bis 18 zu identifizieren und den Diff zu erhalten. Dies ist etwas mühsam und es reicht nicht aus, einfach eine Liste von Versionen zu erhalten und sich das Vorher und Nachher anzusehen. Wir haben eine heuristische Matching-Regel erstellt, um die vorherige und nächste Version zu erhalten. Tag-Datenanalyse. Am Ende wurden 7.200 Repositories und 82.000 Datensätze gesammelt. Außerdem beträgt die durchschnittliche Anzahl der Versionshinweis-Token 63, was für eine Zusammenfassung ziemlich hoch ist. Außerdem ist die Anzahl der eindeutigen Token ziemlich groß, mit 8.830.000. Dies liegt an der großen Anzahl einzigartiger Klassen- und Methodennamen, die im Repository gefunden wurden. Als Nächstes werde ich die vorgeschlagene Methode erklären. Aufgrund der großen Anzahl einzigartiger Klassen- und Methodennamen, die im Repository gefunden wurden. Als Nächstes werde ich die vorgeschlagene Methode erklären. Das klassenweise extraktive-dann-abstraktive Zusammenfassungmodell besteht aus zwei neuronalen Netzen und verwendet einen Klassifikator, um jede Commit-Nachricht in fünf Versionshinweis-Klassen zu klassifizieren. Wir wählen Implementierungen, Fehlerbehebungen, Veraltungen plus und andere. Die als andere klassifizierten Commit-Nachrichten werden verworfen. Dann wendet CEAS den Generator unabhängig auf die vier Dokumentlabels an und generiert Versionshinweise für jede Klasse. In dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Versionshinweisen nicht bekannt. Daher, um den Klassifikator zu trainieren, weisen wir jedem Eingangs-Commit-Nachrichten ein Pseudolabel zu, indem wir die ersten 10 Zeichen jeder Commit-Nachricht verwenden. Wir modellieren die klassenweise abstraktive Zusammenfassung durch unseren Ansatz mit zwei verschiedenen Methoden. Das erste Modell, das wir cssingle nennen, besteht aus einem einzigen Satz-zu-Satz-Netzwerk und generiert einen einzigen langen Versionshinweis-Text, indem es die Eingangs-Commit-Nachrichten zusammenfügt. Das Ausgabetextnetzwerk, von dem jedes einem der am wenigsten bekannten Klassen entspricht. Okay, lassen Sie mich das Experiment erklären. Fünf Methoden wurden verglichen: CAS, CASSingle, CASMatch, PlusSelling und die frühere Studie, GRIF. In Bezug auf Abweichungen wurden in einigen Fällen CSMatch, Blustering und Previous Study Glyph verglichen. In Bezug auf die Bewertung wurden in einigen Fällen am wenigsten Knoten in mehreren Sätzen ausgegeben. Da es schwierig ist, die Anzahl der Sätze zu berechnen, wie sie sind, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt. Blau wird bestraft, wenn das System einen kurzen Satz ausgibt. Diese Strafe führt zu einem niedrigeren Blau-Wert in den im Folgenden beschriebenen Versuchsergebnissen. Schließlich berechnen wir auch die Spezifität, da Rouge und Blau nicht berechnet werden können, wenn die Versionshinweise leer sind. Eine hohe Spezifität bedeutet, dass das Modell korrekt einen leeren Text in Fällen ausgibt, in denen die Versionshinweise leer sind. Hier sind die Ergebnisse. Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den bereinigten Datensatz bewertet, der sie ausschließt. CES und CAS erreichten Rouge L-Werte, die mehr als 10 Punkte höher als die Basislinien waren. Insbesondere auf dem sauberen Testdatensatz sprang die Punktdifferenz zwischen dem vorgeschlagenen Verfahren und den Basislinien auf mehr als 20 Punkte. Diese Ergebnisse deuten darauf hin, dass CAS und CAS signifikant wirksam sind. CAS erhielt einen besseren Root-A-Wert als CAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators bei der Klassifizierung des Klassifikators mit zwei Doppelten wirksam ist. Eine hohe Abdeckung von CAS kann wahrscheinlich erreicht werden, weil der Klassifikator sich darauf konzentrieren kann, relevante Commit-Nachrichten für jede Klasse auszuwählen. CAS Match tendierte dazu, höhere Log L als CAS Single zu erzielen, was darauf hindeutet, dass es auch wirksam ist, unabhängig voneinander unterschiedliche abstraktive Zusammenfassungmodelle für jede Versionshinweis-Klasse zu entwickeln. Hier sind die Fehleranalysen. CAS-Methoden neigen dazu, kürzere Sätze als die menschlichen Referenzsätze auszugeben. In der Abbildung rechts hat der Referenzsatz drei oder vier Sätze, während CAS nur einen hat. Der Grund für diese Zurückhaltung des Modells ist, dass in den Trainingsdaten nur 33% der Sätze im Label Funktionen und 40% im Label Verbesserungen vorhanden sind. Darüber hinaus können CES-Methoden keine genauen Versionshinweise ohne zusätzliche Informationen generieren. Das oberste Beispiel rechts ist ein Beispiel für ein sehr chaotisches Commit, das CS-Methoden nicht ohne zusätzliche Informationen zu einem genauen Versionshinweis generieren können. Das oberste Beispiel rechts ist ein Beispiel für eine sehr chaotische Commit-Nachricht, und der vollständige Satz kann nicht ohne Bezugnahme auf die entsprechende Pro-Anfrage oder das Problem generiert werden. Das Beispiel unten zeigt, dass die beiden Commit-Nachrichten in der Eingabe miteinander verbunden sind und in einen Satz kombiniert werden sollten, aber es versagt dabei. Abschließend eine Schlussfolgerung. Wir haben einen neuen Datensatz für die automatische Generierung von Versionshinweisen erstellt. Wir haben auch die Aufgabe eingerichtet, Commit-Nachrichten einzugeben und sie so zusammenzufassen, dass sie auf alle Projekte anwendbar ist, die in englischer Sprache verfasst sind. Unser Experiment zeigt, dass die vorgeschlagene Methode weniger laute Versionshinweise bei höherer Abdeckung als die Basislinien generiert. Bitte schauen Sie sich unsere Exklusiv-Registerkarte an. Vielen Dank."}
