{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Asaf Harari et je vais présenter notre article, « Enrichissement automatique de données tabulaires à partir de quelques exemples en utilisant des architectures de transformateurs affinés ». Les scientifiques des données analysent les données et se concentrent principalement sur la manipulation des caractéristiques existantes des données. Mais parfois, ces caractéristiques sont limitées. La génération de caractéristiques à partir d'une autre source de données peut ajouter des informations substantielles. Notre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des sources de texte libre externes. La génération de caractéristiques à partir d'une autre source de données peut ajouter des informations substantielles. Notre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des sources de texte libre externes. Supposons que nous ayons un ensemble de données tabulaires et une base de connaissances. Nous avons besoin d'un processus automatique qui implique le couplage d'entités et l'analyse de texte pour extraire de nouvelles caractéristiques à partir du texte libre de la base de connaissances. Notre cadre, FAST, est exactement ce processus automatique. Voyons donc un exemple. Dans un ensemble de données alimenté à FAST. Dans cet exemple, l'ensemble de données est un ensemble de données universitaires lorsque son objectif est de classer les universités en universités de faible rang et en universités de haut rang. l'ensemble de données est un ensemble de données universitaires lorsque son objectif est de classer les universités en universités de faible rang et en universités de haut rang. En tant que base de connaissances, nous utilisons Wikipédia. La première phase de FIST est le couplage d'entités. Lorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances. Et le texte des entités de la base de connaissances est extrait et ajouté à l'ensemble de données. Dans cet exemple, le texte est l'abstract de la page Wikipédia. Maintenant, nous devons générer ou extraire des caractéristiques à partir du texte récupéré. Nous avons donc besoin d'une phase d'extraction de caractéristiques, qui inclut l'analyse de texte. Et c'est la principale nouveauté de cet article, et je vais m'y attarder dans les diapositives suivantes. Après la phase d'extraction de caractéristiques, il y a une phase de génération de caractéristiques lorsque nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques. D'abord, générer des caractéristiques dans le nombre de classes de l'ensemble de données original. Dans cet exemple, l'ensemble de données original a deux classes, donc FAST génère deux nouvelles caractéristiques. Mais si l'ensemble de données a cinq classes, FAST génère cinq nouvelles caractéristiques. Chaque caractéristique représente la probabilité pour chaque classe. Pour analyser le texte, nous utilisons l'état actuel de l'art de l'analyse de texte, qui sont des modèles de langage basés sur des transformateurs comme BERT, GPT, XNL, etc. Mais il est peu probable que nous puissions entraîner un modèle de langage en utilisant les ensembles de données d'entrée. Une approche naïve serait donc un affinement de tâche cible. Donc, dans la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle de langage pré-entraîné, affiner le modèle de langage sur l'ensemble de données cible. Dans cet exemple, pour affiner le modèle de langage, pour classer le texte en classes, l'abstract en classes, bas ou haut, recevoir la sortie du modèle de langage, qui est la probabilité pour chaque classe, et utiliser comme nouvelles caractéristiques. Le problème avec cette approche est que les ensembles de données peuvent avoir peu d'étiquettes d'entités distinctes. nouvelles caractéristiques. Le problème avec cette approche est que les ensembles de données peuvent avoir peu d'étiquettes d'entités distinctes. Dans notre expérience, presque la moitié des ensembles de données contiennent moins de 400 échantillons, et le plus petit ensemble de données contenait 35 échantillons dans son ensemble d'entraînement. Donc, affiner un modèle de langage sur cet ensemble de données serait inefficace. Mais nous pouvons utiliser des connaissances antérieures sur les ensembles de données pré-analysées parce-1 ensembles de données et utiliser cette information lorsque nous analysons l'ensemble de données N. Ce que nous suggérons, c'est d'ajouter une autre phase d'affinement, une phase d'affinement multitâche préliminaire, lorsque vous affinez le modèle de langage sur n-1 ensembles de données, et puis nous exécutons une autre phase d'affinement, qui est un affinement de tâche cible lorsque nous affinons le modèle de langage sur l'ensemble de données cible n. L'état de l'art en affinement multitâche s'appelle mtDNN. Dans mtDNN, mtDNN maintient des têtes dans le nombre de tâches dans l'ensemble d'entraînement. Donc, dans cet exemple, il y a quatre tâches dans l'ensemble d'entraînement donc DNN vide et maintenir quatre têtes comme vous pouvez le voir dans l'image, et il échantillonne un lot aléatoire de l'ensemble d'entraînement. Et si le lot aléatoire appartient, par exemple, à des tâches de classification de phrase unique, il exécute un passage avant et arrière à travers la première tête. Et si le lot aléatoire appartient à une tâche de classement par paire, il exécute un passage avant et arrière à travers la dernière tête. Dans notre scénario, les ensembles de données tabulaires varient en nombre de classes. Donc, dans notre scénario, un ensemble de données Tableau vérifie le nombre de classes. Donc, il y a de nombreuses tâches. MTDNN maintient le nombre de têtes de classes, les couches de sortie, et de plus, MTDNN doit initialiser de nouvelles têtes pour un nouvel ensemble de données avec une nouvelle tâche. Notre approche appelée affinement de reformulation de tâche est que nous, dans notre approche de réforme de tâche, au lieu de maintenir plusieurs têtes, nous reformulons chaque ensemble de données en un problème de classification par phrase, qui est un DAS de deux classes au lieu de maintenir plusieurs têtes, nous reformulons chaque ensemble de données en un problème de classification par phrase, qui est deux tâches de classes. Donc, voyons un exemple. Voici notre ensemble de données d'entrée, qui se compose d'entités, de caractéristiques, de texte et de classes. et nous avons pour tâche de classer le texte en bas et haut pour classer le texte, pour classer l'abstract et la classe, pour l'abstract et la classe, si l'abstract appartient ou non à la classe. Donc, le vecteur d'étiquette dans le cas de Zig, est toujours, qui se compose toujours de deux classes. dans le cas de Zig, il est toujours, qui se compose toujours de deux classes. Et c'est le. Puis il reformule la tâche en tâches de classification par phrase. Appliquer le modèle de langage à la nouvelle tâche et la probabilité de sortie pour chaque classe. Et notez que le modèle de langage est déjà affiné sur N moins un ensemble de données en utilisant un affinement multitâche préliminaire. Puis nous utilisons le vecteur de sortie de l'ensemble de données en utilisant un affinement multitâche préliminaire. Puis nous utilisons le vecteur de sortie du modèle de langage comme une nouvelle caractéristique générée dans le nombre de classes. Pour évaluer notre cadre, nous utilisons un ensemble de classification tabulaire de 17, qui vérifie la taille, les caractéristiques, l'équilibre, le domaine et la performance initiale. Et en tant que base de connaissances, nous utilisons Wikipédia. Nous concevons notre expérience comme une évaluation en temps réel d'un seul, lorsque nous entraînons FAST sur 16 ensembles de données et l'appliquons au 17e ensemble de données. Nous divisons également chaque ensemble de données en quatre défauts et appliquons une validation croisée à quatre défauts. Puis nous générons la nouvelle caractéristique et les évaluons en utilisant cinq classificateurs d'évaluation. pour la validation croisée à quatre défauts. Puis nous générons la nouvelle caractéristique et les évaluons en utilisant cinq classificateurs d'évaluation. Nous utilisons dans notre expérience une architecture basée sur des modèles de langage pré-entraînés. Voici les résultats pour notre expérience. Vous pouvez voir que nous comparons notre cadre à l'affinement d'un ensemble de données cible, à l'affinement de tâche cible et à l'affinement préliminaire de mtdnn et notre affinement reformulé atteint le meilleur résultat, la meilleure performance tandis que DNN vide formule affinement atteint le meilleur résultat, la meilleure performance. Alors que MTDNN a atteint une amélioration de deux pour cent par rapport à l'affinement d'un ensemble de données cible, notre approche a atteint une amélioration de six pour cent. Lorsque nous regardons l'ensemble de données petit, nous pouvons voir que la performance de DNN vide diminue et l'amélioration de l'affinement de tâche cible seule. Pour résumer, FAST permet un enrichissement à partir de 35 échantillons dans notre expérience. Il utilise une architecture pour tous les ensembles de données de tâches et il maintient la tête du modèle. Mais il ajoute une phase de reformulation. Il augmente l'ensemble d'entraînement et il a besoin d'une valeur cible avec un sens sémantique afin que nous puissions la lui fournir et l'utiliser dans le problème de classification par phrase. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à tous, aujourd'hui je vais présenter notre travail de recherche, Apprendre à raisonner de manière déductive, la résolution de problèmes métaboliques comme extraction de raisonnement complexe. Je suis Alan du ByteDance AI Lab, et c'est un travail conjoint avec Jerry de l'Université du Texas à Austin, et Weilu de la SUTD. Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement. Donc, ici nous montrons un exemple où le raisonnement en plusieurs étapes est utile. Donc, cette figure est tirée du stylo sur papier où ils effectuent un prompting pour résoudre le problème mathématique dans un scénario d'apprentissage en quelques exemples. Donc, à gauche, nous pouvons voir si nous donnons quelques exemples avec juste des questions et des réponses, nous ne sommes peut-être pas en mesure d'obtenir les bonnes réponses. Mais si nous donnons une description de raisonnement plus détaillée, le modèle est capable de prédire la description de raisonnement et aussi de faire une prédiction correcte ici. Il est donc bon d'avoir un raisonnement en plusieurs étapes interprétable comme sortie. Et nous pensons également que le problème de méthode est une application directe pour évaluer de telles capacités de raisonnement. Donc, ici dans notre configuration de problème, étant donné les questions, nous devons résoudre cette question. application pour évaluer de telles capacités de raisonnement. Donc, ici dans notre configuration de problème, étant donné les questions, nous devons résoudre cette question et obtenir les réponses numériques. Donc, dans nos ensembles de données, nous avons également l'expression mathématique, qui conduit à cette réponse particulière également. Donc, certaines hypothèses s'appliquent également comme dans les travaux précédents. Nous supposons que la précision des quantités est connue et nous ne considérons que des opérateurs de base tels que l'addition, les soustractions, la multiplication, la division et l'exponentielle. De plus, les opérateurs compliqués peuvent en fait être décomposés en ces opérateurs de base. Donc, les travaux précédents en résolution de problèmes de méthode peuvent en fait être classés en modèles séquence-à-séquence et séquence-à-arbre. Donc, le modèle séquence-à-séquence traditionnel convertit l'expression en une séquence spécifique pour la génération, et il est assez facile à mettre en œuvre, et il peut se généraliser à de nombreux problèmes compliqués différents. Mais les inconvénients de la performance ne sont généralement pas meilleurs que le modèle de structure. Et il manque d'interprétabilité pour la prédiction. Mais en fait, cette direction est toujours assez populaire à cause du modèle transformateur. Donc, dans les modèles basés sur l'arbre, nous structurons en fait ces expressions sous forme d'arbre et suivons un parcours pré-ordre dans trois générations. Donc, dans les modèles basés sur l'arbre, nous structurons en fait ces expressions sous forme d'arbre et suivons un parcours pré-ordre dans les générations d'arbre. Donc, ici nous continuons à générer les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités. Donc, ici la bonne chose est qu'il donne en fait cette structure d'arbre binaire. structure et c'est mais en fait c'est assez contre-intuitif parce que nous générons l'opérateur en premier et puis à la fin nous générons la structure, mais en fait c'est assez contre-intuitif parce que nous générons l'opérateur en premier et puis à la fin nous générons les quantités. Et la deuxième chose est qu'il contient également des calculs répétitifs. Donc, ici si nous regardons cette expression, a fois 3 plus 3, est en fait générée deux fois. Mais en fait, nous devrions réutiliser les résultats donc dans notre approche proposée, nous voulons résoudre ces problèmes de manière étape par étape et interprétable afin par exemple ici dans la deuxième étape nous pouvons obtenir ce diviseur qui est 27 et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent. Et dans ces étapes, nous obtenons les diviseurs. Donc et puis à cette troisième étape, nous obtenons en fait le quotient. Très bien. Et après ces trois étapes, nous pouvons en fait réutiliser les résultats de la deuxième étape et puis obtenir les résultats de la quatrième étape. Et puis finalement nous pouvons obtenir les dividendes. Donc ici nous générons en fait directement toute l'expression plutôt que de générer des opérateurs ou des quantités uniques. Donc cela rend le processus plus précis. Donc, dans notre système déductif, nous commençons d'abord avec un tas de quantités présentées dans les questions, et incluant également quelques constantes comme nos états initiaux. Donc l'expression est représentée par EIJOP, où nous effectuons l'opérateur de QI à QJ, et une telle expression est en fait dirigée. Donc nous avons également la soustraction inverse ici pour représenter la direction opposée. C'est assez similaire à l'extraction de relations. Donc, dans un système déductif formel, au temps t, nous appliquons l'opérateur entre la paire QI et QJ, et nous obtenons alors ces nouvelles expressions. Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité. Donc ce diaporama visualise en fait l'évolution des états où nous continuons à ajouter des expressions aux états actuels. Donc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langage pré-entraîné qui peut être des oiseaux ou des lapins, et puis nous codons une phrase, et nous obtenons alors ces représentations de quantités. Donc une fois que nous obtenons les représentations de quantités, nous pouvons commencer à faire des inférences. Ici nous montrons un exemple de Q1 pour obtenir la représentation pour Q1 divisé par Q2 et puis multiplié par Q3. D'abord nous obtenons la représentation de paire, qui est en fait juste la concaténation entre Q1 et Q2. Et puis nous appliquons un réseau de feedforward, qui est paramétré par l'opérateur. Et puis finalement, nous obtenons la représentation d'expression Q1 divisé par Q2. Mais en pratique, à l'étape d'inférence, nous pourrions être en mesure d'obtenir l'expression incorrecte également. Donc ici toute l'expression possible est égale à trois fois le nombre d'opérateurs. Donc la chose agréable ici est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche. Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche. Donc dans la deuxième étape, nous faisons la même chose, mais la seule différence est une quantité de plus. Donc cette quantité vient de l'expression calculée précédente. Donc finalement, nous pouvons obtenir cette expression finale, Q3 fois Q4. Et nous pouvons également voir le nombre de toutes les expressions possibles est différent de l'étape précédente. Donc de telles différences rendent difficile l'application de la recherche par faisceau parce que la distribution de probabilité entre ces deux étapes est déséquilibrée. Donc la procédure d'entraînement est similaire à l'entraînement d'un modèle séquence-à-séquence, où nous optimisons la perte à chaque temps t. Et ici nous utilisons également ce tau pour représenter quand nous devrions terminer ce processus de génération. Et ici l'espace est différent de séquence-à-séquence, parce que l'espace est différent à chaque temps t, dans le modèle séquence traditionnel à séquence c'est le nombre de vocabulaire et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures donc nous menons des expériences sur les ensembles de données de problèmes de méthode couramment utilisés mawps math23k mathqa MATHQA, et SWAM. Et ici nous montrons brièvement les résultats comparés aux meilleures approches précédentes. Donc notre variante la mieux performante est Robeta déductive raisonner. Et en fait, nous n'utilisons pas la recherche par faisceau en contraste. Ob les meilleures approches sont souvent des modèles basés sur l'arbre. Donc dans l'ensemble notre raisonneur est capable de surpasser de manière significative ce modèle basé sur l'arbre, mais nous pouvons voir le nombre absolu sur MathQA ou SWAMP n'est pas vraiment élevé. Donc nous enquêtons davantage sur les résultats sur Swamp, et cet ensemble de données est difficile parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle de NLP, comme ajouter des informations non pertinentes et des quantités supplémentaires. Donc dans notre prédiction, nous trouvons que certaines des valeurs intermédiaires sont en fait négatives. Par exemple, dans cette question, nous demandons combien de pommes Jake a, mais nous avons des informations supplémentaires comme 17 moins de pêches, et Steven a 8 pêches, ce qui est totalement non pertinent. Donc notre modèle fait une prédiction comme celle-ci, qui produit des valeurs négatives. Et nous observons que ces deux expressions ont en fait des scores similaires. Donc nous pouvons en fait limiter cet espace de recherche en supprimant ces résultats comme négatifs, de sorte que nous puissions rendre la réponse correcte. Donc nous trouvons en outre qu'une telle contrainte améliore considérablement pour certains modèles. Par exemple, pour Donc nous trouvons en outre qu'une telle contrainte améliore considérablement pour certains modèles. Par exemple, pour les oiseaux, nous avons amélioré sept points. Et puis pour le modèle basé sur Robeta, nous avons en fait amélioré deux points. Donc un meilleur modèle de langage a une meilleure capacité de compréhension du langage de sorte que le nombre ici est plus élevé pour Robeta et plus bas pour les oiseaux. Et nous avons également essayé d'analyser la difficulté derrière tous ces ensembles de données. Nous supposons que le nombre de quantités non utilisées peut être considéré comme des informations non pertinentes ici. Donc ici nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités non utilisées, et l'ensemble de données SWAMP a la plus grande proportion. Et ici nous montrons également la performance globale. Pour ces échantillons sans quantités non utilisées, donc la performance globale est en fait plus élevée que la performance globale. Mais avec ces échantillons avec des quantités non utilisées, c'est en fait bien pire que la performance globale. Pour MAWPS, nous n'avons pas vraiment trop de cas de décès, donc j'ignore juste cette partie. Donc finalement, nous voulons montrer l'interprétabilité à travers un exemple d'échec et de participation. Donc ici, notre modèle fait en fait une prédiction incorrecte à la première étape. Donc nous pouvons en fait corréler l'expression Donc ici notre modèle fait en fait une prédiction incorrecte à la première étape. Donc nous pouvons en fait corréler cette expression avec la phrase ici. Donc nous pensons que cette phrase pourrait induire le modèle en erreur pour une prédiction incorrecte. Donc ici, imprimer un autre 35 fait penser au modèle qu'il devrait y avoir un opérateur d'addition. Donc nous essayons de réviser la phrase pour être quelque chose comme le nombre d'arbres de poire est de 55 de moins que les arbres de pomme. Donc nous le rendons pour transmettre une sémantique plus précise afin que le modèle puisse faire la prédiction correcte. Donc cette étude montre comment les prédictions interprétables nous aident à comprendre le comportement du modèle. Donc pour conclure notre travail, donc d'abord notre modèle est en fait assez efficace, et nous sommes capables de fournir une procédure de résolution interprétable. Et nous pouvons facilement incorporer certaines connaissances antérieures comme contrainte, ce qui peut aider à améliorer la performance. Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution de problèmes de réseau, mais aussi à d'autres tâches qui impliquent un raisonnement en plusieurs étapes. Mais nous avons également certaines limitations. Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée. Et la deuxième chose est que, comme mentionné, parce que la distribution de probabilité est déséquilibrée à différents temps t, il est donc également assez difficile d'appliquer la stratégie de recherche par faisceau. Donc voici la fin du discours, et les questions sont les bienvenues. Merci."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine et je viens de l'Université de Maastricht. Je vais présenter mon travail conjoint avec Jerry, qui porte sur un nouveau jeu de données pour la récupération d'articles légaux. Les questions juridiques font partie intégrante de la vie de nombreuses personnes, mais la majorité des citoyens ont peu ou pas de connaissances sur leurs droits et les processus juridiques fondamentaux. En conséquence, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'assistance coûteuse d'un expert juridique se retrouvent sans protection ou, pire, exploités. Notre travail vise à combler le fossé entre les gens et la loi en développant des systèmes de récupération efficaces pour les articles légaux. Un tel système pourrait fournir un service d'aide juridique gratuit pour les humains non qualifiés. Avant de plonger dans la principale contribution de ce travail, décrivons d'abord le problème de la récupération d'articles légaux. Étant donné une simple question sur une question juridique, comme « à quoi risque-je si je viole la confidentialité professionnelle ? », un modèle est nécessaire pour récupérer tous les articles légaux pertinents à partir d'un vaste corpus législatif. Cette tâche de recherche d'information présente son propre ensemble de défis. Premièrement, elle traite de deux types de langage, le langage naturel commun pour les questions et le langage complexe illégal pour les statuts. Cette différence dans les distributions de langage rend plus difficile pour un système de récupérer des candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique qui correspond à la terminologie des statuts. De plus, le droit législatif n'est pas un empilement d'articles indépendants qui peuvent être traités comme une source d'information complète à eux seuls, comme les nouvelles ou les recettes par exemple. Au lieu de cela, c'est une collection de structures de dispositions légales qui n'ont un sens complet que lorsqu'elles sont considérées dans leur contexte global, c'est-à-dire avec les informations supplémentaires de leurs articles voisins, les champs et sous-champs auxquels ils appartiennent, et leur place dans la structure de la loi. Enfin, les articles légaux sont en petits paragraphes, qui sont généralement l'unité de récupération typique dans la plupart des travaux de récupération. Ici, il y a de longs documents qui peuvent atteindre 6 000 mots. Les récentes avancées en NLP ont suscité un grand intérêt pour de nombreuses tâches juridiques, telles que la prédiction de jugements juridiques ou l'examen automatisé de contrats, mais la récupération d'articles légaux est restée principalement intacte en raison du manque de jeux de données de grande taille et de haute qualité. Dans ce travail, nous présentons un nouveau jeu de données centré sur le citoyen de langue maternelle française pour déterminer si un modèle de récupération peut s'approcher de l'efficacité et de la fiabilité des experts juridiques pour la tâche de récupération d'articles légaux. Notre jeu de données de récupération d'articles légaux belges, PSART, se compose de plus de 1 100 questions juridiques posées par des citoyens belges. Ces questions couvrent un large éventail de sujets, allant de la famille, le logement, l'argent, au travail et la sécurité sociale. Chacune d'elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de 22 600 articles légaux des codes de droit belges. Parlons maintenant de la manière dont nous avons collecté ce jeu de données. Tout d'abord, nous avons commencé par compiler un vaste corpus d'articles légaux. Nous avons considéré 32 codes belges disponibles au public et extrait tous leurs articles ainsi que les en-têtes de section correspondants. Ensuite, nous avons rassemblé des questions juridiques avec des références aux statuts pertinents. Pour ce faire, nous avons collaboré avec un cabinet d'avocats belge qui reçoit chaque année environ 4 000 e-mails de citoyens belges qui demandent des conseils sur une question juridique personnelle. Nous avons eu la chance d'accéder à leurs sites web, où leur équipe de juristes expérimentés aborde les problèmes juridiques les plus courants des Belges. Nous avons collecté des milliers de questions, annotées avec des catégories, des sous-catégories et des références juridiques aux statuts pertinents. Enfin, nous avons analysé les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avions considérés. Les références restantes ont été mises en correspondance et converties en les identifiants d'articles correspondants de notre corpus. Nous avons finalement obtenu 1 108 questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre vaste corpus de 22 633 articles légaux. De plus, chaque question est accompagnée d'une catégorie principale et d'une concaténation de sous-catégories, et chaque article est accompagné d'une concaténation de leur en-tête suivant dans la structure de la loi. Ces informations supplémentaires ne sont pas utilisées dans le travail présent, mais pourraient être d'intérêt pour les futures recherches sur la récupération d'informations juridiques ou la classification de textes juridiques. Examinons quelques caractéristiques de notre jeu de données. Les questions sont entre 5 et 44 mots de long, avec une médiane de 40 mots. Les articles sont beaucoup plus longs, avec une longueur médiane de 77 mots, 142 d'entre eux dépassant 1 000 mots, le plus long étant de 5 790 mots. Comme mentionné précédemment, les questions couvrent un large éventail de sujets, avec environ 85 % d'entre elles portant sur la famille, le logement, l'argent ou la justice, tandis que les 15 % restants concernent soit la sécurité sociale, les étrangers ou le travail. Les articles sont également très divers, car ils proviennent de 32 codes belges différents qui couvrent un grand nombre de sujets juridiques. Voici le nombre total d'articles collectés à partir de chacun de ces codes belges. Sur les 22 633 articles, seuls 1 612 sont référencés comme pertinents à au moins une question dans le jeu de données. Et environ 80 % de ces articles cités proviennent soit du code civil, du code judiciaire, du code d'instruction criminelle ou des codes pénaux. Pendant ce temps, 18 des 32 codes ont moins de 5 articles mentionnés comme pertinents à au moins une question, ce qui peut s'expliquer par le fait que ces codes se concentrent moins sur les individus et leurs préoccupations. Dans l'ensemble, le nombre médian de citations pour ces articles cités est de 2, et moins de 25 % d'entre eux sont cités plus de 5 fois. En utilisant nos jeux de données, nous évaluons plusieurs approches de récupération, y compris les architectures lexicales et denses. Étant donné une requête et un article, un modèle lexical attribue un score à la paire requête-article en calculant la somme sur les termes de la requête des poids de chacun de ces termes dans cet article. Nous expérimentons avec les fonctions de classement standard TF-IDF et BM25. Le principal problème avec ces approches est qu'elles ne peuvent récupérer que les articles contenant des mots-clés présents dans la requête. Pour surmonter cette limitation, nous expérimentons avec une architecture basée sur les réseaux neuronaux qui peut capturer les relations sémantiques entre les requêtes et les articles. Nous utilisons un modèle b-encodeur qui mappe les requêtes et les articles dans des représentations vectorielles denses, et calcule un score pertinent entre une paire requête-article par la similarité de leurs emboîtures. Ces emboîtures résultent généralement d'une opération de pooling sur la sortie d'un modèle d'emboîtement de mots. Tout d'abord, nous étudions l'efficacité des b-encodeurs de Siamese dans une configuration d'évaluation sans apprentissage préalable, ce qui signifie que les modèles d'emboîtement de mots pré-entraînés sont appliqués tels quels sans aucun réglage supplémentaire. Nous expérimentons avec des encodeurs de texte indépendants du contexte, à savoir Word2Vec et FastText, et des modèles d'emboîtement contextuels, à savoir Robota et plus spécifiquement Camembert, qui est un modèle Robota français. De plus, nous entraînons nos propres encodeurs b-encodeurs basés sur Camembert sur nos jeux de données. Notez que pour l'entraînement, nous expérimentons avec les deux saveurs de l'architecture b-encodeur. Siamese qui utilise un modèle d'emboîtement de mots unique qui mappe la requête et l'article ensemble dans un espace vectoriel dense partagé, et toTower, qui utilise deux modèles d'emboîtement de mots indépendants qui encodent la requête et l'article séparément dans différents espaces d'emboîtement. Nous expérimentons avec le pooling de moyenne, de maximum et de CLS, ainsi que le produit scalaire et le cosinus pour le calcul des similarités. Voici les résultats de notre référence sur l'ensemble avec les méthodes lexicales ci-dessus, les b-encodeurs de Siamese évalués dans une configuration sans apprentissage préalable au milieu, et les b-encodeurs affinés ci-dessous. Dans l'ensemble, les b-encodeurs affinés surpassent de manière significative toutes les autres références. Le modèle à deux tours améliore son variant Siamese en termes de rappel à 100, mais se comporte de manière similaire sur les autres métriques. Bien que BM25 se soit sous-performé par rapport au B-Encoder entraîné de manière significative, sa performance indique qu'il s'agit encore d'une référence solide pour la récupération spécifique à un domaine. En ce qui concerne l'évaluation sans apprentissage préalable des b-encodeurs de Siamese, nous constatons que l'utilisation directe des emboîtures d'un modèle Camembert pré-entraîné sans optimisation pour la tâche de récupération d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats précédents. De plus, nous avons observé que le biancodeur basé sur Word2Vec s'est nettement surpassé le modèle basé sur FastText et BERT, suggérant que peut-être les emboîtements de mots pré-entraînés sont plus appropriés pour la tâche que les emboîtements de caractères ou de sous-mots. modèle, suggérant que peut-être les emboîtements de mots pré-entraînés sont plus appropriés pour la tâche que les emboîtements de caractères ou de sous-mots lorsqu'ils sont utilisés tels quels. Bien qu'ils soient prometteurs, ces résultats suggèrent une ample opportunité d'amélioration par rapport à un expert juridique qualifié qui peut éventuellement récupérer tous les articles pertinents à toute question et ainsi obtenir des scores parfaits. Concluons en discutant de deux limitations de tous les jeux de données. Premièrement, le corpus d'articles est limité à ceux collectés à partir des 32 codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge, car les articles des décrets, directives et ordonnances sont manquants. Pendant la construction du jeu de données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions se retrouvent avec seulement une fraction du nombre initial d'articles pertinents. Cette perte d'information implique que la réponse contenue dans les articles pertinents restants pourrait être incomplète, bien qu'elle soit toujours complètement appropriée. Deuxièmement, nous devrions noter que toutes les questions juridiques ne peuvent pas être répondues avec les statuts seuls. Par exemple, la question « Puis-je expulser mes locataires s'ils font trop de bruit ? » pourrait ne pas avoir de réponse détaillée dans le droit législatif qui quantifie un seuil de bruit spécifique au-delà duquel l'expulsion est autorisée. Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur le droit jurisprudentiel et trouver des précédents similaires à sa situation actuelle. Par exemple, le locataire fait deux fêtes par semaine jusqu'à 2 h du matin. Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche de récupération d'articles légaux, et le domaine de celles qui sont moins adaptées reste à déterminer. Nous espérons que notre travail suscitera un intérêt pour le développement de modèles de récupération d'articles légaux pratiques et fiables qui peuvent aider à améliorer l'accès à la justice pour tous. Vous pouvez consulter notre article, notre jeu de données et notre code aux liens suivants. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour ! Nous sommes heureux de vous présenter notre travail sur VALS, un benchmark indépendant des tâches destiné à tester les modèles de vision et de langage avec des phénomènes linguistiques spécifiques. Pourquoi avons-nous pris la peine de mettre en place ce benchmark ? Eh bien, au cours des dernières années, nous avons assisté à une explosion de modèles de vision et de langage basés sur les transformateurs, pré-entraînés sur de grandes quantités de paires d'images et de textes. Chacun de ces modèles repousse l'état de l'art sur les tâches de vision et de langage, telles que la réponse visuelle à des questions, le raisonnement sur le sens commun visuel, la récupération d'images, l'ancrage de phrases. Nous avons donc reçu un message : les précisions sur ces benchmarks spécifiques aux tâches augmentent régulièrement. Mais savons-nous ce que les modèles ont réellement appris ? Qu'est-ce qu'un transformateur de vision et de langage a compris lorsqu'il attribue un score élevé à cette image et à cette phrase pour correspondre, et un score faible à celle-ci ? Les modèles de vision et de langage se concentrent-ils sur la bonne chose, ou se concentrent-ils sur les biais comme le montrent les travaux antérieurs ? Pour éclairer davantage cet aspect, nous proposons une direction plus agnostique des tâches et introduisons des vannes qui testent la sensibilité des modèles de vision et de langage à des phénomènes linguistiques spécifiques qui affectent à la fois les modalités linguistiques et visuelles. Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions, la référence d'entité. Mais comment testons-nous si les modèles de vision et de langage ont capturé ces phénomènes ? Par FOILing, une méthode précédemment appliquée pour les modèles de vision et de langage, uniquement pour les phrases nominales par Ravi Shekhar et ses collaborateurs, sur le comptage par nous dans des travaux antérieurs. Foiling signifie essentiellement que nous prenons la légende d'une image et produisons un leurre en modifiant la légende de telle sorte qu'elle ne décrit plus l'image. Et nous faisons ces modifications de phrases en nous concentrant sur six éléments spécifiques, tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la référence d'entité, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous trouverions plus d'une manière intéressante de créer des instances FOIL. Par exemple, dans le cas de l'élément actions, nous avons deux instruments, l'un dans lequel le verbe d'action est changé par une action différente, et un autre dans lequel les actants sont échangés. Le comptage et la référence d'entité sont également des éléments qui ont plus d'un instrument. Et nous créons ces FOILs en nous assurant qu'ils échouent à décrire l'image, qu'ils sont grammaticalement et autrement des phrases valides. Ce n'est pas facile à faire, car une légende FOILée peut être moins probable que la légende originale. Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que des plantes coupent un homme que qu'un homme coupe des plantes, et les grands modèles de vision et de langage pourraient s'en rendre compte. Par conséquent, pour obtenir des FOILs valides, nous devons agir. Premièrement, nous utilisons des modèles de langage puissants pour proposer des FOILs. Deuxièmement, nous utilisons l'inférence de langage naturel, ou NLI, pour filtrer les FOILs qui pourraient encore décrire l'image, puisque lors de la construction des FOILs, nous devons nous assurer qu'ils échouent à décrire l'image. Pour tester cela automatiquement, nous appliquons l'inférence de langage naturel avec la logique suivante. Nous considérons une image comme la prémisse, et sa légende comme son corollaire. De plus, nous considérons la légende comme la prémisse et le FOIL comme son hypothèse. Si un modèle NLI prédit que le FOIL contredit ou est neutre par rapport à la légende, nous prenons cela comme un indicateur d'un FOIL valide. Si un NLI prédit que le FOIL est un corollaire de la légende, il ne peut pas être un bon FOIL, puisque par transitivité, il donnera une description véridique de l'image et nous filtrons ces FOILs. Mais cette procédure n'est pas parfaite. C'est juste un indicateur pour des FOIS valides, par conséquent, comme troisième mesure pour générer des FOIS valides, nous employons des annotateurs humains pour valider les données utilisées dans VALS. Ainsi, après le filtrage et l'évaluation humaine, nous avons autant d'instances de test que décrites dans ce tableau. Notez que VALS ne fournit aucune donnée d'entraînement, mais seulement des données de test, puisqu'il s'agit d'un benchmark de test sans apprentissage préalable. Il est conçu pour tirer parti des capacités existantes des modèles de vision et de langage après l'entraînement préalable. Le réglage fin n'encouragerait que les modèles à exploiter des artefacts ou des biais statistiques dans les données. Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis. Et comme nous l'avons dit, nous sommes intéressés à évaluer quelles capacités les modèles de vision et de langage ont après l'entraînement préalable. Nous expérimentons avec cinq modèles de vision et de langage sur VALS, à savoir avec CLIP, LXMERT, Wil VILBERT, VILBERT 12 in 1 et VISUALBERT. Deux de nos métriques d'évaluation les plus importantes sont la précision des modèles dans la classification des paires d'images et de phrases en légendes et FOILs. Peut-être plus pertinent pour cette vidéo, nous présenterons notre métrique plus permissive, la précision par paire, qui mesure si le score d'alignement de la phrase d'image est plus élevé pour la paire de texte d'image correcte que pour sa paire FOILée. Pour plus de métriques et de résultats à leur sujet, consultez notre article. Les résultats avec la précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus à partir des autres métriques. C'est que la meilleure performance sans apprentissage préalable est réalisée par Wilbert 12 in 1, suivi par Wilbert et ils sont cohérents avec les résultats que nous avons obtenus à partir des autres métriques, c'est que la meilleure performance sans apprentissage préalable est réalisée par Wilbert 12 in 1, suivi par Wilbert, Alex Mert, Klip, et enfin Visual Bird. Il est notable comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par Wilbert 12 in 1, soulignant que les modèles sont capables d'identifier les objets nommés et leur présence dans les images. Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de leurrage adversarial. Nous voyons à partir des instruments de pluralité et de comptage que les modèles de vision et de langage ont du mal à distinguer les références à des objets simples par rapport à des objets multiples ou à les compter dans une image. L'élément relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre les objets dans une image. Ils ont également du mal à distinguer les actions et à identifier leurs participants, même si cela est soutenu par des biais de plausibilité, comme nous le voyons dans l'élément actions. De l'élément référence d'entité, nous découvrons que le suivi de multiples références à un même objet dans une image en utilisant des pronoms est également difficile pour les modèles de vision et de langage. Comme vérification de cohérence, et parce que c'est une expérience intéressante, nous évaluons également deux modèles uniquement textuels, GPT-1 et GPT-2, pour évaluer si VALS est résoluble par ces modèles unimodaux en calculant la perplexité de la légende correcte et FOILée et en prédisant l'entrée avec la perplexité la plus faible. Si la perplexité est plus élevée pour le FOIL, nous prenons cela comme une indication que la légende FOILée peut souffrir d'un biais de plausibilité ou d'autres biais linguistiques. Et il est intéressant de voir que dans certains cas, les modèles GPT uniquement textuels ont capturé la plausibilité du monde mieux que les modèles de vision et de langage. Pour résumer. VALS est un benchmark qui utilise la lentille des constructions linguistiques pour aider la communauté à améliorer les modèles de vision et de langage en testant rigoureusement leurs capacités d'ancrage visuel. Nos expériences montrent que les modèles de vision et de langage identifient bien les objets nommés dans leur présence dans les images, comme le montre l'élément existence, mais ont du mal à ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques. Nous aimerions vraiment encourager la communauté à utiliser VALS pour mesurer les progrès vers l'ancrage linguistique avec les modèles de vision et de langage. Et encore plus, VALS pourrait être utilisé comme une évaluation indirecte des ensembles de données, car les modèles pourraient être évalués avant et après l'entraînement ou le réglage fin pour voir si un ensemble de données aide les modèles à s'améliorer sur l'un des aspects testés par VALS. Si vous êtes intéressé, consultez les données VALS sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Kamizawa de l'Université de Tokyo. Je vais présenter un article intitulé RNSUN, un ensemble de données à grande échelle pour la génération automatique de notes de version via la synthèse de journaux de commit. Je vais expliquer dans l'ordre suivant. Tout d'abord, je vais présenter la génération automatique de notes de version sur laquelle nous travaillons dans cette recherche. La note de version est un document technique qui résume les changements distribués avec chaque version d'un produit logiciel. L'image montre les notes de version pour la version 2.6.4 de la bibliothèque Vue.js. Les notes de version jouent un rôle important dans le développement open source, mais elles prennent beaucoup de temps à préparer manuellement. Il serait donc très utile de pouvoir générer automatiquement des notes de version de haute qualité. Je vais me référer à deux recherches antérieures sur la génération automatique de notes de version. La première est un système appelé Arena, publié en 2014. Il adopte une approche basée sur des règles, par exemple, en utilisant l'extracteur de changements pour extraire les différences principales, les changements de bibliothèque et les changements de documents à partir des différences entre les versions, et en les combinant finalement. La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit, qui doit être lié à Jira, le système de suivi des problèmes, et ne peut être appliqué qu'aux projets qui utilisent Jira. En d'autres termes, il ne peut pas être utilisé pour de nombreux projets sur GitHub la deuxième Glyphe, récemment annoncée en 2020. Elle est disponible sur Internet et peut être stockée via PIP. Ce système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'un des cinq niveaux, tels que les fonctionnalités ou les corrections de bogues, pour chaque message de commit d'entrée. L'image est un exemple d'utilisation qui retourne une étiquette corrective ou de correction de bogues. Les données d'entraînement de Glyph sont assez petites, environ 5000, et dans les expériences décrites ci-dessous, la performance du modèle de classification de texte n'est pas élevée. Je présente deux recherches connexes, mais il y a des problèmes de capacité d'application limitée et de ressources de données rares. Notre article résout ces deux problèmes et génère automatiquement des nœuds de version de haute qualité. Pour le problème de capacité d'application limitée, nous proposons une méthode de résumé de classificateur de haute qualité utilisant uniquement le message de commit comme entrée. Cette méthode proposée peut être utilisée pour tous les dépôts en anglais. Pour le deuxième problème de ressources de données rares, nous avons construit un ensemble de données R et sum composé d'environ 82 000 morceaux de données en collectant des données à partir de dépôts GitHub publics en utilisant RR et certains ensembles de données. Notre ensemble de données Rnsum composé d'environ 82 000 morceaux de données en collectant des données à partir de dépôts GitHub publics en utilisant l'API GitHub. Ensuite, je décris notre ensemble de données. Voici un exemple de données. Le côté gauche est le message de commit et le côté droit est les notes de version. Les notes de version sont étiquetées comme des améliorations de visages, etc. Nous avons défini le message de commit et le côté droit sont les nœuds de version. Les nœuds de version sont étiquetés comme des améliorations, des corrections de bogues, etc. Nous avons mis en place une tâche qui prend les messages de commit en entrée et produit les nœuds de version étiquetés en sortie. Cela peut être considéré comme une tâche de résumé. Nous avons prédéfini quatre étiquettes, fonctionnalités, améliorations, corrections de bogues, dépréciations, suppressions et changements de rupture. Elles ont été définies sur la base de recherches antérieures et d'autres facteurs. Les nœuds du bas à droite sont extraits des nœuds du bas à gauche. À ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été définies à l'avance. Mais les étiquettes ne sont pas toujours cohérentes avec chaque dépôt. Par exemple, l'étiquette améliorations inclut des améliorations, des améliorations, des optimisations, etc. Nous avons préparé une liste de vocabulaire de nos étiquettes d'étude pour chacune de ces variations notationnelles, l'avons utilisée pour détecter la classe de notes de version, et corrigé le texte de la liste qui suit comme la phrase de notes de version pour la classe qui doit identifier la version de version précédente 2.5 à 18 et l'obtenir diff. C'est un peu fastidieux et ce n'est pas suffisant de simplement obtenir une liste de versions et de regarder avant et après. Nous avons créé une règle de correspondance heuristique pour obtenir les versions précédentes et suivantes. Analyse de l'ensemble de données. À la fin, 7 200 dépôts et 82 000 morceaux de données ont été collectés. De plus, le nombre moyen de jetons de nœuds de version est de 63, ce qui est assez élevé pour une tâche de résumé. De plus, le nombre de jetons uniques est assez important, à 8 830 000. Cela est dû au grand nombre de noms de classes et de méthodes uniques trouvés dans le dépôt. Ensuite, j'expliquerai la méthode proposée. Le nombre de classes et de méthodes uniques trouvés dans le dépôt. Ensuite, j'expliquerai la méthode proposée. Le modèle de résumé extractif puis abstractif par classe consiste en deux réseaux neuronaux utilise un classificateur pour classer chaque message de commit en cinq classes de notes de version. Nous choisissons implémente, corrections de bogues, dépréciations plus et autres. Les messages de commit classés comme autres sont écartés. Ensuite, CEAS applique le générateur aux quatre documents d'étiquettes indépendamment et génère des notes de version pour chaque classe. Dans cette tâche, les correspondances directes entre les messages de commit et les notes de version ne sont pas connues. Par conséquent, pour entraîner le classificateur, nous attribuons des pseudo-étiquettes à chaque message de commit d'entrée en utilisant les dix premiers caractères de chaque message de commit. Nous modélisons la synthèse abstractive par classe par notre approche par deux méthodes différentes. Le premier modèle, que nous appelons cssingle, consiste en un réseau de type ensemble-vers-ensemble et génère un long texte de nœud unique, en donnant une concaténation des messages de commit d'entrée. Le texte de sortie des réseaux, chacun correspondant à l'une des classes les moins connues. D'accord, laissez-moi expliquer l'expérience. Cinq méthodes ont été comparées, CAS, CASSingle, CASMatch, PlusSelling et étude précédente, GRIF. En ce qui concerne l'aberration, dans certains cas, CSMatch, Blustering et Previous Study Glyph. En ce qui concerne l'évaluation, dans certains cas, les notes de version les moins sont produites en plusieurs phrases. Puisqu'il est difficile de calculer le nombre de phrases telles qu'elles, elles sont combinées avec des espaces et traitées comme une longue phrase. Le bleu est pénalisé lorsque le système produit une phrase courte. Cette pénalité entraîne une valeur bleue plus faible dans les résultats de l'expérience décrits ensuite. Enfin, nous calculons également la spécificité car rouge et bleu ne peuvent pas être calculés si les nœuds de version sont vides. Une spécificité élevée signifie que le modèle produit correctement un texte vide dans les cas où les nœuds de version supposent vide. Voici les résultats. Étant donné que l'ensemble de données contient des adresses e-mail, des valeurs de hachage, etc., nous avons également évalué l'ensemble de données nettoyé, qui les exclut. CES et CAS ont obtenu des scores rouge L plus de 10 points supérieurs aux valeurs de référence. En particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les valeurs de référence a bondi à plus de 20 points. Ces résultats indiquent que CAS et CAS sont significativement efficaces. CAS a obtenu un score root-A meilleur que CAS, suggérant que la combinaison d'un classificateur et d'un générateur est efficace dans l'entraînement du classificateur en utilisant deux doubles. Une couverture élevée de CAS peut être atteinte, probablement parce que le classificateur peut se concentrer sur la sélection des messages de commit pertinents pour chaque classe. CAS match a tendance à produire un log L plus élevé que CAS single, suggérant qu'il est également efficace de développer indépendamment des modèles de résumé abstraits différents pour chaque classe de notes de version. Voici l'analyse des erreurs. Les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines. Dans la figure à droite, la phrase de référence a trois ou quatre phrases, tandis que CAS n'en a qu'une. La raison de cette réticence du modèle est que, dans les données d'entraînement, seulement 33% des phrases sont présentes dans l'étiquette fonctionnalités et 40% dans l'étiquette améliorations. De plus, les méthodes CES ne peuvent pas générer de notes de version précises sans informations supplémentaires. L'exemple en haut à droite est un exemple d'un engagement très désordonné que les méthodes CS ne peuvent pas générer de notes de version précises sans informations supplémentaires. L'exemple en haut à droite est un exemple d'un message de commit très désordonné, et la phrase complète ne peut pas être générée sans référence au pro-demande ou au problème correspondant. L'exemple ci-dessous montre que les deux messages de commit dans l'entrée sont liés et devraient être combinés en une seule phrase, mais cela échoue à le faire. Enfin, une conclusion. Nous avons construit un nouvel ensemble de données pour la génération automatique de listes de notes. Nous avons également formé la tâche d'entrer des messages de commit et de les résumer de manière à ce qu'elle soit applicable à tous les projets écrits en anglais. Notre expérience montre que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les valeurs de référence. Veuillez consulter notre onglet uniquement Décent. Merci."}
