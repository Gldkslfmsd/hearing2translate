{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lindemann und heute werde ich Ihnen eine kurze Einführung in unseren Artikel über kompositorische Verallgemeinerung ohne Bäume mit Hilfe von Multi-Set-Tagging und latenten Permutationen geben. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositorische Verallgemeinerung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und nicht gesehene Kompositionen von Phrasen zu handhaben, die während des Trainings einzeln gesehen wurden. Im Kontext der semantischen Parsing könnte die Prüfung auf kompositorische Verallgemeinerung so aussehen. Wie üblich haben wir einen Trainingsdatensatz von Äußerungen, in diesem Fall das Mädchen schlief und Maria wusste, dass das Mädchen schlief. Diese Äußerungen werden mit logischen Formen gepaart, die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standard-Maschinelles-Lernen-Evaluation stammt der Testdatensatz nicht aus der gleichen Verteilung, sondern enthält strukturell nicht gesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings tiefere Rekursion gesehen und wird auf ein Beispiel mit tieferer Rekursion getestet. Naive sequenzielle Modelle kämpfen mit dieser Art der Verallgemeinerung außerhalb der Verteilung und produzieren oft Ausgaben, die vom Input losgelöst sind. Insbesondere versagen sie oft darin, die systematischen Korrespondenzen zwischen Input und Output zu reproduzieren, wie sie in dem Beispiel farblich kodiert sind. Eine beliebte Methode, um dies anzugehen, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erhalten werden. Dies kann ein komplizierter und manchmal rechenintensiver Prozess sein. Typischerweise beinhaltet dies erhebliche formalismspezifische Vorverarbeitung der logischen Formen, zum Beispiel, um Variablen-Symbole zu handhaben. Das Erhalten von Bäumen kann auch spezialisierte Grammatikinduktionsprozeduren beinhalten. In diesem Artikel verwenden wir keine Bäume und führen ein neuronales sequenz-zu-sequenz-Modell ein, das direkt die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs modelliert. Zum ersten Mal zeigen wir starke Verallgemeinerung auf tiefere Rekursion, ohne auf Bäume angewiesen zu sein. Unser Ansatz prognostiziert die Ausgabe aus dem Input in zwei Schritten. Zuerst taggen wir jedes Input-Token mit einem unbestellten Multiset von Token, die im Output erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Deshalb verwenden wir in dem zweiten Schritt ein anderes Modell, um eine Permutation zu prognostizieren, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode ein, um eine Permutation zu prognostizieren, die keine harten Einschränkungen auf die möglichen Permutationen legt. Das macht unseren Ansatz ziemlich flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell ungefähr so. Wir gehen von links nach rechts über den Output und bestimmen, welchen Multiset-Token wir in jeder Position setzen sollen. Für die erste Output-Position wählen wir einfach einen aus, wie rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um den zweiten Token im Output zu bestimmen. Auf ähnliche Weise springen wir zu einem anderen Multiset-Token. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen auf dem COGS-Benchmark. Unser Modell übertrifft die anderen mit großem Abstand bei der Verallgemeinerung auf tiefere Rekursion. Einige andere Arten der strukturellen Verallgemeinerung bleiben jedoch sehr herausfordernd. In unserem Artikel lösen wir einige interessante technische Herausforderungen. Zunächst ist die Ausrichtung zwischen Input und Output in den Trainingsdaten nicht gegeben. Folglich wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Zusätzlich gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die sprachlich korrekte ist latent. Wir gehen dies an, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der Permutation mit der höchsten Punktzahl NP-schwer ist. Das liegt daran, dass dies mit dem Reiseverkäuferproblem verwandt ist. Wir approximieren dies mit einer GPU-freundlichen, kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zurückzuprojizieren und die sprachlich plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, werfen Sie bitte einen Blick auf unseren Artikel oder kommen Sie zu unserem Poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra, und heute werde ich über unsere Arbeit „Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models“ sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esen Dermusch und Dan Jorofsky durchgeführt. In den letzten Jahren haben viele die Verbreitung sozialer Vorurteile und Stereotype in großen Sprachmodellen, oder LLMs, dokumentiert. Diese Maßnahmen haben jedoch verschiedene Einschränkungen. Sie stützen sich in der Regel auf handgefertigte Datensätze, deren Erstellung sehr zeitaufwendig ist, und sie messen in der Regel nur sehr spezifische Stereotype, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte verallgemeinern können, oder sie erfassen einfach nur sehr allgemeine, breite Assoziationen wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten in diesem Bereich die Intersektionalität nicht, die die Vorstellung ist, dass vielfältige soziale Identitäten Vorurteile verstärken und einzigartige Schadensquellen sein können. Um diese Einschränkungen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren, instruktionsabgestimmten LLMs sehr gut darin sind, auf Anweisungen in Eingaben zu reagieren. Wir können also das Modell bitten, eine Persona zu generieren, eine Darstellung einer imaginierten Person, indem wir eine Eingabe wie „Stell dir vor, du bist eine asiatische Frau, beschreibe dich selbst“ verwenden. Und wir können sofort sehen, dass dies sehr gut auf jede Demografie übertragbar ist, denn wir können einfach jeden Identitätsmarker in diese Eingabe einfügen. Hier sind einige Beispielgenerierungen von GPT-4. Sofort sehen wir, dass die Ausgaben, obwohl sie nicht offenkundig negativ oder toxisch im traditionellen Sinne dieser Wörter sind, einige interessante Muster aufweisen. Die asiatische Frau wird als unauffällig dargestellt. Die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und mit Verweisen auf eine faszinierende Region bezeichnet. Und beide Frauen of Color-Personas machen Verweise auf die Abstammung, während die weiße Mann-Persona nichts dergleichen hat. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste besteht darin, diese Personas zu generieren. Unsere Eingaben zur Generierung dieser Personas wurden von einer Studie inspiriert, bei der sie diese Eingaben an menschliche Probanden weitergaben und feststellten, dass sie dadurch auch rassistische Stereotype aufdecken konnten. Und auch dies ermöglicht einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten. Der zweite Teil sind markierte Wörter, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten unterscheiden, die ich gleich näher erläutern werde. Der Vorteil dabei ist, dass wir sehr spezifische Stereotype und Muster erhalten, ohne uns auf ein bestimmtes Lexikon verlassen zu müssen. Die Methode der markierten Wörter stützt sich auf das soziolinguistische Konzept der Markierung, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, sprachlich markiert ist. Zum Beispiel wird das Wort „Mann“ oder das Wort „Krieger“ normalerweise mit Männern assoziiert. Wenn Menschen also von einer Kriegerin sprechen, die eine Frau ist, werden sie normalerweise tatsächlich einen männlichen Krieger spezifizieren und den Begriff mit „Frau“ markieren. Im weiteren Sinne sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen normalerweise markiert sind. In unserer Methode bestimmen wir zunächst, was die unmarkierten und markierten Gruppen sind. Und dann vergleichen wir die Personas mit der Methode der kämpfenden Wörter, die im Grunde gewichtete Log-Odds-Verhältnisse verwendet, um die unmarkierten und markierten Gruppen zu unterscheiden. Und dann vergleichen wir die Personas mit der Methode der kämpfenden Wörter, die im Grunde gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas von schwarzen Frauen kämpfende Wörter durchführen und die Log-Odds-Verhältnisse gegen weiße Personas und Mann-Personas vergleichen, da dies zwei entsprechende unmarkierte Gruppen sind. Nun zu einigen Ergebnissen. Zuerst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotype enthalten als die von Menschen geschriebenen. Wenn wir jedoch die Verteilung der Wörter im Lexikon betrachten, finden wir sehr unterschiedliche Dinge. Während die generierten Personas viel höhere Raten der Lexikonwörter aufweisen, haben die von Menschen geschriebenen eine viel breitere Verteilung von Wörtern, während die Stereotypwörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter „groß“ und „athletisch“ sind. Also wirklich nur die positiven oder zumindest nicht negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht. Anstatt das zu tun, werden wir uns den Ergebnissen unserer Methode der markierten Wörter zuwenden, um zu zeigen, wie diese scheinbar positiven Wörter Stereotype und essentialisierende Erzählungen fördern. In unserer Analyse enthüllen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zuerst gehören zu den markierten Gruppen die Top-Wörter wie Kultur, Tradition, Stolz und Exotik. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders als die weiße Norm. Dies trägt zu einem langen Erbe der Diskriminierung und der Anderenmachung für diese Gruppen bei. Darüber hinaus gibt es viele gängige Tropen, die in diesen Wörtern widergespiegelt werden, insbesondere für Frauen of Color. Zum Beispiel beinhalten die Wörter, die lateinamerikanische Frauen beschreiben, Dinge wie lebendig und kurvig, die mit einem Tropismus des Tropentums verbunden sind. Für asiatische Frauen sind die Wörter Dinge wie zierlich, zart und seidig, was mit einer langen Geschichte der Hypersexualisierung asiatischer Frauen, die als sehr sanftmütig und unterwürfig angesehen werden, usw., verbunden ist. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie stark und widerstandsfähig sind. Dies verbindet sich mit einem Archetyp, den die Menschen den Archetyp der starken schwarzen Frau genannt haben. Und obwohl es auf den ersten Blick positiv klingt, gibt es Arbeiten, die zeigen, dass diese Art von Archetyp tatsächlich sehr schädlich ist, weil er diese Demografien unter Druck setzt, widerstandsfähig und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, setzt es Druck auf diese Menschen, sie zu überwinden, was zu sehr negativen gesundheitlichen Ergebnissen für diese Menschen führt, unter anderem. Im weiteren Sinne stellen wir fest, dass die Wörter für jede markierte Gruppe im Grunde nur sehr essentialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotype und essentialisierende Erzählungen ansprechen. Wir sollten auch eine intersektionale Linse verwenden, um Vorurteile und Schäden zu untersuchen. Und schließlich sollte es wirklich mehr Transparenz über Methoden zur Minderung von Vorurteilen geben, denn zum Beispiel, wie diese positiven Stereotype, wissen wir nicht, ob es, weil es eine Art seltsame, übermäßig exzessive Wertanpassung gibt. wie diese positiven Stereotype, wissen wir nicht, ob es, weil es eine Art seltsame übermäßig exzessive Wertanpassung gibt, oder vielleicht andere Anti-Stereotyp-Methoden, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz. Vielen Dank fürs Zuhören. Habt es auf der ACL gut."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABCeval erzählen, einen neuen dimensionellen Ansatz zur Bewertung von konversationeller KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Gino Choi an der Emory University durchgeführt und in Zusammenarbeit mit Amazon Alexa AI. Nehmen wir also an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es sich im Vergleich zum aktuellen Stand der Technik schlägt. Die gängige Praxis besteht darin, menschliche Evaluatoren zu verwenden, indem man menschliche Richter bittet, auszuwählen, welche von zwei Gesprächen besser ist, oder Gespräche anhand einer Likert-Skala zu bewerten. Diese Ansätze eignen sich gut, um ganzheitliche Bewertungen der allgemeinen Dialogqualität zu liefern, aber Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feiner aufgelösten Ebene zu verstehen. Ein Ansatz besteht darin, menschliche Richter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie die Relevanz der Modellantworten, unter Verwendung bestehender vergleichender oder Likert-Skalenmethoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem er explizit annotiert, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt oder nicht, wie das Reagieren mit irrelevanten Informationen oder das Widersprechen sich selbst. Wir nennen diesen Ansatz das Annotieren von Verhaltensweisen im Chat oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um die Verhaltensweisen von Chatmodellen umfassend abzudecken, von denen in der jüngsten Literatur angenommen wird, dass sie die Chatqualität beeinflussen. APC-Eval: Ein Chatmodell ignoriert seinen Partner oder sagt etwas Irrelevantes, widerspricht sich selbst oder seinem Partner, halluziniert falsche Fakten oder verletzt das gesunden Menschenverstandeswissen und wenn das Modell es schafft oder nicht schafft, Empathie zu zeigen. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier modernste Chatmodelle ausgewählt und sie anhand von 100 Mensch-Bot-Gesprächen pro Modell mit ABC-Eval bewertet. Zum Vergleich haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf der Gesprächsebene, Likert-Bewertungen auf der Dialogstufe und Dialogstufenpaarisierte Vergleiche. Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen entlang mehrerer Dimensionen ist. Aus unseren Analysen der Bewertungsergebnisse haben wir festgestellt, dass ABC-Eval-Verhaltensbezeichnungen insgesamt zuverlässiger sind als Bezeichnungen, die mit bestehenden Methoden gesammelt wurden, gemessen an der Inter-Annotator-Übereinstimmung bei 100 doppelt annotierten Gesprächen. Darüber hinaus sind ABC-Eval-Bezeichnungen in Bezug auf die Gesamtgesprächsqualität vorhersehbarer als Metriken, die von bestehenden Methoden produziert werden, wie diese einfache lineare Regressionsanalyse zeigt. Zum Beispiel können Sie sehen, wie die Messung des Anteils der Gesprächsbeiträge mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenzscores nur 4 % oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chatqualität erfasst, indem wir eine schrittweise lineare Regression verwendet haben. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt und wie die meisten von ihnen, wenn man sie nacheinander entfernt, eine beträchtliche Menge an Informationen über die Qualität verlieren. Andererseits erklären die Kombination aller Likert-Metriken auf der Gesprächsebene weit weniger von der Qualität und weniger dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden erreichen können. Sie können in den Ergebnissen unseres Experiments sehen, dass noch mehrere Herausforderungen bestehen und genau quantifiziert wurden. Zum Beispiel haben die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen den gesunden Menschenverstand. Sie produzieren in etwa 15 % der Antworten irrelevante Informationen und widersprechen sich selbst oder ihrem Partner in etwa 10 % der Zeit. Mit dem raschen Verbesserungstempo in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, abnehmen. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken für den Vergleich von Modellen zu verfolgen. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich als bedeutender Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf zu sehen, wie sich die konversationelle KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Vasudha und ich bin Doktorandin im Fachbereich Informatik an der Stony Brook University. Ich möchte unsere Arbeit vorstellen, die für die ACL 2023 als Langpapier angenommen wurde: Transfer Learning for Dissonance Detection, das sich mit der Herausforderung seltener Klassen befasst. Zunächst definieren wir kognitive Dissonanz und warum sie ein wichtiges Problem in der Sprachforschung ist. Einfach ausgedrückt, ist kognitive Dissonanz zwei Meinungen oder Handlungen, die inkonsistent sind. Wie in diesem Beispiel, wo eine Person sagt: „Ich weiß, dass Zigaretten mich töten können“, und dann fährt fort: „Ich habe nach dem Meeting ein paar Zigaretten geraucht.“ Diese Meinung und Handlung sind inkonsistent und befinden sich in Dissonanz. Wenn ich dann erwähne, dass ich glaube, ich könnte meinen Job ohne sie nicht behalten, rechtfertigt das das zweite Auftreten und sie haben eine Konsonanzbeziehung. Obwohl Dissonanz ein sehr häufiges Phänomen ist, das wir bei täglichen Entscheidungen erleben, sind sie in der Sprache unter anderen Arten von Diskursbeziehungen sehr selten zu finden. Warum ist das wichtig? Die Erforschung kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten unter Menschen zu verstehen, Trends in Glauben, Werten und Einstellungen in der Bevölkerung zu verfolgen. Hohe kognitive Dissonanz steht auch im Zusammenhang mit Angststörungen und kann dazu beitragen, die psychische Gesundheit von Menschen besser zu verstehen. Die Erforschung von Dissonanz, die in der Sprache ausgedrückt wird, kann auch hilfreich sein, um Extremismus und Polarisierung von gefährdeten Gruppen zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um persönliche kognitive Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse besser zu verstehen. Um das Ziel zu erreichen, eine Ressource für kognitive Dissonanz zu schaffen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir verwendeten einen dissonanzorientierten Ansatz, wie in dem hier gezeigten Flussdiagramm. Tweets wurden mit einem PDTV-Parser analysiert, und Paare von Diskursenheiten wurden gemäß den Richtlinien, die in unserem Papier beschrieben sind, annotiert. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nachdem wir etwa 1000 Beispiele von Diskursenheitspaaren gesammelt hatten, führten wir eine Schulung für einen anfänglichen Klassifikator durch, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Zu keiner Überraschung führte der Klassifikator nicht viel besser als der Zufall. Angesichts des geringen Auftretens von Dissonanz und der Abwesenheit eines solchen Datensatzes zuvor stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen von Transfer Learning und aktivem Lernen, um so zu annotieren, dass mehr Dissonanzbeispiele in weniger Annotierungsläufen gesammelt werden können, wodurch die Gesamtannotierungs-kosten gesenkt und die Dissonanz-Erkennung verbessert wird. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, starten wir den aktiven Lernprozess, indem wir Gewichte von eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben: Die dissonanzunabhängige Standpunktklassifizierung, eine Aufgabe, die bestimmt, ob zwei Debattenaussagen von verschiedenen Personen übereinstimmen oder nicht, unabhängig vom Thema, das hier als Debatte bezeichnet wird, und die binäre Klassifizierung von Expansions- und Vergleichsklassen von PDTB, da diese beiden eng mit dem Konzept von Konsonanten und Dissonanz verwandt sind, und wir sie hier CEE nennen. Wir stellen fest, dass durch das Übertragen die Null-Shot-Leistung auf dem annotierten Datensatz bereits viel besser als der Zufall ist, mit dem besten Wert von AUC.62. Darüber hinaus wird das Modell iterativ durch Trainingsrunden des aktiven Lernens und Annotationen aktualisiert. Kumulativ sammelt alle Daten, die bisher aus aktiven Annotationen gesammelt wurden, während iterativ das Modell durch Training auf dem neuesten Datensatz aktualisiert. Bei den verschiedenen Strategien stellten wir fest, dass kumulativ gleich oder besser als iterativ war. Als Nächstes, um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Wahrscheinlichkeit für seltene Klassen, um hauptsächlich Beispiele auszuwählen, die nach dem aktuellen Modell in jeder Runde des aktiven Lernens mit hoher Wahrscheinlichkeit dissonant sind. Wir vergleichen dies mit den anderen State-of-the-Art-Strategien, obwohl der Unterschied klein ist. Beachten Sie, dass die Leistung für zufällige Beispiele deutlich niedriger ist. In weiteren Runden des aktiven Lernens mit den beiden besten Strategien verbesserten wir die AUC für die Distanzklassifizierung auf 0,75, was die beste Leistung ist, die wir bisher für diese Aufgabe haben. Wir überprüfen auch die Machbarkeit jeder Strategie für die Qualität der Annotation und die Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz hat und am besten für seltene Klassen geeignet ist. Die Annotatoren finden die Beispiele jedoch auch schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für den Erwerb seltener Klassen und den kalten Start von AL mit angemessen gestalteten Transfer-Learning-Aufgaben ist und erheblich helfen kann. Wir stellen auch fest, dass die iterative Aktualisierung nützlich für das Transfer Learning aus einem anderen Bereich ist, während in-domain aktive Annotationen von der kumulativen Aktualisierung profitieren. Dies sind die Links zu unserem Code, unserem Datensatz und unserem Papier. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, ich bin Akshata und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit, The Kipma Steps, bei der wir die Integration von Wissen aus mehreren Quellen bewerten. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Nationale Sprachverständnismodelle stützen sich auf eine Vielzahl von Wissensquellen, wie das in ihren Parametern enthaltene Wissen, das normalerweise durch Vorabtraining erworben wird, und das in den Eingaben zur Inferenzzeit angegebene Wissen. Jüngere Arbeiten bei Aufgaben wie dem Beantworten von Fragen zeigen, dass Modelle das im Vorfeld getrainte Zeitwissen nutzen können, um die Aufgabe zu lösen. Aber das natürliche Sprachverständnis erfordert oft Wissen, das auch zur Inferenzzeit bereitgestellt wird. Zum Beispiel im Satz: John sah den neu gewählten Präsidenten im Fernsehen. Die im Vorfeld trainierten Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese ereignisspezifische Entität John ist oder wer der neue Präsident ist, da sich der Präsident seit dem Vorabtraining geändert haben könnte. Daher erfordern erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl das im Vorfeld trainierte als auch das zur Inferenzzeit verfügbare Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Coreferenzauflösung auf, die darauf ausgelegt ist, die Fähigkeit zu prüfen, auf Wissen zuzugreifen, das in verschiedenen Quellen verfügbar ist. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablieren Coreferenzauflösungsmodelle. Hier ist ein Beispiel aus unserem Datensatz. Servin ist ein Richter. Kia ist Bäckerin. Servin und Kia trafen sich in einem Park. Nach einem langen Arbeitstag, an dem er Fälle in einem Gericht entschieden hatte, war er froh, sich entspannen zu können. Die Aufgabe hier besteht darin, die korrekte Entität zu identifizieren, auf die das Pronomen er sich bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens, entitätspezifisches Wissen wie zum Beispiel, dass Servin ein Richter ist. Und zweitens, Hintergrundwissen wie zum Beispiel, dass Richter Fälle in Gerichten entscheiden. Im Allgemeinen wird Hintergrundwissen während des Vorabtrainings großer Sprachmodelle erlernt, während entitätspezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von KITMOS definiert. Erstens haben wir die typische Einstellung, background-pretrain, bei der angenommen wird, dass Hintergrundwissen zur Vorabtrainingszeit verfügbar ist. Zweitens gibt es die background both-Einstellung, bei der Hintergrundwissen sowohl zur Vorabtrainingszeit als auch zur Inferenzzeit verfügbar ist. Zuletzt die background inference-Einstellung, bei der beide Wissensarten nur zur Inferenzzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das Hintergrundwissen, das notwendig ist, um eine Aufgabe zu lösen, nicht Teil der vorabtrainierten Daten der Modelle ist, zum Beispiel, weil sich seit der Zeit des Vorabtrainings neue Berufe entwickelt haben. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten und wahren Quellen steuern. In der background pre-trained-Einstellung nehmen wir an, dass das Hintergrundwissen, dass Politiker gewählte Sitze in der Regierung anstreben, in den vorabtrainierten Parametern enthalten ist. Und im 3-Zoll-Zeit-Kontext stellen wir das entitätspezifische Wissen zur Verfügung, dass Chichester ein Politiker ist. In der background both-Einstellung stellen wir zusätzlich nicht nur entitätspezifisches, sondern auch Hintergrundwissen über Politiker im Inferenzzeit-Kontext zur Verfügung. In der background inference-Einstellung stellen wir den fiktiven Beruf Meritur anstelle von Politiker zur Verfügung, da Meritur unwahrscheinlich in den vorabtrainierten Parametern enthalten ist. Wir bewerten den Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreferenzauflösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der best performenden Modelle auf der schwierigsten Variante der background pre-trained-Einstellung. Ohne spezifisches Training auf KITMOS erzielen beide Modelle keine guten Ergebnisse. Wenn sie jedoch auf KITMOS trainiert werden, erzielen sowohl C2F als auch BFQF signifikant bessere Ergebnisse als die zufällige Auswahl. Dies deutet darauf hin, dass Modelle, wenn sie auf allgemeinen Coreferenzauflösungsdatensätzen trainiert werden, lernen, oberflächliche Hinweise auszunutzen, die beim Testen auf KITMOS, wo solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die best performenden Modelle das zur Inferenzzeit bereitgestellte Hintergrundwissen nicht zuverlässig integrieren können. Um die Hauptergebnisse unserer Arbeit zusammenzufassen. Viele Coreferenzauflösungsmodelle scheinen nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu argumentieren, ohne spezifisches Training. Mit spezifischem Training können jedoch einige Modelle Wissen aus mehreren Quellen erfolgreich integrieren. Selbst die best performenden Modelle scheinen jedoch Schwierigkeiten zu haben, zuverlässig integriertes Hintergrundwissen zu präsentieren, das nur zur Inferenzzeit bereitgestellt wird. Wenn Sie mehr Details interessiert sind, lesen Sie bitte unsere Arbeit und sehen Sie sich den Datensatz in Code auf GitHub an. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sara Papi von der Universität Trient und der Fondazione Bruno Kessler, und ich werde kurz das Paper „Attention as a Guide for Simultaneous Speech Translation“ vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprachübersetzung? Simultane Sprachübersetzung oder SimulST ist der Prozess der Übersetzung von gesprochener Sprache in einen Text in einer anderen Sprache in Echtzeit, was eine länderübergreifende Kommunikation ermöglicht. Und was sind die Probleme der aktuellen SimulST-Modelle? Üblicherweise werden spezifische Architekturen trainiert, was die Einführung zusätzlicher Module zur Optimierung bedeutet. Lange und komplizierte Trainingsprozeduren, zum Beispiel mit verschiedenen Optimierungszielen, und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen, zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von 1 Sekunde und eines anderen mit 2 Sekunden Latenz und so weiter. Was ist also unsere Lösung? Erstens, bereits bestehende Offline-SD-Modelle ohne Neu-Training oder die Annahme einer spezifischen Architektur für SimulSD zu verwenden. Verwenden Sie nur ein Modell für jedes Latenzregime und behandeln Sie die Latenz durch spezifische Parameter. Und nutzen Sie das Wissen, das das Modell bereits durch den Aufmerksamkeitsmechanismus zwischen Audioeingang und textlichem Ausgabeergebnis erworben hat, das ist der Kreuzaufmerksamkeitsmechanismus. Und Sie können ein Beispiel rechts sehen. Unsere Lösung besteht darin, EDAT oder Encoder-Decoder-Aufmerksamkeit vorzuschlagen, und es ist eine Strategie, bei der wir entscheiden, ob wir eine Teilausgabe basierend darauf, wohin die Aufmerksamkeit zeigt, emittieren oder nicht. Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht konzentriert ist, das heißt, ihre Summe unter einem bestimmten Schwellenwert alpha liegt, auf weniger als lambda Sprachrahmen, was bedeutet, dass die empfangenen Informationen ausreichend stabil sind. Wenn wir zum Beispiel einen Sprachchunk erhalten, der „Ich werde darüber sprechen“ enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt und wir uns die Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen verweisen, während das letzte Wort auf die zuletzt empfangenen Sprachrahmen verweist, d.h. lambda Sprachrahmen. Das bedeutet, dass die ersten beiden Wörter emittiert werden, während wir, da die Summe der Kreuzaufmerksamkeit über einem bestimmten Schwellenwert alpha liegt, das letzte Wort nicht emittieren und auf einen weiteren Sprachchunk warten. Wenn wir weitermachen und einen weiteren Sprachchunk erhalten und unser Modell andere drei Wörter vorhersagt und wir uns die Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass kein Wort auf die letzten lambda Sprachrahmen verweist. Das bedeutet, dass diese drei Wörter emittiert werden. Wenn wir uns die Hauptergebnisse eines Punktes ansehen, plotten wir die Ergebnisse der simultanen Sprachübersetzung auf Diagrammen, in denen wir auf der einen Seite Blau haben, das die Übersetzungsqualität und die durchschnittliche Verzögerung misst, das ist das Latenzmaß, und wir berücksichtigen auch die rechenbewusste durchschnittliche Verzögerung, die die Rechenzeiten des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten also, dass unsere Kurven in diesem Diagramm so hoch wie möglich sind, aber auch, dass sie nach links verschoben sind. Und wir vergleichen mit geeigneten Strategien, die auch auf Offline-Modellen angewendet werden, das sind die Wet-Key-Strategie und die lokale Übereinstimmung. Und wir vergleichen auch mit der Stand-der-Technik-Architektur, die speziell für die simultane Sprachübersetzung zugeschnitten ist. Dies sind alle Ergebnisse der Strategie der simultanen Sprachübersetzung auf Deutsch, und wir sehen, dass ADDOUT alle Strategien, die auf Offline-Modellen angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächliche verstrichene Zeit oder die rechenbewusste Zeit betrachten, ADAT die schnellste Strategie ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Paper, und wir haben auch den Code und die Modelle Open Source veröffentlicht. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Paper. Wir haben auch den Code und die Modelle Open Source veröffentlicht und die simultane Ausgabe, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Zhu Heng. Heute werde ich unseren Artikel vorstellen, der den Titel trägt: „Funktionieren Kernel 2003-Named-Entity-Tagger noch gut im Jahr 2023?“ Beginnen wir. Unser Artikel untersuchte das Problem der Generalisierung unter Verwendung der Named-Entity-Recognition-Aufgabe oder der NER-Aufgabe. Wir stellten fest, dass Modelle seit fast 20 Jahren Kano 2003 zur Entwicklung von NER verwenden, was natürlich mehrere Probleme aufwirft. Erstens, können diese Modelle auf moderne Daten generalisieren? Und was ist bei der Entwicklung neuer Tagger für eine gute Generalisierung erforderlich? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht dann den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, entwickelten wir den Kano++-Datensatz. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 gesammelt und dann mit den gleichen Cono2003-Anmerkungsrichtlinien annotiert haben. Anschließend haben wir über 20 Modelle auf Cono2003 verfeinert. Wir haben sie sowohl auf Con in F1 bewertet, um die Generalisierung jedes Modells zu beurteilen. Was also ist für eine gute Generalisierung erforderlich? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptbestandteile gibt, die erforderlich sind. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass in der Regel größere Modelle zu einer besseren Generalisierung führen. Und nicht zuletzt wissen wir alle, dass die Anzahl der Feinanpassungsexemplare direkt die Leistung einer nachgeschalteten Aufgabe beeinflusst. Auch hier haben wir festgestellt, dass mehr Feinanpassungsexemplare tatsächlich auch zu einer besseren Generalisierung führen. Zu unserer nächsten Frage, was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist das adaptive Overfitting, also ein Overfitting, das durch die wiederholte Verwendung desselben Testdatensatzes verursacht wird und sich normalerweise als abnehmende Renditen auf dem neuen Testdatensatz manifestiert. Die zweite Hypothese ist der zeitliche Drift, also die Leistungsdegradation, die durch die zunehmende zeitliche Lücke zwischen den Trainings- und Testdaten verursacht wird. Für das adaptive Overfitting haben wir gesehen, dass die rote beste Anpassungslinie im Diagramm auf der rechten Seite einen Gradienten hat, der größer als eins ist. Das bedeutet, dass jede Einheit der Verbesserung, die wir auf CONO 2003 vorgenommen haben, sich in mehr als eine Einheit Verbesserung auf Kano++ umsetzt, was bedeutet, dass es keine abnehmenden Renditen gibt. Und das zeigt uns, dass in diesem Fall kein adaptives Overfitting beobachtet wird. Wie sieht es dann mit dem zeitlichen Drift aus? Für den zeitlichen Drift. Und das bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinanpassungsexemplare benötigen würden. Und diese gehen Hand in Hand. Wir können nicht nur einen Bestandteil haben, sondern alle anderen müssen ebenfalls vorhanden sein. Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch den zeitlichen Drift verursacht wird, und etwas überraschenderweise nicht durch adaptives Overfitting, obwohl Carnal 2003 seit über 20 Jahren verwendet wird. Wenn wir also zur Frage zurückkehren, die wir im Titel unseres Artikels aufgeworfen haben: Funktionieren Kernel 2003-Tagger noch im Jahr 2023? Und wir haben festgestellt, dass die Antwort tatsächlich ein klares Ja ist. Wir hoffen, dass unser Artikel zu mehr Forschung darüber anregt, wie die Generalisierung der Modelle verbessert werden kann. Und zuletzt, stellen Sie bitte sicher, dass Sie unseren Artikel und unseren Datensatz überprüfen. Wenn Sie Fragen haben, können Sie mich gerne kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, willkommen bei unserer Präsentation von d.plain, einem neuen Korpus für die deutsche Textvereinfachung auf Dokument- und Satzebene. Mein Name ist Regina Stodden und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Textvereinfachung definieren. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um das Textverständnis für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Lese-Problemen oder Nicht-Muttersprachlern. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, zum Beispiel von Dokumenten oder Sätzen. In dem hier gezeigten Beispiel können Sie ein parallel ausgerichtetes Satzenpaar eines komplexen deutschen Satzes und dessen Übersetzung in einfache Sprache sehen. Zur Vereinfachung des Satzes sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie lexikale Substitution, Klaustration, Klaustrierungsaufrüstung oder Einfügen von Wörtern. Wir schlagen nun unseren neuen Korpus dplane vor. Denn in den letzten Jahren gab es einige Probleme mit bestehenden Korpora. So sind diese Korpora hier beispielsweise zu klein, um ein Taxonifikationsmodell darauf zu trainieren. Die anderen drei in den letzten Jahren vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können. Daher schlagen wir unseren neuen Korpus dplane vor, der in zwei Teilkorpora, dplane-apa und dplane-web, aufgeteilt ist. dplane-apa basiert auf Nutztexten. In der einfachen APA haben wir 483 Dokumente alle manuell ausgerichtet. Es resultieren ungefähr 30.000-13.000 parallele Satzenpaare. Für DeepLaneWeb umfasst dieser Korpus verschiedene Bereiche und wir richten auch alle diese 750 Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden aus. und wir richten auch alle diese 750 Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden aus. Insgesamt ergeben wir 30.450 Satzenpaare. Wir haben unsere Satzenpaare etwas genauer analysiert, zum Beispiel nach Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als zum Beispiel die Nachrichtentexte oder die Texte für Sprachlerner auf allen Ebenen, was zum Beispiel lexikale Vereinfachung, strukturelle Vereinfachung oder das allgemeine Niveau der Vereinfachung betrifft. Darüber hinaus können Sie sehen, dass unser De Plplane-Korpus eine hohe Priorität verschiedener Vereinfachungstransformationen hat. So haben wir im d.plane API-Korpus viel mehr Umdrehungen und Wortzuweisungen als im d.plane Web-Korpus. Andererseits haben wir im Web-Korpus viel mehr Umschreibungen. Lassen Sie uns nun sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar und jetzt werde ich über die Anwendungsfälle für unseren Datensatz D-plane sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext von maschinellen Übersetzungen, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und wir Ausrichtungen von Sätzen in zwei parallelen Dokumenten mit der gleichen Sprache extrahieren möchten, die den gleichen Inhalt haben, aber auf unterschiedlichen Komplexitätsebenen sind. Und jetzt, da wir unseren Datensatz dplane haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten, und wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und die Codes, um unsere Experimente durchzuführen, im Papier veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für Texte zur deutschen Textvereinfachung die Methode des math align ist und Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten auszuführen, im Papier. Der zweite Anwendungsfall, den wir in unserem Papier gezeigt haben, ist ein Fall der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu produzieren. Wir haben zwei verschiedene Modelle fein abgestimmt. Wir haben long-impart fein abgestimmt, um Satzebene-Vereinfachungen zu produzieren. Sie können auch alle Checkpoints finden und sich in die Details der Scores und der Bewertungsmetriken unserer Experimente im Papier einlesen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Scores als die Basisscores produzieren oder erzielen könnte, und wir haben diese Ergebnisse als Benchmark, als Basis-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vorgeschlagen. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich bin hier, um unsere Arbeit vorzustellen: „Distilling Script Knowledge from Large Language Models for Constraint Language Planning“. Im Alltag planen Menschen oft ihre Handlungen, indem sie schrittweise Anweisungen in Form von garantierten Skripten befolgen. Bisherige Arbeiten konzentrieren sich jedoch hauptsächlich auf die abstrakte Planung. Ein guter Planer sollte Skripte für die ersten Planungsziele schreiben. Ein abstraktes Ziel kann von verschiedenen realen spezifischen Zielen mit vielschichtigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und den Einschränkungen treu sind. In diesem Papier bewerten und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung. Da es keinen Datensatz spezifischer Ziele gibt, der unsere Studie unterstützt, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit vielschichtigen Einschränkungen für die Datenerfassung mit menschlicher Schleife unter Verwendung von InstructGPT. Wir nehmen 100 spezifische Ziele und bewerten die Skripte, die aus großen Sprachmodellen generiert wurden. Diese Tabelle berichtet über die Gesamtnacuranz der Ergebnisse. Wir stellen fest, dass alle großen Sprachmodelle unbefriedigende Ergebnisse bei der Planung für spezifische Ziele erzielen. Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum Lernmodelle scheitern. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann. Wir haben uns eingehender mit den Feinheiten der Einschränkungen beschäftigt, abhängig von der Arbeitsweise. Die Wärmekarte in der Abbildung zeigt, dass die Planungsplattform von instruktiven TPDs erheblich für Mädchen unterschiedlicher Kategorien variiert. Frühere Studien haben gezeigt, dass die Ausgabequalität von Light-Logging-Modellen bei hoher Varianz sinkt, was zu schlechter Leistung führt. Daher übernehmen wir die Idee des übergenerierten Z-Filters, um die Generierungsqualität zu verbessern. Zuerst zeigen wir Einschränkungen mit Beispielen für Instruct GPT und erhalten spezifische Ziele basierend auf den abstrakten Saatgutzielen. Dann übergeneriert Instruct GPT Schlüsselskripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die durchführbaren Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen die Kosinusähnlichkeit und Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Zusätzlich vergeben wir das Skript, das die Schlüsselwörter der Zielbeschränkung enthält. Wir behalten das Skript nur bei, wenn das Ziel den höchsten Wert im Zielsatz erzielt. Mit unserer Methode kann InstructZBT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit erheblich, sowohl in Bezug auf die semantische Vollständigkeit als auch die Treue zu den Einschränkungen. Da große Sprachmodelle kostspielig zu implementieren sind, ist es wesentlich, die Sprachplanungsfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt zu diesem Zweck. Frühere Studien ermöglichen jedoch keine Planung für spezifische Ziele, und die manuelle Annotation von Datensätzen ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um eingeschränkte Sprachplanungssätze von großen Sprachmodellen zu destillieren. Wir wenden unsere Methode zur Erstellung eines Datensatzes der eingeschränkten Sprachplanung an, benannt als CodeScript. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testsites zu gewährleisten, haben wir Cloud-basierte Arbeiter gebeten, überarbeitete falsche Proben zu finden. Diese Abbildung zeigt die Einschränkungsverteilung von CodeScript. Wir stellen fest, dass CodeScript in den generierten spezifischen Zielen hohe Beifallsexpressionsraten zeigt. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung behandeln. Wir stellen fest, dass TFI von Antune bei der Kostenrate die Quadratwurzel von 0 generieren kann. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung behandeln. Wir haben festgestellt, dass T-Dateifunktionen auf CodeScript Skripte von höherer Qualität generieren können als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle größere Modelle unterstützen können, wenn sie richtig auf geeigneten Datensätzen trainiert werden. Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert. Wir bewerten die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle und entwickeln eine Methode zur Übergenerierung von Filtern zur Forschung über die Sprachplanung. Vielen Dank für Ihre Zeit. Weitere Details zu CodeScript finden Sie in unserem Papier."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Yanis Lavrac und ich werde Ihnen unsere Arbeiten zu Dr. BERT vorstellen, einem robusten vorab trainierten Modell in Französisch für den biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Dann werden wir den Hauptbeitrag unseres Artikels vorstellen. Wir stellen das erste biomedizinische Modell in Französisch vor, das Dr. BERT heißt und auf Roberta basiert und auf NACHOS trainiert wurde, einem Datensatz von medizinischen Daten, die aus dem Web gesammelt wurden. Wir haben auch einen Vergleich von Modellen mit mehreren Vorentrainings-Einstellungen und Datenquellen vorgestellt. Dann präsentieren wir unsere Ergebnisse zu 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Und schließlich ziehen wir unsere Schlussfolgerungen aus den Experimenten und geben Ihnen weitere Details darüber, wie Sie auf die Modelle zugreifen können. biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Und schließlich ziehen wir unsere Schlussfolgerungen aus den Experimenten und geben Ihnen weitere Details darüber, wie Sie auf die Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 ist BERT einer der effektivsten Ansätze zur Lösung von Aufgaben der Verarbeitung natürlicher Sprache und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2Vct, FastText oder NWO einen enormen Leistungsgewinn. Seitdem wurde dieses Modell auf viele andere Sprachen adaptiert, wie in Französisch mit Camembert und in anderen Bereichen wie dem biomedizinischen mit permit-bert und bio-bert und im klinischen mit clinical-bert, aber meist in Englisch. Spezialisierte Modelle für andere Sprachen sind rar und basieren oft auf kontinuierlichem Vorentraining aufgrund des Mangels an domänenspezifischen Daten. Für Französisch gab es jedoch bisher keine Open-Source-Modelle für den biomedizinischen Bereich. Wir stellten uns also Fragen darüber, welche Datenquellen für eine breite Anwendung am geeignetsten sind und ob die aktuellen Daten eine gute Ersatz für klinische Daten sind. Um diese Frage zu beantworten, vergleichen wir Dr. BERT mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die von der nichtuniversitären Klinik stammen, die unser Haus ist. Schließlich fragen wir uns, wie viele Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren. Sind es 4 GB, 8 GB oder 4 GB RAM? Eine erste Version von Schubert, einem klinischen Modell, mit 4 GB Sätzen, die aus klinischen Notizen stammen. Und eine endgültige Version von Schubert, mit einer Mischung aus 4 GB Untermenge von NACHOS und 4 GB klinischen Notizen. Zusätzlich zu diesem Vergleich haben wir drei Modelle vorgestellt, die auf kontinuierlichem Vorentraining trainiert wurden, um die Auswirkungen der Vorentrainingsstrategie zu analysieren. Eines basiert auf dem Gewicht von Camembert und wurde auf einer 4-GB-Untermenge von NACHOS trainiert. Ein weiteres basiert ebenfalls auf Camembert, wurde aber mit diesen vier Gigabyte Permit-Belt durch Bio-Geburt und klinische Geburt trainiert. Die Bewertung hebt hervor, dass... 38 GB, Camembert Oscar 4 GB, Camembert CCnet 4 GB, Pumatbert, BioBERT und ClinicalBERT. Die Bewertung hebt hervor, dass Modelle am besten bei Aufgaben mit Daten derselben Art abschneiden, auf die das Modell trainiert wurde. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen. Wir beobachten auch, dass die Verwendung von mehr Daten zu einer besseren Leistung führt. Insgesamt scheint das Vorentraining von Grund auf eine höhere Leistung bei den meisten Aufgaben zu erzielen. Unsere Experimente zum kontinuierlichen Vorentraining mit dem Gewicht und dem Tokenizer von Permet-BERT, trainiert auf der 4-Gigabyte-Untermenge von NACHOS, zeigen jedoch vergleichbare Ergebnisse wie die mit Dr.BERT 4 GB von Grund auf erzielten, was nicht der Fall ist für das Modell, das auf Camembert-Gewichten und Tokenizer basiert, das unter Stabilitätsproblemen leidet. Abschließend bietet unser eigenes System eine bessere Leistung bei 9 der 11 Don't-Trim-Aufgaben und übertrifft global die Ergebnisse des generischen Modells. Zusammenfassend bietet unser eigenes System eine bessere Leistung bei 9 der 11 Don't-Trim-Aufgaben und übertrifft global die Ergebnisse des generischen Modells, hier Camembert. Wir beobachten auch, dass spezialisiertere Daten besser sind, aber nicht gut skalieren. Alle aus NACHOS erhaltenen vorab trainierten Modelle sind auf UGIMFACE frei verfügbar und alle Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation und wir freuen uns auf Gespräche bei der Poster-Session in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xiangbin, Doktorand an der Universität von Washington. Heute stelle ich unsere Arbeit von Vordar-Daten über Sprachmodelle bis hin zu nachgelagerten Aufgaben vor, wobei wir den Spuren politischer Voreingenommenheiten folgen, die zu unfairen NLP-Modellen führen. Sprachmodelle werden also auf groß angelegten Web-Crawling-Daten trainiert. Politische Nachrichtenmedien sind in ihren Vordar-Daten gut vertreten. Laut einer Umfrage des C4-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, The Guardian, die Huffington Post usw. in den Trainingsdaten für Sprachmodelle gut vertreten sind. Dies hat für Sprachmodell-Anwendungen sowohl Segen als auch Fluch bedeutet. Einerseits konnten sie aus verschiedenen Perspektiven lernen, was Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen von Natur aus sozial voreingenommen und könnten zu potenziellen Fairness-Problemen in nachgelagerten Aufgaben führen. Zu diesem Zweck schlagen wir vor, die Pipeline der Weitergabe politischer Voreingenommenheiten von Vordar-Daten über Sprachmodelle bis hin zu nachgelagerten Aufgaben zu untersuchen, indem wir speziell die folgenden Fragen stellen. Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen, und welche Rolle spielt die zugehörige Datenmenge bei solchen politischen Voreingenommenheiten? Zweitens, wie schneiden Sprachmodelle mit unterschiedlicher politischer Ausrichtung tatsächlich bei nachgelagerten Aufgaben ab, und könnte dies zu Fairness-Problemen in NLP-Anwendungen führen? Insbesondere schlagen wir vor, Sprachmodelle mit verschiedenen Prompt-Formaten unter Verwendung der politischen Fragebögen, wie dem politischen Kompass-Test, anzuregen. Dies stellt sicher, dass wir die automatische Bewertung gut in der politikwissenschaftlichen Literatur verankert durchführen können. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle tatsächlich unterschiedliche politische Ausrichtungen haben. Sie besetzen alle vier Quadranten auf dem politischen Kompass. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist, und GPT-Theorien im Allgemeinen liberaler sind. Sie besetzen alle vier Quadranten auf dem politischen Kompass. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist, und GPT-Theorien sind im Allgemeinen sozialliberaler als die BERT-Theorie und ihre Varianten. Zweitens zielen wir darauf ab, zu untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir führen daher ein kontrolliertes Experiment durch, indem wir Sprachmodell-Checkpoints auf sechs verschiedenen parteiischen Korpora weiter vortrainieren, die in Nachrichten und soziale Medien weiter unterteilt in ihre politische Ausrichtung unterteilt sind. Indem wir Sprachmodelle auf solchen parteiischen Korpora weiter vortrainieren, können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel, für Roberta, die weiter verfeinert und auf dem linksgerichteten Reddit-Korpus weiter trainiert wurde, können wir eine erhebliche liberale Verschiebung in Bezug auf ihre politischen Voreingenommenheiten sehen. Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung, die in unserer modernen Gesellschaft vorherrscht, aufnehmen können. Wir teilen daher die Vordar-Korpora in vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten auf und vortrainieren Sprachmodelle separat auf den beiden verschiedenen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung hatten, die nach 2017 weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können. Zuletzt, aber nicht zuletzt, bewerten wir Sprachmodelle mit unterschiedlicher politischer Ausrichtung bei der Erkennung von Hassrede und Falschnachrichten für NLP-Anwendungen, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben könnten. Wir sehen, dass wenn wir die Leistung pro Kategorie betrachten, das heißt, wenn wir die Leistung in verschiedene Demografien oder politische Bedeutungen von Nachrichtenmedien aufteilen, wir ein Muster sehen können, das zum Beispiel für die Erkennung von Hassrede zeigt, dass linksgerichtete Sprachmodelle besser darin sind, Hassrede zu erkennen, die sich gegen sozial benachteiligte Gruppen richtet, jedoch schlechter darin sind, Hassrede zu erkennen, die sich gegen mächtigere Gruppen in unserer Gesellschaft richtet. Und umgekehrt sind rechtsgerichtete Sprachmodelle besser darin, Hassrede zu erkennen, die sich gegen Weiße und Männer richtet, jedoch schlechter darin, Hassrede zu erkennen, die sich gegen Schwarze, LGBTQ+ und andere Minderheitengruppen richtet. Ähnliche Trends finden sich auch bei der Erkennung von Falschnachrichten, wo wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Desinformation von ihrer entgegengesetzten politischen Ausrichtung zu erkennen und umgekehrt. Wir zeigen weiter viele qualitative Beispiele, um zu sehen, dass Sprachmodelle mit unterschiedlicher politischer Ausrichtung unterschiedliche Vorhersagen zu Hassrede- und Desinformationsbeispielen basierend auf ihrer sozialen Kategorie treffen. Im Anhang gibt es eine Reihe von weiteren Beispielen, um dies weiter hervorzuheben. Dies deutet darauf hin, dass es ein sehr drängendes Fairness-Problem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen gibt. Zum Beispiel, wenn rechtsgerichtete Sprachmodelle auf Hassrede oder Desinformation oder was auch immer weiter verfeinert und auf einer beliebten Social-Media-Plattform eingesetzt werden, würde dies bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten. auf Hassrede oder Desinformation oder was auch immer, und auf einer beliebten Social-Media-Plattform einsetzen, würde dies bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten, und die Hassrede, die sich gegen Minderheitengruppen richtet, könnte ohne Kontrolle wüten. Dies hat uns alarmiert, die Fairness-Probleme anzuerkennen und anzugehen, die durch die politischen Ausrichtungen von Sprachmodellen entstehen. Also eine kleine Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen Scylla und Charybdis. Wenn wir also die politischen Meinungen in den Trainingsdaten von Sprachmodellen nicht sanieren, wird die Voreingenommenheit von den Vordar-Daten über Sprachmodelle bis hin zu nachgelagerten Aufgaben weitergegeben, was letztendlich zu Fairness-Problemen führt. Wenn wir jedoch versuchen, die politischen Meinungen irgendwie zu sanieren, riskieren wir auch Zensur oder Ausschluss, und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten von Sprachmodellen beibehalten werden sollte. Es ist also irgendwie das elektrische Trolley-Problem. Okay, toll. Ich denke, das ist so ziemlich alles, was ich für heute habe. Vielen Dank für Ihre Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, ich bin Kostav Sinha und ich freue mich, Sie zu unserem Vortrag über unseren ACL 2023-Artikel „Language Model Acceptability Judgements Are Not Always Robust to Context“ zu begrüßen. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy und Adina Williams. In dieser Arbeit nehmen wir das Minimalpaar-Paradigma erneut unter die Lupe. Das Minimalpaar-Paradigma bewertet im Wesentlichen Sprachmodelle anhand von Akzeptanzurteilen, die auch Grammatikalität einschließen können, wie bei „Blimp“, Syntax, „Gym“ oder Akzeptanz in Bezug auf Stereotype, wie bei „Krauss“-Paaren. Und in diesem Minimalpaar-Paradigma besteht die typische Methode zur Bewertung von Sprachmodellen darin, dass man ein akzeptables oder ein grammatikalisches Satzbeispiel zeigt und dann ein inakzeptables oder ein ungrammatisches Satzbeispiel. Und dann gibt das Modell im Wesentlichen der akzeptablen Aussage eine höhere Wahrscheinlichkeit. Die aktuelle MPP-Pipeline ermöglicht es uns im Grunde nicht, die Akzeptanz von Modellen für längere Sätze zu bewerten. Heutzutage entwickeln große Sprachmodelle immer längere Kontextfenster. Also nehmen wir die Datensätze selbst erneut unter die Lupe und erstellen dann Sätze, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier ein typisches Paar der Grammatikalität aus dem BLIMP-Datensatz aus dem Adjunct Island-Fall ausgewählt. Und was wir tun, ist, dass wir längere Sequenzen erstellen, die akzeptabel sind und die gleiche grammatikalische Struktur aufweisen, indem wir grammatikalische Sätze aus Adjunct Island extrahieren und dann als Präfix sowohl an die akzeptable Abfrage als auch an die inakzeptable Abfrage anhängen. So können wir dasselbe tun, indem wir inakzeptable Sätze aus dem gleichen Match auswählen, und das könnte auch verwendet werden, um die Akzeptanz des Modells zu testen. Und wir können dasselbe tun, indem wir Sätze aus einem anderen Unterdatensatz oder einem anderen Datensatz auswählen. Das nennen wir das Mismatch-Szenario. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber nicht aus dem gleichen Datensatz, mit dem wir evaluieren. Und wir können dasselbe für einen Akzeptanzfall tun. Schließlich können wir Sätze aus einem völlig unverbundenen Bereich auswählen, wie Wikipedia. Das wird uns sagen, ob die Akzeptanzurteile des Modells tatsächlich von einem Kontext beeinflusst werden, der völlig irrelevant für den Satz ist, den wir betrachten. Wie schneidet das Modell ab? Zuerst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Query-Paar sind. Und dort stellen wir fest, dass die MPP-Urteile größtenteils für beliebige Kontextlängen robust sind. Wir erhöhen die Kontextlänge bis auf 1024, um die OPT- und GPT-2-Modelle auszulasten. Und hier sehen wir in der orangefarbenen gestrichelten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert nun, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen oder erstellen wir Sätze aus akzeptablen und inakzeptablen Bereichen aus demselben BLIMP- oder Syntax-Gem-Datensatz. Und dort sehen wir, dass die MPP-Urteile entweder signifikant zunehmen oder abnehmen, wenn man entweder akzeptable oder inakzeptable Präfixe hinzufügt. Aber wenn wir die Struktur abgleichen, das heißt, wenn wir entweder akzeptable oder inakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur abgleichen, das heißt, wenn wir die Sätze aus denselben Phänomenen in blame-person-text-gym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang des MPP-Urteils für das Modell, abhängig davon, ob das gewählte Präfix akzeptabel oder inakzeptabel ist. Nun, das ist ein sehr großer Effekt, der sich im Laufe der Kontextlänge verstärkt und das würde wahrscheinlich neuere Sprachmodelle beeinflussen, die ein großes Kontextfenster haben. Warum beeinflusst das Match-Präfix das Urteil des Sprachmodells so sehr? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingangssatz so zu gestalten, dass die relevante Struktur erhalten bleibt, aber Lärm zum Eingang hinzuzufügen. Und nachdem wir mehrere dieser Störungen durchgeführt haben, stellen wir fest, dass keiner dieser Lärmquellen das Modell tatsächlich dazu bringt, seinen Kurs in Bezug auf die MPP-Urteilstendenz zu ändern. Im Grunde stellen wir fest, dass die Modelle auf ähnliche Weise auf die gestörten Sätze reagieren. Das heißt, wenn wir die Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen. Und wenn wir die Sätze im inakzeptablen Bereich stören, sehen wir einen Rückgang der MPP-Urteile in ähnlicher Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die in den Sätzen gemeinsam sind. Und die MPP-Bewertung, wie wir sie derzeit mit kurzen und einzelnen Satzeingaben durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells über das gesamte Kontextfenster. Bitte lesen Sie unseren Artikel für weitere Details unserer Experimente. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawei, ein Doktorand an der Universität des Saarlandes in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit, „Weaker than you think“, einen kritischen Blick auf schwach überwachtes Lernen, vorstellen. Dies ist eine gemeinsame Arbeit mit Xiao Yusheng, Mario Smusbach und Gia Steffen sowie Dietrich Klackow. Ich möchte mit einer kurzen Einführung in schwache Überwachung und schwach überwachtes Lernen beginnen. Bei schwacher Überwachung kennzeichnen wir die Daten nicht manuell. Stattdessen kennzeichnen wir die Daten mit schwachen Kennzeichnungsquellen, wie einfachen heuristischen Regeln, Wissensbasen oder Crowdsourcing mit geringer Qualität, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwachen Annotationen viel billiger, aber sie sind auch verrauscht, was bedeutet, dass ein gewisser Anteil der Annotationen falsch ist. Wenn wir neuronale Netze direkt auf schwach annotierten Daten trainieren, neigen die neuronalen Netze dazu, den annotierten Rauschen zu merken und verallgemeinern nicht. Im wöchentlich überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solch einem Labelrauschen zu trainieren, sodass die trainierten Modelle weiterhin gut verallgemeinern. In jüngsten Arbeiten im WSL, wobei WSL für wöchentlich überwachtes Lernen steht, ist ein häufiger Anspruch, dass die Leute sagen, dass sie nur Modelle unter den wöchentlichen Labeldaten trainieren und hohe Leistung auf sauberen Testdatensätzen erzielen. Technisch gesehen ist dieser Anspruch nicht falsch, aber es gibt einen Haken, nämlich drei Forschungsfragen. Fragen. Erstens, benötigen wir saubere Validierungsdaten? Sollten wir nur die sauberen Proben für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit angesprochen, und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass neuere WSL-Methoden tatsächlich saubere Validierungsproben benötigen, um richtig zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, wenn es keine sauberen Validierungsproben gibt, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus verallgemeinern, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber annotierte Daten benötigen, um richtig zu funktionieren, und die Annotatkosten für die Beschaffung sauberer Validierungsproben sollten nicht übersehen werden. Unsere zweite Erkenntnis ist, dass die Erhöhung der Anzahl der sauberen Validierungsproben WSL-Ansätzen helfen wird, eine bessere Leistung zu erzielen, wie in der Abbildung links gezeigt. Typischerweise benötigen wir nur 20 Proben pro Klasse, um eine hohe Leistung zu erreichen. Aber das ist nicht das Ende der Geschichte, denn wenn wir uns entscheiden, auf saubere Proben zuzugreifen, dann wird das direkte Training auf ihnen sogar eine bessere Leistung erzielen. Die rote Abbildung zeigt den Leistungsunterschied zwischen Feinabstimmensansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie wir sehen können, wenn wir 10 Proben pro Klasse haben, beginnt das direkte Feinabstimmen, WSL-Ansätze zu übertreffen. Schließlich kann die in früheren WSL-Ansätzen behauptete Leistungsverbesserung leicht erreicht werden, indem die Fortsetzung des Feinabstimmens auf den sauberen Validierungsproben erlaubt wird. Wie wir aus den Abbildungen sehen können, unterlebt das Van Linden-Modell anfangs komplexeren WSL-Methoden wie Cosine. Wenn wir jedoch die Fortsetzung des Feinabstimmens auf den sauberen Proben erlauben, dann erreicht FTW die gleiche Leistung wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Festplattenspeicher erfordern. Zusammenfassend haben wir gezeigt, dass neuere WSL-Ansätze saubere, manuell annotierte Proben benötigen, damit sie richtig funktionieren. Ihr Leistungsgewinn und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Erstens, berichten Sie über die Kriterien für die Modellselektion. Zum Beispiel, berichten Sie, ob die Modellselektion mit sauberen Validierungsproben durchgeführt wurde. Zweitens, WSL-Ansätze sollten mit wenigen kurzen Lernbasismesswerten verglichen werden, wie Arbeiten an sauberen Proben. Drittens, kontinuierliches Feinabstimmen ist eine einfache, aber starke Basismessgröße, die in zukünftigen Arbeiten im WSL berücksichtigt werden sollte. Schließlich haben wir unseren Code open-source gemacht. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte zögern Sie nicht, ihn zu überprüfen. Vielen Dank und viel Spaß auf der Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist David Vilar und ich werde einen kurzen Überblick über den Artikel „Grunting Parm from Translation, assessing strategies and performance“ geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. Parm ist ein 540 Milliarden Parameter großes Sprachmodell, das im letzten Jahr, 2022, vorgestellt wurde. Es wurde auf einer großen Textsammlung mit 780 Milliarden Token trainiert. Zum Zeitpunkt der Veröffentlichung erreicht es den neuesten Stand der Technik bei hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zur Modellierung von Sprachmodellen für die maschinelle Übersetzung. Wir haben die Übersetzungsfähigkeit solcher Modelle unter Verwendung der bewährten Methoden der IMT-Community bewertet. Dazu verwenden wir die neuesten Testsätze, um eine Übereinstimmung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Wir vergleichen zwei modernste Systeme, also die leistungsstärksten Systeme oder die WMT-Bewertung. Wir verwenden modernste neuronale IM-Metriken und zeigen zusätzlich auch die Ergebnisse der fachkundigen menschlichen Bewertung. Schließlich geben wir einige Empfehlungen für Strategien zur Auswahl von Eingabeaufforderungen. Die Eingabeaufforderung hat einen großen Einfluss auf die Leistung von LLMs bei der Übersetzung. Wie wir in einem einfachen Experiment sehen können, bei dem wir eine einmalige Eingabeaufforderung verwenden und für jeden Satz zwei verschiedene Eingabeaufforderungen bereitstellen. Bei der Mehrheit der Sätze, 516 von 1000, beträgt der beobachtete Unterschied mehr als einen Unschärfepunkt. Und in extremen Fällen kann dies bis zu 40 Unschärfepunkte betragen. Es ist also wichtig, eine gute Strategie für die Eingabeaufforderung auszuwählen. In unseren Experimenten haben wir uns für eine Fünf-Schüsse-Eingabeaufforderung entschieden, bei der wir jeden Satz, den wir dem System zur Verfügung stellen, mit der Sprache kennzeichnen, in der er verfasst ist. In diesem Beispiel hier, bei dem wir eine Übersetzung vom Deutschen ins Englische durchführen, werden die deutschen Sätze mit einem deutschen Doppelpunkt und die englischen Übersetzungen mit einem englischen Doppelpunkt gekennzeichnet. Wir haben festgestellt, dass die tatsächliche Form der Eingabeaufforderung im Falle einer Mehrfach-Schüsse-Eingabeaufforderung keinen großen Einfluss hat. Sie ist entscheidend für Null- und Ein-Schüsse-Eingabeaufforderungen, und wenn wir, wie in unserem Fall, zu einer Fünf-Schüsse-Eingabeaufforderung übergehen, gibt es fast keinen Unterschied zur tatsächlichen Form der Eingabeaufforderung. Es sind die Beispiele, die das meiste Gewicht haben. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quelldatensatz. Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl der Eingabeaufforderungen aus den Trainingsdaten der WMT-Bewertungen oder den Dev-Daten. Die Dev-Daten sind viel kuratiert und von höherer Qualität als die Trainingsdaten, die lauter sind, und die Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Dev-Daten. Dennoch haben spezialisierte modernste Systeme einen erheblichen Vorteil gegenüber den Parm-Übersetzungen, aber Parm kommt ziemlich nah an ein kommerzielles System heran. In unserem Fall haben wir uns entschieden, mit Google Translate zu bewerten. Die Erkenntnisse, die wir aus der E-Mail-Analyse gewonnen haben, die wir mit dem MQM-Rahmenwerk durchgeführt haben, sind, dass die Fließfähigkeit von Parm mit den modernsten Systemen vergleichbar ist, aber der Hauptunterschied von der Genauigkeit kommt. Insbesondere sind die häufigsten Fehler Omissionsfehler. Es scheint also, dass Parm manchmal eine bessere klingende Übersetzung produziert, indem es Teile des Quelldatensatzes, die in der Übersetzung animiert sind, weglässt. Die Kategorie „stilistisch unpassend“ ist jedoch für Parm niedriger als für die modernsten Systeme, was ein zusätzliches Signal dafür ist, dass Parm wirklich fließende Ausgaben liefert, aber immer noch mit einigen Genauigkeitsproblemen. Und das war es für diesen wirklich kurzen Überblick. Für weitere Details kommen Sie bitte zur vollständigen Präsentation des Artikels. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Jingwei Yi von der Universität für Wissenschaft und Technologie in China. Es ist mir eine Freude, ein kurzes Werbevideo über Papier zu präsentieren: „Kopieren Sie mein Modell?“ Schutz des Urheberrechts großer Sprachmodelle für Einbettungen und Dienste über Backdoor-Wasserzeichen. Lassen Sie uns zunächst den Hintergrund zu Einbettungen und Diensten vorstellen. Derzeit sind große Sprachmodelle wie GPTT, LAMA, PALM außergewöhnlich in Bezug auf das Verständnis und die Generierung natürlicher Sprache. Einbettungen als Dienste sind einer der Dienste, die auf großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine auf GPT basierende Einbettungs-API an. Neuere Arbeiten haben jedoch gezeigt, dass Angreifer das Modell durch Lernen aus der Einbettung stehlen und ähnliche Dienste anbieten können. Daher ist es notwendig, das Urheberrecht von Einbettungen als Dienste zu schützen. Um das Urheberrecht von Einbettungen als Dienste zu schützen, besteht eine der Lösungen darin, ein Wasserzeichen in den Anbieterdienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode auf Einbettungen als Dienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer geheim genug sein oder der Angreifer sollte das Wasserzeichen leicht entfernen können. Schließlich muss das Wasserzeichen während des Modellauszugsprozesses auf die Dienste des Angreifers übertragbar sein. Bestehende Arbeiten können grob in vier Kategorien eingeteilt werden. Diese Methoden sind jedoch entweder nicht auf Einbettungen als Dienste anwendbar oder sie weisen eine mangelnde Übertragbarkeit auf. Details zu unserem EmbeddingMarker. EmbeddingMarker enthält zwei Hauptschritte, Wasserzeichen-Einbettung und Urheberrechtsprüfung. Bevor wir zu diesen Hauptschritten übergehen, wählen wir zunächst einen Trigger-Satz aus. Der Trigger-Satz ist eine Gruppe von Wörtern in einem mittleren Häufigkeitsintervall. Wir nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeichen-Einbettung definieren wir zunächst eine Ziel-Einbettung. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Die bereitgestellte Einbettung ist die gewichtete Summe der Ziel-Einbettung und der ursprünglichen Einbettung. Das Gewicht der Ziel-Einbettung ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, ist die bereitgestellte Einbettung genau gleich der Ziel-Einbettung. Die Urheberrechtsprüfung besteht darin, zu erkennen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Zuerst konstruieren wir einen Backdoor- und einen gutartigen Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zum Trigger-Satz gehören, während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Trigger-Satz gehören. Dann fordert der Anbieter Einbettungen vom Diebstahldienst mit dem Datensatz an. Die Kosinus- und L2-Ähnlichkeit zwischen der angeforderten Einbettung und der Ziel-Einbettung werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen gutartigen und Backdoor-Datensätzen, die als Delta-Kosinus und Delta-L2 definiert ist. In der Zwischenzeit wenden wir auch den KS-Test an und verwenden dessen p-Wert als dritte Metrik. Wir führen Experimente an vier Datensätzen durch, AGnews, Mind, SSD2 und Eraspam. Wir nehmen an, dass der Anbieter den Wikitext-Datensatz anwendet, um die Wortfrequenz zu zählen. Die Ergebnisse an den vier Datensätzen zeigen, dass unser eingebetteter Marker eine hervorragende Erkennungsleistung erzielen kann, während er eine große Nützlichkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Geheimhaltung der bereitgestellten Einbettung, indem wir die Einbettung von Sätzen an vier Datensätzen über PCA visualisieren. Die Legende der Abbildungen bedeutet die Anzahl der Trigger in jedem Satz. Wie in den Abbildungen gezeigt, ist es schwer, zwischen den Backdoor-Einbettungen und den normalen Einbettungen zu unterscheiden. Das war's, vielen Dank. Willkommen, mit uns zu diskutieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Ying, und mein Kollege Zhiyang und ich werden unsere Forschung zu Multi-Improvement, der Verbesserung des Multi-Model Serial Short Learning durch Instruktionstuning, vorstellen. Mit den Fortschritten bei großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zur Wiederverwendung von vortrainierten Sprachmodellen für verschiedene nachgelagerte Aufgaben auf eine parameter- und dateneffiziente Weise zu erforschen. Kürzlich haben viele Studien gezeigt, dass Instruktionstuning großen Sprachmodellen ermöglicht, auf nicht gesehene Aufgaben auf eine zero-shot-Weise zu reagieren, indem sie natürlichen Anweisungen folgen. Die meisten bisherigen Arbeiten zum Instruktionstuning konzentrieren sich jedoch auf die Verbesserung der zero-shot-Leistung bei rein sprachlichen Aufgaben, während Computer Vision und multimodalen Aufgaben ausgelassen wurden. Daher möchten wir in dieser Arbeit untersuchen, ob Instruktionstuning auf multimodalen vortrainierten Modellen tatsächlich die Generalisierung auf nicht gesehene multimodale Aufgaben verbessern kann. Zusätzlich haben wir bei unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Instruktionssätzen zwischen NLP und multimodalen Modellen entdeckt. Es gibt mehr als 1.600 rein sprachliche Instruktionssätze. Es gibt jedoch keinen großen öffentlich zugänglichen multimodalen Instruktionssatz. Daher motiviert uns dies, einen multimodalen Instruktionstuning-Datensatz aufzubauen. Hier präsentieren wir MultiInstruct, den ersten multimodalen Instruktionstuning-Benchmark-Datensatz, der aus 62 verschiedenen multimodalen Aufgaben besteht, die 10 Hauptkategorien abdecken. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen, und jede Aufgabe ist mit 5 von Experten geschriebenen Anweisungen ausgestattet. Um das Instruktionstuning auf multimodalen Modellen auf unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir OFA, ein einheitliches multimodales vortrainiertes Modell, als unser Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Token und die Koordinaten eines Bounding-Box. Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Instruct-Datensatz. Um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinheitlichen, folgen wir der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-zu-Sequenz-Format, in dem der Eingabestexte, Bilder, Anweisungen und Bounding-Boxen im gleichen Token-Raum dargestellt werden. Okay, jetzt werde ich über multimodales Instruktionstuning sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training, und wir nehmen 10.000 Instanzen pro Aufgabe. Für die Tests behalten wir die gesamte Gruppe des gesunden Menschenverstandes für das Testen und wählen zusätzlich fünf Aufgaben aus der VQA- und der Miscellaneous-Gruppe aus. Wir verwenden alle Instanzen im Test-Split für jede Aufgabe. Zusätzlich nehmen wir zufällig 20 Aufgaben aus dem Test-Split von natürlichen Anweisungen als nicht gesehene Aufgaben für NLP. Wir verwenden also ein vortrainiertes OFA-Großmodell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Während der Tests für jede Aufgabe führen wir insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment bewerten. Wir berichten die durchschnittliche und maximale Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente. Wenn die Aufgabe eine Multi-Model-Klassifizierungsaufgabe ist, berichten wir die Genauigkeit. Wenn es sich um eine Multi-Model-Generierungsaufgabe handelt, berichten wir die Wurzel L. Für eine RP-Aufgabe berichten wir Wurzel jl. Wir haben auch eine zusätzliche Bewertungsmetrik namens Sensitivität eingeführt, die die Fähigkeit des Modells misst, konsistent dieselben Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von der leichten Variation in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann das Instruktionstuning die Leistung von OFA bei Szenen-Multi-Model-Aufgaben erheblich verbessern. Auch das Transfer-Learning von natürlichen Anweisungsdatensätzen kann dem Instruktionstuning zugutekommen. Hier können wir sehen, dass die Leistung des Modells mit zunehmender Anzahl von Aufgaben steigt und gleichzeitig die Sensitivität sinkt. Wir haben auch ein Experiment durchgeführt, bei dem wir eine Anweisung gegenüber fünf Anweisungen verwendet haben. Wie wir sehen können, kann die Verwendung von mehr Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt die Wirkung verschiedener Feinabstimmungsstrategien auf die Sensitivität des Modells. Wie wir sehen können, kann das Modell durch Transfer-Learning von natürlichen Anweisungsdatensätzen eine viel bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell erreichen. Wir können auch sehen, dass das Transfer-Learning von natürlichen Anweisungsdatensätzen OFA helfen kann, eine viel bessere Leistung auf dem Nitro Instruct-Datensatz zu erzielen. Insgesamt schlagen wir den ersten groß angelegten Multi-Model-Instruktionstuning-Datensatz vor. Wir verbessern die zero-shot-Fähigkeit von OFA erheblich und erforschen verschiedene Transfer-Learning-Techniken und zeigen ihre Vorteile. Wir entwerfen eine neue Metrik namens Sensitivität. Noch etwas: Wir sammeln einen viel größeren multimodalen Instruktionstuning-Datensatz mit rund 150 zusätzlichen variantensprachlichen Aufgaben und werden sie bald veröffentlichen. Dies ist ein QR-Code für unsere Daten und Modelle. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Yusheng Zhang von der Penn State University. Heute werde ich unsere Arbeit vorstellen, mehrsprachige semantische Parsing in mehreren natürlichen Sprachen und Bedeutungspräsentationen. Semantische Parsing ist die Aufgabe, semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda-Kalkül zu erstellen. Mehrsprachige semantische Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungspräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir die Anfragen in mehreren natürlichen Sprachen mit neuronalen Modellen in SQL, Lambda oder FunQL usw. übersetzen. Bestehende mehrsprachige semantische Parsing-Modelle werden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Beispielsweise gibt es Lücken bei der Abdeckung bestimmter natürlicher Sprachen. Chinesisch fehlt und aufgrund der Abdeckung bestimmter Mini-Repräsentationen fehlt der Lambda-Kalkül oder sie werden nur auf bestimmten neuronalen Modellen bewertet. Beispielsweise gibt es nur ein einziges Modell zur Bewertung. Daher schlagen wir Exampler vor. Wir stellen einen einheitlichen Datensatz Exampler für mehrsprachige semantische Parsing in mehreren natürlichen Sprachen und Bedeutungspräsentationen zur Verfügung. Er enthält neun Datensätze in verschiedenen Bereichen, fünf semantische Parsing-Aufgaben, acht Bedeutungspräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unseren Benchmark besser bewerten zu können, berücksichtigen wir die sechs Einstellungen für Training und Bewertung. Die erste ist der Übersetzungstest. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein einsprachiges Modell, um eine Bewertung zu trainieren. Und zum Beispiel trainieren wir das englische Modell auf einer englischen Anfrage. Und während der Inferenz übersetzen wir die deutsche Anfrage mit der API ins Englische und verwenden dann das trainierte Modell, um die SQL vorherzusagen. Und wir testen auch ein einsprachiges Modell. In dieser Einstellung ist die Quellsprache dieselbe wie die Zielsprache. Zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch die einsprachige Few-Shot-Einstellung, indem wir einsprachige Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen ein mehrsprachiges Modell, das wir für alle Sprachen trainieren. Zum Beispiel bringen wir die deutschen, englischen und chinesischen Anfragen zusammen, um ein mehrsprachiges Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche Anfragen oder chinesische Anfragen usw. zu übersetzen. Und wir berücksichtigen auch den mehrsprachigen Zero-Shot- und Few-Shot-Transfer. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir es also auf einer englischen Anfrage oder der Kombination aus englischen und deutschen Few-Shot-Anfragen, um ein mehrsprachiges Modell zu trainieren und die SQL-Ausgabe vorherzusagen. Und wir finden auch viele interessante Ergebnisse. Was die Analyse einsprachiger Modelle betrifft, bewerten wir zwei Gruppen von Modellen, einschließlich Encoder PDR, was für mehrsprachige vorab trainierte Encoder mit zeigerbasierten Decodern steht, wie XLMR plus PDR und BERT plus PDR. Und wir bewerten auch Encoder-Decoder-Modelle, die mehrsprachige vorab trainierte Encoder-Decoder-Modelle sind, wie MBART und MT5. Wir fanden heraus, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielen. Und wir bewerten auf MT5 und XLMR plus PDR in der mehrsprachigen Einstellung. Wir fanden heraus, dass Encoder-Decoder oder Encoder-PDR durch Training in einer Mischung aus verschiedenen Sprachen verbessert werden können. Und wir fanden heraus, dass dies daran liegt, dass die meisten großen natürlichen Sprachen eine Leistungsgewinn erzielen können, mit der Ausnahme, dass die englische Leistung in sieben Datensätzen sinkt und nur in drei Datensätzen zunimmt. Ich denke, dies ist als Fluch der Mehrsprachigkeit bekannt. Wir vergleichen auch die mehrsprachige Leistungslücke. In dieser Abbildung ist die blaue Linie der mehrsprachige Few-Shot-Transfer. Die orange Linie ist der mehrsprachige Zero-Shot-Transfer, während die grüne Linie die einsprachige Einstellung ist. Wir fanden heraus, dass wir durch den Vergleich der grünen und der orangen Linie herausfanden, dass die Leistungslücke beim mehrsprachigen Transfer für die Zero-Shot-Einstellung signifikant ist. Und durch den Vergleich der blauen und der orangen Linie fanden wir heraus, dass die Übertragungslücke bei der Few-Shot-Einstellung schnell verkürzt wird. Wir finden auch einige andere interessante Erkenntnisse. Zum Beispiel übertrifft Encoder-Decoder frühere Arbeiten oder erzielte vergleichbare Ergebnisse. Die Darstellung auf der englischen natürlichen Sprache kann die Leistung von Few-Shot auf Zielnatürliche Sprachen erheblich steigern. Und wir fanden heraus, dass mehrsprachige Sprachmodelle wie CODIS und BLUE immer noch angemessen für mehrsprachige semantische Parsing-Aufgaben sind. Zusammenfassend haben wir Exampler erstellt, einen einheitlichen Benchmark für mehrsprachiges semantisches Parsing mit mehreren natürlichen Sprachen und Hauptdarstellungen. Wir führen eine umfassende Benchmark-Studie zu drei repräsentativen Arten von mehrsprachigen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse usw. Und wir laden Sie ein, unseren Artikel und unseren Code zu besuchen. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Szpilkowski und dieser Vortrag handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie vielleicht wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. So wird in den universellen Abhängigkeiten beispielsweise die Struktur der Koordination Lisa, Bart und Maggie so aufgebaut, dass der erste Konjunkte der Kopf der gesamten koordinierten Struktur ist, in diesem Fall also Lisa. Ein ähnlicher Ansatz wird in Igor Milchuks Bedeutungsteorienteorie angenommen, wo wiederum die gesamte koordinierte Struktur vom ersten Konjunkten angeführt wird. Diese beiden Ansätze sind also asymmetrisch, richtig? Sie heben einen der Konjunkten hervor. Nun gibt es auch symmetrische Ansätze zu koordinierten Strukturen wie der Prager Ansatz, der in den Prager Abhängigkeitsbaumkorpora angenommene, vom Konjunktiv angeführte Ansatz, bei dem koordinierte Strukturen vom Konjunktiv angeführt werden. So erhalten wir Abhängigkeiten von n zu allen Konjunkten. Und schließlich gibt es auch einen mehrköpfigen Ansatz, der beispielsweise in Dick Hudsons Wortgrammatik verwendet wird, wo sozusagen alle Konjunkten die Köpfe der koordinierten Struktur sind. Ein mehrköpfiger Ansatz, der beispielsweise in der Cutson-Wortgrammatik verwendet wird, wo sozusagen alle Konjunkten die Köpfe der koordinierten Struktur sind, sodass wir Abhängigkeiten vom Regierenden hier liebt zu allen Kondukten separat erhalten. Das sind Barton-Strukturen. Jetzt ist das Ziel dieses Papiers, zwei Argumente gegen die asymmetrischen Strukturen der Koordination wie diese beiden zu liefern. Okay, das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erklären werde. So bevorzugen in Englisch, wie Sie vielleicht wissen, direkte Objekte, in der Nähe des Verbs zu sein, während Adjunkte weiter entfernt sein können, richtig? Also ist March las es gestern in Ordnung, weil das direkte Objekt es in der Nähe des Verbs ist, während die direkten Objekte von March es vorziehen, in der Nähe des Verbs zu sein, während Adjunkte weiter entfernt sein können, richtig? Also ist March las es gestern in Ordnung, weil das direkte Objekt es in der Nähe des Verbs ist, während March las gestern es viel schlimmer ist, richtig? Denn hier gibt es zwischen dem Verb und dem direkten Objekt einen Adjunkten gestern. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und sehr lang ist, denn dann kann es an die Position nach dem Adjunkten verschoben werden. Das wird hier veranschaulicht. Also sind beide Sätze in Ordnung. March las dieses absolut faszinierende Buch über die BCS heute. Das ist in Ordnung. Anstatt von es haben wir hier eine lange NP. Aber es ist auch in Ordnung zu sagen March las gestern dieses absolut faszinierende Buch über Bienen. Die Begründung hier ist, dass dies möglich ist, weil dieser Satz zwar gegen das allgemeine grammatikalische Prinzip verstößt, dass direkte Objekte neben dem Verb stehen sollten, aber das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also die, die nicht konstant zwischen diesen beiden Strukturen sind. Also haben wir hier eine Abhängigkeit von rot zu dem Adjunkten von sieben Wörtern und von rot zu Buch von vier Wörtern. Zusammen also 11. Wenn Sie diese beiden Konstituenten verschieben, wird die Summe dieser beiden Abhängigkeiten sechs, richtig? Also statt 11, sechs, viel kürzer. Deshalb klingt das ganz in Ordnung. Es verstößt gegen ein Prinzip, erfüllt aber ein anderes. Okay, also haben wir verschiedene Statistiken über die Koordination aus der erweiterten Version des Penn Treebank extrahiert und sehen Sie das Papier, warum wir keine universellen Abhängigkeiten verwendet haben. Und diese Statistiken bestätigen die Beobachtung, die schon oft gemacht wurde, dass linke Konjunkten dazu neigen, kürzer zu sein. Wir haben keine universellen Abhängigkeiten verwendet und diese Statistiken bestätigen die Beobachtung, die schon oft gemacht wurde, dass linke Konjunkten dazu neigen, kürzer zu sein, also Salz und Pfeffer und nicht Pfeffer und Salz, gemessen in Silben, und auch die Beobachtung, die beiläufig gemacht wurde, dass diese Tendenz mit der Länge wächst, also wenn der Unterschied zwischen den Längen der beiden Konjunkten wächst, der kürzere Konjunkte es vorzieht, der Erste zu sein, stärker. Richtig. Also ist der Anteil des linken kurzen Konjunkten größer. Aber was neu an diesem Papier ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Regierende links ist oder abwesend ist, also der Regierende ist in diesem Beispiel ich sah Bart und Lisa, also ist der Regierende links, er ist abwesend im zweiten Beispiel Homer kam und schniefte, hier haben wir die Koordination von zwei Verben und es gibt keinen externen Regierenden, also in solchen Fällen, ähm, der linke Konjunkte zieht es vor, kürzer zu sein, je mehr, ähm, der Unterschied zwischen den beiden Konjunkten ist. Wenn jedoch der Regierende rechts ist, verschwindet dieser Effekt. Das zeigen wir, indem wir die Länge in Zeichen messen, das ist die erste Spalte, in Silben die mittlere Spalte und in Wörtern die rechte Spalte. Also werde ich mich auf die rechte konzentrieren. Was wir hier sehen, ist, dass wenn der Regierende links ist, die Tendenz des linken Konjunkten, kürzer zu sein, stetig mit dem absoluten Unterschied in Wörtern wächst. Und dasselbe wird beobachtet, wenn es keinen Regierenden gibt, wie bei der Koordination von Sätzen. Aber wenn der Regierende rechts ist, verschwindet diese Tendenz. Und wir zeigen im Papier, wie das einen Argument gegen die asymmetrischen Strukturen der Koordination wie diese beiden und für die symmetrischen Strukturen wie diese beiden liefert. Also sehen Sie das Papier für die vollständige Übereinstimmung und Argumente, sorry, und sprechen Sie mit uns über die Postersession. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Hallo! Mein Name ist Kaio Yin, und ich werde unsere Arbeit mit dem Titel „Wann erfordert Übersetzung Kontext? Eine datengesteuerte mehrsprachige Untersuchung“ vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, Andre F.D. Martins und Graham Newbigin durchgeführt. Viele Übersetzungen hängen also vom Kontext ab. Wie würden wir zum Beispiel das Wort „Mole“ in diesem Satz übersetzen? Nun, wenn der vorherige Satz lautete: „Die Lage könnte gefährlich werden, wenn die Minister das herausfinden“, dann bezieht sich „Mole“ auf einen Spion. Wenn der vorherige Satz jedoch lautete: „Könnte es etwas Ernstes sein, Doktor?“ dann bezieht sich „Mole“ auf einen Muttermal. Je nach Kontext ändert sich also die Bedeutung des Wortes, und damit auch seine Übersetzung. Es ist jedoch ziemlich schwierig zu bewerten, wie gut Modelle solche Fälle übersetzen können. Erstens hängt nur ein kleiner Teil der Übersetzungen vom Kontext ab, was bedeutet, dass Metriken auf Korpusniveau wie BLUE diese Übersetzungen nicht erfassen können. Einige Leute haben eine gezielte Bewertung von kontextabhängigen Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachsätze, da sie in der Regel auf Domänenkenntnissen und menschlicher Kuratierung basieren. In dieser Arbeit haben wir versucht, diese zwei Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut bewältigen Modelle diese Fälle? Um die erste Frage zu beantworten, haben wir begonnen, zu messen, wie stark ein Wort vom Kontext während der Übersetzung abhängt. In der vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungssysteme eingeführt. Dies geschieht, indem gemessen wird, wie viel Information der Kontext C über das Ziel Y unter Berücksichtigung der Quelle X liefert. Man kann sich CXMI als die Information vorstellen, die durch die Bereitstellung von Kontext an das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI zu punktweiser CXMI, die die Kontextnutzung auf Satzebene oder Wortniveau messen kann. Wir können Wörter mit hohem P6MI als solche betrachten, die für die Übersetzung Kontext benötigen. Nun analysieren wir Wörter mit hohem P6MI, um Muster zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse auf Transkripten von TED-Talks durch, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst betrachten wir Teile des Sprachgebrauchs, die hohe Mittelwerte von PCXMI aufweisen. Dies ermöglicht es uns, zum Beispiel Dualpronomen im Arabischen zu finden, die relativ hohe PCXMI aufweisen. Dies lässt sich erklären, weil Englisch keine Dualpronomen hat, sodass man Kontext benötigt, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Und ähnlich finden wir, dass bestimmte Sprachen ebenfalls Kontext erfordern, wenn wir die passende Verbform wählen möchten. Anschließend betrachten wir Vokabelartikel, die über alle ihre verschiedenen Vorkommen hinweg hohe PCSXMI aufweisen. Dies hilft uns, Fälle wie den hier zu identifizieren, wo man im Chinesischen Kontext benötigt, um Eigennamen zu übersetzen, um sicherzustellen, dass man innerhalb des Dokuments dieselbe Übersetzung verwendet. Und ähnlich finden wir, dass Kontext unterstützt, um in der richtigen Form zu übersetzen. Und schließlich betrachten wir verschiedene einzelne Token, die hohe PCXMI aufweisen. Dies ermöglicht es uns, Phänomene zu identifizieren, die vom Wort selbst nicht wirklich erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie zum Beispiel die Auflösung von Ellipse. Nun verwenden wir unsere Erkenntnisse aus unserer Analyse, um einen Benchmark für die Übersetzung auf Dokumentebene zu entwerfen. Für jedes der fünf von uns identifizierten Diskursphänomene erstellen wir Tagger, um Wörter, die zum Phänomen gehören, automatisch zu identifizieren, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zum Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich eure Arbeit präsentieren: „Anal Positionality, Characterizing Designed Biases of Datasets and Models“. Diese Arbeit wurde in Zusammenarbeit mit einigen Leuten von der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Sante, Ronan Labrasse, Katarina Aranica und Martin Sapp. \n\nBeginnen wir damit, uns vorzustellen, dass ihr für eine Zeitung arbeitet und Kommentare unter eurem Nachrichtenartikel sichten, um toxische Inhalte zu entfernen. Ihr könntet sich einer beliebten API wie der Perspective API für die Toxizitätserkennung zuwenden, und das funktioniert wirklich gut, wenn ihr Carl Jones seid, wo die Perspective API in der Lage ist, toxische Instanzen korrekt zu erkennen. Aber das ist nicht wirklich der Fall für Aditya Sharma, wo die Perspective API wirklich nicht so empfindlich gegenüber beleidigenden Begriffen ist, die in indischen Kontexten häufiger vorkommen. Das ist ein Beispiel für einen Designbias, bei dem wir systematische Leistungsunterschiede der Technologie zwischen Bevölkerungsgruppen beobachten. Designbiases wie der, den wir gerade zuvor gesehen haben, können aufgrund der Positionalität der NLP-Forscher und Modellentwickler auftreten. Positionalität ist einfach die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen. Das ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen, weit verbreitet ist. Und als Forscher kann die Positionalität den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, verändern kann. \n\nEine Frage, die sich die Leute stellen könnten, ist: Haben Datensätze und Modelle eine Positionalität? Und wir versuchen nicht zu sagen, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können somit bestimmte Positionalitäten über andere repräsentieren. Daher haben frühere Arbeiten einige anekdotische Beweise für die Positionalität vorgeschlagen, wie kulturelle Lücken und Modelle und Datensätze sowie theoretische Definitionen der Modellpositionalität. Diese Arbeiten betrachten jedoch nicht wirklich den Vergleich von Endbenutzern mit den Datensätzen und Modellen selbst. Die Untersuchung der Positionalität von Modellen und Datensätzen wird zunehmend wichtiger, da NLP-Aufgaben subjektiver und sozialer orientiert werden. Es ist herausfordernd, zu charakterisieren, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind. \n\nUm die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir tatsächlich die Annotationen mit echten Benutzern mit bestehenden Datensätzen und Modellen. Wir tun dies durch unser Framework NL-Positionalität. Unser Framework arbeitet in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit verschiedenen Annotatoren neu zu annotieren. Und wir entscheiden uns dafür, dies zu tun, anstatt die Demografie der ursprünglichen Datensatzannotatoren zu betrachten, weil normalerweise nur wenige Annotatoren jede Instanz annotieren und weil Demografie selten gesammelt und geteilt wird. Und so entscheiden wir uns dafür, Daten neu zu annotieren, um viele Annotatoren pro Instanz zu erhalten und um einen reichen Satz an demografischen Daten zu erhalten. Dann nehmen wir die Annotationen nach Demografie und vergleichen sie mit den Modellen und Datensätzen unter Verwendung eines Pearson-R-Korrelationsscores. \n\nUnser Framework unterscheidet sich also tatsächlich von der Literatur zur Diskrepanz zwischen Annotatoren, indem es Endbenutzer mit Modellen und Datensätzen, Vorhersagen und Etiketten vergleicht, im Gegensatz dazu, nur die Übereinstimmung oder die Verteilung der Annotatoren zu betrachten. Unser Framework wird größtenteils durch Lab in the Wild ermöglicht, eine Online-Crowdsourcing-Plattform von unserem HCI-Kollegen. Und Lab in the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können, im Vergleich zu Plattformen wie MTurk, die größtenteils Teilnehmer aus den USA oder Indien haben. Und weiter kann Lab in the Wild immer noch qualitativ hochwertige Daten erhalten. Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist soziale Akzeptabilität. \n\nDie Funktionsweise ist, dass die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz lesen und dann schreiben, wie sozial akzeptabel eine Situation ist. Anschließend können sie, um in der Studie engagiert zu bleiben, ihre Antworten mit einer KI und anderen vergleichen. Wir vergleichen dann diese Annotationen mit Social Chemistry, Delphi und GPT-4. Wir replizieren dann eine sehr ähnliche Einrichtung für den Test zur Toxizitäts- und Hassredeerkennung. Wir verglichen diese Annotationen mit Social Chemistry, Delphi und GPT-4. Wir replizierten dann eine sehr ähnliche Einrichtung für die Toxizitäts- und Hassredeerkennung, bei der sie eine Instanz aus DynaHate lesen und schreiben, ob sie denken, dass es sich um eine Hassrede handelt. Wir verglichen dann diese Annotationen mit DynaHate, Perspective API, Rewire API, Hate Roberta und GPT-4. \n\nUnsere Studie hat am Ende über 16.000 Annotationen von über tausend Annotatoren aus 87 Ländern gesammelt. Jetzt sind wir besser ausgestattet, um zu beantworten, mit wem NLP-Datensätze und -Modelle am meisten übereinstimmen. Wir stellen fest, dass es in NLP eine Positionalität gibt. Zum Beispiel stellen wir fest, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. Für die GPT-4-Analyse der sozialen Akzeptabilität stellen wir fest, dass sie am meisten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir stellen fest, dass DynaHate auch am meisten mit englischsprachigen Ländern übereinstimmt. Wir stellen auch eine stärkere Übereinstimmung mit Menschen mit Hochschulabschluss fest. Für GPT-4 in der Aufgabe zur sozialen Akzeptabilität stellen wir fest, dass es am meisten mit Menschen mit Hochschulabschluss oder Graduiertenausbildung übereinstimmt. Und wir stellen dasselbe für DynaHate fest, wo es am meisten mit Menschen mit Hochschulabschluss übereinstimmt. \n\nWenn Modelle und Datensätze jedoch auf spezifische Populationen abgestimmt sind, werden einige unweigerlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger auf nicht-binäre Menschen abgestimmt sind als auf Männer und Frauen. Wir stellen dies sowohl in der GPT-4-Aufgabe zur sozialen Akzeptabilität als auch in der DynaHEAT-Aufgabenanalyse fest. \n\nAngesichts der Tatsache, dass es in NLP eine Positionalität gibt, was können wir dagegen tun? Wir haben einige Empfehlungen dazu. Die erste ist, alle relevanten Designentscheidungen während des Forschungsprozesses zu dokumentieren. Und die andere ist, NLP-Forschung mit der Linse des Perspektivismus zu betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von vier spezifischen Gemeinschaften aufzubauen. Ein gutes Beispiel dafür ist die Masakane-Initiative. Ich meine, wir möchten betonen, dass inklusive NLP nicht nur darin besteht, alle Technologien für jeden funktionieren zu lassen. \n\nDas war unsere Präsentation. Aber wenn ihr mehr erfahren möchtet, schaut euch gerne unsere Dashboard für die aktuellsten Analysenergebnisse und unser Papier an. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich werde über unsere Arbeit an der Lösung indirekter Verweise zur Entitätssuche sprechen, bei der wir den Alt Entities-Korpus vorstellen. Mein Name ist Jawad Hosseini, und dies ist eine gemeinsame Arbeit mit Philip Radlinski, Sylvia Parity und Annie Lewis. Unser Ziel ist es, die Sprache der Nutzer zu verstehen, wenn sie eine Auswahl treffen möchten. Betrachten Sie diese alternative Frage: Meinten Sie „Easy on Me“ oder „I Got a Feeling“? Ein Nutzer möchte zwischen einem dieser beiden Lieder auswählen. Das Offensichtlichste ist, einen direkten Verweis zu verwenden, z. B. indem man den Namen des Liedes „Easy on Me“ oder seine Position „das erste“ nennt. Manchmal ist jedoch ein indirekter Verweis angemessener, um eine natürlichere Konversation zu führen. Dies kann passieren, wenn sich der Nutzer den Namen des Liedes nicht erinnern kann oder die Aussprachen zu ähnlich sind und schwer zu unterscheiden sind oder wenn der Nutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Unterschiede: z. B. „das neuere“ oder „das Lied, das nicht energiegeladen ist“. Dies ist ein wichtiges Problem in Konversationsystemen und auch für die Bewertung des Entitätsverständnisses von LLM. Uns ist kein öffentlicher Datensatz für diese Aufgabe bekannt, daher erstellen wir einen mit Crowd-Annotation. Unser Datensatz umfasst drei verschiedene Bereiche: Musik, Bücher und Rezepte. Unsere Methodik zur Datensammlung legt Wert auf Unformalisierung und verwendet ein Setup zur Vervollständigung von Cartoons. Der Cartoon hat drei Sprechblasen. Unser Datensatz umfasst drei verschiedene Bereiche: Musik, Bücher und Rezepte. Unsere Methodik zur Datensammlung legt Wert auf Unformalisierung und verwendet ein Setup zur Vervollständigung von Cartoons. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: „Erinnern Sie sich an das Lied, das wir gestern gehört haben?“ Damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: „Meinen Sie „Easy on Me“ oder „I Got a Feeling“?“, was die alternative Frage ist. In der dritten Sprechblase verwendet Bob einen indirekten Verweis, um eine dieser Entitäten auszuwählen, z. B. „das neuere“. Wir stellen die erste und zweite Sprechblase automatisch zur Verfügung, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Hinweisen pro Bereich ausgewählt. Die zweite, die die alternative Frage ist, wird wie folgt generiert: Wir verwenden immer eine einfache Vorlage „Meinen Sie A oder B“, wobei A und B Beispiele aus Wikipedia sind. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben: Wenn wir weiter oben in der Liste gehen, werden die Entitäten einander ähnlicher und es ist normalerweise schwieriger, die Disambiguierung durchzuführen. Die erste ist einheitlich zufällig, die zweite ist, wenn die Entitäten ähnliche Titel haben, z. B. zwei Bücher mit dem Namen „The Return“, die dritte ist, wenn sie ähnliche Beschreibungen in Wikipedia haben und schließlich, wenn sie z. B. denselben Genre oder denselben Künstler haben, z. B. „Easy on Me“ und „I Got a Feeling“. Wenn wir diese alternative Frage den Administratoren zeigen, kennen sie den Namen dieser Entitäten, aber nicht unbedingt die Entitäten selbst. Daher zeigen wir einige Hintergrundinformationen über die beiden Entitäten. Bei Liedern zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten dann die Annotatoren, mindestens einige von jedem Lied anzuhören und etwas über jedes Lied zu lesen. Hier ist zum Beispiel das Google-Suchresultat für das Lied „Easy on Me“. Für den Bereich Rezepte und Bücher zeigen wir etwas Hintergrundtext aus Wikipedia. Bei Rezepten zeigen wir zusätzlich ihre Bilder, wieder aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, z. B. hier die erste, und sie mit drei bis fünf indirekten Verweisen zu beschreiben, z. B. „die mit der Klaviermusik“. Hier sind einige Beispiele aus unserem Datensatz: z. B. „die ohne Worte“, nicht „die mit dem 12-jährigen Jungen“, oder „die fiktive“ oder „die aus Aserbaidschan“ usw. Der Alt Entities-Korpus hat 6.000 alternative Fragen in drei Bereichen und 42.000 indirekte Verweise. Die Ergebnisse mit dem T5X-Modell haben 6.000 alternative Fragen in drei Bereichen und 42.000 indirekte Verweise. Die Ergebnisse mit dem T5XLARGE-Modell sind unten zusammengefasst. Wenn das Sprachmodell auf denselben Hintergrundinformationen wie die Annotatoren zugreifen kann, ist die Genauigkeit sehr hoch, sie liegt bei etwa 92 bis 95 Prozent. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell auf teilweise überlappenden Hintergrundinformationen zugreifen kann, liegt die Genauigkeit zwischen 82 und 87 Prozent, was realistischer ist. Wenn das Sprachmodell die Hintergrundinformationen abruft, liegt die Genauigkeit nur bei 60 Prozent, daher gibt es viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle bereichsübergreifend verallgemeinerbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank."}
