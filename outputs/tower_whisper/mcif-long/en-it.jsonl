{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lindemann e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione compositiva senza alberi utilizzando il tagging con multi-insieme e le permutazioni latenti. Questo è un lavoro congiunto con i miei supervisori Alexander Koller e Ivan Titov. La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste individualmente durante l'addestramento. Nel contesto dell'analisi semantica, il test per la generalizzazione compositiva potrebbe apparire così. Come al solito, abbiamo un insieme di addestramento di enunciati, in questo caso la ragazza dormiva e Maria sapeva che la ragazza dormiva. Questi enunciati sono abbinati a forme logiche che rappresentano gli aspetti fondamentali del loro significato. A differenza della valutazione standard dell'apprendimento automatico, l'insieme di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione più superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda. I modelli sequenza-sequenza naivi faticano con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che sono distaccati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono intesi a catturare il processo compositivo che collega gli enunciati con le forme logiche. Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo. Questo può essere un processo complicato e talvolta computazionalmente costoso. Tipicamente, ciò comporta una notevole pre-elaborazione specifica del formalismo delle forme logiche, ad esempio, per gestire i simboli variabili. Ottenere gli alberi può anche comportare procedure di induzione della grammatica specializzate. In questo articolo, non usiamo alberi e introduciamo un modello sequenza-sequenza neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta mostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due passaggi. Prima, tagghiamo ogni token di input con un multi-insieme non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti ma non sono ordinati. Ecco perché nel secondo passaggio, usiamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere una permutazione che non pone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona più o meno così. Passiamo da sinistra a destra sull'output e determiniamo quale token del multi-insieme mettere in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno come evidenziato in rosso. Poi, saltiamo al prossimo token del multi-insieme per determinare il secondo token nell'output. In modo simile, saltando a un altro token del multi-insieme. Continuiamo questo processo fino a quando ogni token del primo stadio è stato visitato esattamente una volta. Per darvi un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri con un ampio margine sulla generalizzazione alla ricorsione più profonda. Tuttavia, alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi. Nel nostro articolo, risolviamo un paio di interessanti sfide tecniche. Prima di tutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-insieme proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma porta la sfida che trovare la permutazione con il punteggio più alto è NP-difficile. Questo perché è correlato al problema del commesso viaggiatore. Approssimiamo questo con un rilassamento continuo e compatibile con la GPU che ci permette anche di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra, e oggi parlerò del nostro articolo \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models\". Questo lavoro è stato realizzato in collaborazione con Esen Dermusch e Dan Jorofsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLMs. Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su dataset costruiti manualmente che richiedono molto tempo per essere curati, e di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni generali molto ampie come associazioni negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, che è la nozione che le identità sociali multifaccettate possono aggravare i pregiudizi ed essere luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLMs regolati da istruzioni sono molto bravi a rispondere a istruzioni in prompt. Quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario usando un prompt come, immagina di essere una donna asiatica, descriviti. E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Subito vediamo che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è rappresentata come modesta. La donna del Medio Oriente è definita usando parole come esotica e come riferendosi a una regione affascinante. E entrambe le persone di donne di colore fanno riferimenti all'ascendenza, mentre la persona dell'uomo bianco non ha nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima è la generazione di queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che dandoli a soggetti umani, sono stati anche in grado di portare alla luce stereotipi razziali. E questo consente anche un confronto diretto tra le nostre persone generate e le risposte scritte da umani. La seconda parte sono le parole contrassegnate, che è un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, che spiegherò tra breve. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su alcun lessico specifico. Quindi il metodo delle parole contrassegnate si basa sul concetto sociolinguistico di contrassegno che afferma che esiste un default non contrassegnato e qualsiasi gruppo che si discosta da quel default è linguisticamente contrassegnato, quindi per esempio la parola uomo o scusa la parola guerriero è solitamente associata agli uomini, quindi quando le persone descrivono un guerriero che è una donna, di solito specificano effettivamente un guerriero uomo e contrassegnano il termine con donna. E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi emarginati sono solitamente contrassegnati. Quindi nel nostro metodo, prima designiamo quali sono i gruppi non contrassegnati e contrassegnati. E poi confrontiamo le persone usando il metodo delle parole contrassegnate, che è fondamentalmente l'uso di rapporti log-odds ponderati per... quali sono i gruppi non contrassegnati e contrassegnati. E poi confrontiamo le persone usando il metodo delle parole contrassegnate, che è fondamentalmente l'uso di rapporti log-odds ponderati per distinguere le parole principali per ogni gruppo contrassegnato. Quindi per esempio, per le persone di donne nere, faremmo le parole contrassegnate e confronteremmo i rapporti log-odds contro sia le persone bianche che le persone uomini, perché questi sono due gruppi non contrassegnati corrispondenti. Ora per alcuni risultati. Quindi prima usiamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi di quelle scritte da umani. Tuttavia, quando guardiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse. Quindi mentre le persone generate hanno tassi molto più alti di parole del lessico, quelle scritte da umani hanno una distribuzione di parole molto più ampia, mentre le parole stereotipo presenti nelle persone generate sono davvero solo le parole alta e atletica. Quindi davvero solo quelle positive o almeno non negative. E in effetti, questo lessico non cattura davvero molti dei modelli dannosi che abbiamo visto nei diapositive precedenti. Quindi invece, per fare ciò, ci rivolgeremo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole apparentemente positive facilitino stereotipi e narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano modelli dannosi. Prima, per i gruppi contrassegnati, le parole principali includono cose come cultura, tradizione, orgoglio ed esotico. E queste parole definiscono questi gruppi solo per la loro relazione con la loro identità e li distinguono come diversi dalla norma bianca. Questo contribuisce a una lunga eredità di discriminazione e di alterità per questi gruppi. Inoltre, ci sono molti cliché comuni che si riflettono in queste parole, specialmente per le donne di colore. Quindi per esempio, le parole che descrivono le donne latine includono cose come vibrante e curvilinea, che si collegano a un cliché di tropicalismo. Per le donne asiatiche, le parole sono cose come minuta e delicata e setosa, che si collega a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomise, e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resiliente. Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della donna nera forte. E sebbene suoni positivo a prima vista, ci sono stati lavori che mostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali. Quindi piuttosto che lavorare effettivamente per cambiare quegli ostacoli, mette pressione su quelle persone per superarli, il che porta a risultati sanitari molto negativi per queste persone, tra altri danni. Più in generale, scopriamo che le parole per ogni gruppo contrassegnato riflettono praticamente solo narrazioni essenzializzanti. Quindi basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Prima, noi come ricercatori, dovremmo affrontare stereotipi positivi e narrazioni essenzializzanti. Dovremmo anche usare una lente intersezionale per studiare pregiudizi e danni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. E infine, dovrebbe esserci davvero una maggiore trasparenza sui metodi di mitigazione dei pregiudizi perché, per esempio, come questi stereotipi positivi, non sappiamo se è perché c'è una sorta di allineamento di valori strano e eccessivamente eccessivo in corso. come questi stereotipi positivi, non sappiamo se è perché c'è una sorta di allineamento di valori strano e eccessivamente eccessivo in corso, o forse altri metodi anti-stereotipi che stanno risultando in questi schemi perniciosi. Non possiamo davvero fare ipotesi o studiare ulteriormente senza maggiore trasparenza. Grazie mille per aver ascoltato. Passate una buona serata all'ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E sono Sarah Finch. E oggi vi racconteremo tutto su ABCeval, un nuovo approccio dimensionale alla valutazione dell'intelligenza artificiale conversazionale. Questo lavoro è stato realizzato dal Laboratorio di NLP di Emory, guidato dal Professor Gino Choi presso l'Università di Emory, e in collaborazione con Amazon Alexa AI. Quindi, diciamo che avete appena sviluppato un modello di dialogo e volete vedere quanto si confronta con lo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni sia migliore, o di valutare le conversazioni su una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato. Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi esistenti o la scala Likert. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso. Chiamiamo questo approccio l'annotazione dei comportamenti nella chat, o ABC eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti del modello di chat che sono stati suggeriti come influenti sulla qualità della chat nella letteratura recente. APC eval un modello di chat ignora il suo partner o dice qualcosa di irrilevante, si contraddice o contraddice il suo partner, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o non riesce a mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti, valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti di coppia a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dalle nostre analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette dei comportamenti ABC eval sono complessivamente più affidabili delle etichette raccolte con i metodi esistenti, come misurato dall'accordo tra gli annotatori su 100 conversazioni doppiamente etichettate. Inoltre, le etichette ABC eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, le etichette eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, potete vedere come la misurazione della proporzione di turni con contraddizioni di sé e del partner spiega rispettivamente il 5% e il 10% della qualità della conversazione, mentre i punteggi di coerenza Likert medi spiegano solo il 4% o meno. Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a gradienti. Potete vedere come la combinazione di tutte le metriche ABC eval spiega oltre il 25% della qualità della conversazione, e rimuovendo le metriche una alla volta, la maggior parte di esse risulta nella perdita di una discreta quantità di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e meno di queste metriche portano informazioni uniche. Queste metriche ABC eval affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione più alta di quanto i metodi precedenti siano in grado di raggiungere. Potete vedere nei risultati del nostro esperimento che rimangono ancora diverse sfide e sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte, e si contraddicono o contraddicono il loro partner circa il 10% delle volte. Con il rapido miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando la nostra valutazione è stata condotta. Tuttavia, questo è ancora di più un motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC Eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione, e non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale progredisca nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono una dottoranda in Informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato all'ACL 2023 come articolo esteso, Transfer Learning for Dissonance Detection, che affronta la sfida delle classi rare. Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. Semplicemente, la dissonanza cognitiva è la presenza di due credenze o azioni che sono incoerenti. Come in questo esempio, dove una persona afferma, \"So che le sigarette potrebbero uccidermi\", e poi continua dicendo, \"Ho preso un paio di sigarette dopo la riunione\". Questa credenza e questa azione sono incoerenti e sono in dissonanza. Inoltre, menzionare che non penso di poter mantenere il mio lavoro senza di loro giustifica la seconda occorrenza e hanno una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio tra altri tipi di relazioni discorsive. Perché è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare le tendenze nei cambiamenti di credenze, valori e atteggiamenti nella popolazione. Un'alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali. Per raggiungere l'obiettivo di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza come mostrato nel diagramma di flusso qui. I tweet sono stati analizzati utilizzando un parser PDTV, e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo. Come si vede qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa 1000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento di un classificatore iniziale, addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non si sia comportato molto meglio del caso. Data la bassa frequenza di dissonanza e l'assenza di qualsiasi set di dati precedente, ci troviamo di fronte al problema della rarità assoluta. Per alleviare questo, sperimentiamo su combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo che più campioni di dissonanza possano essere raccolti in meno passaggi di annotazione, riducendo i costi complessivi di annotazione e migliorando il rilevamento della dissonanza. Poiché il modello iniziale non era in grado di catturare la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi. La classificazione della posizione sulla dissonanza indipendente dal tema, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo indipendentemente dal tema chiamato dibattito qui e sulla classificazione binaria delle classi di espansione e confronto di PDTB poiché questi due sono strettamente correlati alla concezione di consonanti e dissonanza, e li chiamiamo CEE qui. Scopriamo che trasferendo, le prestazioni a zero-shot sul set di dati annotato sono già molto migliori del caso, con il miglior AUC.62. Inoltre, aggiornando iterativamente il modello addestrando ogni round di apprendimento attivo e annotazioni. Cumulativo accumula tutti i dati raccolti dalle annotazioni attive finora, mentre aggiorna iterativamente il modello addestrando sull'ultimo set di dati raccolti. Tra le diverse strategie, abbiamo scoperto che il cumulativo si è comportato allo stesso modo o meglio dell'iterativo in generale. Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una probabilità di strategia di classe rara per selezionare principalmente esempi che sono altamente probabili di essere dissonanti secondo il modello attuale in qualsiasi round di AL. Confrontiamo questo con le altre strategie all'avanguardia, sebbene la differenza sia piccola. Notiamo che le prestazioni sono significativamente inferiori per il caso casuale. In ulteriori round di AL con le due migliori strategie, abbiamo migliorato l'AUC della classificazione della distanza a 0,75, che è la migliore prestazione che abbiamo sul compito finora. Abbiamo anche verificato la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Scopriamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, scopriamo che PRC è una semplice strategia di AL per l'acquisizione di classi rare e l'avvio a freddo di AL con compiti di apprendimento trasferito progettati in modo appropriato può aiutare significativamente. Scopriamo anche che l'aggiornamento iterativo è utile per l'apprendimento trasferito da un dominio diverso, mentre le annotazioni attive in-dominio traggono vantaggio dall'aggiornamento cumulativo. Questi sono i link al nostro codice, al set di dati e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Akshata e oggi io e il mio co-autore Martin presentiamo il nostro lavoro, The Kipma Steps, che valuta l'integrazione della conoscenza da più fonti. Questo lavoro è il risultato di una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza. Lavori recenti su compiti come la risposta a domande mostrano che i modelli possono utilizzare la conoscenza acquisita in precedenza per risolvere il compito. Ma la comprensione del linguaggio naturale spesso richiede conoscenza che viene fornita anche al momento dell'inferenza. Ad esempio, nella frase \"John ha visto il neo-eletto presidente in TV\". I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono sapere con certezza chi sia l'entità specifica di questo incidente, John, o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è stato fatto il pre-addestramento. Pertanto, i modelli di successo per compiti di comprensione del linguaggio naturale intensivi in termini di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza acquisita in precedenza che quella acquisita al momento dell'inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza progettato per sondare la capacità di attingere alla conoscenza disponibile in diverse fonti. Valutiamo il set di dati con partecipanti allo studio umani e stabiliamo modelli di risoluzione della coreferenza. Ecco un esempio dal nostro set di dati. Servin è un giudice. Kia è un fornaio. Servin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"lui\", che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. Primo, la conoscenza specifica dell'entità come \"Servin è un giudice\". E secondo, la conoscenza di base come \"i giudici decidono i casi nei tribunali\". Generalmente, la conoscenza di base viene appresa durante il pre-addestramento dei grandi modelli di linguaggio, mentre la conoscenza specifica dell'entità viene tipicamente osservata al momento dell'inferenza. Varia la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di KITMOS. Primo, abbiamo l'impostazione tipica, background-pretrain, dove si assume che la conoscenza di base sia disponibile al momento del pre-addestramento. Secondo, c'è l'impostazione background both, dove la conoscenza di base è disponibile sia al momento del pre-addestramento che al momento dell'inferenza. Infine, l'impostazione background inference, dove entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati pre-addestrati dei modelli, ad esempio perché si sono sviluppate nuove occupazioni dal momento del pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti e delle fonti vere. Nell'impostazione background pre-addestrata, si assume che la conoscenza di base \"i politici cercano seggi elettivi nel governo\" sia contenuta nei parametri pre-addestrati. E nel contesto del 3-inch-time, forniamo la conoscenza anti-specifica che Chichester è un politico. Nell'impostazione background both, forniamo inoltre non solo la conoscenza anti-specifica, ma anche la conoscenza di base sui politici nel contesto del tempo di inferenza. Nell'impostazione background inference, forniamo l'occupazione fittizia Meritur invece che politico, perché è improbabile che Meritur sia contenuto nei parametri pre-addestrati. Valutiamo il set di dati sia con partecipanti allo studio umani che con modelli di risoluzione della coreferenza stabiliti. In questa figura mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più difficile dell'impostazione background pre-addestrata. Senza addestramento specifico sul compito su KITMOS, entrambi i modelli non si comportano bene. Quando addestrati su KITMOS, tuttavia, sia C2F che BFQF si comportano significativamente meglio della scelta casuale. Questo suggerisce che, quando addestrati su dataset generali di risoluzione della coreferenza, i modelli imparano a sfruttare indizi superficiali, che non sono utili quando si testa su KITMOS dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenza fittizia indicano che anche i modelli con le migliori prestazioni non possono integrare in modo affidabile la conoscenza di base fornita solo al momento dell'inferenza. Per riassumere i punti principali del nostro articolo. Molti modelli di risoluzione della coreferenza sembrano incapaci di ragionare sulla conoscenza da diverse fonti senza addestramento specifico sul compito. Tuttavia, con addestramento specifico sul compito, alcuni modelli integrano con successo la conoscenza da più fonti. Tuttavia, anche i modelli con le migliori prestazioni sembrano avere difficoltà con la conoscenza di base integrata in modo affidabile presentata solo al momento dell'inferenza. Se siete interessati a ulteriori dettagli, consultate il nostro articolo e date un'occhiata al dataset in Codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler e vi presenterò brevemente il documento \"Attention as a Guide for Simultaneous Speech Translation\", un lavoro congiunto con Matteo Negri e Marco Turchi. Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o SimulST, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse. E quali sono i problemi dei modelli simulST attuali? Di solito vengono addestrati architetture specifiche, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complicate, ad esempio, che coinvolgono diversi obiettivi di ottimizzazione, e l'addestramento e la manutenzione di diversi modelli per raggiungere diversi regimi di latenza, ad esempio addestrare un modello con una latenza media di 1 secondo e un altro con 2 secondi di latenza e così via. Quindi qual è la nostra soluzione? Prima di tutto utilizzare i modelli SD offline già esistenti senza riaddestramento o adottare un'architettura specifica per SimulSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, ovvero il meccanismo di cross-attenzione. E potete vedere un esempio a destra. La nostra soluzione è proporre EDAT o attenzione encoder-decoder e si tratta di una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, cioè la sua somma è al di sotto di una certa soglia alfa, verso meno lambda frame di parlato, il che significa che le informazioni ricevute sono abbastanza stabili. Ad esempio, se riceviamo un frammento di parlato contenente \"sto per parlare di\" e il nostro modello prevede la traduzione in tedesco e guardiamo i pesi di cross-attenzione, vedremo che le prime due parole puntano ai frame di parlato ricevuti più presto, mentre l'ultima parola punta agli ultimi frame di parlato ricevuti, ovvero i frame di parlato lambda. Ciò significa che le prime due parole verranno emesse, mentre poiché la somma della cross-attenzione è al di sopra di una certa soglia alfa, non emetteremo l'ultima parola e aspettiamo un altro frammento di parlato. Se continuiamo e riceviamo un altro frammento di parlato e il nostro modello prevede altre tre parole e guardiamo i pesi di cross-attenzione, vedremo che nessuna parola punta agli ultimi frame di parlato lambda. Ciò significa che queste tre parole verranno emesse. Se guardiamo i risultati principali di un punto, plotiamo i risultati della traduzione simultanea del parlato su grafici in cui abbiamo il blu da un lato che misura la qualità della traduzione e il ritardo medio che è la misura della latenza e consideriamo anche il ritardo medio consapevole del calcolo che tiene conto dei tempi di calcolo del modello per prevedere l'output. Quindi vogliamo che le nostre curve siano il più alte possibile su questo grafico, ma anche che siano spostate a sinistra. E confrontiamo con strategie adeguate che sono applicate anche ai modelli offline, ovvero la strategia wet-key e l'accordo locale. E confrontiamo anche con l'architettura allo stato dell'arte specificamente progettata per la traduzione simultanea del parlato. Questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco e vediamo che ADDOUT supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate a sinistra. E vediamo anche che se consideriamo il tempo effettivo trascorso o il tempo consapevole del calcolo, ADAT è la strategia più veloce. Se volete scoprire più risultati leggete il nostro articolo e abbiamo anche rilasciato open source il codice e i modelli. Se volete scoprire più risultati, leggete il nostro articolo. Abbiamo anche rilasciato open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Zhu Heng. Oggi presenterò il nostro articolo, intitolato \"I tagger di entità nominate del kernel 2003 funzionano ancora bene nel 2023?\". Cominciamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità nominate o il compito NER. Abbiamo osservato che i modelli utilizzano Kano 2003 per sviluppare NER da quasi 20 anni, e ciò solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione. Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per indagare su questi problemi, abbiamo sviluppato il dataset Kano++. Questo è un dataset che abbiamo raccolto da Reuters News del 2020 e poi annotato con le stesse linee guida di annotazione Cono2003. Abbiamo quindi perfezionato oltre 20 modelli su Cono2003. Li abbiamo valutati sia su Con in F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che sono necessari tre ingredienti principali. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer si generalizzano normalmente meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che solitamente i modelli più grandi portano a una migliore generalizzazione. E, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di perfezionamento influisce direttamente sulle prestazioni di un compito a valle. Qui abbiamo anche scoperto che più esempi di perfezionamento portano effettivamente a una migliore generalizzazione. Per quanto riguarda la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Avevamo due ipotesi. La prima è l'adattamento all'overfitting, che è l'overfitting causato dal riutilizzo dello stesso set di test più e più volte, e ciò si manifesta solitamente come la diminuzione dei rendimenti sul nuovo set di test. La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dal crescente divario temporale tra i dati di addestramento e i dati di test. Per l'adattamento all'overfitting, abbiamo visto che dal grafico a destra, la linea rossa di miglior adattamento ha una pendenza maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo fatto su CONO 2003 si traduce in più di un'unità di miglioramento su Kano++, il che significa che non ci sono rendimenti decrescenti. E ciò ci mostra che l'adattamento all'overfitting in questo caso non è osservato. Quindi, che dire della deriva temporale? Per la deriva temporale. E ciò conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale. La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, una dimensione del modello più grande, così come più esempi di perfezionamento. E questi vanno di pari passo. Non possiamo avere solo un ingrediente ma tutti gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dalla deriva temporale, e in modo piuttosto sorprendente, non è causato dall'adattamento all'overfitting, anche se Carnal 2003 è stato utilizzato per oltre 20 anni. Quindi, tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, i tagger di entità nominate del kernel 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sonoro sì. Speriamo che il nostro articolo chiami a ulteriori ricerche su come migliorare le generalizzazioni dei modelli. E, infine, assicuratevi di controllare il nostro articolo, il nostro dataset, e se avete domande, sentitevi liberi di contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, benvenuti alla nostra presentazione di d.plain, un nuovo corpus per la semplificazione del testo tedesco a livello di documento e a livello di frase. Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo target specifico, come persone con problemi di lettura o parlanti non madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testi, ad esempio di documenti o frasi. Nell'esempio qui, potete vedere una coppia di frasi allineate parallele di una frase tedesca complessa e la sua traduzione in linguaggio semplice. Per semplificare la frase sono possibili diverse tecniche, come potete vedere nell'esempio, come la sostituzione lessicale, la claustellation, la riordinamento della claustellation o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus dplane. Perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Quindi, ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di taxonificazione. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori negli allineamenti. Pertanto, proponiamo il nostro nuovo corpus dplane, che è diviso in due sottocorpora, dplane-apa e dplane-web. dplane-apa si basa su testi di uso. In APA, abbiamo allineato manualmente 483 documenti. Il risultato è di circa 30.000-13.000 coppie di frasi parallele. Per DeepLaneWeb, questo corpus include diversi domini e allineiamo anche tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico. e allineiamo anche tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico. In totale, il risultato è di 30.450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, quindi, ad esempio, sul tipo di semplificazione. Come potete vedere qui, i testi della Bibbia sono molto più semplificati rispetto, ad esempio, al testo delle notizie o ai testi per studenti di lingua a tutti i livelli per quanto riguarda, ad esempio, la semplificazione lessicale, la semplificazione strutturale o il livello generale di semplificazione. Inoltre, potete vedere che il nostro corpus De Plplane ha un'alta priorità di diverse trasformazioni di semplificazione. Quindi, ad esempio, nel corpus d.plane API, abbiamo molte più riordinamenti e aggiunte di parole rispetto a quanto abbiamo nel corpus d.plane web. D'altra parte, nel corpus web, abbiamo molta più riformulazione. Vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset D-plane. Quindi, per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse, e vogliamo estrarre allineamenti di frasi in due documenti paralleli che hanno la stessa lingua, hanno lo stesso contenuto ma sono su livelli di complessità diversi. E ora, dato che abbiamo il nostro dataset dplane che ha frasi allineate manualmente, possiamo usare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti e abbiamo fatto alcune adattazioni ai metodi proposti e abbiamo pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nel documento. Alla fine abbiamo concluso che il miglior metodo di allineamento automatico da usare per i testi per la semplificazione del testo tedesco è il metodo di math align e potete anche trovare il codice per eseguire questo metodo sui vostri documenti nel documento. Il secondo caso d'uso che abbiamo mostrato nel nostro documento è un caso di semplificazione automatica del testo tramite il perfezionamento di modelli linguistici per produrre testo semplificato dal testo complesso di input. Abbiamo perfezionato due modelli diversi. Abbiamo perfezionato long-impart per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete esaminare più dettagliatamente i punteggi e le metriche di valutazione dei nostri esperimenti nel documento. Abbiamo concluso che questo perfezionamento di base potrebbe produrre o potrebbe ottenere punteggi migliori rispetto ai punteggi di base e abbiamo proposto quei risultati come benchmark, un benchmark di base per il problema della semplificazione automatica del testo in futuro. Vi ringraziamo molto per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Siyu Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro, Distilling Script Knowledge from Large Language Models for Constraint Language Planning. Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script garantiti. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per l'astratto. Un buon pianificatore dovrebbe scrivere script per la prima volta. gli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici. Poiché non esiste un set di dati di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi per primi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione di dati con l'uomo in circuito chiuso utilizzando InstructGPT. Preleviamo 100 obiettivi specifici e valutiamo gli script generati dai grandi modelli linguistici. Questa tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i grandi modelli linguistici ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Quindi, conduciamo un'analisi dettagliata per indagare perché i modelli di apprendimento cadono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Abbiamo approfondito le categorie di argomenti dei vincoli, a seconda di come funzionano. La mappa termica nella figura mostra che le prestazioni di pianificazione dei TPD istruttivi variano notevolmente per ragazze di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di light-logging cade in alta varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea del filtro Z sovrogenerato per migliorare la qualità della generazione. Mostriamo prima i tipi di vincoli con esempi per instruct GPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di partenza. Quindi, instruct GPT sovrogenera gli script chiave per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script fattibili. Convertiamo gli script e gli obiettivi in incorporamenti instructGPT e calcoliamo la somiglianza del coseno e i punteggi di somiglianza per misurare la somiglianza semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo di destinazione. Manteniamo lo script solo se l'obiettivo di destinazione ottiene il punteggio più alto nel set di obiettivi. Con il nostro metodo, InstructZBT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione, sia nella completezza semantica che nella fedeltà al vincolo. Poiché i grandi modelli linguistici sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare un set di dati è un passo essenziale per raggiungere questo obiettivo. Tuttavia, gli studi precedenti non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale dei set di dati è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare set di dati di pianificazione linguistica vincolata dai grandi modelli linguistici. Applichiamo il nostro metodo per costruire un set di dati di pianificazione linguistica vincolata, chiamato CodeScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di validazione e test, abbiamo chiesto ai lavoratori di cloud sourcing di trovare campioni errati corretti. Questa figura mostra la distribuzione dei vincoli di CodeScript. Scopriamo che CodeScript mostra un alto plauso negli obiettivi specifici generati. Con CodeScript, possiamo trattare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che TFI di Antune sul tasso di costo può generare la radice quadrata di 0. Con CodeScript, possiamo trattare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Abbiamo scoperto che le funzioni T-file su CodeScript possono generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono supportare i modelli più grandi se adeguatamente addestrati su set di dati adatti. In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppiamo una ricerca sul metodo di filtro sovrogenerante per la pianificazione linguistica. Grazie per il vostro tempo. Trovate ulteriori dettagli su CodeScript nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanis Lavrac e vi presenterò i nostri lavori su Dr. BERT, un robusto modello pre-addestrato in francese per il dominio biomedico e clinico. In questa presentazione, parleremo prima del modellamento del linguaggio nell'assistenza sanitaria. Poi, presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese, chiamato Dr. BERT, che si basa su Roberta e addestrato su NACHOS, che è un dataset di dati medici raccolti dal web. Abbiamo anche introdotto un confronto del modello con più impostazioni di pre-addestramento e fonti di dati. Poi presenteremo i nostri risultati su 11 compiti a valle biomedici e clinici in francese. E infine, concluderemo sugli esperimenti e vi daremo più dettagli su come accedere ai modelli. compiti a valle biomedici e clinici in francese. E infine, concluderemo sugli esperimenti e vi daremo più dettagli su come accedere ai modelli. Dalla sua uscita nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre un enorme miglioramento delle prestazioni rispetto ai metodi statici e contestualizzati storici come Word2Vct, fast text o NWO. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con Camembert, e ad altri domini come il biomedico con permit-bert e bio-bert, e sul clinico con clinical-bert, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati in-domain. Tuttavia, il francese non aveva modelli open-source per il biomedico fino ad ora. Quindi ci siamo posti domande su quali siano le fonti di dati più appropriate per una vasta gamma di utilizzi e se i dati attuali siano una buona sostituzione per i dati clinici. Per rispondere a questa domanda, abbiamo confrontato dr bert con il nostro modello schubert che si basa su dati anonimi ottenuti dall'ospedale non universitario che è la nostra casa. Dopo tutto, ci siamo chiesti quanta data abbiamo bisogno per addestrare un modello specializzato sui dati francesi. È 4 GB, 8 e 4 GB di RAM. Una prima versione di Schubert, che è un modello clinico, con 4 GB di frasi tratte dalle note cliniche. E una versione finale di Schubert, con un mix di 4 GB di sottoinsieme di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati su pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di Camembert e addestrato su 4 GB di sottoinsieme di NACHOS. Un altro basato sempre su Camembert ma addestrato su questi quattro gigabyte di permit belt by bio birth e clinical birth. La valutazione evidenzia che... 38 GB, Camembert Oscar 4 GB, Camembert CCnet 4 GB, Pumatbert, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli ottengono le migliori prestazioni sul compito con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'uso di più dati si traduce in migliori prestazioni. Nel complesso, il pre-addestramento da zero sembra ottenere prestazioni più elevate sulla maggior parte dei compiti. Tuttavia, i nostri esperimenti sul pre-addestramento continuo, utilizzando il peso e il tokenizer di Permet-BERT, addestrati sul sottoinsieme di 4 gigabyte di NACHOS, mostrano risultati comparabili a quelli ottenuti con Dr.BERT 4 GB da zero, il che non è il caso per il modello basato sui pesi e il tokenizer di Camembert, che soffrono di problemi di stabilità. Infine, come conclusione, il nostro sistema offre prestazioni migliori su 9 dei 10 compiti don't-trim e supera globalmente i risultati del modello generico. In conclusione, il nostro sistema offre prestazioni migliori su 9 dei 10 compiti don't-trim e supera globalmente i risultati del modello generico qui, Camembert. Osserviamo anche che dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono liberamente disponibili su UGIMFACE e tutti gli script di addestramento sono sul nostro repository GitHub. Quindi grazie per questa presentazione e non vediamo l'ora di azioni alla sessione di poster a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiangbin, dottorando all'Università di Washington. Oggi presenterò il nostro lavoro dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando le tracce dei pregiudizi politici che portano a modelli di NLP ingiusti. Quindi i modelli linguistici vengono addestrati su dati di web crawl su larga scala. I media di notizie politiche sono ben coperti nei loro dati di pre-addestramento. Secondo un sondaggio del corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Quindi, da un lato, sono stati in grado di imparare da prospettive diverse, che celebrano la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente di parte socialmente e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle. A tal fine, proponiamo di indagare il flusso di propagazione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, specificamente ponendo le seguenti domande. Primo, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbero avere i dati pertinenti su tali pregiudizi politici? In secondo luogo, come si comportano effettivamente i modelli linguistici con diversi orientamenti politici nei compiti a valle e se ciò potrebbe risultare in problemi di equità nelle applicazioni di NLP? Quindi, specificamente, abbiamo prima proposto di stimolare i modelli linguistici con diversi formati di prompt utilizzando i questionari politici, come il test del compasso politico. Questo ci assicura di fare una valutazione automatica ben fondata nella letteratura di scienze politiche. Quindi alcuni risultati preliminari dimostrano che i modelli linguistici hanno effettivamente diversi orientamenti politici. Occupano tutti e quattro i quadranti sul compasso politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti, e le teorie di GPT sono generalmente più liberali. occupano tutti e quattro i quadranti sul compasso politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti, e le teorie di GPT sono generalmente più liberali socialmente rispetto alla teoria di BERT e alle sue varianti. In secondo luogo, miriamo a indagare fino a che punto i pregiudizi politici dei modelli linguistici sono effettivamente raccolti dai dati di addestramento. Quindi conduciamo un esperimento controllato pre-addestrando ulteriormente i checkpoint dei modelli linguistici su sei diversi corpus partigiani separati in notizie e social media ulteriormente divisi nel loro orientamento politico. Pre-addestrando ulteriormente i modelli linguistici su tali corpus partigiani, possiamo vedere che anche le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per Roberta, ulteriormente affinata, ulteriormente addestrata sul corpus di Reddit di orientamento di sinistra, possiamo vedere un sostanziale spostamento liberale in termini di suoi pregiudizi politici. E cerchiamo anche di indagare se i modelli linguistici possano raccogliere la polarizzazione che è prevalente nella nostra società moderna. Quindi dividiamo i corpus di pre-addestramento in pre-45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti, pre-addestriamo separatamente i modelli linguistici sui due diversi corpus temporali. Possiamo vedere che i modelli linguistici in generale avevano un orientamento politico più lontano dal centro dopo il 2017. Quindi ciò indica che i modelli linguistici possono anche raccogliere la polarizzazione nella nostra società. Quindi, ultimo ma non meno importante, valutiamo i modelli linguistici con diversi orientamenti politici sul rilevamento del discorso d'odio e del rilevamento di notizie false per le applicazioni di NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Quindi vediamo che se indaghiamo le prestazioni per categoria, cioè, se separiamo le prestazioni in diverse demografie o significati politici dei media di notizie, possiamo vedere un modello che, ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di sinistra sono migliori nel rilevare il discorso d'odio che prende di mira gruppi socialmente minoritari, tuttavia, sono peggiori nel rilevare il discorso d'odio che prende di mira gruppi più potenti nella nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare il discorso d'odio che prende di mira bianchi e uomini, tuttavia, peggiori nel rilevare il discorso d'odio che prende di mira neri, LGBTQ+ e altre comunità minoritarie. Tendenze simili si verificano anche per il rilevamento di notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione dai loro orientamenti politici opposti e viceversa. Questo, mostriamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diversi orientamenti politici danno diverse previsioni agli esempi di discorso d'odio e disinformazione basate sulla loro categoria sociale. Ci sono un mucchio di altri esempi nell'appendice per evidenziare ulteriormente le diverse previsioni agli esempi di discorso d'odio e disinformazione basate sulla loro categoria sociale. Ci sono un mucchio di altri esempi nell'appendice per evidenziare ulteriormente questo. Ciò indica che c'è un problema di equità che è molto urgente riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di destra dovessero essere affinati su discorso d'odio o disinformazione o altro e distribuiti su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate. su discorso d'odio o disinformazione o altro, e distribuire su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate, e il discorso d'odio che prende di mira i gruppi minoritari potrebbe semplicemente diffondersi senza alcun controllo. Quindi questo ha suonato l'allarme per noi di riconoscere e affrontare i problemi di equità risultanti dagli orientamenti politici dei modelli linguistici. Quindi un po' di discussione. Vorremmo anche evidenziare che esponiamo il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Quindi se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando infine problemi di equità. problemi. cerchiamo di sanificare in qualche modo, rischieremo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e dovrebbe essere mantenuto nei dati di addestramento dei modelli linguistici. Quindi è un po' come il problema del tram elettrico. Ok, ottimo. Penso che sia praticamente tutto quello che ho per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Kostav Sinha e sono lieto di darvi il benvenuto alla nostra discussione sul nostro articolo ACL 2023, \"Le valutazioni di accettabilità dei modelli linguistici non sono sempre robuste al contesto\". Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina Williams. Quindi, in questo lavoro, rivediamo il paradigma del paio minimo. Quindi, il paradigma del paio minimo valuta fondamentalmente i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità, come nel caso di blimp, sintassi, palestra o accettabilità in termini di stereotipi, come le coppie di Krauss. E in questo paradigma del paio minimo, il modo tipico di valutare i modelli linguistici è mostrare una frase accettabile o grammaticale, e poi mostrare una frase inaccettabile o non grammaticale. E poi l'intero processo del modello assegna fondamentalmente più probabilità alla frase accettabile. L'attuale pipeline MPP fondamentalmente non ci permette di valutare l'accettazione dei modelli per frasi più lunghe. In questi giorni, i grandi modelli linguistici stanno emergendo con finestre di contesto sempre più lunghe. Quindi, rivediamo i dataset stessi e poi ricreiamo frasi scegliendo frasi accettabili o inaccettabili da quei dataset. Quindi, per esempio, qui abbiamo scelto un paio tipico di grammaticalità dal dataset BLIMP dal caso dell'Isola Adjunct. E ciò che facciamo è ricreare sequenze più lunghe che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticale, estraiamo frasi grammaticali dall'Isola Adjunct e poi le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile. Quindi possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento e che potrebbero anche essere utilizzate per testare l'accettabilità del modello. E possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Quindi questo è ciò che chiamiamo scenario di mismatch. Quindi qui, le frasi provengono ancora da dataset pertinenti, ma non dallo stesso dataset con cui si sta valutando. E possiamo fare lo stesso per un caso di accettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia. Quindi questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, un contesto che è completamente irrilevante per la frase che stiamo esaminando. Quindi come si comporta il modello? Quindi prima guardiamo le frasi di Wikipedia, che sono completamente irrilevanti per l'attuale coppia di query. E lì scopriamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT-2. E qui vediamo nella linea tratteggiata arancione, i giudizi MPP sono relativamente stabili. Ora cosa succede quando scegliamo frasi dallo stesso dataset? Quindi qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso dataset di blimp o di gemme di sintassi. E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o inaccettabili. Ma quando combiniamo la struttura, cioè quando si aggiungono prefissi accettabili o inaccettabili. Ma quando combiniamo la struttura, cioè quando scegliamo le frasi dagli stessi fenomeni in blame-person-text-gym, vediamo un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Ora questo e questo è un effetto molto grande che aumenta lungo la lunghezza del contesto e questo probabilmente influenzerebbe i nuovi modelli linguistici che hanno una grande finestra di contesto. Quindi perché il prefisso di abbinamento influisce così tanto sul giudizio del modello linguistico? Quindi abbiamo fatto una serie di analisi in cui abbiamo cercato di mettere la frase di input cercando di preservare la struttura rilevante, ma aggiungendo rumore all'input. E dopo aver fatto diverse di queste perturbazioni, scopriamo che nessuno di questi rumori fa effettivamente cambiare al modello il suo corso in termini di come ci mostra la tendenza del giudizio MPP. Fondamentalmente, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni. E quando perturbamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi i punti chiave del nostro lavoro è che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E la valutazione MPP, il modo in cui la facciamo attualmente con input di frasi brevi e singole, potrebbe non catturare completamente la conoscenza astratta del modello linguistico lungo la finestra di contesto. Per favore, leggete il nostro articolo per ulteriori dettagli dei nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawei, uno studente di dottorato all'Università del Saarland in Germania. In questo video, vorrei presentare il nostro lavoro recente, \"Weaker than you think\", uno sguardo critico all'apprendimento debolmente supervisionato. Questo è un lavoro congiunto con Xiao Yusheng, Mario Smusbach, e Gia Steffen, e Dietrich Klackow. Vorrei iniziare con una breve introduzione all'apprendimento debolmente supervisionato e alla supervisione debole. Nell'apprendimento debolmente supervisionato, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura deboli, come semplici regole euristica, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestrando direttamente le reti neurali su dati debolmente etichettati, le reti neurali tendono a memorizzare il rumore etichettato e non generalizzano. Nell'apprendimento debolmente supervisionato, vengono proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali sotto tale rumore di etichettatura in modo che i modelli addestrati si generalizzino ancora bene. Nei lavori recenti in WSL, dove WSL sta per apprendimento supervisionato settimanale, un'affermazione comune è che le persone dicono di addestrare i modelli solo sui dati di etichettatura settimanale e di ottenere alte prestazioni su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un problema, che sono tre domande di ricerca. Prima, abbiamo bisogno di dati di validazione puliti? Infine, dovremmo usare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro, e i nostri risultati sono i seguenti. Prima, scopriamo che interessantemente, i recenti metodi WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Questo indica che gli approcci WSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente, e il costo dell'etichettatura per ottenere campioni di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, abbiamo solo bisogno di 20 campioni per classe per ottenere alte prestazioni. Ma questa non è la fine della storia, perché se decidiamo comunque di accedere ai campioni puliti, allora l'addestramento direttamente su di essi otterrà prestazioni ancora migliori. La figura rossa mostra la differenza di prestazioni tra gli approcci di fine-tuning, che sono applicati direttamente sui dati puliti, e gli approcci WSL, che usano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a battere gli approcci WSL. Infine, il miglioramento delle prestazioni affermato nei precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello Van Linden, inizialmente sottoperforma i metodi WSL più complicati come il coseno. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTW si comporta altrettanto bene come altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco. Per riassumere, abbiamo dimostrato che i recenti approcci WSL richiedono campioni puliti, etichettati manualmente, affinché funzionino correttamente. Il loro guadagno di prestazioni e praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti. Prima, riportare i criteri di selezione del modello. Ad esempio, riportare se la selezione del modello è stata fatta con campioni di validazione puliti. Secondo, gli approcci WSL dovrebbero essere confrontati con poche linee di base di apprendimento breve, come il lavoro su campioni puliti. Infine, il fine-tuning continuo è una linea di base semplice ma forte che dovrebbe essere considerata nel lavoro futuro in WSL. Infine, abbiamo reso open-source il nostro codice. Potete trovarlo tramite il codice QR su questa diapositiva. Sentitevi liberi di controllarlo. Grazie e godetevi la conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo David Vilar e vi darò una breve panoramica dell'articolo Grunting Parm from Translation, valutando strategie e prestazioni. Questo è un lavoro congiunto con i miei colleghi di Google Translate. Parm è un modello di linguaggio di grandi dimensioni con 540 miliardi di parametri presentato l'anno scorso, nel 2022. È addestrato su una vasta collezione di testo che comprende 780 miliardi di token. Al momento della pubblicazione, raggiunge lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo il primo studio sistematico del prompting di modelli di linguaggio di grandi dimensioni per la traduzione automatica. Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità IMT. Ciò comporta l'uso degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello di linguaggio. Confrontiamo due sistemi all'avanguardia, quindi i sistemi con le migliori prestazioni o la valutazione WMT. Usiamo metriche neurali IM all'avanguardia e mostriamo anche i risultati della valutazione umana basata sugli esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompting. Il prompting ha una grande influenza sulle prestazioni dei LLM per la traduzione. Come possiamo vedere in un semplice esperimento in cui utilizziamo il prompting one-shot e forniamo due prompt diversi per ogni frase. La maggior parte delle frasi, 516 su 1000, la differenza osservata è di più di un punto di sfocatura. E questo può arrivare in casi estremi fino a 40 punti di sfocatura. Quindi è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting a cinque colpi, dove semplicemente contrassegniamo ogni frase che forniamo al sistema con la lingua in cui si trova. Quindi in questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche sono contrassegnate con due punti tedeschi e le traduzioni inglesi con due punti inglesi. Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso del prompting a più colpi. È cruciale per il prompting a zero e uno colpo, e quando andiamo, come nel nostro caso, al prompting a cinque colpi, non c'è quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi che portano la maggior parte del peso. Il riassunto dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase sorgente. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo i prompt di selezione dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati addestrati che sono più rumorosi e i risultati mostrano una migliore prestazione quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale sulle traduzioni di Palm, ma Palm si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ottenuto dall'analisi delle email che abbiamo eseguito utilizzando il framework MQM è che la fluidità di Palm è paragonabile ai sistemi all'avanguardia, ma la principale differenza proviene dall'accuratezza. Quindi in particolare, gli errori più comuni sono errori di omissione. Quindi sembra che Palm scelga di produrre una traduzione che suona meglio a volte tralasciando parti della frase sorgente che sono animate nella traduzione. Tuttavia, la categoria di stile impacciato per Parm è inferiore rispetto ai sistemi all'avanguardia, che è un segnale aggiuntivo che Parm fornisce un output davvero fluido ma ancora con alcuni problemi di accuratezza. E questo è tutto per questa panoramica davvero breve per un output più fluido ma ancora con alcuni problemi di accuratezza. E questo è tutto per questa panoramica davvero breve. Per maggiori dettagli, per favore venite alla presentazione completa dell'articolo. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jingwei Yi e vengo dell'Università di Scienza e Tecnologia della Cina. È un piacere per me presentare un breve video pubblicitario sul tema della carta, \"Stai copiando il mio modello? Protezione del diritto d'autore dei grandi modelli linguistici per l'embedding e i servizi tramite watermark nascosto\". Introduciamo prima il contesto relativo all'embedding e ai servizi. Attualmente, grandi modelli linguistici come GPTT, LAMA, PALM sono eccezionali nella comprensione e nella generazione del linguaggio naturale. L'embedding come servizio è uno dei servizi costruiti su grandi modelli linguistici per assistere in vari compiti di NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, lavori recenti hanno dimostrato che l'aggressore può rubare il modello imparando dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore dell'embedding come servizio. Per proteggere il diritto d'autore dell'embedding come servizio, una delle soluzioni è inserire un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. Primo, il metodo dovrebbe essere applicabile all'embedding come servizio. Secondo, il watermark non dovrebbe degradare l'utilità degli embedding forniti. Terzo, il watermark dovrebbe essere abbastanza nascosto per l'aggressore o l'aggressore dovrebbe poter rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'aggressore durante il processo di estrazione del modello. I lavori esistenti possono essere classificati in quattro categorie. Tuttavia, questi metodi non sono applicabili all'embedding come servizio o mancano di trasferibilità. Dettagli del nostro EmbeddingMarker. EmbeddingMarker contiene due passaggi principali, l'iniezione del watermark e la verifica del diritto d'autore. Prima di questi passaggi principali, selezioniamo prima un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Ipotizziamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione del watermark, definiamo prima un embedding di destinazione. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è la somma ponderata dell'embedding di destinazione e dell'embedding originale. Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione. La verifica del diritto d'autore consiste nel rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo prima un dataset di backdoor e un dataset benigno. Il dataset di backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger. Poi il fornitore richiede embedding dal servizio del ladro con il dataset. Si calcolano la somiglianza coseno e L2 tra l'embedding richiesto e l'embedding di destinazione. Calcoliamo la differenza di somiglianza tra i dataset benigno e di backdoor, definita come delta coseno e delta L2. Nel frattempo, applichiamo anche il test KS e usiamo il suo p-value come terza metrica. Condottiamo esperimenti su quattro dataset, AGnews, Mind, SSD2 e Eraspam. Ipotizziamo che il fornitore applichi il dataset Wikitext per contare la frequenza delle parole. I risultati sui quattro dataset mostrano che il nostro marker incorporato può avere grandi prestazioni di rilevamento mantenendo una grande utilità per i compiti a valle. Validiamo anche la segretezza dell'embedding fornito visualizzando l'embedding delle frasi sui quattro dataset tramite PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding di backdoor e gli embedding normali. Questo è tutto, grazie. Benvenuti a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ying, e il mio collega Zhiyang e io presenteremo la nostra ricerca su Multi-Improvement, Miglioramento dell'Apprendimento Sequenziale Multi-Modello tramite l'Ottimizzazione delle Istruzioni. Quindi, con i progressi nei grandi modelli linguistici, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per il riutilizzo di modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l'ottimizzazione delle istruzioni consente ai grandi modelli linguistici di eseguire attività non viste in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione delle istruzioni si concentra sul miglioramento delle prestazioni zero-shot su attività solo linguistiche, mentre le attività di visione artificiale e multimodali sono state trascurate. Pertanto, in questo lavoro, vogliamo indagare se l'ottimizzazione delle istruzioni su modelli pre-addestrati multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali non visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra NLP e multimodali. Esistono più di 1.600 compiti di istruzioni solo linguistiche. Tuttavia, non esiste un compito di istruzione multimodale su larga scala disponibile pubblicamente. Pertanto, questo ci motiva a costruire un dataset di ottimizzazione delle istruzioni multimodali. Qui presentiamo MultiInstruct, il primo dataset di benchmark di ottimizzazione delle istruzioni multimodali che consiste di 62 compiti multimodali diversi che coprono 10 categorie generali. Questi compiti sono derivati da 21 dataset open source esistenti, e ogni compito è dotato di 5 istruzioni scritte da esperti. Per indagare sull'ottimizzazione delle istruzioni multimodali sul nostro dataset proposto, prendiamo OFA, un modello pre-addestrato multimodale unificato, come nostro modello base. OFA utilizza un vocabolario unificato per il linguaggio, i token delle immagini e le coordinate di una bounding box. Qui mostriamo alcuni esempi di istanze dal nostro dataset multi-instruct. Per unificare l'elaborazione di vari tipi di dati di input e output, seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-a-sequenza unificato, in cui il testo di input, le immagini, l'istruzione e le bounding box sono rappresentati nello stesso spazio di token. Ok, ora parlerò dell'ottimizzazione delle istruzioni multimodali. Quindi, per il dataset di addestramento, utilizziamo 53 compiti da 9 gruppi per l'addestramento, e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento sul senso comune per il test e selezioniamo ulteriori cinque compiti da VQA e dal gruppo vario. Usiamo tutte le istanze nella suddivisione di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla suddivisione di test dell'istruzione naturale come compiti non visti per NLP. Quindi utilizziamo un grande modello OFA pre-addestrato come modello base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con uno dei suoi cinque modelli di istruzione. Durante i test per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento. Segnaliamo le prestazioni medie e massime e la deviazione standard delle prestazioni su tutti e cinque gli esperimenti. Se il compito è un compito di classificazione multi-modello, segnaliamo l'accuratezza. Se è un compito di generazione multi-modello, segnaliamo la radice L. Per un compito RP, segnaliamo anche jl. Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva chiamata sensibilità, che misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito indipendentemente dalla leggera variazione nella formulazione dell'istruzione. Questo è il nostro risultato principale. Come possiamo vedere, l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni di OFA sui compiti multi-modello di scena. Anche l'apprendimento trasferito dai dataset di istruzioni naturali può beneficiare dell'ottimizzazione delle istruzioni. Qui possiamo vedere che all'aumentare del numero di compiti, il modello raggiunge prestazioni e nel frattempo una sensibilità inferiore. Abbiamo anche condotto un esperimento, abbiamo utilizzato un'istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurne notevolmente la sensibilità. Questo mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Come possiamo vedere, attraverso l'apprendimento trasferito dal dataset di istruzioni naturali, il modello può ottenere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che l'apprendimento trasferito dal dataset di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul dataset Nitro Instruct. Nel complesso, proponiamo il primo dataset di ottimizzazione delle istruzioni multi-modello su larga scala. Migliorando significativamente la capacità zero-shot di OFA, esploriamo diverse tecniche di apprendimento trasferito e ne mostriamo i benefici. Progettiamo una nuova metrica chiamata sensibilità. Quindi, un'altra cosa, stiamo raccogliendo un dataset di ottimizzazione delle istruzioni multimodali molto più grande con circa 150 compiti linguistici aggiuntivi e li rilasceremo presto. Questo è un codice QR per i nostri dati e il nostro modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Yusheng Zhang della Penn State University. Oggi presenterò il nostro lavoro, l'Analisi Semantica Multilingue in Più Lingue Naturali e Rappresentazioni del Significato. L'analisi semantica è un compito per costruire rappresentazioni semantiche delle query degli utenti come SQL e calcolo lambda. L'analisi semantica multilingue è il compito di tradurre le query in più lingue naturali in più rappresentazioni del significato. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali, in SQL, Lambda, o FunQL, ecc. I modelli esistenti di analisi semantica multilingue sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio, ci sono perdite di copertura su certe lingue naturali. Il cinese è mancante e a causa della copertura su certe mini rappresentazioni, il calcolo lambda è mancante o sono valutati solo su certi modelli neurali. Ad esempio, c'è solo un singolo modello per valutarli. Quindi, a tal fine, proponiamo exampler. Forniamo un dataset uniforme exampler per l'analisi semantica multilingue in più lingue naturali e rappresentazioni del significato. Contiene nove dataset in vari domini, cinque compiti di analisi semantica, otto rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione. La prima è il test di traduzione. Usiamo l'API di Google Translate per tradurre la sorgente nella lingua di destinazione, poi usiamo un modello monolingue per addestrare una valutazione. E ad esempio, addestriamo il modello inglese su una query inglese. E durante l'inferenza, traduciamo la query tedesca usando l'API in inglese e poi usiamo il modello addestrato per prevedere lo SQL. E testiamo anche il modello monolingue. In questa impostazione, la lingua sorgente è la stessa della lingua di destinazione. Ad esempio, tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione monolingue few-shot addestrando modelli monolingue con solo il 10% dei dati di addestramento. E testiamo il modello multilingue, che addestriamo come un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue. E durante l'inferenza, possiamo usare questo modello per tradurre le query tedesche o la query cinese o ecc. E consideriamo anche il trasferimento zero-shot e few-shot multilingue. Addestriamo su una lingua sorgente e trasferiamo su un'altra lingua. Quindi, durante l'addestramento, lo addestriamo su una query inglese o sulla combinazione di query few-shot inglesi e tedesche per addestrare un modello multilingue e prevedere l'output SQL. E troviamo anche molti risultati interessanti. Quindi, riguardo all'analisi dei modelli monolingue, valutiamo su due gruppi di modelli, inclusi i decodificatori PDR, che stanno per decodificatori multilingue pre-addestrati con decodificatori basati su puntatori, come XLMR più PDR e BERT più PDR. E valutiamo anche i modelli encoder-decoder, che sono modelli encoder-decoder multilingue pre-addestrati, come MBART e MT5. Abbiamo scoperto che l'encoder-decoder ottiene le migliori prestazioni su tutti e nove i dataset. E valutiamo su MT5 e XLMR più PDR in impostazione multilingue. Abbiamo scoperto che l'encoder-decoder o l'encoder-PDR possono essere migliorati addestrando in una miscela di varie lingue. E abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che le prestazioni dell'inglese scendono in sette dataset e guadagnano solo in tre dataset. Penso che questo sia noto come maledizione della multilingue. Confrontiamo anche il divario di prestazioni multilingue. In questa figura, la linea blu è il trasferimento few-shot multilingue. La linea arancione è il trasferimento zero-shot multilingue, mentre la linea verde è l'impostazione monolingue. Abbiamo scoperto che confrontando la linea verde e arancione, abbiamo scoperto che per l'impostazione zero-shot, il divario di prestazioni del trasferimento multilingue è significativo. E confrontando la linea blu e arancione, abbiamo scoperto che per l'impostazione few-shot, il divario di trasferimento si accorcia rapidamente. Abbiamo anche scoperto alcuni altri risultati interessanti. Ad esempio, l'encoder-decoder supera i lavori precedenti o ha raggiunto risultati comparabili. Rappresentare sulla lingua naturale inglese può aumentare significativamente le prestazioni del few-shot sulle lingue naturali di destinazione. E abbiamo scoperto che i modelli di linguaggio multilingue come CODIS e BLUE sono ancora adeguati per i compiti di analisi semantica multilingue. Per riassumere, abbiamo costruito Examplar, un benchmark unificato per l'analisi semantica multilingue con più lingue naturali e rappresentazioni principali. Condottiamo uno studio di benchmark completo su tre tipi rappresentativi di modelli di linguaggio multilingue. E i nostri risultati mostrano molti risultati interessanti e altro ancora. E benvenuti a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Szpilkowski e questa presentazione riguarda la struttura di dipendenza della coordinazione. Come forse sapete, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci corpus. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie è tale che il primo congiunto è il capo dell'intera struttura coordinata, quindi in questo caso Lisa. Un approccio simile è assunto nella teoria del testo semantico di Igor Milchuk, dove di nuovo, l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici, giusto? Escludono uno dei congiunti. Ora, ci sono anche approcci simmetrici alle strutture coordinate come l'approccio praghese, l'approccio guidato dalla congiunzione assunto nei treebank di dipendenza praghesi, dove le strutture coordinate sono guidate dalla congiunzione. Quindi otteniamo dipendenze da n a tutti i congiunti. E infine c'è anche un approccio multi-testuale che è usato per esempio nella grammatica delle parole di Dick Hudson dove, per così dire, tutti i congiunti sono capi della struttura coordinata. un approccio multi-testuale che è usato per esempio nella grammatica delle parole di Cutson dove per così dire tutti i congiunti sono capi della struttura coordinata quindi otteniamo dipendenze dal governatore qui ama tutti i congiunti separatamente questi sono bartoni che ora l'obiettivo di questo articolo è produrre due e contro le strutture asimmetriche di coordinazione come queste due. Ok, l'argomento si basa sul principio della minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi. Quindi in inglese, come forse sapete, gli oggetti diretti preferiscono essere vicini al verbo mentre gli aggiunti possono essere più lontani, giusto? quindi March ha letto ieri è ok perché l'oggetto diretto it è vicino al verbo mentre gli oggetti diretti di March preferiscono essere vicini al verbo, mentre gli aggiunti possono essere più lontani, giusto? Quindi March ha letto ieri è ok perché l'oggetto diretto it è vicino al verbo, mentre March ha letto ieri è molto peggio, giusto? Perché qui tra il verbo e l'oggetto diretto, c'è un aggiunto ieri. Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo l'aggiunto. Questo è illustrato qui. Quindi entrambe queste frasi sono ok. March ha letto questo libro assolutamente affascinante sul BCS oggi. Va bene. In un certo senso, invece di it, abbiamo questo lungo NP. Ma è anche ok dire March ha letto ieri questo libro assolutamente affascinante sulle api. Quindi il ragionamento qui è che questo è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio della minimizzazione della lunghezza della dipendenza, che dice che sono preferite dipendenze più corte. Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dipendenza da red all'aggiunto di lunghezza sette misurata in parole e da red a book di lunghezza quattro. Quindi insieme è 11. Quando si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Quindi invece di 11, sei, molto più corta. Ecco perché questo suona abbastanza ok. Viola un principio, ma ne soddisfa un altro. Ok, quindi quello che abbiamo fatto, abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vediamo l'articolo perché non abbiamo usato le dipendenze universali. e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono ad essere più corti. non abbiamo usato le dipendenze universali e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono ad essere più corti quindi sale e pepe e non pepe e sale misurati in sillabe e anche l'osservazione che è stata fatta di passaggio che questa tendenza cresce con la lunghezza la differenza di lunghezza quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo più forte. Giusto. Quindi la proporzione è più grande del congiunto corto di sinistra. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente giusto quindi il governo è a sinistra in questo esempio ho visto Bart e Lisa quindi è il governatore è a sinistra uh è assente nel secondo esempio Homer è venuto e ha starnutito qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno giusto quindi in tali casi uh il congiunto di sinistra preferisce essere più corto più è la uh la differenza tra i due congiunti. Tuttavia, quando la governance è a destra qui, la sinistra governa la coordinazione, t e net, questo effetto scompare. Quindi lo dimostriamo misurando la lunghezza in caratteri, questa è la prima colonna in sillabe, la colonna centrale e in parole la colonna di destra. Quindi mi concentrerò su quella di destra. Quello che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto di sinistra ad essere più corto cresce costantemente con la differenza assoluta in parole. E lo stesso è osservato quando non c'è un governatore, come nella coordinazione di frasi. Ma quando il governatore è a destra, questa tendenza scompare. E dimostriamo nell'articolo come questo fornisce un argomento contro le strutture asimmetriche di coordinazione come queste due e a favore delle strutture simmetriche come queste due. Quindi vedere l'articolo per il pieno accordo e gli argomenti, scusa, e parlate con noi della sessione dei poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Ciao! Mi chiamo Kaio Yin, e presenterò il nostro lavoro intitolato, Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati. Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, Andre F.D. Martins e Graham Newbig. Quindi molte traduzioni dipendono dal contesto. Ad esempio, come tradurremo \"mole\" in questa frase? Beh, se la frase precedente fosse, le cose potrebbero diventare pericolose se i ministri lo scoprono, allora \"mole\" si riferisce a uno spia. Ma se la frase precedente fosse, potrebbe essere qualcosa di serio, dottore? Allora \"mole\" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia, e quindi anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre casi come questi è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come il BLUE incapaci di catturare queste traduzioni. E alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana. In questo lavoro, abbiamo cercato di rispondere a queste due domande. Primo, quando la traduzione richiede contesto? E secondo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura per l'uso del contesto da parte dei modelli di traduzione automatica. E questo si fa misurando quanta informazione il contesto C fornisce sull'obiettivo Y, data la sorgente X. Potete pensare a CXMI come all'informazione guadagnata dal dare contesto al modello. In questo lavoro, estendiamo CXMI al CXMI puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare a parole che hanno un alto P6MI come quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con alto P6MI per cercare schemi tra queste parole. E svolgiamo la nostra analisi su trascrizioni di TED Talk che sono state tradotte dall'inglese in 14 lingue diverse. Eseguiamo la nostra analisi a tre livelli diversi. Primo, guardiamo ai tag di parte del discorso che hanno alti mezzi PCXMI. E questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un PCXMI relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. E allo stesso modo, troviamo che alcune lingue richiedono anche contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo agli elementi del vocabolario che hanno un alto PCSXMI medio su tutte le sue diverse occorrenze. Questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di usare la stessa traduzione all'interno del documento. E allo stesso modo, troviamo che il contesto è supportato per tradurre nel giusto livello di formalità. E infine, guardiamo a diversi token individuali che hanno un alto PCXMI. E questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi. Quindi ora usiamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso, o MUDA. Possiamo quindi usare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso, o MUDA. Possiamo quindi usare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso, o MUDA. Possiamo quindi usare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso, o MUDA. Possiamo quindi usare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso, o MUDA. Possiamo quindi usare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso, o MUDA. Possiamo quindi usare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso, o MUDA. Possiamo quindi usare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso, o MUDA. Possiamo quindi usare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso, o MUDA. Possiamo quindi usare il tagger per identificare le parole che appartengono al fenomeno, e le lingue hanno diverse proporzioni di questi fenomeni del discorso. Quindi usiamo il tagger MUDA applicandolo sul corpus parallelo che vogliamo usare per la valutazione. E applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il tagger MUDA ha identificato. E infine, usiamo il nostro benchmark così come altre metriche per valutare diversi modelli sulla traduzione automatica a livello di documento. Prima di tutto, quando usiamo le metriche a livello di corpus, quindi per il blue blue troviamo che i modelli agnostici al contesto hanno le migliori prestazioni, ma poi se usiamo COMET, i modelli consapevoli del contesto hanno le migliori prestazioni. E se usiamo la misura word-f, allora i modelli con o senza contesto hanno prestazioni confrontabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo solo le metriche a livello di corpus. Ora usiamo il benchmark MUDA per valutare i modelli, e troviamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non usano il contesto per certi fenomeni del discorso, come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non hanno usato il contesto su altri fenomeni come ellissi, pronomi e forma verbale. Quindi questo suggerisce in qualche modo dove avremmo bisogno di vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali, e il nostro benchmark mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, eseguiamo un'analisi guidata dai dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto. E poi usiamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni del discorso i modelli possono gestire bene o no, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per l'attenzione. Ci vediamo a Toronto!"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa di dottorato al primo anno alla Carnegie Mellon University, e oggi presenterò il vostro lavoro, Anal Positionality, Characterizing Designed Biases of Datasets and Models. Questo lavoro è stato realizzato in collaborazione con alcuni ricercatori dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Sante, Ronan Labrasse, Katarina Aranica e Martin Sapp. Quindi, iniziamo immaginando che state lavorando per un giornale e state setacciando i commenti sotto il vostro articolo di notizie cercando di rimuovere i contenuti tossici. Potreste rivolgervi a un'API popolare come Perspective API per la rilevazione della tossicità, e questo funziona davvero bene se siete Carl Jones, dove Perspective API è in grado di rilevare correttamente le istanze tossiche. Ma questo non è realmente il caso per Aditya Sharma, dove Perspective API non è altrettanto sensibile ai termini offensivi che sono più comuni nei contesti indiani. Questo è un esempio di pregiudizio di progettazione, dove vediamo differenze sistematiche di performance della tecnologia tra le popolazioni. I pregiudizi di progettazione come quello che abbiamo appena visto prima possono verificarsi a causa della posizionalità dei ricercatori NLP e degli sviluppatori di modelli. La posizionalità è semplicemente le prospettive che le persone detengono come risultato della loro demografia, identità ed esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, specificamente negli spazi accademici femministi e queer. E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati e esiti perché può cambiare le decisioni che i ricercatori prendono. E quindi una domanda che le persone potrebbero porsi è: i set di dati e i modelli hanno una posizionalità? E non stiamo cercando di dire che i modelli stessi e i set di dati stessi hanno identità demografiche ed esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare che i modelli e le celle e i set di dati stessi hanno identità demografiche ed esperienze di vita ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità rispetto ad altre quindi i lavori precedenti hanno suggerito alcune prove aneddotiche di avere una posizionalità come le lacune culturali e i modelli e i set di dati così come definizioni teoriche della posizionalità dei modelli. Tuttavia, questi lavori non esaminano realmente il confronto tra gli utenti finali e i set di dati e i modelli stessi. Studiare la posizionalità dei modelli e dei set di dati è sempre più importante poiché i compiti NLP diventano più soggettivi e socialmente orientati. È difficile caratterizzare come queste posizionalità siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API. Quindi, per studiare la posizionalità dei set di dati e dei modelli, confrontiamo effettivamente le annotazioni con gli utenti reali con i set di dati e i modelli esistenti. Facciamo questo attraverso il nostro framework NL posizionalità. Il nostro framework funziona in due passaggi principali. Il primo passo è ri-annotare i set di dati con annotatori diversi. E scegliamo di farlo guardando la demografia degli annotatori dei set di dati originali, perché di solito solo pochi annotatori annotano ogni istanza e perché la demografia è raramente raccolta e condivisa. E quindi scegliamo di ri-annotare i dati per ottenere molti annotatori per istanza e per ottenere un ricco insieme di dati demografici. Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando un punteggio di correlazione di Pearson. E quindi, il nostro framework si differenzia effettivamente dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con i modelli e i set di dati, le previsioni e le etichette, rispetto a guardare solo l'accordo degli annotatori o le distribuzioni degli annotatori di modellazione. Il nostro framework è in gran parte abilitato da Lab in the Wild, una piattaforma di crowdsourcing online dal nostro collaboratore HCI. E Live in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi. piattaforma di crowdsourcing, ex collaboratore HCI. E Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi rispetto a piattaforme come MTurk, che hanno in gran parte partecipanti dagli Stati Uniti o dall'India. E inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità. Ospitiamo due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggeranno una situazione dal set di dati di chimica sociale e poi scriveranno quanto una situazione è socialmente accettabile. Dopo, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'IA e altri. Poi confrontiamo queste annotazioni con la chimica sociale, Delphi e GPT-4. Poi replichiamo una configurazione molto simile per il test di rilevamento della tossicità e del discorso d'odio. abbiamo confrontato queste annotazioni con la chimica sociale, Delphi e GPT-4. Abbiamo poi replicato una configurazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, dove leggeranno un'istanza da DynaHate e scriveranno se pensano che sia un'istanza di discorso d'odio. Poi abbiamo confrontato queste annotazioni con DynaHate, Perspective API, Rewire API, Hate Roberta e GPT-4. Il nostro studio, alla fine, ha accumulato oltre 16.000 annotazioni da oltre mille annotatori di 87 paesi. Quindi ora siamo meglio attrezzati per rispondere a chi si allineano maggiormente i set di dati e i modelli NLP. Scopriamo che c'è una posizionalità in NLP. Ad esempio, scopriamo che i set di dati e i modelli sono più allineati ai paesi anglofoni. Quindi per l'analisi dell'accettabilità sociale di GPT-4, scopriamo che è più allineato ai paesi confuciani e anglofoni. Scopriamo che dyna-hate è anche più allineato ai paesi anglofoni. Scopriamo anche il maggior allineamento con le persone che hanno un'istruzione universitaria. Quindi per GPT-4 nel compito di accettabilità sociale, scopriamo che è più allineato alle persone con un'istruzione universitaria o di scuola superiore. E troviamo lo stesso per Dynahate, dove è più allineato alle persone con un'istruzione universitaria. Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di ciò è che i set di dati e i modelli sono meno allineati alle persone non binarie rispetto alle controparti maschili e femminili. Troviamo questo nel compito di accettabilità sociale di GPT-4 così come nell'analisi del compito DynaHEAT. Quindi, dato che c'è una posizionalità in NLP, cosa possiamo fare al riguardo? Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutte le scelte di progettazione rilevanti durante il processo di ricerca. E l'altra è fare ricerca NLP con la lente del perspectivismo. La nostra terza raccomandazione è costruire set di dati e modelli specializzati all'interno di quattro comunità specifiche. E un buon esempio di ciò è l'Iniziativa Masakane. Vogliamo sottolineare che l'NLP inclusivo non è solo fare, sapete, tutte le tecnologie funzionano per tutti. E quindi, questo conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di controllare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Salve, parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indirette per la selezione di entità, in cui introduciamo il corpus delle entità alternative. Mi chiamo Jawad Hosseini, e questo è un lavoro congiunto con Philip Radlinski, Sylvia Parity e Annie Lewis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considerate questa domanda alternativa: intendete Easy on Me o I got a feeling? Un utente vuole selezionare tra una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone, Easy on Me, o la sua posizione, la prima. A volte, però, un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone o le pronunce sono troppo simili tra loro e difficili da disambiguare, o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di differenze indirette: ad esempio, quella più recente o la canzone che non è energica. Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità da parte degli LLM. Non siamo a conoscenza di un dataset pubblico su larga scala per questo compito, quindi ne abbiamo creato uno utilizzando l'annotazione collettiva. Il nostro dataset copre tre diversi ambiti: musica, libri e ricette. La nostra metodologia di raccolta del dataset enfatizza l'informalità utilizzando un setup di completamento di vignette. La vignetta ha tre bolle di dialogo. La nostra metodologia di raccolta del dataset enfatizza l'informalità utilizzando un setup di completamento di vignette. La vignetta ha tre bolle di dialogo. Nella prima bolla, Bob dice: \"Ricorda quella canzone che stavamo ascoltando ieri?\" E con questo, Bob imposta il contesto del dialogo. In questa seconda bolla di dialogo, Alice dice: \"Intendi Easy on Me o I got a feeling?\" che è la domanda alternativa. E nella terza bolla di dialogo, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio, quella più recente. Forniamo automaticamente la prima e la seconda bolla di dialogo, ma la terza è compilata dall'annotatore. La prima bolla di dialogo è scelta tra alcuni prompt manuali per ambito. La seconda, che è la domanda alternativa, è generata come segue: usiamo sempre un semplice modello \"Intendi A o B?\" dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato: quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è di solito più difficile fare la disambiguazione. Il primo è uniforme a caso, il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome \"Il ritorno\", il terzo è quando hanno descrizioni simili su Wikipedia e infine quando, ad esempio, hanno lo stesso genere o lo stesso artista, ad esempio. Quando mostriamo questa domanda alternativa agli amministratori, loro conoscono il nome di queste entità ma non necessariamente le entità stesse, quindi ciò che facciamo è mostrare alcune conoscenze di base sulle due entità: per le canzoni, mostriamo semplicemente un link alla ricerca su Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno una parte di ogni canzone e leggere su ogni canzone. Ecco, ad esempio, il risultato della ricerca su Google per la canzone \"Easy on Me\". Per i domini delle ricette e dei libri, mostriamo un po' di testo di sfondo da Wikipedia. Per le ricette, mostriamo anche le loro immagini, di nuovo da Wikipedia, in modo che gli annotatori sappiano come sono fatte. Poi chiediamo agli annotatori di scegliere una di queste entità, ad esempio qui la prima, e di descriverle utilizzando da tre a cinque espressioni di riferimento indirette, ad esempio quella con la musica per pianoforte. Ecco alcuni esempi dal nostro dataset: ad esempio, quella senza parole, non quella con il ragazzo di 12 anni, o quella fittizia, o che viene dall'Azerbaigian e così via. Il corpus delle entità ha 6.000 domande alternative in tre ambiti e ha 42.000 espressioni di riferimento indirette. I risultati con il modello T5XLARGE sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, l'accuratezza è davvero alta, è intorno al 92-95 percento, ma questo non è realistico. Se il modello linguistico ha accesso a conoscenze di base parzialmente sovrapposte, l'accuratezza è tra l'82 e l'87 percento, che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di base, l'accuratezza è solo del 60%, quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili per ambito. Ecco un link al nostro dataset. Grazie."}
