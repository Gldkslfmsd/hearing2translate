{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫马蒂亚斯·林德曼（Matthias Lindemann），今天我将向大家简要介绍我们关于在不使用树的情况下通过多集标记和潜在置换实现组合泛化的论文。这是我和我的导师亚历山大·科勒（Alexander Koller）和伊万·蒂托夫（Ivan Titov）的合作成果。组合泛化可以理解为学习者处理更深层次的递归和在训练过程中单独见过的短语的未见组合的能力。在语义解析的背景下，测试组合泛化可能如下所示。像往常一样，我们有一个训练语料集，在这种情况下是“女孩睡觉”和“玛丽知道女孩睡觉”。这些语料集与表示其核心意义的逻辑形式配对。与标准机器学习评估不同，测试集不是来自同一分布，而是包含结构上未见过的逻辑形式。在这个例子中，模型在训练过程中已经见过更浅层次的递归，并在具有更深层次递归的例子上进行了测试。朴素的序列到序列模型难以应对这种分布外泛化，并且通常会产生与输入脱节的输出。特别是，它们通常无法再现输入和输出之间的系统对应关系，例如在例子中用颜色标注的那些。解决这个问题的一种流行方法是将树集成到模型中。树的目的是捕捉将语料与逻辑形式联系起来的组合过程。这是有效的，但树通常没有给出，需要以某种方式获得。这可能很复杂，有时是一个计算上昂贵的过程。通常，这涉及到对逻辑形式进行相当多的形式特定预处理，例如处理变量符号。获得树也可能涉及到专门的语法感应程序。在这篇论文中，我们不使用树，而是引入了一个直接建模输入片段与输出片段之间对应关系的神经序列到序列模型。我们首次展示了在没有树的情况下对更深层次递归的强大泛化能力。我们的方法通过两步从输入预测输出。首先，我们用一个无序的多集标记每个输入标记，这些标记将出现在输出中。完成第一步后，我们得到了所有正确的标记，但它们没有排序。这就是为什么在第二步，我们使用另一个模型来预测一个置换，将它们排列成正确的顺序。我们引入了一种新的方法来预测一个置换，该方法对可能的置换没有硬约束。这使得我们的方法非常灵活和富有表现力。从概念上讲，我们的置换模型大致是这样工作的。我们从左到右遍历输出，并确定每个位置放置哪个多集标记。对于第一个输出位置，我们简单地选择一个，如图所示。然后，我们跳到下一个多集标记，以确定输出中的第二个标记。我们以类似的方式跳到另一个多集标记。我们继续这个过程，直到第一个阶段的所有标记都被访问过一次。为了给你们一个实验结果的预告，我们在这里将我们的方法与其他无树模型在COGS基准上进行了比较。我们的模型在对更深层次递归的泛化方面远远优于其他模型。尽管如此，其他一些类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的技术挑战。首先，输入和输出之间的对齐在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个多集，这给训练带来了挑战。此外，有时有多个置换与数据一致，但语言学上正确的那个是潜在的。我们通过在训练中诱导对齐来解决这个问题。我们的置换方法非常灵活，但它带来了一个挑战，即找到得分最高的置换是NP难的。这是因为这与旅行商问题有关。我们用一种GPU友好的连续松弛方法来近似这一点，这也使我们能够通过解决方案进行反向传播并学习语言学上更可信的置换。如果你想了解更多关于我们的实验以及我们如何应对这些挑战，请查看我们的论文或来我们的海报。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Myra，今天我要谈谈我们的论文《标记化角色：使用自然语言提示来衡量语言模型中的刻板印象》。这项工作是与 Esen Dermusch 和 Dan Jorofsky 合作完成的。近年来，许多研究已经记录了大型语言模型或 LLM 中社会偏见和刻板印象的普遍存在。然而，这些方法有各种局限性。它们通常依赖于非常耗时的手工构建数据集，而且通常只测量非常具体的刻板印象，这意味着它们不能很好地推广到其他人口或情境，或者它们只是捕捉到非常广泛的联想，如对特定群体的负面联想。此外，这个领域的多数工作都没有考虑交叉性，即多方面社会身份可以加剧偏见，并成为伤害的独特来源。为了克服这些局限性，我们依赖于这些较新的指令微调 LLM 非常擅长响应提示中的指令这一特性。因此，我们可以要求模型生成一个角色，即使用像“想象你是一个亚洲女性，描述自己”这样的提示来描绘一个虚构的个体。我们可以立即看到，这种方法对任何人口都非常具有可推广性，因为我们只需在提示中指定任何我们想要的身份标记。以下是 GPT-4 生成的几个示例。我们立即看到，虽然输出不是传统意义上的明显消极或有毒，但有一些有趣的模式。亚洲女性被描绘成不引人注目。中东女性则被用词如“异域风情”来描述，就像描述一个迷人的地区一样。而两个有色人种角色都提到了祖先，而白人角色则没有任何这样的描述。为了捕捉这些模式，我们的方法有两部分。第一部分是生成这些角色。我们的提示是为了生成这些角色，灵感来自一项研究，他们在研究中给人类受试者提供了这些提示，发现通过给人类受试者这些提示，他们也能揭示出种族刻板印象。这也使得我们可以直接比较我们生成的角色与人类书面回应。第二部分是标记词，这是一种识别区分标记组和非标记组的词的方法，我稍后会详细解释。这种方法的好处是我们能得到非常具体的刻板印象和模式，而无需依赖任何特定的词汇。因此，标记词方法借鉴了社会语言学中的标记性概念，该概念指出存在一个非标记的默认值，任何与该默认值不同的群体在语言上都是标记的，例如，词“男人”或“战士”通常与男性相关联，因此当人们描述一个女性战士时，他们通常会具体说明“一个男人战士”，并用“女性”标记该词。更广泛地说，社会中的主导群体在语言和社会上都是非标记的，而边缘化群体通常是标记的。因此，在我们的方法中，我们首先指定非标记和标记群体是什么。然后，我们使用战斗词方法比较角色，这基本上是使用加权对数几率比来区分每个标记群体的顶级词。因此，例如，对于黑人女性的角色，我们会进行战斗词比较，并将对数几率比与白人角色和男性角色进行比较，因为这两个是两个对应的非标记群体。现在，让我们来看看一些结果。首先，我们使用刻板印象词汇表，发现生成的字符包含比人类书面角色更多的刻板印象。然而，当我们实际查看词汇表中词的分布时，我们发现了一些非常不同的东西。因此，虽然生成的字符具有更高的词汇表词率，但人类书面角色具有更广泛的词分布，而生成的字符中的刻板印象词实际上只是“高大”和“运动健将”。所以，实际上只有积极的或至少是非消极的词。事实上，这个词汇表根本没有很好地捕捉到我们在早期幻灯片中看到的许多有害模式。因此，相反，为了做到这一点，我们将转向我们标记词方法的结果，以展示这些看似积极的词如何促进刻板印象和本质化叙述。在我们的分析中，我们揭示了这些看似积极的描绘反映了哪些有害模式。首先，对于标记群体，顶级词包括文化、传统、自豪和异域风情。这些词只通过它们与身份的关系来定义这些群体，并使它们与白人规范区分开来。这为这些群体造成了长期的歧视和异化的遗产。此外，这些词反映了许多常见的陈词滥调，特别是对于有色人种女性。例如，描述拉丁美洲女性的词包括充满活力和曲线美，这与热带主义的陈词滥调相连。对于亚洲女性，这些词包括娇小、细腻和丝滑，这与亚洲女性长期以来被过度性化、被视为非常温顺和顺从等长期的历史相连。最后，对于黑人女性，我们看到一些顶级词包括坚强和韧性。这与人们所谓的“坚强黑人女性”原型相连。虽然乍一看这听起来很积极，但已有研究表明，这种原型实际上非常有害，因为它给这些人口施加了很大的压力，要求他们在面对社会障碍时保持韧性和坚强。因此，与其真正致力于改变这些障碍，它反而给这些人施加了克服它们的压力，这导致了这些人非常消极的健康结果，以及其他危害。更广泛地说，我们发现每个标记群体的词几乎只反映了非常本质化的叙述。基于这些模式，我们对模型所有者提出了三条建议。首先，我们作为研究人员，应该关注积极的刻板印象和本质化叙述。我们还应该使用交叉视角来研究偏见和危害，因为如果不这样做，可能会忽略很多东西。最后，关于减少偏见的透明度应该真正提高，因为，例如，像这些积极的刻板印象一样，我们不知道这是因为存在某种奇怪的、过度夸张的价值对齐，还是因为其他一些反刻板印象方法导致了这些有害模式。我们真的无法做出任何假设或进一步研究，除非有更多的透明度。非常感谢大家的聆听。祝你在 ACL 玩得开心。"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是詹姆斯·芬奇，我是萨拉·芬奇。今天我们将向大家介绍ABCeval，这是一种全新的评估对话式人工智能的维度方法。这项工作由埃默里大学的吉诺·崔教授领导的埃默里大学自然语言处理实验室完成，并与亚马逊Alexa AI合作完成。假设您刚刚开发了一个对话模型，并且想知道它与当前的先进技术相比表现如何。常见的做法是使用人工评估，例如让人工评判员选择两个对话中哪一个更好，或者根据李克特量表对对话进行评分。这些方法在提供整体对话质量的全面评估方面效果良好，但对话质量有许多方面。因此，您可能希望评估对话质量的多个维度，以便更细致地了解模型的优势和劣势。一种方法是简单地让人工评判员评估对话质量的几个维度，例如模型响应的相关性，使用现有的比较方法或李克特量表方法。然而，我们相信存在一种更精确、更可靠的维度对话评估策略。我们的方法试图通过明确标注每个模型响应是否表达了某些行为（例如，用无关信息回应或自相矛盾）来减少人工评估的主观性。我们称这种方法为对话行为标注，简称ABCeval。我们开发了这种方法，以全面覆盖最近文献中建议影响对话质量的对话模型行为。ABCeval评估模型是否忽视其对话伙伴，是否说无关的话，是否自相矛盾或与对话伙伴矛盾，是否产生错误的事实或违反常识，以及模型是否成功或未能表现出同理心。为了确定哪种评估方法最有效，我们选择了四种最先进的对话模型，并使用ABCeval对每种模型的100个人机对话进行了评估。为了进行比较，我们还使用三种现有方法对这些对话进行了评估：回合级别的李克特评分、对话级别的李克特评分和对话级别的配对比较。对于每种现有方法，我们收集了对对话中最常见的八个方面的评估，因为这是评估对话模型的标准做法。从我们对这些评估结果的分析中，我们发现ABCeval行为标签总体上比现有方法收集的标签更可靠，这是通过对100个双重标注对话的评判员间一致性来衡量的。此外，ABCeval标签比现有方法产生的指标更能预测整体对话质量，这可以通过这个简单的线性回归分析来证明。例如，eval标签比现有方法产生的指标更能预测整体对话质量，这可以通过这个简单的线性回归分析来证明。例如，您可以看到测量自相矛盾和对话伙伴矛盾的回合比例分别解释了对话质量的5%和10%，而平均李克特一致性得分只解释了4%或更少。最后，我们使用逐步线性回归检查了每个评估指标是否捕捉了对话质量的独特方面。您可以看到，所有ABCeval指标的组合解释了超过25%的对话质量，而当您逐个移除这些指标时，大多数都会导致丢失大量关于质量的信息。另一方面，所有回合级别的李克特指标的组合解释了远低于质量的百分比，而且这些指标中很少有携带独特信息。这些可靠、信息丰富且独特的ABCeval指标使我们能够以比以前方法所能实现的更高的分辨率评估对话式人工智能。您可以在我们的实验结果中看到，仍然存在一些挑战，并且这些挑战已经被精确量化。例如，我们在测试的机器人中，大约20%的响应违反了常识。它们在大约15%的响应中产生无关信息，并且在大约10%的时间里自相矛盾或与对话伙伴矛盾。随着该领域的快速进步，许多这些错误率可能会在新发布的模型中有所下降。然而，这更说明了追求可靠且精确的评估指标以比较模型的重要性。我们希望ABCeval可以被该领域的其他人作为朝着这个方向迈出的有意义的一步，我们期待看到对话式人工智能在未来几个月和几年中的发展。感谢您的观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫 Vasudha，是 Stony Brook 大学计算机科学博士生。我将介绍我们被 ACL 2023 接收的长文论文《用于检测认知失调的迁移学习》，该论文解决了稀有类别的挑战。我们首先定义了认知失调，并解释了为什么它是语言研究中一个重要的课题。简单来说，认知失调是指两种信念或行为不一致。例如，一个人说：“我知道香烟会害死我”，然后又说：“会议结束后，我抽了几口烟。”这种信念和行为不一致，处于失调状态。此外，提到“如果没有它们，我可能无法保住工作”来为第二次吸烟行为辩护，说明它们之间存在共鸣关系。虽然认知失调是我们日常决策中非常常见的一种现象，但在其他类型的语篇关系中，它们在语言中表达的非常罕见。那么，为什么这很重要呢？研究认知失调可以帮助我们理解人们之间的分歧影响，跟踪信念、价值观和态度变化的趋势。高认知失调也与焦虑症有关，可以更好地理解人们的心理健康。研究语言中表达的认知失调也有助于理解极端主义和弱势群体的两极分化。最后，理解认知失调对于理解个体的认知风格非常重要，有助于我们更好地理解决策过程。为了实现认知失调资源的目标，我们对失调关系进行了大规模标注。我们采用了如图所示的失调优先方法。我们使用 PDTV 解析器解析推文，并根据我们的论文中描述的指南对语篇单位对进行标注。如图所示，只有 3.5% 的标注对中发现了失调。在收集了大约 1000 对语篇单位示例后，我们对初始分类器进行了训练，仅训练了 43 个失调示例。不出所料，分类器的表现并没有比随机猜测好多少。鉴于失调出现的频率低，且之前没有任何此类数据集，我们面临着绝对稀有性的问题。为了缓解这个问题，我们尝试了迁移学习和主动学习的组合，以便在较少的标注轮次中收集更多的失调样本，降低整体标注成本，同时提高失调检测。由于初始模型根本无法捕捉到失调类别，我们通过从密切相关任务中转移权重来启动主动学习过程。我们从两个不同的任务中转移：独立于主题的失调立场分类，该任务确定两个人在不同主题上的辩论陈述是否一致或不一致（这里称为辩论），以及 PDTB 的扩展和比较类别的二元分类，因为这两个与共鸣和失调的概念密切相关，我们称它们为 CEE。我们发现，通过转移，在标注数据集上的零样本性能已经远好于随机猜测，最佳的 AUC 为 0.62。此外，通过训练轮次的迭代更新模型。累积更新了迄今为止从主动标注中收集的所有数据，而迭代更新则在最新的数据集中进行训练。在不同的策略中，我们发现累积的性能在各个方面都等于或优于迭代。接下来，为了提高失调示例的数量，我们使用稀有类别概率策略，选择在任何主动学习轮次中当前模型认为高度可能失调的示例。我们将此与其他最先进的策略进行比较，尽管差异很小。请注意，随机猜测的性能明显较低。在使用两种最佳策略进行进一步的主动学习轮次后，我们将距离分类 AUC 提高到 0.75，这是我们迄今为止在该任务上取得的最佳性能。我们还检查了每种策略对标注质量和标注员成本的可行性。我们发现 PRC 在失调比例最高，并且对稀有类别最有效。然而，标注员也发现这些示例很困难。总之，我们发现 PRC 是一种简单的主动学习策略，用于稀有类别获取和冷启动主动学习，通过适当设计的迁移学习任务可以显著帮助。我们还发现，迭代更新对于从不同领域进行迁移学习很有用，而领域内的主动标注则受益于累积更新。这些是我们代码、数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Akshata，今天我和我的合著者 Martin 将展示我们的工作，Kipma 步骤，评估从多个来源整合知识的方法。这项工作是麦吉尔大学、Mila 和微软研究院的合作项目。国家语言理解模型利用各种知识来源，例如参数中包含的知识，通常通过预训练获得，以及推理时输入中给出的知识。最近在问答等任务中的工作表明，模型可以使用预训练的时间知识来解决任务。但是，自然语言理解通常需要在推理时也提供知识。例如，在句子“约翰在电视上看到了新当选的总统。”中，预训练的参数可能包含关于总统做什么和电视是什么的信息，但它们无法可靠地知道约翰这个特定事件的实体是谁，或者新总统是谁，因为自预训练以来总统可能已经换了。因此，成功处理知识密集型 NLU 任务的模型需要能够整合和使用预训练时间和推理时间知识。在这项工作中，我们提出了一套用于知识整合的诊断测试。我们引入了一个核心参照解析任务，旨在探究利用不同来源可用知识的能力。我们使用人类研究参与者评估数据集，并建立核心参照解析模型。以下是我们数据集的一个例子。Servin 是法官。Kia 是面包师。Servin 和 Kia 在公园见面。经过一天在法庭上审理案件后，他很高兴放松一下。这里的任务是确定代词 he 指的是哪个正确的实体，在这种情况下是 Servin。给定代词的解析需要两种类型的信息。首先，实体特定知识，例如 Servin 是法官。其次，背景知识，例如法官在法庭上审理案件。一般来说，背景知识是在大型语言模型的预训练过程中学习的，而实体特定知识通常在推理时观察到。我们改变这两种信息的可用性，使其可能只在一个来源中找到，或者在多个来源中找到。我们定义了三个 KITMOS 设置。首先，我们有典型的设置，背景预训练，其中假设背景知识在预训练时可用。其次，有一个背景两者设置，其中背景知识在预训练时间和推理时间都可用。最后，背景推理设置，其中两种知识类型只在推理时可用。最后一个设置特别有趣，因为它模拟了解决任务所需的背景知识不是模型预训练数据的一部分的情况，例如，自预训练以来出现了新的职业。以下是关于我们如何控制事实和真实来源可用性的一个例子。在背景预训练设置中，我们假设背景知识政治家寻求政府选举席位包含在预训练参数中。而在 3 英寸时间范围内，我们提供了特定于 Chichester 的知识，即 Chichester 是政治家。在背景两者设置中，我们还提供了不仅是特定于实体的知识，还有推理时间范围内关于政治家的背景知识。在背景推理设置中，我们提供了虚构职业 Meritur 而不是政治家，因为 Meritur 不太可能包含在预训练参数中。我们使用人类研究参与者和建立的核心参照解析模型对数据集进行了评估。在这个图中，我们展示了在背景预训练设置的最困难变体上表现最好的模型的结果。没有在 KITMOS 上进行任务特定训练，两个模型的表现都不好。然而，当在 KITMOS 上进行训练时，C2F 和 BFQF 的表现都显著优于随机选择。这表明，当在一般核心参照解析数据集上进行训练时，模型学会利用表面线索，这些线索在测试 KITMOS 时已被移除，因此对模型无用。使用虚构知识的额外实验表明，即使是表现最好的模型也无法可靠地整合仅在推理时提供的背景知识。总结我们论文的主要结论。许多核心参照解析模型似乎无法在没有任务特定训练的情况下对来自不同来源的知识进行推理。然而，通过任务特定训练，一些模型成功地整合了来自多个来源的知识。尽管如此，即使是表现最好的模型似乎也难以可靠地整合仅在推理时呈现的背景知识。如果您对更多细节感兴趣，请参阅我们的论文并在 GitHub 上查看数据集。感谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是特伦托大学和布鲁诺·凯斯勒基金会的萨拉·帕皮，我将简要介绍我们的论文《注意力作为同步语音翻译的指导》，这是我和马泰奥·内格里、马可·图尔奇合作完成的。什么是同步语音翻译？同步语音翻译或SimulST是指将口语实时翻译成另一种语言的文本，实现跨语言交流。那么，当前的SimulST模型有哪些问题呢？通常会训练特定的架构，引入额外的模块进行优化。例如，训练过程漫长且复杂，涉及不同的优化目标，以及训练和维护多个模型以达到不同的延迟机制，例如训练一个平均延迟为1秒的模型和另一个延迟为2秒的模型等。那么，我们的解决方案是什么呢？首先，使用现有的离线SD模型，无需重新训练或采用特定的SimulSD架构。对于每个延迟机制只使用一个模型，并通过特定参数处理延迟。此外，利用模型通过音频输入和文本输出之间的注意力机制（即交叉注意力机制）获得的知识。您可以在右侧看到一个例子。我们的解决方案是提出EDAT或编码器-解码器注意力，这是一种策略，我们根据注意力指向的位置决定是否发出部分翻译。如果注意力不集中，即其总和低于某个阈值alpha，指向较少的lambda语音帧，这意味着接收到的信息足够稳定，则发出一个词。例如，如果我们接收一个包含“我要谈论”的语音片段，我们的模型预测德语翻译，我们将查看交叉注意力权重，我们会看到前两个词指向最早接收到的语音帧，而最后一个词指向最后接收到的语音帧，即lambda语音帧。这意味着前两个词将被发出，而由于交叉注意力的总和高于某个阈值alpha，我们将不发出最后一个词，而是等待另一个语音片段。如果我们继续接收另一个语音片段，我们的模型预测其他三个词，我们将查看交叉注意力权重，我们会看到没有词指向最后的lambda语音帧。这意味着这三个词将被发出。如果我们查看主要结果的点，我们在图表上绘制了同步语音翻译结果，其中一边用蓝色测量翻译质量和平均延迟（即延迟度量），我们还考虑了考虑到模型计算时间的计算感知平均延迟，该延迟考虑了模型预测输出的计算时间。所以，我们希望我们的曲线在这个图表上尽可能高，但也要向左移动。我们还与适用于离线模型的适当策略进行了比较，即湿键策略和局部一致性。我们还与专门为同步语音翻译定制的最先进架构进行了比较。这些是同步语音翻译策略在德语上的所有结果，我们看到ADDOUT优于所有应用于离线模型的策略，因为曲线向左移动。我们还看到，如果我们考虑实际的经过时间或计算感知时间，ADAT是最快的策略。如果您想发现更多结果，请阅读我们的论文，我们还发布了开源代码和模型。如果您想发现更多结果，请阅读我们的论文。我们还发布了开源代码和模型，并进行了同步输出，以促进我们工作的可重复性。感谢您的关注。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫朱恒。今天我要介绍我们的论文《2003年核命名实体标注器在2023年是否仍然有效？》让我们开始吧。我们的论文研究了使用命名实体识别任务或NER任务进行泛化的问题。我们观察到，近20年来，模型一直在使用Kano 2003来开发NER，这自然引发了几个问题。首先，这些模型能否泛化到现代数据？当我们开发新的标注器时，需要什么才能实现良好的泛化？同时，如果我们观察到泛化效果不佳，这些模型性能下降的原因是什么？为了研究这些问题，我们开发了Kano++数据集。这是一个我们从2020年路透社新闻中收集的数据集，然后使用相同的Cono2003标注指南对它们进行了标注。我们随后在Cono2003上微调了20多个模型。我们在Con in F1上对它们进行了评估，以评估每个模型的泛化能力。那么，实现良好泛化的条件是什么？通过我们的实验，我们发现需要三个主要条件。第一个是模型架构。通过我们的实验，我们发现Transformer模型通常能更好地泛化到新数据。第二个条件是模型大小。我们发现，通常较大的模型能更好地泛化。最后，但同样重要的是，我们都知道，微调示例的数量直接影响下游任务的性能。在这里，我们还发现，更多的微调示例实际上也能带来更好的泛化效果。关于我们的下一个问题，导致某些模型性能下降的原因是什么？我们有两个假设。第一个是自适应过拟合，这是由于反复使用相同的测试集导致的过拟合，通常表现为在新测试集上的回报递减。第二个假设是时间漂移，这是由于训练数据和测试数据之间的时间差距越来越大导致的性能下降。对于自适应过拟合，我们从右侧的图表中看到，红色的最佳拟合线有一个大于1的梯度。这意味着我们在CONO 2003上做出的每一项改进都转化为在Kano++上的超过一项改进，这意味着没有回报递减。这表明在这种情况下没有观察到自适应过拟合。那么时间漂移呢？对于时间漂移，这证实了我们的假设，即性能下降的主要原因是时间漂移。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这些条件是相辅相成的。我们不能只具备其中一个条件，而忽略其他条件。同时，我们还发现，这里的性能下降是由时间漂移引起的，而且令人惊讶的是，它不是由自适应过拟合引起的，尽管Kano 2003已经使用了20多年。所以，回到我们论文标题提出的问题，Kano 2003标注器在2023年是否仍然有效？我们发现答案实际上是肯定的。我们希望我们的论文能促使更多人研究如何提高模型的泛化能力。最后，请务必查看我们的论文和数据集，如果您有任何问题，请随时与我联系。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，欢迎来到我们的演示，我们将展示 d.plain，这是一个用于德语文本简化的新语料库，涵盖文档级别和句子级别。我叫 Regina Stodden，我将引导大家完成演示的第一部分。我们首先定义文本简化。文本简化是将文本进行改编，以提高特定目标群体（如阅读障碍者或非母语者）对文本的理解。为了训练文本简化模型，我们需要文本的平行对，例如文档或句子的平行对。在下面的例子中，您可以看到一个复杂的德语句子及其通俗语言翻译的平行对齐句子对。为了简化句子，可以采用不同的技术，例如词汇替换、短语重组、短语重新排序或插入单词。我们现在提出我们新的语料库 dplane。因为近年来，现有的语料库存在一些问题。例如，这些语料库太小，无法训练分类模型。近年来提出的其他三个模型都是自动对齐的，这意味着它们的对齐可能存在错误。因此，我们提出了新的语料库 dplane，它分为两个子语料库，dplane-apa 和 dplane-web。dplane-apa 基于使用文本。在简单的 APA 中，我们手动对齐了 483 个文档。这大约产生了 30,000-13,000 对平行句子对。对于 DeepLaneWeb，这个语料库包括不同的领域，我们还手动对齐了这 750 个文档，另一方面也使用了自动对齐方法。总的来说，我们产生了 30,450 对句子对。我们对句子对进行了更详细的分析，例如简化类型。您可以看到，圣经文本在所有级别上都比新闻文本或语言学习者文本的简化要强，例如词汇简化、结构简化或整体简化水平。此外，您可以看到，我们的 De Plplane 语料库具有不同简化转换的高优先级。例如，在 d.plane API 语料库中，我们有更多的重组和单词添加，而在 d.plane web 语料库中，我们有更多的改写。那么，我们现在来看看我们可以用这个语料库做什么。大家好，我是 Omar，我现在将谈谈我们数据集 D-plane 的使用案例。第一个使用案例，我们可以评估自动对齐方法。近年来，出现了很多对齐方法，但在机器翻译的背景下，我们有两个用不同语言编写的平行文档，我们想要从两个平行文档中提取对齐的句子，这些句子具有相同的语言、相同的内容，但它们在复杂性水平上有所不同。现在，我们有了手动对齐句子的 dplane 数据集，我们可以使用这些句子作为金标准对齐，来评估一些提出的对齐方法。我们对提出的方法进行了一些改编，并在论文中发表了所有这些改编和运行实验的代码。最后，我们得出结论，用于德语文本简化的最佳自动对齐方法是 math align 方法。您也可以在论文中找到在您自己的文档上运行此方法的代码。我们在论文中展示的第二个使用案例是通过微调语言模型来自动简化文本的案例。我们微调了两个不同的模型。我们微调了 long-impart 模型，以生成句子级别的简化。您也可以找到所有检查点，并在论文中查看我们实验的详细分数和评估指标。我们得出结论，这种基本的微调可以产生或获得比基准分数更好的分数，我们提出了这些结果作为未来的自动文本简化问题的基准。非常感谢大家的关注，我们希望在会议期间见到大家。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是复旦大学的袁思雨。我在这里介绍我们的工作，即从大型语言模型中提取脚本知识以进行约束语言规划。在日常生活中，人类通常通过遵循形式为保证脚本的逐步指令来规划自己的行动。然而，之前的研究主要集中在抽象规划上。一个好的规划者应该第一次编写脚本。规划的目标。一个抽象目标可以被不同的现实生活具体目标继承，这些具体目标具有多方面的约束。一个好的规划者应该编写合理且忠实于约束的脚本。在本文中，我们首先评估和改进大型语言模型的约束语言规划能力。由于没有具体的目标数据集来支持我们的研究，我们必须首先获取这些目标。如表所示，我们使用InstructGPT扩展了人类在循环数据获取中具有多方面约束的抽象目标。我们抽取了100个具体目标，并评估了大型语言模型生成的脚本。该表报告了结果的总体准确性。我们发现所有大型语言模型在规划具体目标方面都取得了不令人满意的结果。然后，我们进行详细分析，以调查学习模型为何会失败。图中的结果显示，生成脚本的语义完整性是可以接受的，但无法保证对约束的忠实度。我们深入研究了更细粒度的约束主题类别，具体取决于工作方式。图中的热图显示，指示性TPD的规划性能在不同类别的女孩中差异很大。之前的研究表明，轻日志模型的输出质量在高方差下下降，导致性能不佳。因此，我们采用了过度生成的Z-过滤器来提高生成质量。我们首先展示了对Instruct GPT的约束类型示例，并根据种子抽象目标获得了具体目标。然后，Instruct GPT过度生成特定目标的关键脚本。接下来，开发了一个过滤器模型来选择可行的脚本。我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度和相似度分数以衡量语义相似度。此外，我们奖励包含目标约束关键词的脚本。如果目标在目标集中得分最高，我们才保留该脚本。通过我们的方法，InstructZBT可以生成更高质量的脚本。我们的方法大大提高了规划能力，无论是在语义完整性还是对约束的忠实度方面。由于大型语言模型的部署成本高，因此必须使较小和专业模型具备语言规划能力。创建数据集是实现这一目标的必要步骤。然而，之前的研究并未实现对具体目标的规划，手动数据集标注成本高昂。因此，我们遵循符号知识蒸馏的理念，从大型语言模型中提取约束语言规划数据集。我们应用我们的方法来构建一个约束语言规划的数据集，命名为CodeScript。总共，我们生成了55,000个带有脚本的具体目标。为了确保验证和测试网站的质量，我们请云端工人找到修订的错误样本。该图显示了代码脚本的约束分布。我们发现代码脚本在生成的具体目标中表现出高度的赞赏。有了代码脚本，我们可以将较小但专业的模型用于约束语言规划。我们发现Antune的TFI在成本率上可以生成0的平方根。有了代码脚本，我们可以将较小但专业的模型用于约束语言规划。我们发现CodeScript上的T-file函数可以生成比大多数大型语言模型更高质量的脚本，表明在适当的数据集上进行适当训练的小型模型可以支持大型模型。总之，我们建立了约束语言规划问题。我们评估了大型语言模型的约束语言规划能力，并开发了一个过度生成过滤器方法研究语言规划。感谢您的时间。有关CodeScript的更多详细信息，请参阅我们的论文。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Yanis Lavrac，我将向大家介绍我们在 Dr. BERT 上的工作，这是一个针对法语生物医学和临床领域的强大预训练模型。在本次演讲中，我们首先讨论医疗保健中的语言建模。然后，我们将介绍我们文章的主要贡献。我们介绍了第一个法语生物医学模型，名为 Dr. BERT，它基于 Roberta，并在 NACHOS 上进行训练，NACHOS 是一个从网络上抓取的医学数据集。我们还介绍了多个预训练设置和数据源的模型比较。然后，我们展示了我们在法语的 11 个生物医学和临床下游任务上的结果。最后，我们总结了实验，并向大家提供了更多关于如何访问这些模型的详细信息。法语的生物医学和临床下游任务。最后，我们总结了实验，并向大家提供了更多关于如何访问这些模型的详细信息。自 2018 年发布以来，BERT 已成为解决自然语言处理任务的最有效方法之一，相比历史上的静态和上下文方法（如 Word2Vct、fast text 或 NWO），BERT 提供了巨大的性能提升。从那时起，该模型已被适应到许多其他语言，如法语中的 Camembert，以及其他领域如生物医学中的 permit-bert 和 bio-bert，以及临床中的 clinical-bert，但大多数是英语。其他语言的专用模型很少，并且通常基于持续预训练，因为缺乏领域内的数据。然而，到目前为止，法语还没有任何开源的生物医学模型。所以我们问自己，对于广泛的用途，最合适的 数据来源是什么？当前的数据是否可以很好地替代临床数据？为了回答这个问题，我们比较了 dr bert 和我们的 schubert 模型，后者基于我们医院非大学医院获得的匿名数据。最后，我们问自己，我们需要多少数据来训练一个专门针对法语数据的模型？是 4 GB、8 GB 还是 4 GB 的 RAM？Schubert 的第一个版本是一个临床模型，包含 4 GB 的从临床笔记中提取的句子。Schubert 的最终版本则混合了 4 GB 的 NACHOS 子集和 4 GB 的临床笔记。除了这个比较，我们还介绍了三个基于持续预训练的模型，以分析预训练策略的影响。一个基于 Camembert 的权重，并在 4 GB 的 NACHOS 子集上进行训练。另一个也是基于 Camembert，但训练了四个吉字节的 permit belt by bio birth 和 clinical birth。评估结果显示... 38 GB，Camembert Oscar 4 GB，Camembert CCnet 4 GB，Pumatbert，BioBERT 和 ClinicalBERT。评估结果显示，模型在与训练数据性质相同的任务上表现最佳。然而，我们可以观察到，来自异构来源的数据似乎更具通用性。我们还观察到，使用更多的数据可以带来更好的性能。总的来说，从头开始的预训练似乎在大多数任务上都能获得更高的性能。然而，我们对持续预训练的实验，使用 Permet-BERT 的权重和分词器，并在 4 GB 的 NACHOS 子集上进行训练，显示的结果与 Dr.BERT 4 GB 从头开始的结果相当，但基于 Camembert 权重和分词器的模型则存在稳定性问题。最后，作为结论，我们的适当系统在 11 个 don't-trim 任务中的 9 个任务上表现更好，并且在全球范围内超越了通用模型的结果。总结，我们的适当系统在 11 个 don't-trim 任务中的 9 个任务上表现更好，并且在全球范围内超越了通用模型（这里指的是 Camembert）的结果。我们还观察到，更专业的 数据更好，但它并没有很好地扩展。从 NACHOS 获得的所有预训练模型都可以在 UGIMFACE 上免费获得，所有的训练脚本都在我们的 GitHub 仓库中。所以，感谢大家的聆听，我们期待在多伦多的海报环节与大家互动。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是华盛顿大学的博士生Xiangbin。今天我将介绍我们从预训练数据到语言模型再到下游任务的工作，追踪导致不公平自然语言处理模型的政治偏见的轨迹。因此，语言模型是在大规模网络爬虫数据上训练的。政治新闻媒体在他们的预训练数据中得到了很好的覆盖。根据对C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。这为语言模型的应用带来了既是福又是祸。因此，一方面，它们能够从多样化的视角学习，这庆祝了民主和思想的多元性。另一方面，这些不同的政治观点在本质上具有社会偏见，可能会在下游任务应用中导致潜在的公平问题。为此，我们提议研究从预训练数据到语言模型再到下游任务的政治偏见传播管道，具体来说，通过以下问题。首先，我们如何评估语言模型的政治倾向，以及相关数据可能对这些政治偏见产生什么作用？其次，具有不同政治倾向的语言模型在实际的下游任务中表现如何，这是否会导致自然语言处理应用中的公平问题？因此，具体来说，我们首先提议使用政治问卷（如政治指南针测试）对不同提示格式的语言模型进行提示。这确保了我们的自动评估基于政治科学文献。因此，一些初步结果表明，首先，语言模型确实具有不同的政治倾向。它们占据了政治指南针上的四个象限。我们还可以看到，GPT-4是所有语言模型中最自由派的一个，GPT理论通常更为自由。它们占据了政治指南针上的四个象限。我们还可以看到，GPT-4是所有语言模型中最自由派的一个，GPT理论通常比BERT理论及其变体更为社会自由。其次，我们的目标是研究语言模型的政治偏见实际上是从训练数据中获得的程度。因此，我们通过进一步在六个不同的党派语料库上对语言模型检查点进行预训练，这些语料库分为新闻和社交媒体，进一步分为其政治倾向，来进行受控实验。通过在这些党派语料库上进一步对语言模型进行预训练，我们可以看到语言模型的意识形态坐标也相应地发生了变化。例如，对于Roberta，进一步微调，进一步训练于左翼的Reddit语料库，我们可以看到其政治偏见发生了实质性的自由派转变。我们还试图研究语言模型是否能够捕捉到我们现代社会中普遍存在的极化。因此，我们将预训练语料库分为美国第45任总统当选前和当选后，我们分别在两个不同的时间语料库上对语言模型进行预训练。我们可以看到，语言模型在2017年后通常具有更远离中心的政治倾向。因此，这表明语言模型也可以捕捉到我们社会中的极化。最后，但并非最不重要的一点，我们评估了不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测方面的表现，这些应用通常涉及语言模型，可能具有非常重大的影响。因此，我们看到，如果我们按类别评估性能，也就是说，如果我们将性能分为不同的人口统计或新闻媒体的政治意义，我们可以看到一个模式，例如，对于仇恨言论检测，左翼语言模型在检测针对社会少数群体的仇恨言论方面表现更好，然而，在检测针对我们社会中更强大群体的仇恨言论方面表现较差。反之，右翼语言模型在检测针对白人和男性的仇恨言论方面表现更好，然而，在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现较差。虚假新闻检测也出现了类似的趋势，我们看到左翼语言模型在检测来自其相反政治倾向的虚假信息方面表现更好，反之亦然。我们进一步展示了许多定性例子，以证明具有不同政治倾向的语言模型根据其社会类别对仇恨言论和虚假信息例子给出了不同的预测。附录中有更多例子，进一步强调了根据社会类别对仇恨言论和虚假信息例子做出不同的预测。附录中有更多例子，进一步强调了这一点。这表明语言模型的政治偏见存在一个非常紧迫的公平问题。例如，如果右翼语言模型被微调用于仇恨言论或虚假信息或其他，并部署到一个流行的社交媒体平台，这意味着具有相反政治观点的人可能会被边缘化。在仇恨言论或虚假信息或其他方面，并部署到一个流行的社交媒体平台，这意味着具有相反政治观点的人可能会被边缘化，而针对少数群体的仇恨言论可能会不受控制地蔓延。因此，这为我们敲响了警钟，要求我们承认并解决语言模型政治倾向导致的公平问题。因此，有一点讨论。我们还希望强调，我们揭示了语言模型政治偏见的独特困境。这就像在斯库拉和喀里布狄斯之间。因此，如果我们不净化语言模型训练数据中的政治观点，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平问题。问题。我们试图以某种方式净化，我们也会冒着审查或排斥的风险，而且很难确定什么才是真正中立的，应该保留在语言模型训练数据中。所以，这有点像电车难题。好吧，很好。我想这就是我今天要讲的全部。感谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是科斯塔夫·辛哈，很高兴欢迎大家来到我们的ACL 2023论文讨论会，论文题目是《语言模型的可接受性判断并非总是对上下文鲁棒》。这是一项与约翰·高蒂埃、亚伦·穆勒、卡尼什卡·米什拉、卡伦·富恩特斯、罗杰·莱维和阿迪娜·威廉姆斯合作完成的联合研究。因此，在这项工作中，我们重新审视了最小对概念。最小对概念基本上是通过可接受性判断来评估语言模型，其中还可能包括语法性，如blimp、语义、gym，或从刻板印象角度来看的可接受性，如Krauss对。在最小对概念中，评估语言模型的典型方法是展示一个可接受的句子或语法句子，然后展示一个不可接受的句子或非语法句子。然后，模型的基本过程基本上会给可接受的句子赋予更高的概率。当前的MPP管道基本上不允许我们评估模型对更长句子的可接受性。如今，大型语言模型正在采用越来越长的上下文窗口。因此，我们重新审视了数据集本身，然后通过从这些数据集中选择可接受或不可接受的句子来重新创建句子。例如，这里我们从BLIMP数据集的Adjunct Island案例中选择了典型的一对语法性句子。我们所做的是重新创建更长的序列，这些序列是可接受的，并且具有相同的语法结构匹配，我们从Adjunct Island中提取语法句子，然后将其作为前缀添加到可接受查询和不可接受查询中。因此，我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情，这也可以用来测试模型的可接受性。我们也可以通过从不同的子集或不同的数据集选择句子来做同样的事情。这就是我们所谓的不匹配场景。因此，这里的句子仍然来自相关的数据集，但不是你正在评估的同一数据集。我们也可以对可接受性案例做同样的事情。最后，我们可以从完全不相关的领域选择句子，例如维基百科。这将告诉我们，模型的可接受性判断是否实际上受到任何上下文的影响，上下文与我们正在查看的句子完全无关。那么模型的表现如何呢？首先，我们查看维基百科句子，这些句子与当前查询对完全无关。我们发现，MPP判断对于任意上下文长度大多是鲁棒的。我们将上下文长度增加到1024，以最大化OPT和GPT-2模型。我们在这里看到，在橙色虚线中，MPP判断相对稳定。那么，当我们选择来自同一数据集的句子时会发生什么？这里，我们从相同的blimp或语义宝石数据集的可接受和不可接受领域选择或创建句子。我们看到，当添加可接受前缀或不可接受前缀时，MPP判断要么显著增加，要么显著减少。但是，当我们匹配结构时，即当我们添加可接受前缀或不可接受前缀时。但是，当我们匹配结构时，即当我们从blame-person-text-gym中的相同现象中选择句子时，我们看到模型的MPP判断有显著增加或显著减少，具体取决于所选择的前缀是可接受还是不可接受。现在，这个效果随着上下文长度的增加而增加，这可能会影响到具有大上下文窗口的较新的语言模型。那么，为什么匹配前缀会如此大地影响语言模型的判断呢？我们进行了一系列分析，试图通过保留相关结构来展示输入句子，但向输入中添加噪声。经过几次这样的扰动，我们发现这些噪声实际上并没有使模型在显示MPP判断趋势方面改变其方向。基本上，我们发现模型以相似的方式对扰动句子敏感。也就是说，当我们在可接受领域中扰动句子时，我们看到所有扰动中的相似增加。当我们在不可接受领域中扰动句子时，我们看到MPP判断减少。我们看到所有扰动中的相似增加。当我们在不可接受领域中扰动句子时，我们以相似的方式看到MPP判断减少。因此，我们工作的关键结论是，语言模型对句子中共享的潜在句法和语义特征敏感。我们目前通过短句和单句输入进行的MPP评估可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取我们实验的更多详细信息。感谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是德国萨尔兰大学的博士生大伟。在这个视频中，我想介绍我们最近的工作，名为“比你想象的更弱”，这是一次对弱监督学习的批判性审视。这是与萧玉生、马里奥·斯穆斯巴赫、吉亚·斯泰芬和迪特里希·克拉考合作完成的。我想先简要介绍一下弱监督和弱监督学习。在弱监督中，我们不手动标注数据。相反，我们使用弱标注源来标注数据，例如简单的启发式规则、知识库或低质量的众包，如图右侧所示。与人工标注相比，弱标注的成本要低得多，但它们也存在噪声，这意味着一定数量的标注是错误的。如果我们直接在弱标注数据上训练神经网络，神经网络往往会记住标注的噪声，并且无法泛化。在弱监督学习中，提出了训练算法，以便在这样的标签噪声下稳健地训练神经网络，以便训练的模型仍然能很好地泛化。在最近的WSL（弱监督学习）工作中，一个常见的说法是，人们说他们只在弱标签数据下训练模型，并在干净的测试集上取得了高性能。从技术上讲，这个说法并没有错，但有一个问题，那就是三个研究问题。首先，我们是否需要干净的验证数据？最后，我们是否应该只使用干净的样本进行验证，还是有更好的利用它们的方法？我们在工作中解决了这些研究问题，我们的发现如下。首先，我们发现，有趣的是，最近的WSL方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如图所示，如果没有干净的验证样本，那么训练的模型无法超越原始的弱标签进行泛化，这意味着训练是没有意义的。这表明WSL方法实际上需要干净的标注数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净的验证样本数量将有助于WSL方法取得更好的性能，如图左侧所示。通常，我们只需要每个类别20个样本就能达到高性能。但这还不是故事的全部，因为如果我们决定访问干净样本，那么直接在它们上训练甚至会取得更好的性能。红色图形显示了直接应用于干净数据的微调方法与仅用于验证的WSL方法之间的性能差异。如图所示，如果我们每个类别有10个样本，直接微调开始超越WSL方法。最后，之前WSL方法中声称的性能提升可以通过允许在干净验证样本上继续微调来轻松实现。如图所示，Van Linden模型最初的性能低于更复杂的WSL方法如余弦。然而，如果我们允许在干净样本上继续微调，那么FTW的性能与其他方法一样好。所以，在实践中，没有理由选择更复杂的WSL方法，这些方法需要更多的计算时间和磁盘空间。总结一下，我们表明，最近的WSL方法需要干净的、手动标注的样本才能正常工作。它们的性能提升和实用性被严重高估了。我们对未来工作的具体建议如下。首先，报告模型选择标准。例如，报告模型选择是否基于干净的验证样本。其次，WSL方法应该与一些基于干净样本的短学习基线进行比较。最后，连续微调是一个简单但强大的基线，应该在未来的WSL工作中考虑。最后，我们开源了我们的代码。您可以通过此幻灯片上的二维码找到它。请随时查看。谢谢，祝会议愉快。"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫大卫·维拉尔，我将简要介绍一篇论文《从翻译中学习参数提示：评估策略和性能》。这是我和谷歌翻译同事的合作成果。Parm 是一款去年（2022 年）推出的参数量达 5400 亿的参数大型语言模型。它在包含 7800 亿个标记的庞大文本集合上进行了训练。在发布时，它在数百个 NLP 任务中取得了最先进的水平。在这项工作中，我们提出了对机器翻译进行大型语言模型提示的首次系统研究。我们使用 IMT 社区的最佳实践来评估这些模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。我们比较了两个最先进的系统，因此是性能最佳的系统或 WMT 评估。我们使用了最先进的神经 IM 指标，并还展示了基于专家的人类评估结果。最后，我们提供了一些提示选择策略的建议。提示对翻译大型语言模型的性能有很大影响。我们可以通过一个简单的实验看到这一点，我们使用一次性提示，并为每句话提供了两个不同的提示。大多数句子（1000 个中有 516 个），观察到的差异超过一个模糊点。在极端情况下，这可以达到 40 个模糊点。因此，选择一个好的提示策略很重要。在我们的实验中，我们选择了五次提示策略，我们只需将我们提供给系统的每句话标记为其所在的语言。所以在这个例子中，我们从德语翻译成英语，德语句子用德语冒号标记，英语翻译用英语冒号标记。我们发现，在多次提示的情况下，实际形式的提示对性能没有太大影响。这对于零次和一次提示至关重要，而在我们的情况下，我们使用了五次提示，实际形式的提示几乎没有区别。重要的是例子本身。我们实验结果的总结是，例子质量比与源句子的相似性更重要。因此，选择高质量翻译的例子非常重要。特别是，我们比较了从 WMT 评估的训练数据或开发数据中选择提示。开发数据经过精心挑选，质量更高，而训练数据则更嘈杂，结果显示使用开发数据时性能更好。尽管如此，专业最先进的系统在性能上仍比 Palm 翻译有显著优势，但 Palm 已经接近商业系统。在我们的情况下，我们选择与谷歌翻译进行评估。我们使用 MQM 框架进行的电子邮件分析结果表明，Palm 的流畅度与最先进的系统相当，但主要区别在于准确性。特别是，最常见的错误是遗漏错误。因此，Palm 有时会选择生成听起来更好的翻译，有时会省略源句子的某些部分，这些部分在翻译中是生动的。然而，Parm 的风格生硬类别低于最先进的系统，这是一个额外的信号，表明 Parm 提供了非常流畅的输出，但仍然存在一些准确性问题。这就是这个非常简短的概述的全部内容。有关更多详细信息，请参阅论文的完整介绍。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自中国科学技术大学的易静威。我很高兴能为您带来一个关于论文的简短广告视频《你是在抄袭我的模型吗？通过后门水印保护嵌入和服务的大型语言模型》。让我们先介绍一下嵌入和服务的基础知识。目前，GPTT、LAMA、PALM等大型语言模型在自然语言理解和生成方面表现出色。嵌入即服务是建立在大型语言模型之上的服务之一，用于辅助各种自然语言处理任务。例如，OpenAI提供基于GPT的嵌入API。然而，最近的研究表明，攻击者可以通过学习嵌入来窃取模型，并提供类似的服务。因此，保护嵌入即服务的版权是必要的。为了保护嵌入即服务的版权，一个解决方案是将水印嵌入到提供商服务中，并检测另一个服务是否包含水印。水印方法需要满足以下属性。首先，该方法应适用于嵌入即服务。其次，水印不应降低所提供嵌入的实用性。第三，水印对攻击者来说应该是隐蔽的，或者攻击者可以轻松移除水印。最后，水印需要在模型提取过程中转移到攻击者的服务中。现有工作可以大致分为四类。然而，这些方法要么不适用于嵌入即服务，要么缺乏可转移性。我们嵌入标记器的详细信息。嵌入标记器包含两个主要步骤，水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组频率适中的词语。我们假设提供商可以收集一个通用的文本语料库，并计算词频。在水印注入中，我们首先定义一个目标嵌入。当用户向提供商服务发送一句话时，提供商计算句子中的触发次数。所提供的嵌入是目标嵌入和原始嵌入的加权求和。目标嵌入的权重与句子中的触发次数成正比。当句子中的触发次数大于m时，所提供的嵌入正好等于目标嵌入。版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有词语都属于触发集的句子，而良性数据集中的句子中所有词语都不属于触发集。然后，提供商使用数据集从窃取服务请求嵌入。计算请求嵌入与目标嵌入之间的余弦相似度和L2相似度。我们计算良性数据集和后门数据集之间的相似度差异，定义为delta余弦和delta L2。同时，我们还应用KS检验，并使用其p值作为第三个指标。我们在四个数据集上进行了实验，AGnews、Mind、SSD2和Eraspam。我们假设提供商使用Wikitext数据集来计算词频。四个数据集的结果表明，我们的嵌入标记器可以在保持出色实用性的同时，具有出色的检测性能。我们还通过PCA可视化四个数据集中的句子嵌入，验证了所提供嵌入的隐蔽性。图例表示每句话中的触发次数。如图所示，很难区分后门嵌入和正常嵌入。以上就是全部内容，谢谢。欢迎与我们讨论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫 Ying，我的同事 Zhiyang 和我将为大家介绍我们关于多改进的研究，即通过指令微调改进多模型序列短学习。随着大型语言模型的进步，许多研究开始探索以参数和数据高效的方式，将预训练语言模型用于不同下游任务的新学习范式。最近，许多研究表明，指令微调使大型语言模型能够以零样本方式在未见过的任务上表现出色，只需遵循自然指令即可。然而，大多数关于指令微调的先前工作都集中在提高零样本语言任务的性能上，而计算机视觉和多模态任务则被忽略了。因此，在这项工作中，我们想要研究多模态预训练模型的指令微调是否真的可以提高对未见的多模态任务的泛化能力。此外，在我们进行研究时，我们发现 NLP 和多模态之间在指令数据集的可用性上存在显著差异。存在超过 1,600 个仅语言的指令任务。然而，没有大规模公开的多模态指令任务。因此，这促使我们构建了一个多模态指令微调数据集。在这里，我们介绍了 MultiInstruct，这是第一个多模态指令微调基准数据集，包含 62 个多样化的多模态任务，涵盖 10 个主要类别。这些任务来自 21 个现有的开源数据集，每个任务都配备了 5 个专家撰写的指令。为了研究我们提出的数据集上的多模态指令微调，我们以 OFA 作为我们的基模型，OFA 使用统一的词汇表来表示语言、图像标记和边界框的坐标。在这里，我们展示了我们多任务数据集的一些示例实例。为了统一处理各种输入和输出数据类型，我们遵循 OFA 的方法，并将所有任务表述为统一的序列到序列格式，其中输入文本、图像、指令和边界框以相同的标记空间表示。好的，现在我要谈谈多模态指令微调。对于训练数据集，我们使用 9 个组中的 53 个任务进行训练，每个任务抽样 10,000 个实例。对于测试，我们保留整个常识推理组进行测试，并从 VQA 和杂项组中选择另外五个任务。我们使用每个任务测试集中的所有实例。此外，我们从自然指令测试集中的 20 个任务中随机抽样，作为 NLP 的未见任务。因此，我们使用预训练的 OFA 大模型作为基模型。在训练过程中，我们将所有任务的所有实例混合在一起。每个实例随机与其五个指令模板中的一个结合。在每个任务的测试中，我们总共进行五个实验，每个实验使用五个指令中的一个来评估模型。我们报告所有五个实验的平均和最大性能以及性能的标准差。如果任务是多模型分类任务，我们报告准确率。如果是多模型生成任务，我们报告根 L。对于 RP 任务，我们报告根 jl，我们还引入了一个额外的评估指标，称为敏感性，它衡量模型在指令措辞略有变化的情况下，对同一任务始终产生相同输出的能力。这是我们的主要结果。我们可以看到，指令微调可以显著提高 OFA 在场景多模型任务上的性能。此外，从自然指令数据集进行的迁移学习可以使指令微调受益。在这里，我们可以看到，随着任务数量的增加，模型的性能提高，同时敏感性降低。我们还进行了一个实验，我们使用了一个指令与五个指令进行比较。我们可以看到，使用更多指令可以提高模型的整体性能，并大大降低其敏感性。因此，这表明了不同的微调策略对模型敏感性的影响。我们可以看到，通过从自然指令数据集进行迁移学习，模型可以比原始 OFA 模型实现更好的敏感性。我们还可以看到，从自然指令数据集进行的迁移学习可以帮助 OFA 在 Nitro Instruct 数据集上实现更好的性能。总的来说，我们提出了第一个大规模多模型指令微调数据集。我们显著提高了 OFA 的零样本能力，并探索了不同的迁移学习技术并展示了它们的优势。我们设计了一个名为敏感性的新指标。还有一点，我们正在收集一个更大的多模态指令微调数据集，包含大约 150 个额外的变体语言任务，我们很快就会发布它们。这是一个用于我们数据和模型的二维码。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自宾夕法尼亚州立大学的张宇胜。今天我要介绍的是我们的工作，跨语言多自然语言语义解析与意义表示。语义解析是构建用户查询语义表示的任务，如SQL和λ演算。跨语言语义解析是将多自然语言查询翻译成多意义表示的任务。如图所示，我们需要使用神经模型将多自然语言查询翻译成SQL、λ演算或FunQL等。现有的跨语言语义解析模型分别针对有限任务和应用的数据集提出和评估。例如，某些自然语言的覆盖范围存在漏洞。中文缺失，由于某些微表示的覆盖范围，λ演算缺失，或者它们只在某些神经模型上进行评估。例如，只有一个模型进行评估。因此，为此目的，我们提出了Exampler。我们为跨语言多自然语言语义解析与意义表示提供了一个统一的数据集Exampler。它包含九个来自各个领域的语料库，五个语义解析任务，八种意义表示，以及15个语系中的22种自然语言。为了更好地评估我们的基准测试，我们考虑了六种训练和评估设置。第一种是翻译测试。我们使用Google翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们用英语查询训练英语模型。在推理过程中，我们使用API将德语查询翻译成英语，然后使用训练好的模型预测SQL。我们还测试了单语模型。在这种设置中，源语言与目标语言相同。例如，德语到德语或英语到英语。我们还通过训练单语模型，只使用10%的训练数据，测试了单语少样本设置。我们还测试了多语言模型，我们为所有语言训练了一个多语言模型。例如，我们将德语、英语、中文查询一起训练一个多语言模型。在推理过程中，我们可以使用这个模型翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少样本迁移。我们在一个源语言上进行训练，然后迁移到另一种语言。因此，在训练过程中，我们用英语查询或英语和德语少样本查询的组合训练一个多语言模型，并预测SQL输出。我们还发现了许多有趣的成果。因此，关于单语模型的分析，我们在两组模型上进行评估，包括编码器PDR，即多语言预训练编码器加基于指针的解码器，如XLMR加PDR和BERT加PDR。我们还评估了编码器-解码器模型，即多语言预训练编码器-解码器模型，如MBART和MT5。我们发现编码器-解码器在所有九个数据集上获得最佳性能。我们在多语言设置上对MT5和XLMR加PDR进行了评估。我们发现，通过在各种语言的混合中进行训练，可以改进编码器-解码器或编码器-PDR。我们发现这是因为大多数主要自然语言都可以获得性能提升，除了英语在七个数据集上性能下降，只在三个数据集上有所提升。我认为这被称为多语言的诅咒。我们还比较了跨语言性能差距。如图所示，蓝色线是跨语言少样本迁移，橙色线是跨语言零样本迁移，绿色线是单语设置。我们发现，通过比较绿色和橙色线，我们发现对于零样本设置，跨语言迁移性能差距显著。通过比较蓝色和橙色线，我们发现对于少样本设置，迁移差距迅速缩小。我们还发现了一些其他有趣的发现。例如，编码器-解码器优于之前的工作或取得了可比拟的结果。在英语自然语言上的表现可以显著提升目标自然语言上的少样本表现。我们发现多语言模型如CODIS和BLUE仍然适用于跨语言语义解析任务。总结起来，我们构建了Exampler，这是一个统一的跨语言语义解析基准测试，包含多自然语言和主要表示。我们对三种代表性的多语言模型进行了全面的基准测试研究。我们的结果显示了许多有趣的发现等。欢迎访问我们的论文和代码。谢谢大家。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫亚当·施皮尔科夫斯基，今天我要讲的主题是并列句的依存结构。众所周知，不同的理论和语料库方法对依存结构有不同的假设。例如，在普遍依存理论中，丽莎、巴特和玛姬的并列结构是，第一个并列成分是整个并列结构的词头，在这个例子中是丽莎。伊戈尔·米尔丘克的意义文本理论也采用了类似的方法，即整个并列结构由第一个并列成分领导。因此，这两种方法都是不对称的，对吗？它们都将某个并列成分单独挑出来。现在，也有对并列结构的对称方法，例如布拉格方法，布拉格依存树库假设的以连词为词头的对称方法，其中并列结构由连词领导。因此，我们从n得到对所有并列成分的依存关系。最后，还有多头方法，例如迪克·哈德森的词法语法中使用的方法，可以说，所有并列成分都是并列结构的词头。多头方法，例如在卡特逊的词法语法中使用的方法，可以说，所有并列成分都是并列结构的词头，因此我们从这里的支配词loves得到对所有并列成分的单独依存关系，这些是巴顿现在正在制作的目标。本文的目的是提出两种反对并列结构不对称性的观点，如上述两种。好的，这个论点是基于依存长度最小化的原则，我将通过这些例子来解释。所以，在英语中，如你所知，直接宾语倾向于靠近动词，而附属成分可能更远，对吗？所以，March昨天读了它没问题，因为直接宾语it靠近动词，而March的直接宾语倾向于靠近动词，而附属成分可能更远，对吗？所以，March昨天读了它没问题，因为直接宾语it靠近动词，而March昨天读了它则糟糕得多，对吗？因为这里动词和直接宾语之间有一个附属成分yesterday。然而，当直接宾语非常冗长时，这种影响可能会得到缓解，因为这时它可以移到附属成分之后的位臵。这里有例子说明。所以，这两个句子都很好。March今天读了这本书，这本书关于BCS，非常有趣。没问题。从某种意义上说，用it代替，我们有这个长NP。但是，说March昨天读了这本书，关于蜜蜂，非常有趣，也是可以的。所以，这里的推理是，这是可能的，因为即使这个句子违反了直接宾语应该紧靠动词的一般语法原则，它满足了依存长度最小化的原则，即倾向于较短的依存关系。所以，这两个树只显示了关键依存关系的长度，即这两个结构中不常出现的依存关系。所以，这里我们从red到附属成分的依存关系长度为7个词，从red到book的依存关系长度为4个词。所以，总共是11。当你移动，当你交换这两个成分时，这两个依存关系的总和变为6，对吗？所以，从11变为6，短得多。这就是为什么这个听起来相当好的原因。它违反了一个原则，但满足了另一个原则。好的，所以我们做了什么，我们从宾夕法尼亚树库的增强版中提取了关于并列的各种统计数据，并查看了为什么我们没有使用普遍依存理论。这些统计数据证实了之前多次观察到的现象，即左并列成分往往较短。我们没有使用普遍依存理论，这些统计数据证实了之前多次观察到的现象，即左并列成分往往较短，所以盐和胡椒，而不是胡椒和盐，以音节为单位，还有在经过观察中发现的趋势，即随着长度的增加，这个趋势增强，即当两个并列成分的长度差异增大时，较短的并列成分更倾向于排在前面。对。所以，左短并列成分的比例更大。但是，本文的新颖之处在于，我们观察到这种趋势只有在支配词在左边或不存在时才会发生，所以在这个例子中，我看到了巴特和丽莎，所以支配词在左边，呃，在第二个例子中，霍默来了，打了个喷嚏，这里有两个动词的并列，没有外部支配词，对吗？所以在这种情况下，呃，左并列成分更倾向于较短，尤其是两个并列成分之间的差异越大。然而，当右边的支配词在这里支配并列结构t和net时，这种效应就不复存在了。所以我们通过测量字符长度，即第一列，音节，中间列，以及单词，右列来展示这一点。所以，我将重点关注右边的。我们在这里看到，当支配词在左边时，左并列成分较短的趋势随着单词的绝对差异稳步增长。当没有支配词时，也会观察到同样的现象，例如在句子并列中。但是，当支配词在右边时，这种趋势就不复存在了。我们在论文中展示了这一点如何为反对并列结构不对称性（如上述两种）和支持对称结构（如上述两种）提供了论据。所以，请参阅论文以获取完整的协议和论据，抱歉，并与我们讨论海报会议。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "大家好！我叫殷凯奥，我将为大家介绍我们的研究成果，题目是《翻译何时需要上下文？基于数据的多语言探索》。这项研究是与帕特里克·费尔南德斯、艾米·刘、安德烈·F.D. 马丁斯和格雷厄姆·纽比格合作完成的。因此，很多翻译确实依赖于上下文。例如，我们如何翻译这句话中的“mole”？如果前一句是“如果部长们知道了，事情可能会变得危险”，那么“mole”指的是间谍。但如果前一句是“这可能有什么严重的事情，医生？”那么“mole”指的是胎记。因此，根据上下文，这个词的含义会发生变化，因此它的翻译也会随之改变。然而，评估模型在处理这类情况时的表现非常困难。首先，因为只有小部分翻译依赖于上下文，这使得像BLUE这样的语料库级指标无法捕捉到这些翻译。有些人建议对依赖上下文的翻译进行有针对性的评估，但这些资源通常依赖于领域知识和人工整理，因此只能支持有限类型的依赖上下文的翻译和有限的语言集合。在这项研究中，我们试图回答这两个问题。首先，翻译何时需要上下文？其次，模型在处理这些情况时的表现如何？为了回答第一个问题，我们首先测量了一个词在翻译过程中对上下文的依赖程度。在之前的研究中，我们引入了CXMI作为机器翻译模型使用上下文的度量。CXMI通过测量上下文C给定源X的情况下，对目标Y提供了多少信息来实现这一目标。你可以将CXMI视为给模型提供上下文所获得的信息。在这项研究中，我们将CXMI扩展为逐点CXMI，可以测量句子级或词级上下文的使用情况。我们可以将PCXMI高的词视为需要上下文进行翻译的词。现在，我们分析PCXMI高的词，寻找这些词之间的模式。我们对从英语翻译成14种不同语言的TED演讲的转录本进行了分析。我们的分析在三个不同的层面上进行。首先，我们查看了PCXMI平均值高的词性标注。这使我们能够找到例如，阿拉伯语中的双重代词具有相对高的PCXMI。这可以解释为英语没有双重代词，因此在翻译成阿拉伯语时，你需要上下文来确定代词是否是双重代词。同样，我们发现某些语言在选择适当的动词形式时也需要上下文。然后，我们查看了在所有不同出现情况下PCSXMI平均值高的词汇项目。这有助于我们识别出像这里的情况，即在中文中，你需要上下文来翻译专有名词，以确保在文档中使用相同的翻译。同样，我们发现上下文有助于以正确的正式程度进行翻译。最后，我们查看了PCXMI高的不同个体标记。这使我们能够识别出无法真正由词本身捕捉到的现象，而是通过句子结构表达的，例如省略的解析。所以，现在我们使用分析结果来设计一个文档级翻译的基准。对于我们确定的五个语篇现象，我们创建了标记器来自动识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。然后，我们可以使用标记器来识别与现象相关的词，我们称我们的标记器为多语言语篇意识（MUDA）标记器。最后，我们使用我们的基准以及其他指标来评估不同模型在文档级机器翻译中的表现。首先，当我们使用语料库级指标时，我们发现无上下文模型的性能最好，但如果我们使用COMET，有上下文意识的模型表现最好。如果我们使用word-f度量，那么有上下文和无上下文的模型具有可比的性能。这再次表明，如果我们仅使用语料库级指标，就很难确定最佳的文档级翻译系统。现在，我们使用MUDA基准来评估模型，我们发现有上下文意识的模型在某些语篇现象（如正式性和词汇连贯性）中比不使用上下文的模型更准确。但这些模型在其他现象（如省略、代词和动词形式）上并没有比不使用上下文的模型表现得更好。因此，这表明了我们需要在文档级翻译方面取得更多进展。我们还比较了不同的商业系统，我们的基准显示DeepL通常比Google Translate在文档级翻译中更准确。总结一下，我们对14对语言对进行了数据驱动的分析，以确定何时需要上下文进行翻译。然后，我们使用我们的发现建立了一个文档级机器翻译的基准，这有助于我们确定模型可以很好地处理哪些语篇现象，以及哪些翻译系统擅长文档级翻译。非常感谢您的聆听。在多伦多见！"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是珍妮，卡内基梅隆大学的一名一年级博士生，今天我将为大家介绍你们的论文《语义位置性：数据集和模型中设计偏见的特征描述》。这项工作是与华盛顿大学和艾伦人工智能研究所的一些研究人员合作完成的，其中包括塞巴斯蒂安·桑特、罗南·拉布拉塞、卡塔里娜·阿拉尼卡和马丁·萨普。让我们先想象一下，你为一家报纸工作，正在筛选新闻文章下面的评论，试图删除有毒内容。你可能会转向像Perspective API这样的流行API进行毒性检测，如果你是卡尔·琼斯，Perspective API能够正确检测出有毒实例，那么这会非常有效。但对于阿迪亚·沙尔马来说，情况并非如此，因为Perspective API对印度语境中更常见的攻击性用词并不敏感。这是一个设计偏见的例子，我们在这里看到技术在不同人群之间的系统性性能差异。我们之前看到的像这样的设计偏见可能源于自然语言处理研究人员和模型开发人员的语义位置性。语义位置性是指人们由于其人口统计特征、身份和生活经历而持有的观点。这是一个在批判性研究中广泛使用的概念，特别是在女权主义和酷儿学术领域。作为研究人员，语义位置性会影响研究过程及其结果和结论，因为它会改变研究人员做出的决策。因此，人们可能会问，数据集和模型是否有语义位置性？我们并不是说模型本身和数据集本身具有人口统计特征和生活经历，但它们确实汇集了真实的人们的判断和观点，因此可以代表模型和数据集本身具有人口统计特征和生活经历，但它们确实汇集了真实的人们的判断和观点，因此可以代表某些语义位置性优于其他语义位置性。之前的研究已经提出了关于语义位置性的轶事证据，例如文化差距和模型与数据集，以及模型语义位置性的理论定义。然而，这些研究并没有真正比较最终用户与数据集和模型本身。随着自然语言处理任务变得更加主观和社会导向，研究模型和数据集的语义位置性变得越来越重要。要描述这些语义位置性的偏差是具有挑战性的，因为并非所有决策都被记录下来，许多模型隐藏在API后面。因此，为了研究数据集和模型的语义位置性，我们实际上是比较了真实用户与现有数据集和模型的注释。我们通过我们的框架NL语义位置性来实现这一点。我们的框架主要分为两个步骤。第一步是用来自不同背景的注释员重新注释数据集。我们选择这样做，而不是查看原始数据集注释员的人口统计数据，因为通常只有少数注释员对每个实例进行注释，而且人口统计数据很少被收集和分享。因此，我们选择重新注释数据，以便每个实例有多个注释员，并获得丰富的社会人口数据。然后，我们将按人口统计数据进行的注释与模型和数据集进行比较，使用皮尔逊相关系数。因此，我们的框架实际上与注释员意见不一致的文献不同，它通过比较最终用户与模型和数据集、预测和标签，而不是仅仅关注注释员的意见一致性或模型注释员分布，来比较最终用户与模型和数据集。我们的框架在很大程度上得益于我们的HCI合作者的在线众包平台Lab in the Wild。Live in the Wild是一个在线实验平台，我们可以在这里招募来自不同背景的志愿者。众包平台，前HCI合作者。Lab in the Wild是一个在线实验平台，我们可以在这里招募来自不同背景的志愿者，相比于MTurk这样的平台，后者主要有来自美国或印度的参与者。此外，Lab in the Wild仍然能够获得高质量的数据。我们在Lab in the Wild上举办了两项任务，其中一项是社会可接受性。它的工作原理是参与者将阅读来自社会化学数据集的某个情境，然后他们将写出这个情境在社会上是多么可接受。之后，为了保持参与研究，他们可以将自己的回答与AI和其他人的回答进行比较。然后，我们将这些注释与社会化学、德尔菲和GPT-4进行比较。然后，我们为毒性检测和仇恨言论检测任务复制了一个非常相似的设置。我们将这些注释与社会化学、德尔菲和GPT-4进行比较。然后，我们为毒性检测和仇恨言论检测任务复制了一个非常相似的设置，参与者将阅读DynaHate中的某个实例，并写出他们是否认为这是一个仇恨言论的实例。然后，我们将这些注释与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT-4进行比较。我们的研究最终收集了来自87个国家的1000多名注释员的16000多条注释。所以，我们现在更有能力回答自然语言处理数据集和模型最符合谁的问题。我们发现自然语言处理中存在语义位置性。例如，我们发现数据集和模型最符合英语国家的用户。因此，对于GPT-4的社会可接受性分析，我们发现它最符合儒家和英语国家的用户。我们发现DynaHate也最符合英语国家的用户。我们还发现与受过大学教育的人群的额外对齐。因此，对于GPT-4的社会可接受性任务，我们发现它最符合受过大学教育或研究生教育的人群。我们还发现DynaHate也是如此，它最符合受过大学教育的人群。然而，当模型和数据集与特定人群对齐时，有些人不可避免地会被抛在后面。一个例子是，数据集和模型与非二元人群的对齐程度低于男性和女性人群。我们在GPT-4的社会可接受性任务以及DynaHEAT任务分析中都发现了这一点。因此，鉴于自然语言处理中存在语义位置性，我们能对此做些什么呢？我们有几个建议。第一，在整个研究过程中记录所有相关的设计选择。第二，从视角主义的角度进行自然语言处理研究。我们的第三个建议是在四个特定社区内构建专门的数据集和模型。一个很好的例子是Masakane倡议。我的意思是，我们希望强调包容性自然语言处理不仅仅是让所有技术为所有人工作。因此，我们的演讲到此结束。但如果您想了解更多，请随时查看我们的仪表板以获取最新的分析结果和我们的论文。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我将谈谈我们在解决间接指称表达以进行实体选择方面的工作，我们在此介绍了备选实体语料库。我的名字是贾瓦德·霍赛尼，这是一项与菲利普·拉德林斯基、西尔维亚·帕里蒂和安妮·刘易斯合作完成的工作。我们的目标是理解用户在做出选择时使用的语言。请考虑以下备选问题：你是说《Easy on Me》还是《I Got a Feeling》？用户想要在这两首歌曲中进行选择，最明显的方法是使用直接引用，例如说出歌曲的名称《Easy on Me》或其位置第一首，但有时使用间接引用更合适，可以进行更自然的对话。这种情况可能发生在用户记不起歌曲名称时，或者发音过于相似，难以区分时，或者当用户想要指定偏好时。以下是几个示例间接差异，例如：较新的那首或不是充满活力的那首。这是对话系统中的一个重要问题，也是用于基准测试大型语言模型实体理解能力的任务。我们没有发现一个公共数据集，一个针对该任务的大型公共数据集，因此我们通过众包标注收集了一个。我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，使用卡通完成设置。卡通中有三个对话气泡。我们的数据集收集方法强调非正式性，使用卡通完成设置。卡通中有三个对话气泡。在第一个气泡中，鲍勃说，还记得我们昨天听的那首歌吗？然后鲍勃设置了对话上下文。在第二个对话气泡中，爱丽丝说，你是说《Easy on Me》还是《I Got a Feeling》？这是一个备选问题。在第三个对话气泡中，鲍勃使用间接引用来选择其中一个实体，例如：较新的那首。我们自动提供第一个和第二个对话气泡，但第三个由标注者填写。第一个对话气泡是从每个领域的一些手动提示中选出的。第二个对话气泡，即备选问题，是通过以下方式生成的：我们始终使用一个简单的模板：你指的是A还是B，其中A和B是从维基百科中选取的样本。以下是我们使用的不同抽样方法：当我们在列表中向上移动时，实体变得越来越相似，通常更难进行消歧。第一个是均匀随机，第二个是实体有相似标题的情况，例如两本书都名为《回归》；第三种情况是它们在维基百科上的描述相似，最后一种情况是例如同一类型或同一艺术家。当我们向管理员展示这个备选问题时，他们知道这些实体的名称，但并不一定了解实体本身，所以我们展示了一些关于这两个实体的背景知识。对于歌曲，我们只需为每首歌曲提供一个谷歌搜索链接，然后要求标注者至少收听每首歌曲的一部分，并阅读每首歌曲的介绍。以下是《Easy on Me》的谷歌搜索结果示例。对于食谱和书籍领域，我们展示了一些维基百科的背景文本。对于食谱，我们还从维基百科再次展示了它们的图片，以便标注者了解它们的样子。然后我们要求标注者选择其中一个实体，例如：第一首，并使用三到五个间接指称表达来描述它们，例如：钢琴音乐的那首。以下是我们数据集中的几个示例：例如：没有歌词的那首，不是有12岁男孩的那首，也不是虚构的那首，或者来自阿塞拜疆的那首等等。备选实体语料库包含三个领域中的6,000个备选问题，以及42,000个间接指称表达。使用T5X模型的结果如下。如果语言模型可以访问与标注者完全相同的背景知识，那么准确率非常高，大约在92%到95%之间，但这并不现实。如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87%之间，这更现实。例如，当语言模型检索背景知识时，准确率只有60%，因此还有很大的改进空间。我们还表明，模型具有领域泛化能力。以下是我们的数据集链接。谢谢大家。"}
