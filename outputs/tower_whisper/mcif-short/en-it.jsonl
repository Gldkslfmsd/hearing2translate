{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, benvenuti alla nostra presentazione di d.plain, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La semplificazione del testo è un processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come persone con problemi di lettura o parlanti non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testi, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Nell'esempio qui riportato, potete vedere un paio di frasi allineate in parallelo di una frase tedesca complessa e la sua traduzione in linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, come la sostituzione lessicale, la claustellazione, la riorganizzazione della claustellazione o l'inserimento di parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "proponiamo ora il nostro nuovo corpus, dplane. Perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Quindi, per esempio, questi corpora qui sono troppo piccoli per addestrare un modello di taxonificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli che ho proposto negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori negli allineamenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, proponiamo il nostro nuovo corpus dplane, che è suddiviso in due sottocorpus, dplane-apa e dplane-web. Il dplane-apa si basa su testi di uso."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Nel semplice APA, abbiamo allineato manualmente 483 documenti, ottenendo circa 30.000 coppie di frasi parallele e 13.000 coppie di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "per DeepLaneWeb. Questo corpus include diversi ambiti e tutti questi 750 documenti vengono allineati, da un lato manualmente e dall'altro con metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, otteniamo 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato un po' di più le nostre coppie di frasi. Ad esempio, sul tipo di semidificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come potete vedere qui, i testi biblici sono molto più semplici rispetto, ad esempio, al testo delle notizie o ai testi per studenti di lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "a tutti i livelli, per esempio per quanto riguarda la semplificazione lessicale, la semplificazione strutturale, e anche il livello generale di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, si può notare che il nostro corpus Deplane presenta una grande varietà di diverse trasformazioni di semplificazione. Quindi, ad esempio, nel corpus Deplane API, abbiamo molte più riorganizzazioni e aggiunte di parole rispetto al corpus Deplane Web."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, nel corpus web, abbiamo molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo quindi cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset D-plane. Quindi, per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni sono stati sviluppati numerosi metodi di allineamento, ma nel contesto delle traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi nei documenti post."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "ma nel nostro caso d'uso stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli nella stessa lingua con lo stesso contenuto, ma con un livello di complessità diverso"}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora, dato che abbiamo il nostro set di dati D-plane, che contiene frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo di allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "E potete trovare anche il codice per eseguire questo metodo sui vostri documenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è un caso di semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "raffinando i modelli linguistici per produrre un testo semplificato dal testo complesso di input."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo perfezionato due modelli diversi. Abbiamo perfezionato il modello a lungo impatto per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche perfezionato la base normale lunga, la base normale in parte per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "È inoltre possibile trovare tutti i punti di controllo e approfondire i dettagli dei punteggi e delle metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questo aggiustamento fine di base potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo questi risultati come un punto di riferimento, un punto di riferimento di base per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Szpilkowski e questo discorso riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come forse sapete, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci corpus. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie"}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "in modo tale che il primo congiunto sia il capo dell'intera struttura coordinata, quindi in questo caso Lisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio simile è adottato nella teoria del testo-significato di Igor Milczuk, dove anche in questo caso l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici. Essi individuano uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Ora esistono anche approcci simmetrici per coordinare le strutture, come l'approccio praghese, l'approccio a testa congiuntiva adottato nei treebank di dipendenza di Praga, dove le strutture coordinate sono guidate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo dipendenze dall'inizio alla fine di tutti i congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "Infine, viene utilizzato anche un approccio multi-testuale, ad esempio nella grammatica delle parole di Dick Hudson."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove, per così dire, tutti i congiunti sono teste della struttura coordinata. Quindi otteniamo dipendenze dal governatore, qui loves, a tutti i congiunti separatamente. Queste sono di Barton."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "Ora, lo scopo di questo articolo è di presentare un nuovo argomento a favore delle strutture di coordinazione simmetriche come queste due e contro le strutture di coordinazione asimmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Ok, l'argomento si basa sul principio della minimizzazione della lunghezza della dipendenza che spiegherò in base a questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Quindi in inglese, come forse sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli accessori possono essere più lontani, giusto? Quindi \"March, read it yesterday\" è corretto perché l'oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Mentre March leggeva ieri, è molto peggio, giusto? Perché qui tra il verbo e l'oggetto diretto c'è un avverbio di tempo ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo l'adjuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. Oggi marzo ha letto questo libro assolutamente affascinante sul BCS. Va bene. Al posto di questo, abbiamo questa lunga NP."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma va bene anche dire: \"Ho letto ieri March, un libro assolutamente affascinante sulle api.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il ragionamento qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere accanto al verbo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Wojciech Czaja - Soddisfa il principio della minimizzazione della lunghezza della dipendenza, che afferma che sono preferibili dipendenze più corte. Wojciech Czaja - dipendenze più corte."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui abbiamo una dipendenza da \"leggere\" all'aggettivo di lunghezza sette misurato in parole e da \"leggere\" al libro di lunghezza quattro. Quindi insieme è 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6, giusto? Quindi invece di 11, 6, molto più breve. Ecco perché questo suona abbastanza bene, giusto? Viola un principio, ma ne soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Ok, quindi abbiamo estratto varie statistiche sul coordinamento dalla versione migliorata della Penn Treebank e vediamo il documento per cui non abbiamo utilizzato le dipendenze universitarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "Mateusz Piorkowski - E le statistiche confermano l'osservazione fatta molte volte prima che i contratti di sinistra tendono ad essere più brevi, anche il sale e il pepe e il sale misurati in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "e anche l'osservazione che è stata fatta di passaggio che questa tendenza cresce con la differenza di lunghezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra le lunghezze dei due coniugati cresce, il coniugato più corto preferisce essere il primo più forte. Esatto. Quindi la proporzione è maggiore per i coniugati corti a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma la novità di questo studio è che abbiamo osservato che questa tendenza si verifica solo quando le governanti a sinistra sono assenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il governatore è a sinistra in questo esempio. Ho visto Bart e Lisa, quindi il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "Nel secondo esempio è assente. Omero venne e starnutì. Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. Quindi, in tali casi, il congiunto a sinistra preferisce essere più breve, tanto più quanto maggiore è la differenza tra le due congiunzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance di destra è qui, la sinistra governa il coordinamento, tel e net, questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo quindi che misurando la lunghezza in caratteri, la prima colonna in sillabe, la colonna centrale in parole, la colonna a destra. Mi concentrerò quindi su quella di destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Quello che vediamo qui è che quando il governatore è a sinistra,"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza del congiunto a sinistra ad essere più breve cresce costantemente con la differenza assoluta di parole. E lo stesso si osserva quando non c'è un governatore, come nel coordinamento delle frasi. Ma quando il governatore è a destra, questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "E nel documento mostriamo come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche come queste due e per le strutture simmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per l'accordo e gli argomenti completi, si veda il documento, scusatemi, e ci parli della sessione di presentazione dei poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiangbin, dottorando presso l'Università di Washington. Oggi presenterò il nostro lavoro dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando le tracce dei pregiudizi politici che portano a modelli di NLP ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici vengono addestrati su dati raccolti su larga scala tramite il web scraping."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media di notizie politiche sono ben rappresentati nei loro dati di pre-addestramento. Secondo un'indagine sul corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben rappresentati nei dati di addestramento dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una situazione ambivalente per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, da un lato, sono stati in grado di imparare da prospettive diverse, il che celebra la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente influenzate da pregiudizi sociali e potrebbero portare a potenziali problemi di equità nelle applicazioni di compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo di indagare il flusso di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici e ai compiti a valle, chiedendoci specificamente le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbero avere i dati pertinenti su tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano effettivamente i modelli linguistici con diversi limiti politici nei compiti a valle e se ciò potrebbe comportare problemi di equità nelle applicazioni di NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in particolare, abbiamo prima proposto di stimolare i modelli linguistici con diversi formati di input utilizzando i questionari politici, come il test del compasso politico. Questo ci garantisce di fare una valutazione automatica ben fondata sulla letteratura di scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che i primi modelli linguistici hanno effettivamente orientamenti politici variabili. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche osservare che GPT-4 è il modello linguistico più liberale di tutti, e le teorie di GPT sono generalmente più liberali socialmente rispetto alla teoria di BERT e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, ci proponiamo di indagare fino a che punto i pregiudizi politici dei modelli linguistici siano effettivamente tratti dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Potremmo quindi condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint del modello linguistico su sei diversi corpus partigiani separati in notizie e social media, ulteriormente divisi nelle loro inclinazioni politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Con un ulteriore pre-addestramento dei modelli linguistici su tali corpus partigiani, possiamo osservare che anche le coordinate ideologiche del modello linguistico si spostano di conseguenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, nel caso di Roberta, ulteriormente affinata e addestrata sul corpus di Reddit di orientamento di sinistra, possiamo osservare un notevole spostamento verso posizioni liberali"}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "per quanto riguarda i suoi pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare se i modelli linguistici possano cogliere la polarizzazione che è prevalente nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi dividiamo i corpora di pre-addestramento in pre-45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti, e addestriamo separatamente i modelli linguistici sui due diversi corpora temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "possiamo vedere che i modelli linguistici in generale avevano una tendenza politica più lontana dal centro dopo il 2017. Quindi ciò indica che i modelli linguistici possono anche cogliere la polarizzazione nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, valutiamo i modelli linguistici con diverse inclinazioni politiche per quanto riguarda il rilevamento del discorso d'odio e il rilevamento di notizie false, per le applicazioni di NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vediamo che se analizziamo le prestazioni per categoria, cioè se separiamo le prestazioni in"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "Analizzando diverse demografie o significati politici dei media, possiamo osservare un modello secondo cui, ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di sinistra sono migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "nel rilevare l'incitamento all'odio rivolto ai gruppi sociali minoritari"}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, sono meno bravi a rilevare l'hate speech rivolto a gruppi più potenti nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "e viceversa, i modelli linguistici di destra sono migliori nel rilevare il discorso d'odio rivolto ai bianchi e agli uomini, ma peggiori nel rilevare il discorso d'odio rivolto ai neri, agli LGBTQ+ e ad altre comunità minoritarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Eventi simili si verificano anche per il rilevamento delle notizie false, dove si osserva che i modelli linguistici di sinistra sono più bravi a rilevare la disinformazione proveniente dalla loro opposta tendenza politica e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, mostriamo inoltre molti esempi qualitativi per vedere che i modelli linguistici con significati politici diversi"}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "Forniscono previsioni diverse per gli esempi di discorsi d'odio e disinformazione in base alla loro categoria sociale. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente questo aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che esiste un problema di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se un modello linguistico di orientamento conservatore venisse ottimizzato su discorsi d'odio o disinformazione o altro, e venisse distribuito su una popolare piattaforma di social media,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e gli incitamenti all'odio contro i gruppi minoritari potrebbero diffondersi senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha quindi suonato l'allarme per farci riconoscere e affrontare le questioni di equità derivanti dalle inclinazioni politiche dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Quindi un po' di discussione. Vorremmo anche sottolineare che mettiamo in evidenza il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come essere tra Scilla e Cariddi,"}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici e poi ai compiti a valle, creando in ultima analisi problemi di equità"}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se proviamo in qualche modo a sanificare, rischiamo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia realmente neutrale e cosa debba mantenere i dati di monotonia linguistica. È un po' come il problema del tram elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Va bene, ottimo. Penso che sia praticamente tutto quello che ho per oggi. Grazie per il suo tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa di dottorato al primo anno alla Carnegie Mellon University, e oggi presenterò il vostro lavoro, \"Anal Positionality, Characterizing Designed Biases of Data Sets and Models\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto in collaborazione con alcuni ricercatori dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santee, Ronan Labrosse, Katarina Reinecke e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Quindi iniziamo immaginando che lavoriate per un giornale e che stiate setacciando i commenti sotto il vostro articolo di notizie cercando di rimuovere i contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "potresti rivolgerti a un'API popolare come Perspective API per il rilevamento della tossicità. E questo funziona davvero bene se sei Carl Jones, dove Perspective API è in grado di rilevare correttamente le istanze tossiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è proprio il caso di Aditya Sharma, dove l'API di prospettiva non è davvero così sensibile ai termini offensivi che sono più comuni nei contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio di progettazione in cui si osservano differenze sistematiche di prestazioni della tecnologia tra le popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "I pregiudizi di progettazione come quello che abbiamo appena visto prima possono verificarsi a causa della posizionalità dei ricercatori di NLP e degli sviluppatori di modelli. La posizionalità è semplicemente la prospettiva che le persone assumono a causa delle loro caratteristiche demografiche, della loro identità e delle loro esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi esiti e risultati perché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi una domanda che le persone potrebbero porsi è se i set di dati e i modelli abbiano una posizionalità?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "E non stiamo cercando di dire che i modelli, le cellule e i set di dati abbiano essi stessi identità demografiche ed esperienze di vita, ma che essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, i lavori precedenti hanno suggerito alcune prove aneddotiche dell'esistenza della posizionalità, come le lacune culturali nei modelli e nei set di dati, oltre alle definizioni teoriche della posizionalità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non si concentrano sul confronto tra gli utenti finali e i set di dati e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E studiare la modellazione e la posizionalità dei set di dati diventa sempre più importante man mano che i compiti di NLP diventano più soggettivi e socialmente orientati."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "E è difficile caratterizzare come queste posizioni siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Per studiare il set di dati e la posizionalità del modello, confrontiamo le annotazioni con gli utenti reali con i set di dati e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo facciamo attraverso il nostro quadro di riferimento della posizionalità NL."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro modello funziona in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo è ri-annotare i set di dati con diversi annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E scegliamo di farlo piuttosto che esaminare la demografia degli annotatori dei dataset originali, perché di solito solo alcuni annotatori annotano ogni istanza e perché la demografia viene raramente raccolta e condivisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "Perciò abbiamo scelto di ri-annotare i dati, in modo da avere molti annotatori per ogni istanza, e di ottenere un ricco insieme di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Prendiamo quindi le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando un punteggio di correlazione R di Pearson."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E quindi il nostro quadro si differenzia effettivamente dalla letteratura sul disaccordo tra annotatori confrontando gli utenti finali con modelli e set di dati, previsioni ed etichette, anziché limitarsi a osservare il semplice accordo tra annotatori o la modellazione delle distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è in gran parte reso possibile da Lab in the Wild, una piattaforma di crowdsourcing online del nostro collaboratore HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "And Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversificati rispetto a piattaforme come MTurk, che hanno in gran parte partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Nel Lab in the Wild abbiamo due compiti, uno dei quali riguarda l'accettabilità sociale. Il modo in cui funziona è che i partecipanti leggono una situazione del dataset di chimica sociale e poi scrivono quanto quella situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con quelle di un'intelligenza artificiale e di altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con la chimica sociale, Delphi e GPT-4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Poi replichiamo una configurazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, dove leggeranno un'istanza da DynaHate e scriveranno se pensano che sia un'istanza di discorso d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo quindi queste annotazioni con DynaHate, Perspective API, Rewire API, Hate Roberta e GPT-4. Il nostro studio ha infine raccolto oltre 16.000 annotazioni da oltre mille annotatori di 87 paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora siamo meglio attrezzati per rispondere a chi si allineano maggiormente i set di dati e i modelli di NLP? Scopriamo che esiste una posizionalità nel NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo riscontrato che i dataset e i modelli sono più allineati ai paesi anglofoni. Quindi, per l'analisi dell'accettabilità sociale del GPT-4, abbiamo riscontrato che è più allineato ai paesi confuciani e anglofoni. Abbiamo riscontrato che anche dyna-hate è più allineato ai paesi anglofoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche riscontrato una maggiore corrispondenza con le persone che hanno un'istruzione universitaria. Quindi, per GPT-4 nel compito di accettabilità sociale, abbiamo riscontrato che è più in linea con le persone che hanno un'istruzione universitaria o un'istruzione post-laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo la stessa cosa per Donahate, dove è più in linea con le persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i set di dati e i modelli sono meno allineati alle persone non binarie rispetto agli uomini e alle donne. Lo troviamo nel compito di accettabilità sociale di GPT-4, così come nell'analisi del compito DynaHATE."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dato che nell'NLP esiste la problematicità della posizione, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo alcune raccomandazioni al riguardo. La prima è tenere traccia di tutte le scelte di design pertinenti durante il processo di ricerca. E l'altra è fare ricerca NLP con la lente del perspectivismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è di creare set di dati e modelli specializzati all'interno di quattro comunità specifiche. E un buon esempio di ciò è l'Iniziativa Masakane. Vogliamo sottolineare che l'NLP inclusivo non consiste solo nel far sì che tutte le tecnologie funzionino per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "E con questo concludo la nostra presentazione, ma se desiderate saperne di più, non esitate a consultare la nostra dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Siyu Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro, Distilling Script Knowledge from Large Language Models for Constraint Language Planning."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo interazioni passo dopo passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "Il lavoro precedente ha sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come fare una torta, e ha dimostrato che i grandi modelli linguistici possono scomporre efficacemente gli obiettivi in passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione degli obiettivi astratti di attività stereotipate. La pianificazione degli obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane ancora poco studiata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "che impongono diverse limitazioni agli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Poiché non esiste un insieme di dati di obiettivi specifici per supportare il nostro studio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo prima raggiungere questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati. Per l'acquisizione di dati con intervento umano, utilizzare InstructGPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Preleviamo un campione di 100 ragazze specifiche e valutiamo gli script generati da grandi modelli locali."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "La tabella seguente riporta l'accuratezza complessiva dei risultati. Abbiamo riscontrato che tutti i modelli di linguaggio leggeri ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, conduciamo un'analisi dettagliata per indagare perché i modelli di apprendimento delle linee cadono."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma non si può garantire la fedeltà ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato categorie di argomenti più dettagliate dei vincoli definiti in WikiHow. La mappa termica nella figura mostra che le prestazioni di pianificazione dei PD didattici variano notevolmente per le ragazze delle diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di vento leggero è caratterizzata da un'elevata varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea del filtro Z sovragenerato per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo prima i tipi di vincolo con esempi per intract CPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di partenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, istruisci GPT a sovra-generare script di casi per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, viene sviluppato un modello di filtro per selezionare gli script fattibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo gli script e gli obiettivi in rappresentazioni astratte GPT e calcoliamo la similarità del coseno e i punteggi di similarità per misurare la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, evitiamo lo script che contiene le parole chiave del vincolo di destinazione. Manteniamo lo script solo se l'obiettivo di destinazione ottiene il punteggio più alto nell'insieme degli obiettivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, InstructZBT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica che di fedeltà al vincolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché l'implementazione di modelli di linguaggio di grandi dimensioni è costosa, è essenziale consentire la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di un set di dati è un passaggio essenziale per raggiungere questo obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, gli studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale dei set di dati è costosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare set di dati di pianificazione linguistica vincolata da grandi modelli linguistici"}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applichiamo il nostro metodo per la creazione di un dataset di pianificazione linguistica vincolata, chiamato Codescript."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di validazione e di test, chiediamo ai lavoratori provenienti da fonti cloud di trovare campioni errati corretti."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione delle costrizioni dello script del codice. Abbiamo riscontrato che lo script del codice presenta un alto grado di applausone negli obiettivi specifici generati. Con lo script del codice, possiamo tracciare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che la funzione T-file sul tasso di costo può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono supportare i modelli più grandi quando sono stati adeguatamente addestrati su dataset idonei."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppiamo un metodo di filtro per i modelli linguistici sovra-generati."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli linguistici di grandi dimensioni per generare un set di dati di script di alta qualità per la pianificazione linguistica vincolata. Speriamo che il set di dati CodeScript possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il suo tempo. Si prega di trovare ulteriori dettagli dello script del codice nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Zhu Heng. Oggi presenterò il nostro articolo, \"Gli etichettatori di entità nominate del kernel 2003 funzionano ancora bene nel 2023?\" Cominciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento degli enti nominati o il compito NER"}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo osservato che i modelli utilizzano CONO 2003 per sviluppare il NER da quasi 20 anni. E ciò solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, se osserviamo una cattiva generalizzazione, cosa causa il calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare su questi problemi, abbiamo sviluppato il dataset CONO++. Questo è un dataset che abbiamo raccolto da Reuters News del 2020 e poi annotato con le stesse linee guida di annotazione CONO 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi perfezionato oltre 20 modelli su Kano 2003. Li abbiamo valutati sia sul set di test Kano 03 che sul set di test Kano++."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che sono necessari tre ingredienti principali:"}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer si generalizzano normalmente meglio sui nuovi dati,"}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione fine influisce direttamente sulle prestazioni di un compito a valle. Anche in questo caso abbiamo scoperto che più esempi di ottimizzazione fine portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Alla nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due ipotesi. La prima è il sovradattamento adattivo, che è un sovradattamento causato dal riutilizzo dello stesso set di test più e più volte. E questo si manifesta solitamente come la diminuzione dei rendimenti su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra il treno e i dati di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea rossa di miglior adattamento ha una pendenza maggiore di 1."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo apportato a Carnot 2003 si traduce in più di un'unità di miglioramento su Carnot++, il che significa che non ci sono rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che in questo caso non si osserva un sovra-adattamento adattivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, che dire del Temporal Drift?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni peggiorano con un divario temporale più ampio."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E ciò conferma la nostra ipotesi che la causa principale del calo delle prestazioni sia la deriva temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura di modello migliore, di un modello più grande, così come di più esempi di affinamento. E questi vanno di pari passo. Non possiamo avere solo un ingrediente, ma tutti gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato da un errore temporale e, in modo piuttosto sorprendente, non è causato da un sovra-adattamento adattivo, anche se KONO 2003 è stato utilizzato per oltre 20 anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, tornando alla domanda che abbiamo posto nel titolo del nostro articolo, i tagger di Connell 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sonoro sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo solleciterà ulteriori ricerche su come migliorare le generalizzazioni dei modelli"}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "Infine, vi preghiamo di consultare il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Salve, parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indiretto per la selezione delle entità, in cui introduciamo gli AltEntityScorers."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Jawad Hosseini e questo è un lavoro congiunto con Philip Radlinski, Sylvia Parity e Annie Lewis."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considerate questa domanda alternativa. Volevate dire \"Easy on Me\" o \"I Got a Feeling\"? Qui un utente vuole scegliere tra una di queste due canzoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è fare un riferimento diretto. Ad esempio, dicendo che il nome della canzone è Yami o la sua posizione, la prima."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "A volte, però, è più appropriato fare un riferimento indiretto per avere una conversazione più naturale. Questo può accadere quando l'utente non riesce a ricordare il titolo di una canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "oppure le pronunce sono troppo simili tra loro e difficili da distinguere."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di differenze dirette. Ad esempio, il più recente o la canzone che non è energica."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità degli LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per un compito. Quindi ne raccogliamo uno utilizzando l'annotazione di massa. Il nostro set di dati copre tre diversi ambiti: musica, libri e ricette."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta dei dati si basa sull'informalità, utilizzando un set di completamento di vignette."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il fumetto ha tre bolle di dialogo. Nella prima bolla, Bob dice: \"Ricordi quella canzone che ascoltavamo ieri?\" E con questo, Bob imposta il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nella seconda bolla di dialogo, Alice dice: \"Intendi per favore verso di me o ho una sensazione?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "che è la domanda alternativa. E nella terza bolla, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio, la nuova"}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniamo automaticamente la prima e la seconda bolla di discorso, ma la terza viene compilata dall'annotatore. La prima bolla di discorso viene scelta tra alcuni suggerimenti manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "La seconda, che è la domanda alternativa, viene generata come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo sempre un modello semplice. Vuoi dire A o B? Dove A e B sono esempi tratti da Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro, e di solito è più difficile fare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'at-rand uniforme."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, ad esempio, due libri con lo stesso nome, il ritorno."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso si verifica quando hanno descrizioni simili su Wikipedia. Infine, quando presentano caselle informative o attributi simili su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista, per esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando mostriamo questa domanda alternativa agli annotatori, essi conoscono il nome di queste entità, ma non necessariamente conoscono l'entità stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo quindi alcune informazioni di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "e poi chiedete agli annotatori di ascoltare almeno alcune di ciascuna canzone e di leggere le informazioni su ciascuna canzone. Ecco, ad esempio, il risultato della ricerca Google per la canzone Easy Annotation."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini di nuovo da Wikipedia in modo che gli annotatori sappiano come si presentano."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Poi chiediamo agli annotatori di scegliere una di queste entità, per esempio, qui la prima, e di descriverle utilizzando da tre a cinque espressioni di riferimento indirette."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica per pianoforte. Ecco alcuni esempi del nostro dataset. Ad esempio, quello senza parole, non quello con il ragazzo di 12 anni, o quello fittizio, o proveniente dall'Azerbaigian, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus Altentities contiene 6000 domande alternative distribuite in tre domini e 42.000 espressioni di riferimento indirette. I risultati ottenuti con il modello T5XLARGE sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, allora l'accuratezza è davvero alta. È intorno al 92-95%. Ma questo non è realistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso ad alcune conoscenze pregresse parzialmente sovrapposte, l'accuratezza è compresa tra l'82% e l'87%, il che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze pregresse."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 60%. Quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili a diversi ambiti. Ecco un link al nostro dataset. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sara Pape dell'Università di Trento e della Fondazione Bruno Kessler e vi presenterò brevemente il documento \"Attention as a Guide for Simultaneous Speech Translation\", un lavoro congiunto con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o simulST, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli SimulST attuali? Di solito vengono addestrate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "procedure di formazione lunghe e complesse, ad esempio la formazione che coinvolge diversi obiettivi di ottimizzazione,"}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza, ad esempio addestrare un modello con una latenza media di 1 secondo e un altro con una latenza di 2 secondi e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, qual è la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "I primi due utilizzano modelli SD offline già esistenti senza riaddestramento o adozione di un'architettura specifica per un singolo SD. Utilizzano un solo modello per ogni regime di latenza e gestiscono la latenza tramite parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "e sfruttare le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, ovvero il meccanismo di cross-attenzione. Potete vedere un esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre l'attenzione ADAT o encoder-decoder e si tratta di una strategia per la quale decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Una parola viene emessa se la tensione non è concentrata, cioè, la sua somma è al di sotto di una certa soglia α, verso l'ultima riga di frame di discorso, il che significa che le informazioni ricevute sono..."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se riceviamo un frammento di discorso contenente \"I'm going to talk about\" e il nostro modello prevede la traduzione in tedesco"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E noi esamineremo i pesi di cross-attention"}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "vedremo che le prime due parole indicano i frame di discorso ricevuti per primi, mentre l'ultima parola indica gli ultimi frame di discorso ricevuti come frame di discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che le prime due parole verranno omesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, poiché la somma dell'attenzione incrociata è superiore a una certa soglia alfa, non emetteremo l'ultima parola e attendiamo un altro blocco di discorso."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro blocco di discorso e il nostro modello prevede altre tre parole, daremo un'occhiata ai pesi di cross-attention."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "vedremo che nessuna parola indica gli ultimi fotogrammi del discorso di lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che verranno emesse queste tre parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se osserviamo i principali risultati di ciò,"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Rappresentiamo i risultati della traduzione simultanea del discorso su grafici in cui abbiamo da un lato il blu che misura la qualità della traduzione e il ritardo medio"}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Questa è la misura della latenza. E consideriamo anche il ritardo medio consapevole del calcolo che tiene conto dei tempi di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vogliamo che le nostre curve siano il più possibile alte in questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "ma vogliamo anche che siano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E li confrontiamo con strategie adeguate che si applicano anche ai modelli offline, come la strategia del wet-key e l'accordo locale. E li confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la pre-traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea del discorso in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo che supera tutte le strategie applicate ai modelli offline poiché le curve sono spostate verso sinistra"}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, se consideriamo il tempo effettivo trascorso o il tempo di elaborazione consapevole, questa è la strategia più veloce."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate scoprire ulteriori risultati, leggete il nostro articolo. Inoltre, abbiamo reso disponibili in open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ying, e il mio collega Zhiyang e io presenteremo la nostra ricerca su Multi-Improvement, Migliorare l'Apprendimento Breve Seriale Multimodale tramite l'Ottimizzazione dell'Istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Grazie ai progressi nei grandi modelli linguistici, molte ricerche hanno iniziato a esplorare nuovi paradigmi di apprendimento per il riutilizzo di modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che il tuning dell'istruzione consente ai grandi modelli linguistici di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione dell'istruzione si concentra sul miglioramento delle prestazioni della tabella seriale nei compiti relativi solo al linguaggio, mentre la visione artificiale e i compiti multimodali sono stati trascurati."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, vogliamo indagare se l'ottimizzazione dell'istruzione su modelli pre-addestrati multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali non visti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di set di dati di istruzione tra NLP e multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono più di 1600 compiti di istruzione basati solo sul linguaggio. Tuttavia, non esiste un compito di istruzione multimodale su larga scala disponibile pubblicamente. Pertanto, ciò ci motiva a creare un dataset di sintonizzazione per l'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi-Instruct, il primo dataset di riferimento per l'accordatura delle istruzioni multimodali che consiste in 62 compiti multimodali diversi che coprono 10 categorie di tavole."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi compiti sono derivati da 21 dataset open source esistenti e ogni compito è corredato da 5 istruzioni scritte da esperti."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare sul tuning dell'istruzione multimodale sul nostro dataset proposto, prendiamo OFA, un modello pre-addestrato multimodale unificato, come nostro modello di base. OFA utilizza un vocabolario unificato per il linguaggio, i token delle immagini e le coordinate di una casella delimitante."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui mostriamo alcuni esempi tratti dal nostro dataset Multi-Instra."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "Per unificare l'elaborazione di una varietà di tipi di dati di input e output,"}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo dell'OFA e formuliamo tutti i compiti in un formato sequenza-a-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentati nello stesso spazio di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Ok, ora parlerò del tuning dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per il set di dati di addestramento, utilizziamo 53 compiti di 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento sul senso comune per il test e selezioniamo ulteriori 5 compiti dal gruppo VQA e vario."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutti i casi nel set di test per ogni compito. Inoltre, selezioniamo casualmente 20 compiti dal set di test delle istruzioni naturali come compiti non visti per l'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Quindi utilizziamo un grande modello OFA pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante il test, per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Segnaliamo le prestazioni medie e massime e la deviazione standard delle prestazioni in tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un compito di classificazione multi-modello, riportiamo l'accuratezza. Se è un compito di generazione multi-modello, riportiamo ROUGE-L. Per il compito di NLP, riportiamo anche ROUGE-L."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Quindi, questa misura la capacità del modello di produrre costantemente gli stessi risultati per lo stesso compito, indipendentemente dalla leggera variazione nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il nostro risultato principale. Come possiamo vedere, la regolazione dell'istruzione può migliorare significativamente le prestazioni dell'OFA nei compiti multi-modello in scena."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Anche l'apprendimento trasferito da set di dati di istruzione naturali può essere utile per l'ottimizzazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo vedere come, all'aumentare del numero di compiti, il modello raggiunge prestazioni migliori e, nel frattempo, una sensibilità inferiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento, utilizzando un'istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurne notevolmente la sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo mostra l'effetto di diverse strategie di ottimizzazione sulla sensibilità del modello. Come possiamo vedere, attraverso l'apprendimento trasferito da un set di dati di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto al modello OFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche osservare che l'apprendimento trasferito dal set di istruzioni di Nitro può aiutare OFA a ottenere prestazioni molto migliori sul set di istruzioni di Nitro."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "Nel complesso, proponiamo il primo set di dati per l'ottimizzazione dell'istruzione di più modelli su larga scala. Miglioreremo in modo significativo la capacità zero-shot di OFV e esploreremo diverse tecniche di apprendimento per trasferimento, dimostrandone i vantaggi. Progetteremo una nuova metrica chiamata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Un'altra cosa, stiamo raccogliendo un set di dati di apprendimento per istruzioni multimodali molto più ampio, con circa 150 ulteriori compiti linguistici varianti, e li renderemo disponibili. Questo è un codice QR per i nostri dati e il nostro modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Kostav Sinha e sono lieto di darvi il benvenuto alla nostra discussione sul nostro articolo ACL 2023, \"Le valutazioni di accettabilità dei modelli linguistici non sono sempre robuste al contesto\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con John Gauthier, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo lavoro, rivediamo i paradigmi dei coppie minime."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il paradigma accoppiato minimo valuta fondamentalmente i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità come in blimp, sintassi, gem o accettabilità in termini di stereotipi, come le coppie incrociate."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma di coppie minime, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammaticalmente corretta, e poi mostrare una frase accettabile o una frase grammaticalmente scorretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi la speranza è che il modello attribuisca fondamentalmente una maggiore probabilità alla frase accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP fondamentalmente non ci permette di valutare l'accettazione di un modello verso frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Oggi i grandi modelli linguistici stanno sviluppando finestre contestuali sempre più lunghe. È quindi fondamentale valutare l'accettabilità del modello in tutta la finestra contestuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivisitare la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Questo è l'approccio. Quindi, quello che facciamo è simulare queste sequenze più lunghe. Rivediamo i set di dati stessi e poi ricreiamo le frasi scegliendo frasi accettabili o inaccettabili da quei dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, qui abbiamo scelto un tipico esempio di grammaticalità dal set di dati del blimp, relativo al caso dell'isola di complemento."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è ricreare sequenze più lunghe e accettabili, che abbiano la stessa corrispondenza della struttura grammaticale, estraendo frasi grammaticali dall'Isola di Argent"}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che a quella non accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento. E ciò potrebbe anche essere utilizzato per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è ciò che chiamiamo scenario di non corrispondenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui, le frasi provengono ancora da dataset pertinenti, ma non dallo stesso dataset che si sta valutando. E possiamo fare lo stesso per il caso di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "Come se il contesto provenisse da un sottoinsieme diverso del dataset o se fosse del tutto irrilevante per il momento attuale, per la frase che stiamo analizzando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, come si comporta il modello? Prima di tutto, guardiamo le frasi di Wikipedia, che sono completamente irrilevanti per l'attuale coppia di query. E qui scopriamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT-2. E qui, nella linea tratteggiata arancione, vediamo che i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Cosa succede quando scegliamo frasi dallo stesso insieme di dati?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili dallo stesso set di dati di palloncino o di gemma sintattica"}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E poi vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o prefissi inaccettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando combiniamo la struttura, cioè quando scegliamo le frasi che descrivono gli stessi fenomeni nel testo sulla persona da incolpare, Jim,"}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "osserviamo un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora questo, e questo è molto grande, come questo effetto aumenta con la lunghezza del contesto. E questo probabilmente influenzerebbe i nuovi modelli linguistici, che hanno una grande finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Quindi perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto una serie di analisi in cui abbiamo cercato di mantenere la frase di input preservandone la struttura rilevante, ma aggiungendo del rumore all'input. E dopo aver effettuato diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "constatiamo che nessuno di questi rumori sta effettivamente facendo cambiare al modello, uh, il suo corso in termini di come ci mostra la tendenza del giudizio MPP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "In generale, abbiamo riscontrato che i modelli sono sensibili alle perturbazioni e alle frasi in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "Cioè, quando perturbamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni. E quando perturbamo le frasi nel dominio inaccettabile, osserviamo una diminuzione dei giudizi MPP in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti, che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione dell'MPP, il modo in cui la eseguiamo attualmente con input brevi e di una sola frase, potrebbe non catturare appieno la conoscenza astratta del modello linguistico in tutta la finestra contestuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Si prega di leggere il nostro articolo per ulteriori dettagli sui nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Yusheng Zhang e vengo dalla Penn State University. Oggi presenterò il nostro lavoro, Cross-lingual Semantic Parsing in Multiple Natural Languages and Minimal Representations."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Quindi l'analisi semantica è un compito che consiste nella costruzione di rappresentazioni semantiche delle query degli utenti, come SQL e il calcolo Lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "L'analisi semantica interlinguistica è il compito di tradurre le query in più lingue naturali in più rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Hau-Tieng Wu, Ph.D.: Come mostrato nella sua figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali per sequel lambda o fun QL e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "I modelli esistenti di analisi semantica interlinguistica sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "mancano alcune coperture per certi linguaggi naturali. Il cinese è assente e"}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "a causa della copertura su alcune mini rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il calcolo lambda manca."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "oppure vengono valutati solo su un determinato modello neurale. Ad esempio, esiste un solo modello per valutarli."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo un esemplare. Forniamo un set di dati esemplare uniforme per il collegamento incrociato dell'analisi semantica in più lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene nove set di dati in vari ambiti, cinque compiti di analisi semantica, otto rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è un test di traduzione. Usiamo l'API di Google Translate per tradurre la fonte nella lingua di destinazione, poi utilizziamo un modello monolingue per addestrare una valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, addestreremo il modello inglese su una query in inglese e, durante l'inferenza, traduciamo la query tedesca utilizzando l'API in inglese e poi utilizziamo il modello addestrato per prevedere la query SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E testeremo anche il modulo monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di partenza è la stessa della lingua di arrivo, ad esempio, dal tedesco al tedesco o dall'inglese all'inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testiamo anche l'impostazione del campo di tiro monolingue addestrando modelli monolingue con solo il 10% dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "e abbiamo testato un modello multilingue, che abbiamo addestrato su un unico modello multilingue per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo unito le query in tedesco, inglese e cinese per addestrare un modello multilingue. E durante l'inferenza, possiamo utilizzare questo modello"}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "per tradurre query tedesche o query cinesi o et cetera."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "Consideriamo anche il trasferimento multilingue senza esempi e con pochi esempi. Noi ci alleniamo su una lingua sorgente e trasferiamo su un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante l'addestramento, ci addestriamo su query in inglese o sulla combinazione di query few-shot in inglese e tedesco per addestrare un modello multilingue e prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche ottenuto molti risultati interessanti. Quindi, per quanto riguarda l'analisi dei modelli monolingue, valutiamo due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "incluso l'encoder PDR, che sta per encoder multilingue pre-addestrati con decodificatori basati su puntatori, come XLMR più PDR e BERT più PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo anche i modelli encoder-decoder multilingue pre-addestrati, come mBART e MT5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che l'Encoder-Decoder ottiene le migliori prestazioni su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "e valutiamo su MT5 e XLMR oltre all'impostazione multilingue PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo scoperto che l'encoder-decoder o l'encoder-PDR possono essere migliorati addestrandoli in una miscela di varie lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo scoperto che ciò è dovuto al fatto che la maggior parte dei principali linguaggi naturali può ottenere un miglioramento delle prestazioni, tranne che per l'inglese, le cui prestazioni peggiorano in sette dataset e migliorano solo in tre dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Penso che questo sia noto come la maledizione della multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche il divario di prestazione tra le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento multilingue a pochi esempi. La linea arancione rappresenta il trasferimento multilingue a zero esempi, mentre la linea verde rappresenta l'impostazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che confrontando la linea verde e quella arancione, abbiamo scoperto che per l'impostazione zero-shot, il divario di prestazioni nel trasferimento interlinguistico è significativo. E confrontando la linea blu e quella arancione, abbiamo scoperto che per l'impostazione few-shot, il divario di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche riscontrato altri risultati interessanti. Ad esempio, l'encoder-decoder supera i lavori precedenti o ottiene risultati comparabili. La rappresentazione in lingua inglese può migliorare significativamente le prestazioni dei pochi linguaggi naturali presi in considerazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo scoperto che modelli linguistici multilingue come CODIS e BLUE sono ancora inadeguati per compiti di analisi semantica interlinguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo creato Examplar, un benchmark unificato per l'analisi semantica incrociata con più lingue naturali e rappresentazioni principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto uno studio di riferimento completo su tre tipi rappresentativi di modelli linguistici multilingue, e i nostri risultati mostrano molti risultati interessanti, ecc. Vi invitiamo a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo David Villar e vi darò una breve panoramica dell'articolo \"Traduzione della piattaforma Grunt, valutazione delle strategie e delle prestazioni\". Questo è un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "PARM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri presentato l'anno scorso nel 2022. È stato addestrato su una vasta collezione di testi che comprende 780 miliardi di documenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "Al momento della pubblicazione, raggiunge lo stato dell'arte in centinaia di compiti di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, presentiamo il primo studio sistematico di un grande modello linguistico per l'elaborazione di traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità AMT. Ciò comporta l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo due sistemi all'avanguardia. Quindi i sistemi con le migliori prestazioni sono la valutazione WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche MT neurali all'avanguardia e mostriamo inoltre i risultati della valutazione umana basata sugli esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei PROM."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "Il prompt ha una grande influenza sulle prestazioni degli LLM per la traduzione. Come possiamo vedere in un semplice esperimento in cui utilizziamo un prompt one-shot e forniamo due prompt diversi per ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "Nella maggior parte delle frasi, 516 su 1000, la differenza osservata è di più di un punto di sfocatura."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "E questo può arrivare, nei casi estremi, fino a 40 punti di sfocatura. Quindi è importante selezionare una buona strategia di prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nei nostri esperimenti, abbiamo optato per una strategia di prompt a cinque colpi in cui abbiamo semplicemente contrassegnato ogni frase fornita al sistema con la lingua in cui è scritta."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "Quindi in questo esempio, in cui eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi sorgente, sono contrassegnate con due punti tedeschi e le traduzioni in inglese con due punti inglesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo visto che la forma effettiva del prompt non ha una grande influenza nel caso di diversi prompt brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È fondamentale per il prompting a zero e a un colpo. E quando si passa, come nel nostro caso, al prompting a cinque colpi, non c'è quasi nessuna differenza rispetto alla forma effettiva del prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a portare il peso maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, i risultati dei nostri esperimenti dimostrano che la qualità dell'esempio è più importante della somiglianza con la frase di partenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "È quindi importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo i prompt di selezione dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, e anche i risultati sono migliori. Quindi, una migliore performance quando si utilizzano i dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale rispetto alle traduzioni di Palm. Ma Palm si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Dalle analisi che abbiamo condotto sull'inibizione umana eseguita utilizzando il framework MQM, abbiamo appreso che la fluidità di PALM è paragonabile a quella dei sistemi più avanzati, ma la differenza principale risiede nella precisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Quindi sembra che Palm scelga di produrre una traduzione che suona meglio a volte tralasciando parti della frase di origine che sono presenti nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria \"style-outward\" per PAN è inferiore rispetto ai sistemi all'avanguardia, il che rappresenta un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "che PARM fornisce un output davvero fluido, ma comunque con alcuni problemi di precisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa breve panoramica. Per maggiori dettagli, si prega di consultare la presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawei, uno studente di dottorato presso l'Università del Saarland in Germania. In questo video, vorrei presentare il nostro lavoro recente, Più debole di quanto pensi, uno sguardo critico all'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Xiao Yusheng, Mario Smusbach, Gia Steffen e DT Schlaukel."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione al supervisionamento debole e all'apprendimento debolmente supervisionato."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura deboli, come semplici regole euristica, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se addestreremo direttamente le reti neurali su dati debolmente etichettati, le reti neurali tenderanno a memorizzare il rumore etichettato e non generalizzeranno."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Nell'apprendimento debolmente supervisionato, vengono proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali in presenza di tale rumore di etichetta, in modo che i modelli addestrati generalizzino comunque bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In recenti lavori in WSL, dove WSL sta per Weekly Supervised Learning, una affermazione comune è che le persone dicono di addestrare i modelli solo sui dati di etichettatura settimanali e di ottenere alte prestazioni su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è sbagliata, ma c'è un problema."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "cioè che le persone presumono che esista un ulteriore set di validazione pulito disponibile per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo fermarci su questo problema, poiché ciò implica che sono necessarie ulteriori annotazioni manuali nell'apprendimento settimanale di SuperWise. Ma, come un elefante nella stanza, questa necessità viene spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il suddetto dubbio ci porta a porci tre domande di ricerca. Primo, i dati di validazione puliti sono necessari per WSL? O possiamo forse utilizzare un set di validazione rumoroso?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, se i dati puliti sono necessari, o se i dati puliti sono obbligatori affinché WSL funzioni, quante etichette pulite ci servono? Infine, dovremmo utilizzare solo i campioni puliti per la validazione, o esistono modi migliori per utilizzarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, scopriamo che, interessantemente, i recenti metodi WSL richiedono effettivamente campioni di piatti bianchi e puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "Altrimenti, si verifica un forte calo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli originali."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "cioè che la formazione è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che i metodi WSL richiedono dati accuratamente etichettati per funzionare correttamente, e il costo dell'annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "In genere, per ottenere prestazioni di alta qualità, abbiamo bisogno di soli 20 campioni per classe."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questa non è la fine della storia, perché se in ogni caso decidiamo di accedere a campioni puliti, allora l'addestramento diretto su di essi otterrà risultati ancora migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di prestazione tra gli approcci di ottimizzazione fine, che vengono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a battere gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni ottenuto nei precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare l'ottimizzazione sui campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere dai dati, il modello Van Lina denominato FTW inizialmente ha prestazioni inferiori rispetto ai metodi WSL più complessi come i coseni."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se permettiamo di continuare il perfezionamento sui campioni puliti, allora FTW si comporta altrettanto bene come altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni puliti e annotati manualmente affinché funzionino correttamente. Il loro miglioramento delle prestazioni e la loro praticità sono fortemente sopravvalutati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre raccomandazioni concrete per i lavori futuri sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, riportare i criteri di selezione del modello. Ad esempio, indicare se la selezione del modello è stata effettuata su campioni di validazione ben puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, i metodi WSL dovrebbero essere confrontati con le linee guida per l'apprendimento futuro, poiché entrambi lavorano su campioni puliti. In terzo luogo, il continuo perfezionamento è una linea guida semplice ma efficace che dovrebbe essere presa in considerazione nei futuri lavori nel campo WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso open source il nostro codice. Potete trovarlo tramite il codice QR su questa diapositiva. Vi invitiamo a consultarlo. Grazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E sono Sarah Finch. E oggi vi racconteremo tutto su ABCeval, un nuovo approccio dimensionale alla valutazione dell'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal laboratorio di elaborazione del linguaggio naturale di Emory, diretto dal professor Gino Choi presso l'Università di Emory, in collaborazione con l'intelligenza artificiale di Amazon Alexa."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo quindi che tu abbia appena sviluppato un modello di dialogo e desideri vedere quanto sia performante rispetto allo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La pratica comune è quella di utilizzare una valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni assegnando un punteggio su una scala Likert."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio consiste nel chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi esistenti o scale di tipo Likert."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se la risposta di ciascun modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddire se stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio \"annotating behaviors in chat\" o, in breve, \"ABC eval\". Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che, secondo la letteratura recente, influenzano la qualità della chat."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABC eval è in grado di misurare la frequenza con cui i modelli di chat commettono vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, la misurazione ABC eval valuta il numero di turni in cui un modello di chat ignora il suo interlocutore o dice qualcosa di irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o contraddice il suo partner, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o non riesce a mostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC eval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo valutato queste conversazioni utilizzando anche tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti di coppia a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi di questi risultati di valutazione, abbiamo scoperto che le etichette di comportamento di valutazione ABC sono nel complesso più affidabili rispetto alle etichette raccolte con i metodi esistenti, come misurato dall'accordo tra gli annotatori su 100 conversazioni etichettate in duplice copia."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette di valutazione ABC sono più predittive della qualità complessiva della conversazione rispetto ai parametri ottenuti con i metodi esistenti, come dimostra questa semplice analisi di regressione lineare."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, si può vedere come la misurazione della proporzione di turni con contraddizioni personali e di coppia spieghi rispettivamente il 5% e il 10% della qualità della conversazione, mentre i punteggi medi di coerenza Likert spieghino solo il 4% o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a gradi."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Si può vedere come la combinazione di tutte le metriche di valutazione ABC spieghi oltre il 25% della qualità della conversazione. E quando si rimuovono le metriche una alla volta, la maggior parte di esse comporta la perdita di una quantità significativa di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità e meno di queste metriche contengono informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste metriche di valutazione ABC, affidabili, informative e distinte, ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione maggiore rispetto ai metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "Si può vedere che nei risultati del nostro esperimento rimangono ancora diverse sfide, che sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato presentano violazioni del senso comune in circa il 20% delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Circa il 15% delle risposte fornisce informazioni irrilevanti. E si contraddicono o contraddicono il proprio partner circa il 10% delle volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Grazie al rapido progresso nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è un motivo in più per perseguire metriche di valutazione affidabili e precise per confrontare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Ci auguriamo che ABC Eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'IA conversazionale progredisca nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Kaio-Yin, e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, Andre F.D. Martins e Graham Newbig."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi molte traduzioni dipendono dal contesto. Ad esempio, come tradurremo \"mole\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Bene, se la frase precedente era, le cose potrebbero diventare pericolose se i ministri lo scoprono, allora Mo si riferisce a uno spia. Ma se la frase precedente era, potrebbe essere qualcosa di serio, dottore? Allora Mo si riferisce a un neo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a seconda del contesto, il significato della parola cambia e quindi cambia anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come il Blue incapaci di catturare queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "Alcune persone hanno suggerito una valutazione mirata delle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla cura umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, cerchiamo di rispondere a queste due domande:  \n1. Quando la traduzione richiede un contesto?  \n2. Quanto bene i modelli gestiscono questi casi?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. Ciò si ottiene misurando quanta informazione il contesto C fornisce sull'obiettivo Y data la fonte X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "si può pensare a CXMI come alle informazioni ottenute dando un contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo CXMI al CXMI puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo considerare le parole con un alto PSXMI come quelle che richiedono un contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un alto PCXMI per cercare schemi tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "E noi eseguiamo la nostra analisi su trascrizioni di TED Talk che sono state tradotte dall'inglese in 14 lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo la nostra analisi a tre livelli diversi. In primo luogo, esaminiamo le etichette dei tipi di parte del discorso che hanno un alto valore di PCXMI."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un P6MI relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "E allo stesso modo, ci accorgiamo che alcune lingue richiedono anche un contesto quando vogliamo scegliere la forma verbale appropriata. Esaminiamo quindi gli elementi del vocabolario che hanno un PCSXMI elevato in media su tutte le sue diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario il contesto per tradurre i nomi propri e assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "E allo stesso modo, ci accorgiamo che il contesto è supportato per tradurre con la giusta formalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esaminiamo i diversi token individuali che hanno un alto p6mi. Questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora utilizziamo i risultati della nostra analisi per progettare un punto di riferimento per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, abbiamo creato dei tagger per identificare automaticamente le parole che appartengono al fenomeno, e abbiamo chiamato il nostro tagger Multilingue Consapevole del Discorso, o tagger MUDA."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi anche osservare che le diverse lingue presentano proporzioni diverse di questi fenomeni discorsivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Quindi utilizziamo il tagger Muda applicandolo al corpus parallelo che vogliamo utilizzare per la valutazione. E applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il tagger Muda ha identificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "Infine, utilizziamo il nostro parametro di riferimento e altri indicatori per valutare i diversi modelli di traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, quando utilizziamo metriche a livello di corpus, per il blu, scopriamo che i modelli agnostici al contesto hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma poi, se utilizziamo COMET, i modelli sensibili al contesto ottengono i migliori risultati. E se utilizziamo la misura F delle parole, allora i modelli con o senza contesto hanno prestazioni confrontabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo metriche a livello di corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark Muda per valutare i modelli e scopriamo che i modelli consapevoli del contesto sono significativamente più accurati rispetto ai modelli che non utilizzano il contesto per certi fenomeni del discorso, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori di altri modelli che non utilizzano il contesto su altri fenomeni come le ellissi, i pronomi e la forma dei verbi. Quindi questo suggerisce dove dovremmo vedere maggiori progressi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche diversi sistemi commerciali e il nostro benchmark dimostra che DeepL è generalmente più accurato di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, eseguiamo un'analisi basata sui dati su 14 coppie linguistiche per identificare quando le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizziamo i nostri risultati per costruire un punto di riferimento per la traduzione automatica a livello di documento, che può aiutarci a identificare quali modelli di fenomeni del discorso possono gestire bene o male, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione. Ci vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanis Lavrak e vi presenterò i nostri lavori su Dr. BERT, un modello robusto pre-addestrato in francese per il dominio biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, parleremo prima del modellamento linguistico nell'assistenza sanitaria. Poi presenteremo il contributo principale del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in francese chiamato Dr. Bert, che si basa su Roberta e viene addestrato su NACHOS, un insieme di dati medici raccolti dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto un confronto tra modelli con più impostazioni di pre-addestramento e fonti di dati. Quindi, presentiamo i nostri risultati su 11 compiti a valle biomedici e clinici in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "E infine, concludiamo gli esperimenti e vi diamo ulteriori dettagli su come accedere ai modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Dal suo lancio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre notevoli miglioramenti delle prestazioni rispetto ai metodi statici e contestualizzati storici come Word2Vec, FastText o NWO."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert, e ad altri settori come il biomedico con PAMED-BERT e BioBERT, e il clinico con Clinical-BERT, ma principalmente in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "I modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati specifici del settore."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora, la Francia non disponeva di un moderno software open source per il settore biomedico."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci chiediamo quindi quale sia la fonte di dati più appropriata per un'ampia gamma di utilizzi. E i dati attuali sono una buona sostituzione per i dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il dottor Burt con il nostro modello Schubert, che si basa su dati anonimi ottenuti dall'ospedale non universitario che ospita la nostra casa."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, ci chiediamo: quanti dati abbiamo bisogno di utilizzare per addestrare un modello specializzato su dati francesi? Sono 4 gigabyte, 8 gigabyte o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero. Una prima versione del Dr. Bert con sette gigabyte di nachos, una seconda versione di quattro gigabyte di set di nachos,"}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che è un modello clinico, con 4 GB di frasi tratte dalle cartelle cliniche. E una versione finale di Schubert con un mix di un sottoinsieme di 4 GB di dati naturali e 4 GB di cartelle cliniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con un pre-addestramento continuo per analizzare l'impatto delle strategie di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso del Camembert e addestrato su quattro gigabyte di nachos. Un altro basato anch'esso sul Camembert, ma addestrato questa volta sui quattro gigabyte di clink e lots."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "Infine, uno basato sul modello biomedico inglese, Bermud-Bert, addestrato su un set di quattro gigabyte di estratti. In totale, abbiamo sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, abbiamo raccolto diversi compiti pubblici e privati non eccitanti come il riconoscimento di nomi e identità, la classificazione, il tagging delle parti del discorso e la risposta alle domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questi modelli sono confrontati con sei modelli di riferimento, ovvero Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCnet 4 GB, Pumatbert, BioBERT e ClinicalBERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "L'evoluzione evidenzia che il modello funziona meglio con dati della stessa natura di quelli con cui è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo ottenere i dati da fonti eterogenee e osservare che questi ultimi sembrano essere più versatili. Osserviamo inoltre che l'utilizzo di più dati si traduce in prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "Nel complesso, l'addestramento da zero sembrava ottenere prestazioni superiori nella maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento sul pre-addestramento continuo utilizzando il peso e il tokenizer di Pumet-BERT, addestrato sul sottoinsieme di 4 gigabyte di NACHOS, ha mostrato risultati comparabili a quelli ottenuti con Dr.BERT 4 gigabyte da zero."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "questo non vale per il modello basato su pesi camembert e pelle token, che soffre di problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, in conclusione, il nostro sistema offre prestazioni migliori su 9 dei 11 compiti a valle e supera globalmente i risultati del modello generico qui presente, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo anche che i dati specializzati sono migliori, i dati più specializzati sono ancora migliori, ma non si adattano bene all'ampliarsi delle dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli pre-addestrati ottenuti da NACHOS sono liberamente disponibili sulla faccia UGIM, e tutti gli script di addestramento sono nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi grazie per questa presentazione, e non vediamo l'ora di vedere le azioni che verranno intraprese dopo la sessione a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lindemann e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione compositiva senza alberi utilizzando il tagging multiset e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei consulenti, Alexander Koller e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste singolarmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto dell'analisi semantica, il test per la generalizzazione compositiva potrebbe essere così. Come al solito, abbiamo un insieme di enunciati di addestramento, in questo caso, la ragazza dormiva e Mary sapeva che la ragazza dormiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Questi enunciati sono abbinati a forme logiche che rappresentano gli aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "A differenza della valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha mostrato una ricorsione poco profonda durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli sequenza-sequenza naivi faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono risultati che sono distaccati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono codificate a colori nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono destinati a catturare il processo compositivo che collega le enunciazioni con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta computazionalmente costoso. Tipicamente, ciò comporta una notevole pre-elaborazione delle forme logiche specifica per il formalismo, ad esempio, per gestire i simboli variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L'acquisizione degli alberi può anche comportare procedure di induzione grammaticale specializzate."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, non utilizziamo alberi e introduciamo un modello sequenza-sequenza neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, dimostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede la previsione dell'output dall'input in due passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Prima, etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passaggio, abbiamo tutti i token corretti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Per questo motivo, nel secondo passaggio, utilizziamo un altro modello per prevedere una permutazione e metterli nell'ordine corretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona più o meno così."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Passiamo da sinistra a destra sull'output e determiniamo quale token del multiset inserire in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Poi saltiamo al prossimo token multiset per determinare il secondo token dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token dell'output in modo simile saltando a un altro token multiset. Proseguiamo con questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "fino a che ogni token del primo stadio non sia stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per darvi un'anticipazione dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri con un ampio margine nella generalizzazione alla ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Altri tipi di generalizzazione strutturale rimangono comunque molto difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo, risolviamo un paio di interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multisetter proviene, il che rappresenta una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte della formazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è un problema NP-difficile. Questo perché è legato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Approssimiamo questo con un rilassamento continuo ottimizzato per la GPU che ci permette anche di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desidera saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, la invitiamo a consultare il nostro articolo o a venire al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Akshata e oggi io e il mio co-autore Martin presentiamo il nostro lavoro, The Kipma Steps, che valuta l'integrazione della conoscenza da più fonti. Questo lavoro è il risultato di una collaborazione tra l'Università McGill, Mila e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione del linguaggio nazionale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Lavori recenti su compiti come la risposta alle domande dimostrano che i modelli possono utilizzare la conoscenza temporale pre-addestrata per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale richiede spesso conoscenze che vengono fornite anche al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, nella frase \"John ha visto il presidente appena eletto in TV\","}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su cosa fanno i precedenti e cosa sia una TVA, ma non possono sapere in modo affidabile chi sia l'entità specifica di questo incidente, John, o chi sia il nuovo presidente, perché il precedente potrebbe essere cambiato dopo l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i modelli di successo per compiti di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un compito di risoluzione della coreferenza progettato per verificare la capacità di attingere alle conoscenze disponibili in diverse fonti. Valutiamo il dataset con partecipanti allo studio umani e stabiliamo modelli di risoluzione della coreferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro dataset. Thirvin è un giudice. Kia è una fornaia. Thirvin e Kia si incontrarono in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"lui\", che in questo caso è \"servo\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un dato pronome richiede due tipi di informazioni. In primo luogo, la conoscenza specifica dell'entità, come ad esempio \"il giudice è un giudice\". In secondo luogo, la conoscenza di base, come ad esempio \"i giudici decidono i casi nei tribunali\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, le conoscenze di base vengono apprese durante l'addestramento preliminare dei grandi modelli linguistici, mentre le conoscenze specifiche delle entità sono tipicamente osservate al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Varia la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre impostazioni di KITMOS. In primo luogo, abbiamo l'impostazione tipica, pretrain di background, in cui si presume che la conoscenza di base sia disponibile al momento del pretrain."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è lo scenario di background, in cui le conoscenze di base sono disponibili sia al momento del pre-addestramento che al momento dell'inferenza. Infine, lo scenario di inferenza di background, in cui entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questa ultima impostazione è particolarmente interessante, poiché simula il caso in cui le conoscenze di base necessarie per risolvere un compito non fanno parte dei dati pre-addestrati dei modelli. Ad esempio, perché nuove occupazioni si sono sviluppate dal momento dell'addestramento pre-liminare."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controlliamo la disponibilità dei fatti nella vera fonte"}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di apprendimento pre-addestrato di base, assumiamo che la conoscenza di base, I politici cercano di ottenere seggi elettivi nel governo, sia contenuta nei parametri pre-addestrati. Nel contesto temporale raro, forniamo la conoscenza anti-specifica, Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di impostazione \"sullo sfondo-sopra\", forniamo inoltre non solo informazioni anti-specifiche, ma anche conoscenze di base sui politici nel contesto di tipo interferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "e per l'impostazione dell'interferenza di sfondo, forniamo l'occupazione fittizia Meritur invece che Politico, perché è improbabile che Meritur sia contenuto nel paradigma pre-addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato il dataset sia con partecipanti allo studio umani che con modelli di risoluzione della co-referenza consolidati. In questa figura, mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più difficile dell'impostazione di pre-addestramento di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "Con la nostra formazione specifica per il compito su KITMOS, entrambi i modelli non ottengono buoni risultati. Tuttavia, quando addestrati su KITMOS, sia C2F che BFQF ottengono risultati significativamente migliori rispetto alla scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Ciò suggerisce che, quando vengono addestrati su dataset generali di risoluzione della co-referenza, i modelli imparano a sfruttare le indicazioni superficiali, che non sono utili quando si effettua il test su kitmos, dove tali indicazioni sono state rimosse."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Esperimenti aggiuntivi con conoscenze fittizie indicano che anche i modelli con le prestazioni migliori non riescono a integrare in modo affidabile le conoscenze retroattive fornite solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i punti principali del nostro articolo, molti modelli di rivoluzione co-referenziati sembrano incapaci di ragionare sulla conoscenza proveniente da fonti diverse senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo la conoscenza proveniente da più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, anche i modelli con le prestazioni migliori sembrano avere difficoltà con la conoscenza retrospettiva integrata in modo affidabile presentata solo al momento dell'inferenza. Se siete interessati a ulteriori dettagli, consultate il nostro articolo e date un'occhiata al dataset nel codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra, e oggi parlerò del nostro articolo, \"Persone contrassegnate: l'uso di prompt in linguaggio naturale per misurare gli stereotipi nei modelli linguistici\". Questo lavoro è stato realizzato in collaborazione con Esen Dermush e Dan Jorofsky."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su dataset costruiti manualmente che richiedono molto tempo per essere curati."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie come associazioni negative con gruppi particolari."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte del lavoro in questo ambito non tiene conto dell'intersezionalità, ovvero l'idea che le identità sociali multifaccettate possano amplificare i pregiudizi e rappresentare punti unici di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà per cui questi nuovi LLM regolati tramite istruzioni sono molto bravi a rispondere alle istruzioni nei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario utilizzando un prompt come, immagina di essere una donna asiatica, descriviti."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco quindi alcune generazioni di esempio da GPT-4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Subito si nota che, sebbene i risultati non siano apertamente negativi o tossici nel senso tradizionale di queste parole,"}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono alcuni schemi interessanti"}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è rappresentata come modesta. La donna del Medio Oriente è descritta con parole come esotica e come se si riferisse a una regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le donne di colore fanno riferimento alla loro discendenza, mentre il personaggio dell'uomo bianco non ha nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "Per catturare questi schemi, il nostro metodo si compone di due parti. La prima consiste nella creazione di queste persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri suggerimenti per generare queste persone sono stati ispirati da uno studio in cui sono stati dati questi suggerimenti a soggetti umani, scoprendo che, dandoli a soggetti umani, sono stati anche in grado di far emergere stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre ciò consente un confronto diretto tra le nostre persone generate e le risposte scritte umane."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte è rappresentata dalle parole contrassegnate, un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, su cui tornerò tra breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di questo approccio è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su un lessico specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo delle parole contrassegnate si basa quindi sul concetto sociolinguistico di contrassegnatezza, che afferma che esiste un valore predefinito non contrassegnato e che qualsiasi gruppo che si discosta da tale valore predefinito è linguisticamente contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, la parola \"guerriero\" è solitamente associata agli uomini. Quindi, quando le persone descrivono una guerriera, di solito specificano \"guerriero di sesso femminile\" e indicano il termine con \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "In senso più ampio, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi emarginati sono solitamente contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel nostro metodo, per prima cosa indichiamo quali sono i gruppi non marcati e quelli marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "Poi confrontiamo le persone utilizzando il metodo delle parole di contrasto, che consiste essenzialmente nell'utilizzare rapporti log-odds ponderati per distinguere le parole principali per ciascun gruppo contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, per le persone di donne nere, useremmo parole offensive e confrontarliamo i rapporti con gli dei della legge sia con le persone bianche che con le persone maschili, perché questi sono i due gruppi non contrassegnati corrispondenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "E ora alcuni risultati. Quindi, per prima cosa, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte dagli esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, mentre le persone generate hanno tassi molto più elevati di parole Luxon, quelle scritte dagli esseri umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate presenti nelle persone generate sono davvero solo le parole \"alto\" e \"atletico\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in realtà, solo quelli positivi o almeno non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "E infatti, questo lessico non riesce a cogliere molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, per fare ciò, ci rivolgeremo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole apparentemente positive favoriscano stereotipi e narrazioni essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, riveleremo come queste rappresentazioni apparentemente positive riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, per i gruppi contrassegnati, le parole principali includono cose come cultura, tradizione, orgoglio e esotico. E queste parole definiscono questi gruppi solo in base alla loro relazione con la loro identità e li distinguono come diversi dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Questo contribuisce a una lunga eredità di discriminazione e di emarginazione per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molti luoghi comuni che si riflettono in queste parole, specialmente per le donne di colore. Quindi, per esempio, le parole che descrivono le donne latine includono termini come vibranti e curvilinee."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "che si collegano a un tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come piccola, delicata e setosa,"}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "che si collega a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomissae, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "Infine, per le donne di colore, vediamo che alcune delle parole più frequenti sono forti e resilienti."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si collega a un archetipo che la gente ha chiamato l'archetipo della donna nera forte. E sebbene a prima vista possa sembrare positivo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché esercita una forte pressione su queste categorie demografiche affinché siano resilienti e forti contro gli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, invece di lavorare per cambiare quegli ostacoli, si mette pressione su quelle persone affinché li superino, il che porta a risultati sanitari molto negativi per queste persone, tra gli altri danni."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "In senso più ampio, scopriamo che le parole per ogni gruppo contrassegnato riflettono in gran parte solo narrazioni essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, noi ricercatori dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una lente intersezionale per studiare i pregiudizi e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "Perché, per esempio, come questi stereotipi positivi, non sappiamo se sia perché c'è una sorta di strano..."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "un allineamento dei valori eccessivamente esagerato è in corso, o forse altri metodi anti-stereotipi che stanno portando a questi schemi perniciosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo davvero fare ipotesi o studiare ulteriormente senza maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato. Buon divertimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jingwei Yi e vengo all'Università di Scienza e Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È un piacere per me presentare un breve video pubblicitario sulla carta, \"Stai copiando il mio modello? Protezione del diritto d'autore dei grandi modelli linguistici per l'embedding e i servizi tramite watermark nascosto\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo prima il contesto relativo all'Embedding come Servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i grandi modelli linguistici come GPT, LAMA, PALM sono eccezionali nella comprensione e nella generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'integrazione come servizio è uno dei servizi basati su grandi modelli linguistici per assistere in vari compiti di elaborazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, OpenAI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, recenti studi hanno dimostrato che l'aggressore potrebbe rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore dell'embedding come servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "per proteggere il diritto d'autore dei servizi incorporati. Una delle soluzioni è incorporare una filigrana nel servizio del fornitore e rilevare se un altro servizio contiene la filigrana."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo della filigrana deve soddisfare le seguenti proprietà. In primo luogo, il metodo dovrebbe essere applicabile all'incorporamento dei servizi pubblicitari. In secondo luogo, la filigrana non dovrebbe compromettere l'utilità degli incorporamenti forniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "In terzo luogo, la filigrana dovrebbe essere abbastanza occulta per l'aggressore o quest'ultimo dovrebbe poterla rimuovere facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la filigrana deve essere trasferibile ai servizi dell'aggressore durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Le opere esistenti possono essere classificate in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo non è applicabile all'integrazione dei servizi pubblicitari o manca di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo, proponiamo EmbeddingMarker, un metodo di watermark basato su backdoor applicabile ai servizi di inserimento di annunci."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "Permettetemi quindi di illustrare i dettagli del nostro Embedding Marker. L'Embedding Marker comprende due passaggi principali: l'inserimento della filigrana e la verifica del diritto d'autore."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi passaggi principali, selezioniamo prima un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di filigrana, si definisce prima un obiettivo di incorporamento. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'incorporamento fornito è una somma ponderata dell'incorporamento di destinazione e dell'incorporamento originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del diritto d'autore serve a rilevare se un modello alla base di un altro servizio contiene il marchio verbale."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Per prima cosa, creiamo un set di dati di backdoor e un set di dati benigno. Il set di dati di backdoor contiene frasi in cui tutte le parole appartengono al set di trigger. Mentre tutte le parole nelle frasi del set di dati benigno non appartengono al set di trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il fornitore richiede incorporamenti dal servizio Steeler con il dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "Vengono calcolate la somiglianza cosinica e L2 tra l'embedding richiesto e l'embedding di destinazione. Calcoliamo la differenza di somiglianza tra i dataset benigni e quelli con backdoor, definita come delta cosinico e delta L2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto esperimenti su quattro dataset, AGnews, Mind, SSD2 ed Eraspam. Ipotizziamo che il fornitore applichi il dataset Wikitext per contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro set di dati mostrano che il nostro marcatore incorporato può avere ottime prestazioni di rilevamento mantenendo al contempo un'ottima utilità per i compiti successivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Confermiamo inoltre la segretezza dell'embedding fornito visualizzando l'embedding delle frasi su 4DataSet VOPCA. La legenda delle figure indica il numero di trigger in ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nelle figure, è difficile distinguere tra le rappresentazioni fattorizzate e le rappresentazioni normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Questo è tutto. Grazie. Benvenuti a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono una dottoranda in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato all'ACL 2023 come un lungo articolo sul transfer learning per il rilevamento della dissonanza affrontando la sfida delle classi rare."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel contesto del linguaggio. In parole semplici, la dissonanza cognitiva si verifica quando due credenze o azioni sono incoerenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "Come in questo esempio, in cui una persona afferma: \"So che le sigarette potrebbero uccidermi\", e poi continua dicendo: \"Ho fumato un paio di sigarette dopo la riunione\". Questa convinzione e questa azione sono incoerenti e sono in dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, il fatto che non credo di poter mantenere il mio lavoro senza di loro giustifica la seconda occorrenza e hanno una relazione di consonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio tra altri tipi di relazioni discorsive."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Perché è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, monitorare le tendenze e i cambiamenti nei valori delle credenze e negli atteggiamenti della popolazione,"}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "L'alta dissonanza cognitiva è anche legata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "Studiare la dissonanza espressa nel linguaggio può essere utile anche per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Per raggiungere l'obiettivo di creare una risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza in primo luogo, come mostrato nell'organigramma qui presente."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "I tweet sono stati analizzati utilizzando un parser PDTV e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, la dissonanza è stata riscontrata solo nel 3,5% delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Dopo aver raccolto circa 1000 esempi di coppie di unità di discorso, abbiamo eseguito la formazione di un classificatore iniziale, addestrato solo su 43 esempi di disnets. Non sorprende che il classificatore non abbia ottenuto risultati molto migliori del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Data la bassa frequenza di dissonanza e l'assenza di qualsiasi precedente serie di dati, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "Per alleviare questo problema, sperimentiamo combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo tale da poter raccogliere più campioni di dissonanza con meno passaggi di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Poiché il modello iniziale non era in grado di catturare la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Passiamo a due compiti diversi. La dissonanza indipendente dal tema riguarda la classificazione, un compito che determina se due affermazioni di dibattito provenienti da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamiamo dibattito qui e sulla classificazione binaria delle classi di espansione e di confronto di PDTB, poiché questi due concetti sono strettamente legati alla concezione delle consonanti e della dissonanza e li chiamiamo CEE qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che, dopo il trasferimento, le prestazioni zero-shot sul set di dati annotato sono già molto migliori del caso, con il miglior valore AUC pari a 0,62."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ottimizzando iterativamente entrambi i compiti, scopriamo che l'ottimizzazione dei compiti di CE seguita da un ulteriore ottimizzazione sul dibattito produce prestazioni molto migliori a zero scatti. Pertanto, questo è il modello che utilizziamo per avviare l'apprendimento attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni ciclo di apprendimento attivo e annotazioni. Cumulatore accumula tutti i dati raccolti finora dalle annotazioni attive, mentre aggiorna in modo iterativo il modello addestrandolo sull'ultimo insieme di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Tra le diverse strategie, abbiamo scoperto che quella cumulativa ha ottenuto risultati uguali o migliori rispetto a quella iterativa in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità di classe rara, PRC, per selezionare principalmente gli esempi che hanno un'alta probabilità di essere dissonanti secondo il modello attuale in qualsiasi round di AL."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "La confrontiamo con le altre strategie all'avanguardia comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che la strategia proposta per la PRC funziona meglio rispetto ad altre strategie all'avanguardia, anche se la differenza è minima. Si noti che le prestazioni sono significativamente inferiori per il caso casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "Nei successivi cicli di AL con le due migliori strategie, abbiamo migliorato la classificazione della distanza AUC a 0,75, che è la migliore performance che abbiamo ottenuto finora per questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Verifichiamo anche la fattibilità di ogni strategia in termini di qualità dell'annotazione e costi per gli annotatori. Abbiamo riscontrato che la PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo scoperto che la PRC è una semplice strategia di apprendimento automatico per l'acquisizione di classi rare e che l'avvio a freddo dell'apprendimento automatico può essere notevolmente facilitato da compiti di apprendimento per trasferimento progettati in modo appropriato."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre riscontrato che l'aggiornamento iterativo è utile per l'apprendimento trasferibile da un dominio diverso, mentre le annotazioni attive in-domain traggono vantaggio dall'aggiornamento cumulativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro set di dati principale e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
