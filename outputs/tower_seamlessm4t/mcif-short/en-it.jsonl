{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, benvenuti alla nostra presentazione del nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò nella prima parte della presentazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La semplificazione del testo è il processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come persone con problemi di lettura o parlanti non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testi, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "E nell'esempio qui, potete vedere un paio di frasi allineate in parallelo di una frase tedesca complessa e la sua traduzione in linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come possiamo vedere nell'esempio, come la sostituzione lessicale, l'eliminazione delle proposizioni, la riorganizzazione o l'inserimento di parole"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo ora il nostro nuovo corpus di planum, perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti, quindi, ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di tassonomia."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori negli allineamenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto proponiamo il nostro nuovo corpus di plane, che è diviso in due sotto-corpora, di plane APA e di plane web. Di plane APA si basa su testi di notizie"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Nel piano AP abbiamo allineato quattrocentottantatrè documenti tutti manualmente, il risultato è di circa trentamila coppie di frasi parallele"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Per il Deep Web, questo corpus include diversi domini e allineiamo tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo ottenuto trentamila quattrocentocinquanta coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Analizziamo un po' di più le nostre frasi, ad esempio sul tipo di semantizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come potete vedere qui, i testi biblici sono molto più semplici rispetto, ad esempio, al testo delle notizie o al testo per studenti di lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "a tutti i livelli, ad esempio, ad esempio, semplificazione lessicale, semplificazione strutturale, tutti gli altri livelli di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, si può notare che il nostro corpus di profondità presenta una grande varietà di diverse trasformazioni di amplificazione. Ad esempio, nel corpus dell'API di profondità abbiamo molte più riordinazioni e aggiunte di parole rispetto al corpus del web di profondità."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, nel corpus web abbiamo molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Omar, e ora parlerò dei casi d'uso per il nostro set di dati D-plane."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni sono stati sviluppati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi nei documenti post."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma nel nostro caso, stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli che hanno la stessa lingua, lo stesso contenuto, ma sono a un livello di complessità diverso."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora che abbiamo il nostro set di dati, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine abbiamo concluso che il miglior metodo di allineamento per la semplificazione del testo in tedesco è il metodo di allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "E potete trovare anche il codice per eseguire questo metodo sui vostri documenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è il caso della semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "ottimizzando i modelli linguistici per produrre un testo semplificato a partire dal testo complesso di input."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo perfezionato due diversi modelli. Abbiamo perfezionato il modello di input lungo per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E noi affiniamo anche la base normale in parte per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "È inoltre possibile trovare tutti i punti di controllo e visualizzare ulteriori dettagli nei punteggi e nelle metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questo aggiustamento fine di base potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo questi risultati come un punto di riferimento, un punto di riferimento di base per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Shvirkovsky e questo discorso riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Forse sapete che diverse strutture di dipendenza sono definite da diverse teorie e processi, quindi, per esempio, nell'universo le dipendenze sono la struttura della struttura coordinata di Lisa e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "è tale che il primo congiunto è il capo dell'intera struttura centrale, quindi in questo caso Lisa"}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "La prima è che l'intera struttura è controllata dalla prima congettura, quindi questi due approcci sono simmetrici, quindi quello fuori dalla congettura."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Ora l'approccio simmetrico per coordinare strutture come l'approccio prag, il processo di congiunzione, il processo sincrono, le strutture sincrone sono guidate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo alcune dipendenze dall'inizio alla fine di tutti i contratti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "Infine, questo è anche un approccio multiuso che viene utilizzato, ad esempio, nella grammatica di Catchers World."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "Quindi tutte le congetture sono teste della struttura coordinata, quindi otteniamo dipendenze dal governatore, qui ama condurre separatamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo di questo articolo è di presentare un nuovo argomento a favore delle strutture di coordinazione simmetriche come questa e contro le strutture di coordinazione asimmetriche come questa."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Ok, l'argomento si basa sul principio della minimizzazione della lunghezza della dipendenza che spiegherò in base a questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Quindi in inglese, come forse sapete, si preferisce che l'oggetto diretto sia vicino alla rete, mentre un salto potrebbe essere più lontano, tanto che va bene perché l'oggetto diretto è vicino alla rete."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Mentre March ha letto ieri, è molto peggio qui, perché qui tra il verbo e l'oggetto diretto c'è ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere migliorato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo il salto in aria."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui, quindi entrambe le frasi sono corrette, tanto che il libro sul B.C. di ieri è assolutamente affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma va bene anche dire che Marge ha letto ieri questo libro assolutamente affascinante sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il ragionamento qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale secondo cui un oggetto diretto dovrebbe essere vicino al verbo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio della minimizzazione della lunghezza della dipendenza, che afferma che sono preferibili dipendenze più corte."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture"}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Ecco quindi la dipendenza dal rosso fino al bordo sette in parole e dal rosso al libro di quattro, per ottenerlo."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando si sposta, quando si scambiano queste due circoscrizioni, la somma di queste due dipendenze diventa sei, quindi è sedici, ma è per questo che suona piuttosto bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Ok, quindi quello che abbiamo fatto è estrarre varie statistiche dalla versione coordinata della Banca Pentium e vedere il documento per capire perché non abbiamo utilizzato le dipendenze universali."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "E queste statistiche confermano l'osservazione fatta molte volte prima che i gemelli siamesi di sinistra tendono ad essere più bassi, quindi sale e pepe e non sale e pepe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione che è stata fatta di passaggio, che questa tendenza cresce con differenze lunghe, lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra le lunghezze delle due articolazioni congiunte aumenta, le articolazioni congiunte più corte sono le prime a diventare più forti, quindi la proporzione è maggiore rispetto alle articolazioni congiunte sinistre."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma la novità di questo studio è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è di sinistra o assente."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo esempio, il governatore è a sinistra, ho visto Bart e Lisa, quindi il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "È assente nel secondo esempio, la casa del Kamen e dello Sneeze, dove abbiamo la coordinazione di due parole e ora il governatore esterno #ah a destra, quindi in tali casi la conch sinistra preferisce essere la più corta, #ah maggiore è la differenza tra le due."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance è a destra, come qui, la sinistra governa il coordinamento della rete, questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Quindi mostriamo che misurando la lunghezza in caratteri, la prima colonna in sillabe, la colonna centrale in parole, la colonna a destra, quindi mi concentrerò su quella di destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Ciò che stiamo dicendo è che quando il governatore è a sinistra"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza per la sinistra di essere più corta cresce costantemente con la differenza assoluta di parole e lo stesso si osserva quando non c'è un governatore come nel coordinamento delle frasi, ma quando il governatore è a destra questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "E nel documento mostriamo come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche come queste due e per le strutture asimmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Si veda il documento per l'accordo completo e gli argomenti di scusa e parli con noi della sessione postale. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Sono uno studente di dottorato all'Università di Washington e oggi presenterò il nostro lavoro dal modello linguistico al modello linguistico al modello linguistico al modello linguistico al modello linguistico al modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici sono addestrati su dati di webcrawl su larga scala."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media politici sono trattati nella fase di pre-formazione, secondo un sondaggio sui quattro giornali, si possono vedere il New York Times, il Los Angeles Times, il Guardian, l'Huffington Post, ecc. Siamo trattati nella formazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una situazione ambivalente per l'applicazione dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, da un lato, possono essere visti da diverse prospettive, che celebrano la democrazia e il pluralismo delle idee, dall'altro, queste diverse visioni politiche sono socialmente distorte e potenzialmente ingiuste in termini di applicazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "Per questo motivo proponiamo di indagare il flusso di propaganda politica dai modelli linguistici ai modelli linguistici, chiedendoci specificamente le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo le inclinazioni politiche dei modelli linguistici e quale ruolo giocano i dati personali in tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si utilizzano diversi modelli linguistici con diversi partiti politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in particolare, proponiamo di proporre due diversi modelli linguistici con formati diversi utilizzando i questionari politici come il test del compasso politico, ciò garantisce che la valutazione automatica sia data in scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari mostrano che i primi modelli linguistici hanno ancora diverse tendenze politiche, occupano tutti e quattro i quadranti del campo politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche osservare che GPT4 è il modello linguistico più liberale di tutti e che la teoria GPT è generalmente più liberale socialmente rispetto alla teoria BERT e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, studieremo in che misura i modelli linguistici politici siano effettivamente tratti dai dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Quindi possiamo controllare l'esperimento testando ulteriormente i punti di controllo linguistici e le sei diverse parti dell'azienda sono divise in notizie e social media e sono divise in quelle politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Con ulteriori addestramenti dei modelli linguistici e confrontando i due, possiamo vedere che le coordinate ideologiche del modello linguistico corrispondono anch'esse alle stesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, nel caso di Robert, un ulteriore risultato, un ulteriore allenamento sul corpo rosso per mancini, possiamo osservare un sostanziale cambiamento liberale in termini di"}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "per quanto riguarda i suoi pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare come i modelli linguistici possano cogliere la polarizzazione che è prevalente nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi stiamo dividendo il corpo di pre-addestramento in due, il quarantacinquesimo presidente degli Stati Uniti e il quarantacinquesimo presidente degli Stati Uniti, e poi stiamo separando i modelli linguistici in due corpi temporanei diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo vedere che i modelli linguistici hanno generalmente un significato politico che risale a più di ventisette anni fa, quindi questo modello linguistico può essere utilizzato anche per descrivere la polarizzazione nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Quindi non saremo in grado di valutare i modelli linguistici con diverse prospettive politiche e il rilevamento del discorso e la segnalazione delle notizie, quindi avremo due applicazioni che sono modelli linguistici e possono avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi diremo che se analizziamo le prestazioni per categoria, cioè se separiamo le prestazioni in"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "In base a diverse demografie o media politici, possiamo osservare che, ad esempio, per il riconoscimento vocale i modelli linguistici per mancini sono migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "nel rilevare l'incitamento all'odio rivolto ai gruppi sociali minoritari"}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, siamo all'inizio del rilevamento del discorso d'odio rivolto a gruppi più potenti nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "E a proposito, i modelli linguistici sono migliori nel prendere di mira il discorso dei bianchi e il discorso dei bianchi, ma sono migliori nel prendere di mira il discorso dei neri e degli LGBTIQ e di altre comunità minoritarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Eventi simili si verificano anche per il rilevamento delle notizie false, dove si osserva che i modelli linguistici di sinistra sono più bravi a rilevare la disinformazione rispetto ai loro opposti politici e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo vi mostreremo diversi esempi qualitativi per vedere i modelli linguistici con diversi significati politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "È possibile fornire diverse previsioni per gli esempi di discorsi e informazioni nelle categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare questo aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che esiste un problema di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se si trovano i modelli linguistici giusti, è possibile conoscere il discorso e le informazioni e utilizzarle sulle piattaforme dei social media."}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e che l'incitamento all'odio contro i gruppi minoritari potrebbe diffondersi senza alcun controllo"}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo sembra l'allarme per voi per riconoscere e affrontare le questioni di equità causate dal modello linguistico politico"}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel corso della discussione, vorremmo anche sottolineare che spiegheremo il linguaggio unico del linguaggio politico, che è come quello tra i due."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non standardizziamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici e poi ai compiti successivi, creando in ultima analisi problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se proviamo a sanificarlo in qualche modo, otterremo anche censura o esclusione ed è incredibilmente difficile determinare cosa sia realmente neutrale e dovrebbe essere conservato nella lingua, quindi è un po' come il problema elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Va bene, ottimo. Penso che sia praticamente tutto quello che ho per oggi. Grazie per il suo tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Sono uno studente di dottorato al primo anno alla Carnegie Mellon University e sto presentando il mio lavoro in una posizione di responsabilità, progettando i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato realizzato in collaborazione con l'Università di Washington e l'Istituto per lo Studio della Rivoluzione Americana, in particolare con Sebastian Santee, Ronan Labrina, Catherine Rankin e Martin Sap."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Quindi iniziamo immaginando che lavoriate per un giornale e che stiate commentando il vostro articolo di notizie cercando di rimuovere i contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Puoi rivolgerti all'APP popolare come l'APP per il rilevamento della tossicità e questo è davvero utile se sei un fumettista."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è proprio il caso di Aditya Sharma, la cui prospettiva non è particolarmente sensibile ai termini offensivi e ai contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio di progettazione in cui si osservano differenze sistematiche di prestazioni tecnologiche tra le popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "L'unica cosa che è simile a quella che abbiamo appena visto è il posizionamento dei ricercatori di NLP e degli sviluppatori di modelli. Il posizionamento è semplicemente la prospettiva che le persone hanno come risultato della loro demografia, identità ed esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi femministi e accademici."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E come ricercatore, il posizionamento può influenzare il processo di ricerca e i suoi esiti e risultati perché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi una domanda che le persone potrebbero porsi è: i set di dati e i modelli hanno una posizionalità?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "Non stiamo cercando di dire che i modelli e le modelle abbiano identità demografiche ed esperienze di vita, ma le opinioni e le posizioni aggregate di persone reali possono rappresentare certe posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Il primo compito è quindi quello di suggerire alcune delle prove dell'esistenza di una posizione, come le lacune culturali, i modelli e i dati, oltre alle definizioni del posizionamento del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non si concentrano sul confronto tra gli utenti finali e i set di dati e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio del posizionamento dei modelli e dei dati diventa sempre più importante man mano che i test di NLP diventano più soggettivi e socialmente orientati."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "E è difficile caratterizzare come queste proprietà siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Per studiare il set di dati e la posizionalità del modello, confrontiamo le annotazioni con gli utenti reali con i set di dati e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo facciamo attraverso il nostro quadro di riferimento, la posizionalità NL."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro modello funziona in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo è ri-annotare i set di dati con diversi annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E noi esamineremo la demografia dei set di dati originali, perché di solito solo alcuni dei set di dati vengono raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi deciso di rianalizzare i dati per ottenere più entità per istanza e per ottenere un ricco insieme di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando il nostro punteggio di correlazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "Ed è per questo che il nostro framework è diverso dall'Annotator Agreement, confrontando gli utenti con modelli e set di dati ed etichette, e guardando solo l'Annotator Agreement o l'Annotator Distribution."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro quadro di riferimento è in gran parte reso possibile grazie a Lab and Wild, una piattaforma di crowdsourcing online per ex collaboratori HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "Nel mondo della sperimentazione online, possiamo reclutare volontari per confrontare le piattaforme con quelle degli Stati Uniti e dell'India, e il mondo dei dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due test nel mondo, uno è l'accettabilità sociale e l'altro è il funzionamento, che consiste nel fatto che i partecipanti saranno in grado di vedere la situazione dai dati di chimica sociale e quanto la situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con quelle dell'IA e di altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi replicato in modo molto simile per il test di tossicità e di riconoscimento del parlato, dove abbiamo osservato esempi dei sordi e dei destri e qual è il significato del parlato."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo poi questi confronti con i dati dell'A.P.I. (A.P.I.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.) e del G.P.D. (G.P.D.E.R.E.R.) nello studio di sedici mila osservazioni provenienti da ottantasette paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora dovremo capire chi si occuperà dei set di dati NLP con il maggior numero di righe di dati. Scopriremo che è posizionato nell'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo scoperto che i dati sono per lo più nei paesi anglofoni, quindi per l'analisi del GPD per la responsabilità sociale abbiamo scoperto che è per lo più nei paesi anglofoni, e abbiamo scoperto che è anche nei paesi anglofoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche riscontrato che la maggior parte delle persone con un'istruzione universitaria ha maggiori probabilità di avere un'istruzione universitaria, quindi per il G.P.D. nel compito di socializzazione troviamo che la maggior parte delle persone con un'istruzione universitaria o post-laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo la stessa cosa per Danny Hate, dove è più in linea con le persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i dati non sono buoni quanto quelli delle persone non binarie rispetto agli uomini e alle donne. Lo troviamo nei quattro test di accettazione sociale del G.P.D. così come nel test D.N.H."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dato che esiste una posizione nel LED e nell'LP, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo alcune raccomandazioni al riguardo. La prima è tenere traccia di tutte le scelte di design rilevanti durante il processo di ricerca e l'altra è condurre ricerche di NLP sullo spettro della percezione."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di creare set di dati e modelli specializzati con comunità specifiche e un buon esempio di ciò è l'iniziativa Masakani. Vogliamo sottolineare che non stiamo semplicemente facendo in modo che tutte le tecnologie funzionino per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "Questa è la presentazione, ma se volete vedere di più, non esitate a consultare i risultati e gli articoli più aggiornati. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono C. Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro. Distinguere la conoscenza degli script dai modelli linguistici leggeri per la pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo dopo passo sotto forma di script guidati."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "I lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come fare un calcio, e hanno dimostrato che i grandi modelli linguistici possono scomporre efficacemente gli obiettivi in passaggi"}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione degli obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con vincoli specifici, come la preparazione di una torta al cioccolato, rimane ancora inesplorata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "che impone vincoli diversi agli obiettivi di pianificazione un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli sfaccettati un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli"}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo prima la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nulla al di fuori degli obiettivi specifici esiste per individuare il nostro sguardo."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo acquisire questi obiettivi prima, come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'uomo nell'acquisizione dei dati di sguardo, utilizziamo GPT didattico"}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato un centinaio di obiettivi specifici e valutato gli script generati dai modelli più grandi."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo riscontrato che tutti i modelli lineari ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, procediamo con un'analisi dettagliata per indagare a cosa servono i modelli di livello del suolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "i risultati nelle figure mostrano che la completezza semantica negli script generati è accettabile, ma non si può garantire la fedeltà ai vincoli"}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Analizziamo le categorie tematiche più finalizzate dei vincoli definiti nel modo di lavorare. La mappa principale nella figura mostra che le prestazioni di pianificazione delle istruzioni variano notevolmente per le ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei grandi modelli varia notevolmente, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di sovra-generare il filtro per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo prima i tipi vincolati con esempi per ppt intransitivi e otteniamo obiettivi specifici basati sugli obiettivi astratti menzionati."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, si istruisce GPT affinché generi script di casi per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, viene sviluppato un modello di filtro per selezionare gli script visivi"}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo gli script e gli obiettivi in incorporamenti intrinseci di GPT e calcoliamo la somiglianza cosinica e i punteggi di somiglianza per misurare la somiglianza semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, evitiamo lo script che contiene le parole chiave del vincolo di destinazione, lo conserviamo solo se la ragazza di destinazione ottiene il punteggio più alto."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, l'intuitività può generare punteggi di qualità superiore. Il nostro metodo migliora notevolmente la semplificabilità, sia nella completezza semantica che nella fedeltà al vincolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché l'implementazione di grandi modelli linguistici è costosa, è essenziale consentire la pianificazione linguistica con modelli più piccoli e specializzati. La creazione di set di dati è un passaggio essenziale per raggiungere questo obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, gli studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale dei dataset è costosa"}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Perciò, seguiamo l'idea della distillazione della conoscenza simbolica per distillare i dati dei siti di pianificazione linguistica vincolata dai grandi modelli linguistici"}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "pianifichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata denominato codescript"}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di validazione e di test, chiediamo ai lavoratori del crowdsourcing di trovare e rivedere i campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questo grafico mostra la distribuzione limitata di coscript. Abbiamo riscontrato che coscript presenta un'alta probabilità nei risultati specifici generati. Con coscript, possiamo scegliere modelli più piccoli ma specializzati per la pianificazione linguistica limitata."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Con l'aiuto di T-File, T-File, Tune e Courseraid, è possibile generare script di qualità superiore rispetto alla maggior parte dei moduli su larga scala, indicando che i moduli più piccoli possono supportare moduli più grandi quando vengono adeguatamente addestrati su siti di dati idonei."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, stabiliamo il problema della pianificazione linguistica vincolata, valutiamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppiamo un metodo di filtro per la sovra-generazione per i grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli linguistici di grandi dimensioni per generare un insieme di dati di script di alta qualità, codescript, per la pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il Suo tempo. Si prega di trovare ulteriori dettagli dello script del codice nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Xu Hong. Oggi presenterò il nostro articolo \"Gli etichettatori di entità Do Cornell 2003 funzionano ancora bene nel 2023?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento degli enti nominati o il compito NER."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli utilizzano CONSO 2003 per sviluppare il NER da quasi 20 anni, e ciò solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "al tempo stesso, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare su questi problemi, abbiamo sviluppato il set di dati Carneau+, che è un set di dati raccolti da Reuters News dal 2020 e poi annotati con le stesse linee guida di annotazione Carneau 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi perfezionato oltre 20 modelli sul Corno 2003 e li abbiamo valutati sia sul set di test Corno 3 che sul set di test Corno +."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che sono necessari tre ingredienti principali:"}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'architettura del modello. Attraverso i nostri esperimenti abbiamo scoperto che i modelli transformer si generalizzano normalmente meglio sui nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione fine influisce direttamente sulle prestazioni di un compito a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Alla nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due ipotesi: la prima è il sovradattamento adattivo, che è un sovradattamento causato dal riutilizzo dello stesso set di test più e più volte, e questo si manifesta solitamente come una diminuzione dei rendimenti sul nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra il treno e i dati di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea rossa di miglior adattamento ha una pendenza maggiore di uno."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo apportato a Color 2003 si traduce in più di un'unità di miglioramento su Color +, il che significa che non ci sono rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che in questo caso non si osserva un sovra-adattamento adattivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, e la temperatura?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni peggiorano con intervalli temporali più ampi."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E ciò conferma la nostra ipotesi che la causa principale del calo delle prestazioni sia la deriva temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un'architettura di modello migliore, di una dimensione del modello più grande, oltre che di più esempi di ottimizzazione fine."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato da un'errata temporizzazione, e sorprendentemente non è causato da un sovra-adattamento adattivo, anche se Conal 2003 è stato utilizzato per oltre 20 anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, tornando alla domanda che abbiamo posto nel titolo del nostro articolo, i tag del 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sonoro sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo solleciterà ulteriori ricerche su come migliorare la generalizzazione dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "Infine, vi preghiamo di consultare il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indiretto per la selezione delle entità, in cui presentiamo il corpus delle entità alternative."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Jawad Hosseini e questo è un lavoro congiunto con Philip Radlinsky, Silvia Parati e Annie Joyce."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è comprendere il linguaggio dell'utente quando vuole fare una scelta."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è fare un riferimento diretto, ad esempio dicendo che il nome della canzone è su di me o la sua posizione, la prima."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "A volte, però, è più appropriato fare un riferimento indiretto per avere una conversazione più naturale. Questo può accadere quando l'utente non riesce a ricordare il titolo di una canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "tutte le pronunce sono troppo simili tra loro e difficili da capire"}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "o quando l'utente vuole specificare una preferenza, ecco alcuni esempi di preferenze indirette, ad esempio il più recente o la canzone che non è energica"}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un problema importante nei sistemi di conservazione e anche per il benchmarking della comprensione delle entità LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per questo compito, quindi ne raccogliamo uno utilizzando il crowdsourcing. Il nostro set di dati copre tre diversi ambiti: musica, libri e"}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando il tuo set di completamento dei fumetti."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il fumetto ha tre bolle di dialogo. Nella prima bolla, Bob dice: \"Ricorda quella canzone che ascoltavamo ieri?\" e con questo, Bob imposta il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nella seconda bolla di dialogo, Alice dice: \"Vuoi dire che è facile per me o ho una sensazione?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "che è la domanda alternativa. E nella terza bolla, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio, la nuova"}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniamo automaticamente la prima e la seconda bolla di discorso, ma la terza viene compilata dall'annotatore. La prima bolla di discorso viene scelta tra alcuni suggerimenti manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "la seconda, che è la domanda alternativa, viene generata come segue"}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo sempre un modello semplice. Vuoi dire A o B? Dove A e B sono esempi tratti da Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è generalmente più difficile ottenere la stessa equazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è uniforme."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, ad esempio due libri con lo stesso nome del rivenditore."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso si verifica quando hanno descrizioni simili su Wikipedia e quando presentano caselle informative o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando mostriamo questa domanda alternativa agli editori, loro conoscono il nome di queste entità, ma non necessariamente conoscono le entità."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo quindi alcune informazioni di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "E poi chiedete ai commentatori di ascoltare almeno una parte di ogni canzone e di leggere le informazioni su ogni canzone. Ecco, per esempio, il risultato della ricerca su Google per la canzone Easy."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo anche le loro immagini da Wikipedia in modo che gli annotatori sappiano come si presentano."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Poi chiediamo agli editori di selezionare una di queste entità, ad esempio la prima, e di descriverla utilizzando da tre a cinque riferimenti indiretti."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica per pianoforte. Ecco alcuni esempi del nostro dataset. Ad esempio, quello senza parole, non quello con il ragazzo di dodici anni o quello fittizio o proveniente dall'Azerbaigian e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus Identity comprende 6.000 domande alternative distribuite su tre ambiti e 42.000 espressioni di riferimento indirette. I risultati ottenuti con il modello T5X Large sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso alle stesse conoscenze di base degli analisti, allora l'accuratezza è davvero alta, è intorno al novanta-due-novantacinque percento, ma questo non è realistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso ad alcune conoscenze pregresse parzialmente sovrapposte, allora l'accuratezza è compresa tra l'ottantadue e l'ottantasette percento, che è più realistico, ad esempio, quando il modello linguistico recupera le conoscenze pregresse."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso solo a due nomi di entità, l'accuratezza è solo del 60%, quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili a diversi ambiti. Ecco un link al nostro set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Serapapi dell'Università di Trento e della Fondazione Bruno Kessler e vi presenterò brevemente l'attenzione come guida per la traduzione simultanea del discorso, un lavoro congiunto con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Cos'è la traduzione simultanea del discorso  \nLa traduzione simultanea del discorso, o simulesc, è il processo di traduzione di un linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli di simulazione attuali? Le architetture specifiche sono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "procedure di formazione lunghe e complesse, ad esempio la formazione che coinvolge diversi obiettivi di ottimizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E addestrare e mantenere diversi modelli per ottenere diversi regimi di latenza, ad esempio, addestrare un modello con una latenza media di un secondo e un altro con due secondi di latenza e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Qual è quindi la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, utilizzare i modelli ST offline esistenti senza riaddestramento o adottare architetture specifiche per semplicità. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E la conoscenza è già acquisita dal modello attraverso il meccanismo dell'input audio e dell'output testuale, che è il meccanismo dell'output audio, e potete vedere un esempio proprio lì."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre un codice o codificare l'attenzione al codice, ed è una strategia che ci permette di decidere se effettuare o meno una traduzione parziale in base a dove l'attenzione si concentra."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "una parola viene emessa se la tensione non è concentrata, cioè se questa somma è inferiore a una certa soglia alfa, verso l'ultima struttura del discorso lambda, il che significa che le informazioni ricevute sono sufficientemente stabili"}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, se riceviamo un discorso abbreviato contenente \"sto per parlare di\" e il nostro modello prevede la traduzione in tedesco"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "e vedremo i pesi di cross-attention"}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i frame di discorso ricevuti per primi, mentre l'ultima parola indica gli ultimi frame di discorso ricevuti, almeno i frame di discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che verranno emessi le prime due parole"}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "mentre, poiché la somma delle tensioni incrociate è superiore a una certa soglia alfa, non emetteremo l'ultima parola e attenderemo un altro segmento di discorso"}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro speech tank e il nostro modello prevede altre tre parole, daremo un'occhiata ai pesi di cross-attention"}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "vedremo che nessuna parola indica gli ultimi fotogrammi del discorso di lambda"}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che verranno emesse queste tre parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se si osservano i risultati principali di ciò,"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Progetteremo i risultati della traduzione simultanea del discorso su grafici in cui abbiamo da un lato il blu che misura la qualità della traduzione e il ritardo medio"}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Questa è la misura della latenza e consideriamo anche la media computazionale che tiene conto del tempo di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vogliamo che le nostre code siano il più alte possibile in questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Ma vogliamo anche che siano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E li confrontiamo con strategie adeguate che si applicano anche ai modelli offline, ovvero la strategia Whitecaps e l'accordo locale, e li confrontiamo anche con l'architettura di ultima generazione specificamente progettata per la traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea del discorso in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "e vediamo che ed supera tutte le strategie applicate ai modelli offline poiché le loro curve sono spostate verso sinistra"}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, se consideriamo il tempo effettivo o il tempo di calcolo, questa è la strategia più veloce."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate scoprire ulteriori risultati, leggete il nostro articolo. Abbiamo inoltre reso disponibili il codice open source, i modelli e le simulazioni per facilitare la riproducibilità del nostro lavoro."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ying e il mio collega Ji Yong e io presenteremo la nostra ricerca sull'apprendimento sociale multimodale migliorato con più istruttori tramite l'accordatura didattica."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Grazie ai progressi nei grandi modelli linguistici, molte ricerche hanno iniziato a esplorare nuovi paradigmi di apprendimento per il riutilizzo di modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che l'ottimizzazione dell'istruzione consente ai grandi modelli linguistici di eseguire compiti non visti in modo approfondito, seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione dell'istruzione si è concentrata sul miglioramento delle prestazioni a somma zero nei compiti basati esclusivamente sul linguaggio, mentre la visione artificiale e i compiti multimodali sono stati trascurati."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, vogliamo indagare se l'ottimizzazione dell'istruzione su modelli di modelli multimodal possa effettivamente migliorare la generalizzazione a compiti multimodal non visti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo scoperto una significativa discrepanza nella disponibilità del set di dati di istruzione tra l'LP e il modello multi."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono oltre 1.600 compiti di istruzione basati esclusivamente sul linguaggio, ma non esiste un compito di istruzione multimodale su larga scala disponibile pubblicamente, quindi questo ci spinge a creare un insieme di dati di sintonizzazione didattica multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo MultiInstructor, il primo set di dati di riferimento per l'accordatura delle istruzioni multimodali, che consiste in sessantadue compiti multimodali diversi che coprono dieci categorie differenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi compiti sono derivati da 21 set di dati open source esistenti e ogni compito è accompagnato da 5 istruzioni scritte aggiuntive."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare sul bilanciamento dell'istruzione multimodale sul nostro set di dati proposto, prendiamo OFA, un modello multimodale unificato come modello di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui mostriamo alcuni esempi tratti dal nostro set di dati multi-istari."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "per unificare l'elaborazione di vari tipi di dati di input e output"}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo dell'OFA e formuliamo tutti i compiti in un formato sequenza-a-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e le bounding box sono rappresentati nello stesso spazio di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Ok, ora parlerò dell'accordatura dell'istruzione multimodale"}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Per il set di dati di addestramento, utilizziamo 53 compiti di 9 gruppi per l'addestramento e campioniamo 10.000 per compito per il test, dove riserviamo l'intero gruppo del senso comune per il test e selezioniamo ulteriori 5 compiti dal gruppo VQV e dal gruppo vario."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutti gli esempi nel test per ogni compito e preleviamo anche un campione casuale del compito dal test dell'istruzione naturale, come si vede nel test per l'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Quindi utilizziamo un modello OFA Large pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante il test, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Segnaliamo le prestazioni medie e massime e la deviazione standard delle prestazioni in tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza. Se è un compito di generazione multimodale, riportiamo l'RGL. Per i compiti RLP, riportiamo anche l'RGL."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva chiamata sensibilità, che misura la capacità del modello di produrre costantemente lo stesso output per lo stesso compito, indipendentemente da lievi variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere, il nostro risultato principale è che la regolazione delle istruzioni può migliorare significativamente le prestazioni del sistema operativo negli stessi compiti multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, l'apprendimento trasferito da set di dati di istruzioni naturali può essere utile per l'ottimizzazione delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo vedere come, all'aumentare della quantità di compiti, il modello raggiunge prestazioni migliori e, nel frattempo, una sensibilità inferiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento, utilizzando un'istruzione rispetto a cinque istruzioni, e come possiamo vedere, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurne notevolmente la sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo mostra quindi l'effetto di diverse strategie di front-loading sulla sensibilità del modello. Come possiamo vedere, trasferendo l'apprendimento dal set di dati, il modello può raggiungere una sensibilità molto migliore rispetto al modello OFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche osservare che l'apprendimento trasferito dai dati delle istruzioni NITURE può aiutare OFA a ottenere prestazioni molto migliori sui dati delle istruzioni NITURE."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, proponiamo un set di dati di sintonizzazione didattica multimodale del suo genere che migliora significativamente la capacità a breve termine dell'OIF, esplora diverse tecniche di apprendimento trasferito e ne dimostra i vantaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Un'altra cosa, stiamo raccogliendo un set molto più ampio di dati di regolazione delle istruzioni multimodali con circa 150 ulteriori compiti di linguaggio visivo e li rilasceremo."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Kostas Senna e sono lieto di darvi il benvenuto alla nostra discussione sul nostro articolo ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre solidi in base al contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "È un lavoro congiunto con John Gautier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi in questo lavoro rivediamo il paradigma della coppia minima."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Il paradigma di abbinamento minimo valuta fondamentalmente i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità, come difetti, sintassi o accettabilità in termini di stereotipi, come le coppie incrociate."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma minimalista, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammaticalmente corretta e poi mostrare una frase inaccettabile o una frase grammaticalmente scorretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi la speranza è che il modello attribuisca fondamentalmente una maggiore probabilità al set accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP fondamentalmente non ci permette di valutare l'accettazione di un modello verso frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici stanno emergendo con finestre sempre più lunghe, quindi è importante che valutiamo l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Questo è l'approccio, quindi quello che faremo è simulare queste sequenze più lunghe, esamineremo i set di dati stessi e poi creeremo frasi scegliendo frasi accettabili o inaccettabili da quei set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, qui abbiamo scelto un tipico esempio di grammaticalità dal set di dati del blimp, relativo al caso dell'isola di complemento."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è, per ricreare sequenze più lunghe, accettabili e con la stessa struttura grammaticale corrispondente, estrarre frasi grammaticali dal"}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che a quella non accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento, e ciò potrebbe anche essere utilizzato per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso, quindi è quello che chiamiamo scenario di non corrispondenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui, le frasi provengono ancora da set di dati pertinenti, ma non dallo stesso set di dati che si sta valutando, e possiamo fare lo stesso per i casi di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Ci dirà quindi se i giudizi di accettabilità del modello sono effettivamente influenzati da un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "come ad esempio se il contesto proviene da un sottoinsieme diverso del dataset o se è completamente irrilevante rispetto alla frase attuale che stiamo esaminando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, come si comporta il modello? Prima di tutto, guardiamo le frasi di Wikipedia che sono completamente irrilevanti per l'attuale coppia di query e lì scopriamo che i giudizi MPP sono per lo più robusti per un contesto arbitrario."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino al 2024 per massimizzare i modelli OPT e GPT2 e abbiamo visto qui nella riga orange.de che i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Cosa succede invece quando scegliamo frasi dallo stesso dataset?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili dallo stesso set di dati blim o sintattici."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E qui vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o prefissi inaccettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando combiniamo la struttura, cioè quando scegliamo le frasi dello stesso fenomeno nel testo di accusa."}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "Si osserva un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Questo effetto è molto significativo e aumenta all'aumentare della lunghezza del contesto, e probabilmente influenzerebbe i modelli linguistici più recenti che hanno finestre di contesto più ampie."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo condotto una serie di analisi in cui abbiamo cercato di preservare la frase di input cercando di mantenere la struttura pertinente, ma aggiungendo rumore all'input e poi facendo un mucchio di queste cose."}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che nessuno di questi rumori sta effettivamente facendo cambiare il modello in termini di come ci mostra l'andamento del giudizio MPP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "In generale, abbiamo riscontrato che i modelli sono sensibili alle frasi di partenza in modi simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "Cioè, quando disturbiamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutti i disturbi, e quando disturbiamo le frasi nel dominio inaccettabile, osserviamo una diminuzione dei giudizi MPP in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti, che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, il modo in cui la eseguiamo correttamente, con input di frasi brevi e singole, potrebbe non catturare appieno la conoscenza astratta del modello linguistico in tutta la finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Si prega di leggere il nostro articolo per ulteriori dettagli sui nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Yusof John e vengo dalla Penn State University. Oggi presenterò il nostro lavoro, \"Example, Cross-Lingual Semantic Parsing in Multiple Natural Languages and Many Representations\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Il parsing semantico è quindi il compito di costruire rappresentazioni semantiche delle query degli utenti, come Sequel e il calcolo Lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "La semantica interlinguistica è il compito di tradurre le query in più lingue naturali in più rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli più recenti: C, C, C, L, D, F, Q, ecc."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "i modelli di analisi semantica interlinguistica esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati, ad esempio"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "ci sono delle lacune nella copertura di certi linguaggi naturali, manca il cinese e"}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "potrebbero coprire molte rappresentazioni incerte"}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il cocktail Lambda manca."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "oppure vengono valutati solo su certi modelli più recenti, ad esempio c'è solo un singolo modello da valutare"}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo un esempio, forniamo un esempio di dataset uniforme per l'analisi semantica incrociata in più lingue naturali e molte rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene novanta set in vari ambiti, cinque compiti di analisi semantica, otto rappresentazioni del significato e ventidue lingue naturali in quindici famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è un test di traduzione, utilizziamo l'API di Google Translate per tradurre la fonte nella lingua di destinazione, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, addestreremo il modello inglese su una query in inglese e, durante l'inferenza, traduciamo la query in tedesco utilizzando l'API in inglese, quindi utilizziamo il modello addestrato per prevedere la sequenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E testeremo anche un modello monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di partenza è la stessa della lingua di arrivo, ad esempio dal tedesco al tedesco o dall'inglese all'inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testiamo anche la configurazione del fusibile monolingue addestrando modelli monolingue con solo il dodici percento dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "e che ha un modello multilingue che noi addestreremo come un unico modello multilingue per tutte le lingue"}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo messo insieme il tedesco, l'inglese e il cinese per addestrare un modello multilingue, e durante l'infanzia possiamo utilizzare questo modello per"}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "per tradurre query in tedesco o query in cinese o altro"}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "E consideriamo anche il collegamento incrociato tra zero-shot e trasferimento visivo, tra una lingua sorgente e il trasferimento a un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante l'addestramento, lo addestrerò su query in inglese, o sulla combinazione di query in inglese e tedesco, per addestrare un modello multilingue a prevedere l'output sequenziale."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo anche molti risultati interessanti. Quindi, per quanto riguarda l'analisi dei modelli monolingue, valutiamo due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "Incluso Encoder.pdf, che sta per Encoder multilingue pre-addestrati con decoder basati su puntatori, come XLR+PDF e Bert+PDF."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo anche i modelli encoder-decoder che sono modelli encoder pre-addestrati multilingue come #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um"}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che l'encoder decoder ottiene le migliori prestazioni su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "e valutiamo su MT5 e l'esempio XLMR più PDR in impostazione multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che Encoder Decoder o Encoder PDF possono essere migliorati addestrandoli in una miscela di varie lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "e quando viene trovato, è perché la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, tranne che le prestazioni dell'inglese diminuiscono in sette dataset e migliorano solo in tre dataset"}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Penso che questo sia noto come la maledizione del multilinguismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche il divario di prestazione tra le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento interlinguistico di campo, la linea arancione rappresenta il trasferimento interlinguistico senza precedenti, mentre la linea verde rappresenta l'impostazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che confrontando le linee verdi e arancioni, abbiamo riscontrato che per le impostazioni zero-shot, il divario di prestazioni nel trasferimento dei collegamenti incrociati è significativo, e confrontando le linee blu e arancioni, abbiamo scoperto che per le impostazioni few-shot, il divario di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche riscontrato altri risultati interessanti, ad esempio, che l'encoder-decoder esegue più lavoro o ottiene risultati comparabili, ma l'apprendimento dell'inglese come lingua madre può migliorare significativamente le prestazioni delle lingue di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo scoperto che i modelli linguistici multilingue come Codex e Blue sono ancora inadeguati per la comunicazione tra lingue diverse e da persona a persona"}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo creato Exemplar, un punto di riferimento unificato per l'analisi semantica incrociata, con più lingue naturali e molte rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto uno studio di riferimento completo su tre tipi rappresentativi di modelli linguistici multilingue, e i nostri risultati mostrano molti risultati interessanti, ecc. E benvenuti a visitare il nostro articolo e il codice."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo A.V. Villar e vi farò una breve recensione dell'articolo \"Printing Power for Translation, Assessing Strategies and Performance\". Si tratta di un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "Faram è un modello linguistico con 540 miliardi di parametri, presentato l'anno scorso nel 2022. Si tratta di un vasto insieme di testo che comprende 780 miliardi"}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "La pubblicazione Tamil raggiunge lo stato dell'arte in centinaia di compiti NRP."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro presentiamo il primo studio sistematico sull'uso di prompt per grandi modelli linguistici nella traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo la capacità di traduzione del modello utilizzando le migliori pratiche della comunità MT. Ciò comporta l'utilizzo dei test più recenti per evitare la sovrapposizione dei dati con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo due sistemi all'avanguardia, i sistemi con le migliori prestazioni e la valutazione WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche MT neurali all'avanguardia e mostriamo anche i risultati della valutazione umana basata sugli esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "Il prompt ha un grande impatto sulle prestazioni della traduzione, come possiamo vedere in un semplice esperimento in cui utilizziamo un prompt one-shot e forniamo due prompt diversi per una frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "Nella maggior parte delle frasi, 516 su 1.000, la differenza osservata è di più di un punto di sfocatura."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "E questo può arrivare, nei casi estremi, fino a quaranta punti, quindi è importante selezionare la buona strategia promozionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nei nostri esperimenti, abbiamo deciso di utilizzare una strategia a cinque colpi, in cui abbiamo semplicemente contrassegnato ogni frase fornita al sistema con la lingua in cui è scritta."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, in cui eseguiamo traduzioni dal tedesco all'inglese, le frasi in tedesco sono contrassegnate con la colonna \"Tedesco\" e le traduzioni in inglese con la colonna \"Inglese\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo visto che la forma effettiva della promozione non ha una grande influenza nel caso di una promozione seriale breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È fondamentale per la promozione a zero e a un solo colpo e quando passiamo al nostro caso di promozione, non c'è differenza rispetto alla forma effettiva della promozione."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a portare il peso maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, i risultati dei nostri esperimenti dimostrano che la qualità del campione è più importante della somiglianza con la frase di partenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "Quindi è importante selezionare gli esempi da traduzioni di alta qualità, in particolare confrontiamo i prompt di selezione dai dati di addestramento delle valutazioni WMT o i dati del"}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati sono molto più accurati e, maggiore è la qualità dei dati, migliori sono i risultati ottenuti utilizzandoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi specializzati hanno un vantaggio sostanziale rispetto alle traduzioni Palm, ma Palm si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di operare con Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Dalle valutazioni umane che otteniamo utilizzando il framework MQM, emerge che la fluidità del palm è paragonabile a quella dei sistemi più avanzati, ma la differenza principale deriva dall'accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Quindi sembra che Palm scelga di produrre una traduzione migliore, a volte tralasciando parti della frase che sono presenti nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria di abbigliamento esterno per Palm è inferiore rispetto ai sistemi di ultima generazione, il che rappresenta un segnale aggiuntivo"}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "che fornisce un output davvero fluido ma con alcuni problemi di precisione"}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa recensione davvero breve. Per maggiori dettagli, si prega di consultare la mia presentazione completa dell'articolo. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Davey, uno studente di dottorato all'Università di Salen in Germania. In questo video vorrei presentare il nostro lavoro recente, \"Più debole di quanto pensiate: uno sguardo critico all'apprendimento a sorpresa settimanale\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Shaul Usher, Marius Muzpah, Andreas Stefan e Dietrich Klarko."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "Nella supervisione debole non etichettiamo manualmente i dati, ma li etichettiamo utilizzando fonti di etichettatura deboli, come semplici regole euristica, basi di conoscenza o servizi di cloud sourcing di bassa qualità, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se addestriamo direttamente le reti neurali e etichettiamo i dati in modo debole, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Nell'addestramento debolmente supervisionato, vengono proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali in presenza di tale rumore di etichetta, in modo che i modelli di addestramento generalizzino comunque bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In un recente lavoro presso il WSL, dove WSL sta per Weekly Supervisory Learning, si sostiene comunemente che le persone addestrino i modelli solo con dati a livello settimanale e ottengano alte prestazioni su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è sbagliata, ma c'è un problema."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "Il problema è che le persone presuppongono che esista un ulteriore set di validazione pulito disponibile per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Mettiamo in dubbio questa impostazione del problema, poiché implica che siano necessarie ulteriori annotazioni manuali nei materiali didattici settimanali, ma, come un elefante nella stanza, questa necessità viene spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il dubbio sopra esposto ci porta a porci tre domande di ricerca: in primo luogo, i dati di validazione puliti sono necessari per WSL, o possiamo forse utilizzare un set di validazione rumoroso?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, se sono necessari dati puliti, o se i dati puliti sono obbligatori affinché WSL funzioni, quanti campioni puliti ci servono?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro lavoro affrontiamo queste domande di ricerca e i nostri risultati sono i seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, abbiamo scoperto che i recenti metodi WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "altrimenti si verifica un notevole calo delle prestazioni, come mostrato in questa figura: se non ci sono campioni di validazione puliti, i modelli di tendenza non possono generalizzare oltre le etichette di bit originali"}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "cioè che la dottrina è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che i metodi WSL richiedono dati accuratamente etichettati per funzionare correttamente, e il costo dell'annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "In genere, abbiamo bisogno di soli venti campioni per classe per ottenere alte prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questa non è la fine della storia, perché se decidiamo di accedere a campioni puliti, allora l'addestramento diretto su di essi porterà a prestazioni ancora migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di prestazione tra gli approcci di ottimizzazione fine, che vengono applicati direttamente con dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere, se abbiamo dieci campioni per classe, il fine-tuning diretto inizia a battere gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni ottenuto nei precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare l'ottimizzazione su campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere dai dati, il modello Wallina, denominato FTW, inizialmente ha prestazioni inferiori rispetto ai metodi WSL più complessi come il metodo del coseno."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se permettiamo di continuare l'ottimizzazione sui campioni di clic, allora FTP funziona altrettanto bene quanto altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi, che richiedono più tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, dimostriamo che i recenti approcci WSL richiedono campioni puliti e annotati manualmente affinché funzionino correttamente. Il loro miglioramento delle prestazioni e la loro praticità sono fortemente sopravvalutati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre raccomandazioni concrete per i lavori futuri sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, riportare i criteri di selezione del modello; ad esempio, indicare se la selezione del modello avviene tramite campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, i metodi WSL dovrebbero essere confrontati con le linee guida per l'apprendimento futuro, un presunto lavoro su campioni chiari. In terzo luogo, il continuo perfezionamento è una linea guida semplice ma efficace che dovrebbe essere presa in considerazione nei lavori futuri nel campo del WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso open source il nostro codice. Potete trovarlo tramite il codice QR su questa diapositiva. Vi invitiamo a consultarlo. Grazie e unitevi alla conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch e sono Sarah Finch. E oggi vi racconteremo tutto su ABC EVEL, un nuovo approccio dimensionale alla valutazione dell'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal laboratorio di elaborazione del linguaggio naturale di Emory, diretto dal professor Gino Choi presso l'Università di Emory e in collaborazione con l'intelligenza artificiale di Amazon Alexa."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che tu abbia appena sviluppato un modello di dialogo e desideri vedere quanto sia performante rispetto allo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La pratica comune è quella di utilizzare una valutazione umana, come ad esempio chiedere ai giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala prestabilita."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti, quindi potresti voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio consiste nel chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello utilizzando metodi comparativi o scalabili esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio cerca di ridurre la soggettività della valutazione umana, segnalando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddirsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio \"Annotating Behaviors in Chat\" o ABC in breve, abbiamo sviluppato questo metodo per coprire in modo completo i modelli di comportamento dei chat che sono stati suggeriti per influenzare la qualità della chat e la letteratura recente."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "A B C E è in grado di misurare le percentuali con cui i modelli di chat commettono vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, A B C E V A misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o contraddice il suo partner, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o non riesce a mostrare empatia"}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni in chat condotte da esseri umani per ciascun modello utilizzando ABC."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Licart a livello di turno, valutazioni Licart a livello di dialogo e confronti di coppia a livello di dialogo"}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalla nostra analisi di queste valutazioni, abbiamo scoperto che le etichette comportamentali ABC sono generalmente più affidabili delle etichette esistenti, come misurato dall'Accordo Interinale su cento conversazioni in doppio cieco."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette ABC sono più predittive della qualità complessiva della conversazione rispetto ai parametri ottenuti con i metodi esistenti, come dimostrato dall'analisi di regressione lineare semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, si può vedere come la misurazione della proporzione delle autocontraddizioni e delle controparti del cinque e dieci percento della qualità della conversazione, mentre i punteggi di coerenza medi sono solo del quattro percento o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico del controllo della qualità utilizzando una regressione lineare a gradi."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Si può vedere come la combinazione di tutte le metriche ABC spieghi oltre il venticinque percento della qualità della conversazione e, rimuovendo una metrica alla volta, la maggior parte di esse comporta la perdita di una quantità significativa di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche del livello di rotazione della liquirizia spiega molto meno della qualità e meno di queste metriche portano informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un metodo affidabile, informativo e distinto. La metrica A B C E V può essere utilizzata per valutare l'intelligenza artificiale conversazionale con una risoluzione maggiore rispetto ai metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "Dai risultati del nostro esperimento si evince che permangono ancora diverse sfide, che sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato presentano violazioni del senso comune in circa il venti per cento delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Forniscono informazioni pertinenti in circa il 15% delle risposte e si contraddicono o contraddicono il proprio partner in circa il 10% dei casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Data la rapida evoluzione del settore, molti di questi errori potrebbero essere riscontrati nei nuovi modelli rilasciati dalla valutazione, tuttavia, questo è un motivo in più per perseguire metriche di valutazione affidabili e accurate per confrontare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "speriamo che a b c eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione e non vediamo l'ora di vedere come l'IA conversazionale progredisca nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Kyoyan e sto presentando il nostro lavoro intitolato \"Quando si traducono i dati contestuali\". Questa è una collaborazione con Patrick Furness, M.D., M.F. Martin e Gram."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi molte traduzioni dipendono dal contesto, per esempio, come tradurremo \"più\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Bene, se la frase precedente era \"le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprissero\", allora Moe si riferisce a uno spia. Ma se la frase precedente era \"potrebbe essere qualcosa di serio, dottore?\", allora Moe si riferisce a un neo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a seconda del contesto, il significato della parola cambia e quindi cambia anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come il blue incapaci di catturare queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni hanno suggerito una valutazione mirata delle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza umana e sulla creazione umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro stiamo cercando di rispondere a queste due domande: primo, quando la traduzione richiede un contesto, e secondo, quanto bene i modelli gestiscono questi casi?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto della traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "E nel lavoro precedente abbiamo introdotto XMI come misura per i modelli di traduzione automatica, misurando quanta informazione la C fornisce sull'obiettivo e perché."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "Puoi pensare a CXMI come alle informazioni ottenute fornendo contatti al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro estendiamo il CXM al punto YXM, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo considerare le parole con un alto PXM come quelle che richiedono un contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un alto P.S.M.I. per cercare schemi tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "E noi eseguiamo la nostra analisi su trascrizioni di TED Talk che sono state tradotte dall'inglese in 14 lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo la nostra analisi a tre livelli diversi. Prima di tutto esaminiamo i tag del discorso che hanno un alto significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "E questo è il motivo per cui si può trovare, ad esempio, la pronuncia araba di un proverbio arabo che ha una \"i\" acuta. Questo può essere spiegato perché l'inglese non ha un proverbio simile, quindi è necessario sapere se il proverbio è stato tradotto in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "E scopriamo anche che alcune lingue richiedono un contesto quando vogliamo scegliere la forma verbale appropriata. Esaminiamo quindi gli elementi del vocabolario che hanno un alto valore di p-sezionale in tutte le sue diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo aiuta a identificare casi come quello qui, dove in cinese è necessario assicurarsi di utilizzare la stessa traduzione nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "E allo stesso modo scopriamo che il contesto è supportato con la giusta formalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esamineremo diversi #um e diversi #qualcuno con un alto p.s.m., e ciò ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono più espressivi nella struttura, quindi basta risolverlo."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora utilizziamo i risultati della nostra analisi per progettare un benchmark per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni che abbiamo identificato, creeremo automaticamente dei tag per identificare le parole correlate al fenomeno, e chiameremo il nostro tag \"fenomeno multilingue\" o \"mutag\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi anche notare che diverse lingue hanno diverse proporzioni di questi fenomeni."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo quindi il Mudah Tagger applicandolo al corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il Mudah Tagger ha identificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "Infine, utilizziamo il nostro parametro di riferimento e altri indicatori per valutare diversi modelli di #um nella traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, quando utilizziamo una metrica a livello di corpus, quindi per il blu, scopriamo che i modelli complessi agnostici hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma poi, se utilizziamo Comet, i modelli consapevoli del contesto ottengono i migliori risultati, e se utilizziamo la misura Word F, allora i modelli con e senza contesto hanno prestazioni confrontabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione di documenti se si utilizza solo una metrica a livello di corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark Muad'Dib per valutare i modelli e scopriamo che i modelli contestuali sono significativamente più accurati rispetto ai modelli che non utilizzano il contesto per certi fenomeni del discorso come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori rispetto ai modelli che non utilizzano altre forme di comunicazione come i fonemi, quindi dobbiamo fare ulteriori progressi per la documentazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche diversi sistemi commerciali e il nostro benchmark dimostra che Google Translate è generalmente più accurato di Google Translate per la traduzione di documenti locali."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, eseguiamo un'analisi basata sui dati su quattordici coppie linguistiche per identificare una traduzione che richiede contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizzeremo i nostri risultati per creare un punto di riferimento per la traduzione a livello di documento, che può aiutare a identificare quali modelli di fenomeni possono essere utilizzati e quali sistemi di traduzione sono adatti per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione, siete a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yannis Lavaque e vi presenterò il nostro lavoro su Dr. Bert, un robusto modello britannico in francese per i settori biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, parleremo prima del modellamento linguistico nell'assistenza sanitaria, poi presenteremo il contributo principale del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Stiamo introducendo il primo modello biomedico in francese, chiamato Dr. Bert, che si basa su Roberta e viene addestrato su Nachos, che è un insieme di dati medici provenienti da internet."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo anche un confronto tra modelli con più impostazioni platoniche e fonti di dati, quindi presentiamo i nostri risultati su undici compiti non stereotipati biomedici e clinici in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "Infine, concluderemo con gli esperimenti e vi forniremo ulteriori dettagli su come accedere al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Dal suo lancio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre un enorme miglioramento delle prestazioni rispetto ai metodi statici e contestualizzati storici come Word to Vect, Fast Text o Word."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert e altri ambiti come il biomedico con biomedical e il clinico con clinical, ma soprattutto in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "I modelli specializzati per altre lingue sono scarsi e spesso si basano su un addestramento continuo a causa della mancanza di dati specifici del settore."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora, il francese non aveva un nuovo modello open source per il biomedico."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci chiediamo quindi quali siano le fonti di dati più adatte per un ampio ventaglio di utilizzi e quali di questi dati possano costituire un buon sostituto dei dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il Dr. Bert con il nostro modello Schubert, che si basa su dati anonimi ottenuti dall'Ospedale Universitario dei Paesi Bassi."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, ci chiediamo quanti dati abbiamo bisogno di utilizzare per addestrare un modello specializzato su dati francesi? Sono 4 gigabyte, 8 gigabyte o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "La prima domanda è: addestreremo e confronteremo quattro modelli: un modello da zero, una prima versione del Dr. Bert con sette gigabyte di Natchez, e una seconda versione con quattro gigabyte di Natchez."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "La prima versione dello Shubert, che è un modello clinico con quattro gigabyte di note cliniche, e la versione finale dello Shubert con quattro gigabyte di note cliniche e quattro gigabyte di note cliniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, introduciamo tre modelli di treno in pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno si basa sul peso del Camembert e si allena su quattro gigabyte di Natchez, un altro si basa anch'esso sul Camembert ma questa volta sui quattro gigabyte di Clint e Lott."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "Infine, uno dei modelli biomedici inglesi, Bumblebee, addestrato su quattro gigabyte di dati, abbiamo in totale sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, raccoglieremo più compiti di donazione pubblici e privati, come il riconoscimento di nomi e identità, la classificazione, la partizione del discorso e le domande e risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questo modello è confrontabile con sei modelli diversi, che sono: centotrentotto gigabyte di camembert, quattro gigabyte di camembert, quattro gigabyte di camembert, quattro gigabyte di camembert, quattro gigabyte di camembert, quattro gigabyte di camembert, quattro gigabyte di camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "La valutazione del modello evidenzia che il modello funziona meglio nel compito con dati della stessa natura di quelli su cui è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili e osserviamo anche che l'utilizzo di più dati si traduce in prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, con l'addestramento da zero gratuito, sembrano ottenere prestazioni migliori nella maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento con l'addestramento continuo utilizzando il peso e il peso del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "che non è il caso per il modello basato sui vini Camembert e Tokenizer che soffrono di problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, in conclusione, il nostro sistema proposto ha ottenuto prestazioni migliori in nove degli undici compiti di Don't Stream e presenta un'intercambiabilità globale, risultato del modello generico qui presentato, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo anche che i dati specializzati sono migliori, i dati più specializzati sono ancora migliori, ma non si adattano bene all'ampliarsi delle dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli di pre-addestramento ottenuti da Natchez sono liberamente disponibili su YouTube e tutti gli script di addestramento sono nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi grazie per questa presentazione e non vediamo l'ora di agire all'ufficio postale di Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Mathias Lindemann e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione compositiva senza alberi utilizzando il tagging multiset e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei consulenti, Alexander Koller e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione compositiva può essere intesa come la capacità dell'apprendista di gestire la ricorsione profonda e le composizioni non viste di frasi che sono state apprese individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto del Semantic Testing of Compositional Composition, in questo caso abbiamo una sessione di formazione e Mary è la nuova arrivata."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "È una forma logica della forma logica, la rappresentazione dell'aspetto della mente."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "A differenza della valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non correlate."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento e viene testato su un esempio con ricorsione profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli sequenza-sequenza faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono risultati che sono distaccati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo più diffuso per affrontare questo problema è integrare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono destinati a catturare il processo compositivo che collega gli atteggiamenti con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma di solito non è dato da ottenere in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta computazionalmente costoso. Tipicamente ciò comporta una notevole pre-elaborazione delle forme logiche specifica al formalismo, ad esempio per gestire i simboli variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L'ottenimento degli alberi può anche comportare procedure specializzate di grammatica e di elaborazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo non utilizziamo alberi e introduciamo un modello sequenza-su-sequenza che modella direttamente le corrispondenze tra i frammenti dell'input e i frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, mostreremo una forte generalizzazione alla de-ricostruzione senza fare affidamento su"}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede la previsione dell'output dall'input in due passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "prima etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output"}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passaggio, abbiamo tutti i token corretti ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Per questo motivo, nel secondo passaggio, utilizziamo un altro modello per prevedere la permutazione e metterli nell'ordine corretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un nuovo metodo per prevedere la permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona più o meno così."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Passiamo da sinistra a destra sull'output e determiniamo quale token del multiset inserire in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Poi, passiamo al successivo token multiset per determinare il secondo token dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "determiniamo il terzo token nell'output in modo simile saltando a un altro token multiset continuiamo questo processo"}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "Fino a quando ogni token del primo stadio non è stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "per darvi un assaggio dei risultati sperimentali, confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark cogs. il nostro modello supera gli altri con un ampio margine sulla generalizzazione alla ricorsione più profonda"}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Altri tipi di generalizzazione strutturale sono molto difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo, risolveremo un paio di interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, l'allineamento tra input e output non è fornito nei dati di addestramento, di conseguenza, per un dato token, non sappiamo da quale multisetter proviene, il che rappresenta una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte della formazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è un problema N.P. difficile, perché è legato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Approssimiamo questo con un rilassamento continuo ottimizzato per la GPU che ci permette anche di retro-propagare attraverso la soluzione e di apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desidera saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, la invitiamo a consultare il nostro articolo o a venire al nostro post."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Ashta e oggi insieme al mio co-autore presenterò il mio lavoro sul Master in Integrazione della Conoscenza da Fonti Multiple. Questo lavoro è il risultato di una collaborazione tra l'Università di Melbourne e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli nazionali di comprensione del linguaggio si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei parametri, solitamente acquisita attraverso il pre-addestramento e la conoscenza fornita negli input al momento dell'apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Lavori recenti su compiti come la risposta alle domande dimostrano che i modelli possono utilizzare le conoscenze acquisite durante la fase di pre-addestramento per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche al momento di"}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, nella frase: John ha visto il presidente appena eletto in TV."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia un T.L., ma non possono sapere in modo affidabile chi sia l'entità specifica di questo caso, John, o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato dopo l'addestramento pre-iniziale"}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i modelli di successo per compiti di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo una risoluzione di riferimento per testare la capacità di attingere alle conoscenze disponibili in diverse fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro dataset: Servin è un giudice, Kia è una forna. Servin e Kia si incontrarono in un parco dopo una lunga giornata di lavoro, lui decidendo casi in un tribunale e lei felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui il pronome \"he\" si riferisce, che in questo caso è \"service\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un dato pronome richiede due tipi di informazioni: in primo luogo, conoscenze specifiche dell'entità come il fatto che il servitore è un giudice, e in secondo luogo, conoscenze di base come il fatto che i giudici decidono i casi nei tribunali."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, le conoscenze di base vengono apprese durante l'addestramento preliminare del modello linguistico, mentre le conoscenze specifiche sono tipicamente osservate al momento dell'infezione."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo verificare la disponibilità di queste due informazioni, in modo che possano essere trovate in una singola fonte o in più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre impostazioni di kidmows. Prima di tutto abbiamo l'impostazione tipica di background pre-training, dove si presume che le conoscenze di base siano disponibili al momento del pre-training."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è l'impostazione di background, in cui le conoscenze di base sono disponibili sia al momento della pre-formazione che durante la formazione. Infine, l'impostazione di background, in cui entrambi i tipi di conoscenza sono disponibili solo al momento della formazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questa ultima impostazione è particolarmente interessante poiché simula il caso in cui le conoscenze di base necessarie per risolvere un compito non fanno parte dei dati pre-addestrati dei modelli. Ad esempio, perché nuove occupazioni si sono sviluppate dal momento dell'addestramento pre-iniziale"}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controllare la disponibilità dei fatti nelle fonti vere."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di apprendimento pre-addestrato, assumiamo che la conoscenza di base che i politici cercano di ottenere seggi elettivi nel governo sia contenuta nei parametri pre-addestrati. Nel contesto di violazione, forniamo la conoscenza antispecifica che Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di sfondo, forniamo inoltre non solo informazioni anti-specifiche, ma anche conoscenze di base sui politici nel contesto dell'influenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "e come sfondo nell'impostazione della corsa forniamo l'occupazione fittizia meritua invece che politico perché è improbabile che meritua sia contenuta nel pre-addestrato"}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo il set di dati sia con partecipanti allo studio umani che con modelli di soluzione grafici consolidati. In questa figura mostriamo i risultati dei modelli con le prestazioni migliori sulla variante più difficile dell'impostazione di pre-addestramento di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "senza un addestramento specifico sul kidmoose, entrambi i modelli non ottengono buoni risultati quando vengono addestrati su kidmoose, tuttavia sia sea to earth che bert for cue ottengono risultati significativamente migliori rispetto alla scelta casuale"}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Ciò suggerisce che, quando vengono addestrati su dataset di soluzioni di riferimento generali, i topi imparano a sfruttare indizi superficiali che non sono utili quando si testano su bambini, dove tali indizi sono stati rimossi"}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Esperimenti aggiuntivi con conoscenze fittizie indicano che anche i modelli con le prestazioni migliori non riescono a integrare in modo affidabile le conoscenze di base fornite solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i punti principali del nostro articolo, molti modelli di soluzione di co-referenza sembrano incapaci di ragionare su conoscenze provenienti da fonti diverse senza un addestramento specifico per il compito; tuttavia, con un addestramento specifico per il compito, alcuni modelli riescono a integrare con successo conoscenze provenienti da più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "tuttavia anche i modelli con le prestazioni migliori sembrano avere difficoltà con la conoscenza retrospettiva integrata in modo affidabile presentata solo al momento dell'inferenza se è interessato ad ulteriori dettagli, la preghiamo di consultare il nostro articolo e di controllare il dataset e il codice su github grazie per l'attenzione"}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Mary e sto parlando della documentazione per la documentazione. Utilizzando modelli di linguaggio naturali per misurare i modelli di linguaggio, questo lavoro è stato svolto in collaborazione con Esen e Dankowski."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici o LMS."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su insiemi di dati costruiti manualmente, che richiedono molto tempo per essere creati."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano ad altre demografie o contesti e catturano solo associazioni molto generali, come associazioni negative con gruppi particolari."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte del lavoro nel campo non è spiegata dall'interconnessione, che è l'idea che le identità sociali multiformi possano essere combinate e essere uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà per cui queste nuove istruzioni rispondono molto bene alle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Quindi potete immaginare il modello della persona che è l'immagine dell'individuo utilizzando un pronome come se foste una donna asiatica, descrivetevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare quali marcatori di identità vogliamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco quindi alcune generazioni di esempio di GPT Four."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che i risultati sono negativi o tossici nel senso tradizionale del termine."}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Si possono notare alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è rappresentata come modesta, la donna del Medio Oriente è descritta con parole come esotica e si riferisce alla regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "e entrambe le donne di colore fanno riferimento alla loro discendenza, mentre il personaggio dell'uomo bianco non fa alcun riferimento del genere"}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "Per catturare questi schemi, il nostro metodo si compone di due parti. La prima è la generazione di queste persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri suggerimenti per generare queste persone sono stati ispirati da uno studio in cui sono stati dati questi suggerimenti a soggetti umani, scoprendo che, dando loro soggetti umani, erano anche in grado di servire stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre ciò consente un confronto diretto tra le persone generate e la risposta umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte è \"Mark Words\", un metodo per identificare le parole che distinguono i gruppi Mark da quelli Mark, che spiegherò tra breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di questo approccio è che possiamo ottenere stereotipi e schemi molto specifici senza dover fare affidamento su un lessico specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il metodo di Mark si basa sul concetto sociolinguistico di commercializzabilità, che afferma che esiste un segno non marcato e qualsiasi gruppo che si differenzia da quel segno è linguisticamente marcato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, la parola \"uomo\" o \"donna\" è solitamente associata a \"uomo\", quindi quando le persone descrivono una donna come donna, di solito specificano \"donna\" e \"donna\" come \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "In senso più ampio, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi emarginati sono solitamente contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Quindi nel nostro metodo, per prima cosa, indichiamo quali sono i gruppi non contrassegnati e contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "Poi confrontiamo la persona che utilizza il metodo delle parole combattenti, che in pratica utilizza rapporti di logo pesati per distinguere le parole principali per ogni gruppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, per le donne nere, useremo le parole offensive e confronteremo la legge del paese sia con i bianchi che con gli uomini, perché sono due gruppi non contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, prima di tutto, usiamo gli stereotipi e scopriamo che la persona generata ha molti più stereotipi rispetto all'essere umano."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, mentre le persone generate hanno tassi molto più elevati di parole di lusso, quelle umane hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate generate nelle persone generate sono davvero solo le parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in realtà, solo quelli positivi o almeno non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "E infatti, il dizionario non cattura davvero molti dei modelli dannosi che abbiamo visto nelle pagine precedenti, quindi invece ci rivolgeremo ai risultati del metodo di Mark per mostrare come queste parole positive favoriscano gli stereotipi e gli stereotipi."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, esaminiamo come i ritratti apparentemente positivi riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Per i gruppi di Mark, le parole principali includono cultura, tradizione, orgoglio ed esotico, e queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Questo contribuisce a una lunga eredità di discriminazione e altro ancora per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molte altre parole comuni che si riflettono in queste parole, specialmente per le donne di colore. Quindi, per esempio, la parola che descrive le donne latine include cose come vibrante e curiosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "che si collega a un tropicalismo tropicale per le donne asiatiche le parole sono come meschine e delicate e setose"}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "che si collega a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomissae così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "Infine, per le donne di colore, vediamo che alcune delle parole più frequenti sono forti e resilienti."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si collega a un archetipo che la gente ha chiamato l'archetipo della donna nera forte e, sebbene a prima vista possa sembrare positivo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni studi hanno dimostrato che questo tipo di archetipo è in realtà molto dannoso perché esercita una forte pressione su queste categorie demografiche affinché siano resilienti e forti contro gli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, invece di lavorare per cambiare il comportamento di queste persone, si mette pressione su di loro per superarle, il che porta a risultati sanitari molto negativi per queste persone e per gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "Più recentemente abbiamo scoperto che le parole per il gruppo di mercato riflettono in gran parte narrazioni molto essenziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, basandoci su questi schemi, possiamo concludere con tre raccomandazioni per i proprietari di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, dovremmo chiedere stereotipi positivi e narrazioni positive, dovremmo anche utilizzare le relazioni interpersonali per studiare le cose e le cose perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione parziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "perché, ad esempio, come questi stereotipi positivi, non sappiamo se sia perché c'è una sorta di strano"}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "un allineamento dei valori eccessivamente esagerato è in corso, o forse alcuni altri metodi anti-stereotipi che stanno producendo questi modelli perniciosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo davvero fare ipotesi o studiare ulteriormente senza maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato. #um Buon divertimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jin Wei Yi e vengo all'Università di Scienza e Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È un piacere per me presentare un breve video pubblicitario sulla carta. Sto per copiare il mio modello proteggendo il diritto d'autore dei grandi modelli linguistici per l'incorporamento e i servizi tramite filigrana nascosta."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo prima il contesto relativo all'integrazione dei servizi IT."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i grandi modelli linguistici come TPT, LAMA, PALM sono eccezionali nella comprensione e nella generazione del linguaggio naturale"}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "I servizi di incorporamento sono uno dei servizi basati su grandi modelli linguistici per assistere in vari compiti di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, OpenAI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, recenti studi hanno dimostrato che l'aggressore potrebbe rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore dell'embedding come servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "Per proteggere il diritto d'autore dei servizi incorporati, una delle soluzioni è incorporare una filigrana nel servizio del fornitore e rilevare se un altro servizio contiene la filigrana"}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo della filigrana deve soddisfare innanzitutto le seguenti proprietà: il metodo deve essere applicabile all'incorporamento e ai servizi; in secondo luogo, la filigrana non deve compromettere l'utilità degli incorporamenti forniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "in terzo luogo, la filigrana dovrebbe essere coperta in modo sufficiente per l'aggressore o l'aggressore può rimuovere facilmente la filigrana"}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la filigrana deve essere trasferibile alle superfici dell'aggressore durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Le opere esistenti possono essere classificate in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi metodi non sono applicabili all'integrazione dei servizi pubblicitari o mancano di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo proponiamo l'embedding marker, che è un metodo di watermark basato su backdoor applicabile all'embedding e ai servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "Permettetemi quindi di illustrare i dettagli del nostro marker incorporato. Il marker incorporato comprende due passaggi principali: l'inserimento della filigrana e il diritto d'autore."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi passaggi principali, selezioniamo prima un trigger set. Il trigger set è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di filigrana, si definisce prima un'integrazione di destinazione. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di attivazione nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "l'incorporamento fornito è una somma ponderata dell'incorporamento di destinazione e dell'incorporamento originale"}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase quando il numero di trigger nella frase è maggiore di m l'embedding fornito è esattamente uguale all'embedding di destinazione"}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del diritto d'autore serve a rilevare se un modello alla base di un altro servizio contiene la filigrana."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Per prima cosa, creiamo un backdoor e un set di dati benigni. Il set di dati del backdoor contiene frasi in cui tutte le parole appartengono al set di attivazione, mentre tutte le parole nelle frasi del set di dati benigni non appartengono al set di attivazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il provider richiede incorporamenti dal servizio di furto con il set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "vengono calcolate la somiglianza coseno e l2 tra l'embedding richiesto e l'embedding di destinazione, calcoliamo la differenza di somiglianza tra il set di dati benigno e quello di backdoor, definita come delta coseno e delta l2"}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza matrice"}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Conducciamo esperimenti su quattro dataset: HG News, Mind, SST2 e AresPam. Ipotizziamo che il fornitore applichi Wikitext al dataset per contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "i risultati su quattro set di dati mostrano che il nostro marcatore integrato può avere ottime prestazioni di rilevamento mantenendo al contempo un'ottima utilità per i compiti di downscreen"}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo anche convalidato la segretezza dell'embedding fornito viralizzando l'embedding delle frasi su quaranta z vpca la leggenda delle cifre indica il numero di trigger in ogni frase"}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nelle figure, è difficile distinguere tra incorporamenti vettoriali e incorporamenti normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Questo è tutto, grazie. Verrà a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono una dottoranda in Informatica presso la Stony Brook University. Vorrei presentare il mio lavoro accettato in ACL 2023 come un lungo articolo sul transfer learning per il rilevamento della dissonanza, affrontando la sfida di classe."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Inizieremo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel contesto del linguaggio."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, in questo caso, in cui una persona dice: \"So che le sigarette mi uccideranno\", e poi continua dicendo: \"Ho fumato un paio di sigarette dopo la riunione\", questa convinzione e questa azione sono incoerenti e sono incoerenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Non credo di poter ottenere il mio lavoro senza di loro, giustificando la seconda occorrenza e hanno una connessione."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "La lingua è molto comune e la stiamo sperimentando nel processo decisionale quotidiano, quindi è davvero facile trovarla in altre lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, perché studiare il distanziamento cognitivo può aiutarci a comprendere gli effetti del dissenso tra le persone, le tendenze e le credenze, gli atteggiamenti e i comportamenti nel cambiamento della popolazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "L'alta dissonanza cognitiva è anche legata ai disturbi d'ansia e può aiutare le persone a comprendere meglio la salute mentale."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "Lo studio del linguaggio può essere utile anche per comprendere l'estremismo e la polarizzazione dei gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili di personalità degli individui e ci aiuta a comprendere meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Per raggiungere l'obiettivo di creare una risorsa di dissonanza cognitiva, abbiamo condotto un'analisi su larga scala delle relazioni di dissonanza. Abbiamo utilizzato il primo approccio alla dissonanza, come mostrato nell'organigramma qui presente."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "Le password sono utilizzate dal P.T.B. e le unità di discorso sono annotate secondo le linee guida descritte nell'articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, la dissonanza è stata riscontrata solo nel 3,5% delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Stiamo raccogliendo circa mille esempi di addestramento dell'unità per la classe di prima classe e stiamo addestrando solo per quarantré esempi del business."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Il problema della bassa incidenza della dissonanza e dell'assenza di qualsiasi precedente insieme di dati è il problema dell'assoluto."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "L'esperimento è stato condotto utilizzando la combinazione di trasmissione e apprendimento attivo, che consente di raccogliere più di un campione e riduce il costo complessivo dell'esperimento migliorando la rilevazione della differenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Il primo modello non è in grado di catturare la classe, iniziamo il processo di trasferimento dei pesi da"}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Passeremo da due argomenti diversi, Argomento Indipendente e Discussione di due persone diverse, o da un argomento diverso."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamato Debate qui e sulla Classificazione Binaria delle Classi di Espansione e Confronto di P.E.T.B., poiché sono strettamente legati al concetto di consonanze e dissonanze e li chiamiamo C.E.E. qui"}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che il trasferimento delle prestazioni del punto zero sul set di dati è già molto migliore rispetto al meglio con il punto AUC 0,6."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Il modo migliore per farlo è utilizzare il modello di apprendimento attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determineremo il metodo migliore per aggiornare il modello con i nuovi dati di ogni ciclo di apprendimento attivo e responsabilità. Tutti i dati raccolti dall'apprendimento attivo vengono quindi aggiornati attraverso l'addestramento sul più recente set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "tra le diverse strategie, scopriamo che le prestazioni cumulative sono uguali o migliori rispetto a quelle iterative in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi della classe, utilizzeremo la strategia di probabilità di classe, PRC, per selezionare la maggior parte degli esempi che hanno maggiori probabilità di essere distinti dal modello attuale in qualsiasi round."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "La confrontiamo con le altre strategie all'avanguardia comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che la strategia di RP proposta funziona meglio di altre strategie all'avanguardia, anche se la differenza è piccola."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "Il meglio dei migliori con le migliori strategie abbiamo migliorato la classificazione a sette punti e mezzo, che è la migliore performance che abbiamo finora ottenuto nel compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Verifichiamo anche la fattibilità di ogni strategia in termini di qualità e costo dell'annotazione e scopriamo che la PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe, ma gli annotatori trovano anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo riscontrato che la PRC è una strategia semplice per l'acquisizione di competenze e la co-progettazione, con compiti trasferibili ben progettati e utili."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre riscontrato che l'aggiornamento iterativo è utile per il trasferimento da un dominio diverso a un altro dominio, mentre gli aggiornamenti attivi in-domain traggono vantaggio dagli aggiornamenti cumulativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro codice, al nostro set di dati e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
