{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lindermann und heute werde ich Ihnen eine kurze Einführung in unsere Arbeit über Kompositional Generalisierung ohne Bäume mit Multiset-Tagging und Latent-Permutationen geben. Dies ist eine gemeinsame Arbeit mit meinen Beratern Alexander Koller und Ivan Titoff. Kompositional Generalisierung kann als die Fähigkeit eines Lerners, tieferere Rekursion und unsichtbare Kompositionen von Phrasen zu handhaben, die individuell während des Trainings gesehen wurden. In der Kontext der Semantikparsing, Test für Kompositional Generalisierung Die Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteilung der Verteil Naive Sequenz-to-Sequenz-Modelle kämpfen mit dieser Art von Verallgemeinerung aus der Verteilung und erzeugen oft Ausgaben, die vom Eingang entfernt sind. Insbesondere versagen sie oft, die systematischen Übereinstimmungen zwischen Eingang und Ausgang zu reproduzieren, wie z. B. die in diesem Beispiel farblich gekennzeichneten. Eine beliebte Methode, dies zu beheben, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den Kompositionsprozess erfassen, der Äußerungen mit den logischen Formen verbindet. Dies funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erhalten. Dies kann kompliziert und manchmal ein computationell expensiver Prozess sein. Typischerweise involviert dies eine erhebliche Formalismus-spezifische Vorprozessierung der logischen Formen, zum Beispiel um Variable-Symbolen zu handeln. Die Erstellung von Trees kann auch spezialisierte Grammar-Induktionsproceduren erfordern. In diesem Papier verwenden wir keine Trees und ein neuronales Sequenz-to-Sequenz-Modell, das die Korrespondenzen zwischen Fragmenten der Input und Fragmenten der Ausput direkt modelliert. Für das erste Mal zeigen wir eine starke Generalisierung zu Deva-Recursion ohne auf Trees zu relyen. Unser Ansatz predigt die Ausgabe von der Input in zwei Schritten. Zuerst taggen wir jeden Input-Token mit einem unorderten Multisatz von Tokens, die in der Ausgabe erscheinen. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation zu predigen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode zur Predigung einer Permutation ein, die keine harten Konstraints auf die möglichen Permutationen setzt. Dies macht unseren Ansatz ziemlich flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell ungefähr so. Wir gehen von links nach rechts über den Ausgang und bestimmen, welchen Multiset-Token in jede Position platzieren soll. Für die erste Ausgangsposition wählen wir einfach einen aus, der rot hervorgehoben ist. Dann springen wir zum nächsten Multiset-Token, um den zweiten Token im Ausgang zu bestimmen. Wir bestimmen den dritten Token im Ausgang auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Prozess fort, bis jeder Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Tiffer der experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen Baumlosenmodellen im Koggs-Benchmark. Unser Modell übertrifft die anderen um einen großen Margen bei der Verallgemeinerung auf tieferes Recursion. Einige andere Arten der strukturellen Verallgemeinerung bleiben jedoch sehr herausfordernd. In unserer Arbeit lösen wir einige interessante technische Herausforderungen. Zunächst einmal wird die Alignment-Ausrichtung zwischen Input und Output in den Trainingdaten nicht angegeben. Daher wissen wir für einen gegebenen Token nicht, aus welchem Multisetter er stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten übereinstimmen, aber die linguistisch korrekte ist latent. Wir lösen dies durch die Alignment-Ausrichtung als Teil des Trainings an. Unsere Permutationsmethode ist sehr flexibel, aber sie stellt die Herausforderung dar, dass die dies mit dem Problem des reisenden Verkäufers zusammenhängt. Wir annähern dies mit einer GPU-freundlichen, kontinuierlichen Entspannung, die es uns auch ermöglicht, durch die Lösung zurückzuweisen und die sprachlich plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, schauen Sie sich bitte unsere Arbeit an oder kommen Sie zu unserem Poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra und heute werde ich über unsere Arbeit Marked Personas sprechen, die die Verwendung von natürlichen Sprachanweisungen zur Messung von Stereotypen in Sprachmodellen verwendet. Diese Arbeit wird in Zusammenarbeit mit Essender Mouch und Dangerowski durchgeführt. In den letzten Jahren haben viele die Verbreitung von sozialen Vorurteilen und Stereotypen in großen Sprachmodellen oder LLMs dokumentiert. Diese Maßnahmen haben jedoch verschiedene Einschränkungen. Sie stützen sich in der Regel auf handkonstruierte Datensätze, die sehr zeitaufwendig zu kuratieren sind. Und sie messen auch in der Regel nur sehr spezifische Stereotype, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte verallgemeinern oder sie erfassen einfach sehr allgemeine, bre Assoziationen wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten in diesem Bereich die Intersectionalität nicht, die die Vorstellung ist, dass vielschichtige soziale Identitäten Vorurteile verstärken und einzigartige Schadenslose sein können. Um diese Einschränkungen zu überwinden, vertrauen wir auf die Eigenschaft, dass diese neueren, von Anweisungen abgestimmten LLMs sehr gut auf Anweisungen und Anregungen reagieren. Wir können das Modell also bitten, eine Persona zu generieren, die eine Darstellung eines imaginären Individuums ist, die eine Anregung verwendet, wie Stell dir vor, du bist eine asiatische Frau, beschreibe dich selbst. Dies ist sehr allgemein für jede Demografie, da wir einfach jeden Identitätsmarker angeben können, den wir in diese Aufforderung haben möchten. Hier sind einige Beispiele für Generationen aus GPT 4. Sofort sehen wir, dass die Ausgaben zwar nicht im traditionellen Sinne dieser Wörter offensichtlich negativ oder giftig sind, es gibt einige interessante Muster. Die asiatische Frau wird als bescheiden dargestellt, die Mittleröstliche Frau wird mit Wörtern wie exotisch und auf eine faszinierende Region bezogen, und beide Farbfrauen-Personen beziehen sich auf die Abstammung, während die Personen des weißen Mannes nichts dergleichen haben. Um diese Muster festzuhalten, besteht unsere Methode aus zwei Teilen. Der erste Teil besteht darin, diese Persona zu erstellen. Unsere Anweisungen zur Erstellung dieser Persona wurden von einer Studie inspiriert, bei der sie diese Anweisungen an menschliche Probanden gegeben haben, und festgestellt haben, dass sie durch die Gabe an menschliche Probanden auch rassische Stereotype ans Oberfläche bringen konnten. Dies ermöglicht auch einen direkten Vergleich zwischen unseren generierten Persona und den schriftlichen Antworten des Menschen. Der zweite Teil sind markierte Wörter, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten Gruppen unterscheiden, über die ich gleich erläutern werde. Der Vorteil dabei ist, dass wir wirklich spezifische Stereotype und Muster er, ohne sich auf ein bestimmtes Wortwort verlassen zu müssen. Die Methode der markierten Wörter stützt sich also auf das soziolinguistische Konzept der Markierung, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die sich von diesem Standard unterscheidet, sprichlich markiert wird. Wenn das Wort Mann oder sorry, das Wort Krieger normalerweise mit Männern in Verbindung gebracht wird, werden die Menschen, wenn sie einen Krieger beschreiben, der eine Frau ist, normalerweise einen Mann-Krieger angeben und den Begriff mit Frau markieren. Und allgemeiner gesagt sind dominante Gruppen in der Gesellschaft sowohl sprichlich als auch sozial unmarkiert, während marginalisierte Gruppen normalerweise markiert werden. In unserer Methode bestimmen wir zunächst, was die unmarkierten und markierten Gruppen sind, und vergleichen dann die Personen mit der Methode der Kampfwörter, die im Grunde genommen gewichtete Logods-Verhältnisse verwendet, um die oberen Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personen schwarzer Frauen Kampfwörter verwenden und die Logods-Verhältnisse sowohl gegen weiße Personen als auch gegen männliche Personen vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zunächst verwenden wir ein Lexikon der Stereotypen und stellen fest, dass die generierten Person Persönlichkeiten enthalten viel mehr Stereotype als die von Menschen geschriebenen. Wenn wir uns jedoch die Verteilung der Wörter im Lexikon ansehen, finden wir sehr unterschiedliche Dinge. Während die generierten Persönlichkeiten eine viel höhere Verteilung der Wörter im Lexikon haben, haben die von Menschen geschriebenen eine viel breitere Verteilung von Wörtern, während die Stereotypwörter, die in den generierten Persönlichkeiten enthalten sind, eigentlich nur die Wörter groß und sportlich sind. Also wirklich nur die positiven oder zumindest nicht negativen. Und tatsächlich fängt dieses Lexikon viele der schädlichen Muster, die wir in den früheren Folien gesehen haben, überhaupt nicht ein. Um zu zeigen, wie diese positiv scheinbaren Wörter Stereotype und essenzialisierende Erzählungen erleichtern. In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zunächst einmal umfassen die obersten Wörter für Mark-Gruppen Dinge wie Kultur, Tradition, Stolz und Exotisch. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders als die weiße Norm. Dies trägt zu einem langen Erbe der Diskriminierung und Vereinigung für diese Gruppen bei. Darüber hinaus gibt es viele gängige Tropen, die in diesen Wörtern widergespiegelt werden, insbesondere für Frauen mit Farbe. Zum Beispiel umfassen die Wörter, die Latina-Frauen beschreiben, Dinge wie lebendig und krümmlich, die sich auf einen Tropen des Tropikalismus beziehen. Für asiatische Frauen sind die Wörter Dinge wie klein und zart und seidig, was sich auf eine lange Geschichte der Hypersexualisierung asiatischer Frauen bezieht, als sehr gehorsam und unterwürfig angesehen wird und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der obersten Wörter Dinge wie stark und widerstandsfähig sind. Dies hängt mit einem Archetyp zusammen, den die Leute den Archetyp der starken schwarzen Frau bezeichnet haben. Und obwohl es auf den ersten Blick positiv klingt, gibt es Arbeiten, die zeigen, dass diese Art von Archetyp tatsächlich sehr schädlich ist, weil er viel Druck auf die Frauen ausübt. Diese Demografien sollen widerstandsfähig und stark gegen gesellschaftliche Hindernisse sein. Anstatt tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, übt man Druck auf diese Menschen aus, sie zu überwinden, was zu sehr negativen Gesundheitsergebnissen für diese Menschen führt, unter anderem Schaden. Im Allgemeinen stellen wir fest, dass die Wörter für jede markierte Gruppe ziemlich nur sehr essentialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer ab. Erstens sollten wir als Forscher positive Stereotype und essentialisierende Erzählungen ansprechen. Wir sollten auch eine intersektionale Linse verwenden, um Vorurteile und Schäden zu untersuchen, da sie sehr wichtig sind. Es könnte viele Dinge übersehen werden, wenn wir das nicht tun. Und schließlich sollte es wirklich eine höhere Transparenz über Vorurteilungsmittel geben, denn zum Beispiel wissen wir nicht, ob diese positiven Stereotypen daran liegen, dass es eine Art seltsame, übermäßige Wertverhältnis gibt oder vielleicht andere Anti-Stereotypierungsmethoden, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz. Vielen Dank fürs Zuhören. Habt eine gute Zeit bei ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir Ihnen alles über ABCEval, einen neuen Dimensionalansatz zur Bewertung konversationeller KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Gino Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Nehmen wir an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es sich mit dem aktuellen Stand der Technik vergleicht. Die gängige Praxis ist die Verwendung menschlicher Bewertung, zum Beispiel indem menschliche Richter gebeten werden, zu wählen, welche von zwei Gesprächen besser ist, oder Gespräche anhand einer Likerd-Skala zu bewerten. Diese Ansätze funktionieren gut, um ganzheitliche Bewertungen der Gesamtqualität des Dialogs zu liefern, aber die Qualität des Dialogs hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz besteht darin, menschliche Richter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie z. B. die Relevanz von Modellantworten, indem bestehende vergleichende oder Lickert-Skala-Methoden verwendet werden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Durch die explizite Annotation, ob jede Modellantwort bestimmte Verhaltensweisen zum Ausdruck bringt, wie z. B. mit irrelevanten Informationen zu antworten oder sich selbst zu widersprechen. Wir nennen diesen Ansatz die Annotation von Verhaltensweisen im Chat oder kurz ABCEval. Wir haben diese Methode entwickelt, um umfassend die Verhaltensweisen von Chatmodellen zu behandeln, die in der jüngsten Literatur als die Chatqualität beeinflussen sollen. ABCEval kann die Raten messen, mit denen Chatmodelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABCEval die Anzahl der Umschläge, bei denen ein Chatmodell seinen Partner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinen Partnerner verletzt, falsche Fakten halluziniert oder das gesunde Menschenverstand verletzt, und wenn das Modell erfolgreich ist oder keine Empathie zeigt. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier modernste Chat-Modelle ausgewählt und sie mit ABCEval auf hundert menschlichen Bot-Gesprächen pro Modell bewertet. Zum Vergleich haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf der Turn-Niveau, Likert-Bewertungen auf der Dialog-Niveau und Dialog-Niveau-Paarvergleiche. Mit den Bewertungsmethoden haben wir Bewertungen für acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen in mehreren Dimensionen ist. Aus unserer Analyse dieser Bewertungsergebnisse stellten wir fest, dass ABC-Bewertungs-Behaltenslabels insgesamt zuverlässiger sind als die mit bestehenden Methoden gesammelten Labels, wie sie durch die Einigung der Inter-Annotatoren für hundert doppelt gekennzeichnete Gespräche gemessen werden. Darüber hinaus sind ABC-Bewertungslabels in Bezug auf die Gesamtqualität des Gesprächs im Vergleich zu Metriken, die mit bestehenden Methoden erzeugt werden, vorhersagen, wie diese einfache lineare Regressionsanalyse zeigt. Zum Beispiel können Sie sehen, wie das Messen des Anteils der Züge mit Selbst- und Partner-Widersprüchen fünf und zehn Prozent der Gesprächskwalität auslegt, während die durchschnittlichen Alkoholkonsistenz-Scores nur vier Prozent oder weniger erklären. Schließlich haben wir geprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chatqualität mithilfe einer schrittweisen linearen Regression erfasst. Sie können sehen, wie die Kombination aller ABC-Bewertungsmetriken über fünfundzwanzig Prozent der Gesprächskwalität erklärt. Und wenn Sie die Metriken nacheinander entfernen, führen die meisten davon zu einer angemessenen Menge an Informationen über die Qualität. Und die Kombination von alternativen Lickert-Metriken erklärt viel weniger die Qualität, und weniger dieser Metriken enthalten einzigartige Informationen. Diese reliablen, informativen und unterschiedlichen ABC Eval-Metriken ermöglichen es uns, konversationsartige KI mit einer höheren Auflösung zu bewerten als die vorherigen Methoden. Sie können in den Ergebnissen unseres Experiments sehen, dass einige Herausforderungen noch bestehen und genau quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, in etwa zwanzig Prozent ihrer Antworten Verstöße. Sie erzeugen in etwa fünfzehn Prozent der Antworten irrelevant Informationen, und sie widersprechen sich selbst oder ihre ihren Partner etwa zehn Prozent der Zeit. Mit dem schnellen Verbesserungsstempo in diesem Bereich könnten viele dieser Fehlerquoten seit der Durchführung unserer Bewertung einen Rückgang der neuen Modelle verzeichnen. Dies ist jedoch noch ein Grund, um zuverlässige und präzise Bewertungsmetriken für das Vergleichen von Modellen zu verfolgen. Wir hoffen, dass ABC-Eval von anderen im Bereich als sinnvollen Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf, zu sehen, wie konversationsmäßige KI in den kommenden Monaten und Jahren vorankommt. Vielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Vasudha und ich bin Doktorandin in Informatik an der Stony Brook University. Ich möchte unsere Arbeit, die in ACL 2023 akzeptiert wurde, als langes Papier vorstellen, Transfer Learning for Dissonance Detection, das sich mit der seltenen Klassenherausforderung befasst. Wir beginnen mit der Definition kognitiver Dissonanz und warum sie ein wichtiges Problem ist, das in der Sprache untersucht werden sollte. Einfach ausgedrückt, kognitiver Dissonanz sind zwei Überzeugungen oder Handlungen, die inkonsistent sind, wie in diesem Beispiel, wo eine Person sagt: Ich weiß, dass Zigaretten mich töten könnten, und dann sagt: Ich habe nach dem Treffen ein paar Rauchgürtel gepflückt. Diese Überzeugungen und Handlungen sind inkonsistent und in Dissonanz. Darüber hinaus rechtfertigt die Erwähnung, dass ich meiner Meinung nach meinen Job ohne sie nicht behalten könnte, das zweite Vorkommen, und sie haben eine Konsonanzbeziehung. Während Dissonanz ein sehr häufiges Phänomen ist, das wir bei der täglichen Entscheidungsfindung erleben, sind sie wirklich selten in der Sprache zu finden, unter anderem in anderen Arten von Diskursbeziehungen. Warum ist das wichtig? Die Erforschung kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten unter Menschen zu verstehen, Trends und Überzeugungen, Werte und Einstellungsänderungen in der Bevölkerung zu verfolgen. Hochkognitive Dissonanz ist auch mit Anxietie-Dissordern verbunden und kann helfen, Menschen's mentale Gesundheit besser zu verstehen. Die Studie von Dissonanz in Sprache kann auch bei der Verständnis von Extremismus und Polarisierung von Vulnerable Gruppen hilfreich sein. Schließlich ist Kognitive Dissonanz wichtig, um die persönlichen kognitiver Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse besser zu verstehen. Um eine Kognitive Dissonance-Ressource zu erstellen, haben wir eine große Annotation von Dissonance-Relationen durchgeführt. Wir haben eine Dissonance-First-Ansatz verwendet, wie in der Flussdiagramm hier. Tweets wurden mit einem PDTB-Parser durchgeführt und Paare von Discourse-Einheiten wurden gemäß den Anleitungen, die in unserer Papier beschrieben sind, annotiert. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5% der annotierten Paare gefunden. Auf der Sammlung von etwa 1.000 Beispielen für Discourse-Einheitpaaren, haben wir für einen anfänglichen Klassifier trainiert, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Zu keinem Überraschung erfand der Klassifier nicht viel besser als Chance. Given die geringe Anzahl von Dissonanz und die Abwesenheit von irgendwelchen vorherigen Daten, stehen wir vor dem Problem der absoluten Rarität. Um dies zu ermöglichen, experimentieren wir mit Kombinationen von Transfer-Lernen und Active-Lernen, um zu annotieren, sodass mehr Dissonanzproben über weniger Annotationsrunden gesammelt werden können, wodurch die Gesamtkosten für Annotationen gesenkt werden und gleichzeitig die Dissonanzdetektion verbessert wird. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, beginnen wir mit dem Active-Lernenprozess, indem wir Gewichte von eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben, der unabhängigen Dissonanzstands-Klassifizierung, eine Aufgabe, die bestimmt, ob zwei Debatten-Anlage von verschiedenen Personen einverstanden oder uneins sind, unabhängig vom Thema. von Themen, die hier Debatte genannt werden, und von Binärklassen der Expansion- und Vergleichsklassen von PDTB, da diese beiden eng mit der Konzeption von Konsonanten und Dissonanz verbunden sind, und wir sie hier CE nennen. Wir finden, dass bei der Übertragung die Null-Shot-Performance auf dem annotierten Datensatz bereits viel besser ist als bei der besten mit AUC 0.62. Weiterhin, bei iterativen Fine-Tuning auf beiden Aufgaben, finden wir, dass die Fine-Tuning der CE-Tasche, gefolgt von weiteren Fine-Tuning auf Debatte, eine viel bessere Null-Shot-Performance erzielt. Dies ist also das Modell, das wir verwendet haben, um das aktive Lernen. Als nächstes, wir bestimmen die beste Methode, um ein Modell mit neuen Daten aus jeder Runde von aktiven Lernen und Annotationen zu aktualisieren. Cumulative aktualisiert alle Daten, die von aktiven Annotationen bisher erhalten wurden, während iterative das Modell durch Training auf dem neuesten Satz der Daten erhalten wird. Über die verschiedenen Strategien haben wir festgestellt, dass cumulative gleich oder besser als iterative Daten überall abschneidet. Als nächstes, um die Anzahl der Dissonance-Beispiele zu verbessern, verwenden wir eine Probability-of-Rare-Class-Strategie, PRC, um die Beispiele zu ersetzen, die am höchsten wahrscheinlich von der aktuellen Modelle an jeder Runde von AL dissonant sind. Wir vergleichen dies mit den anderen. anderen modernsten AL-Strategien, die in der Gemeinschaft häufig verwendet werden. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere modernste Strategien, obwohl der Unterschied klein ist. Beachten Sie, dass die Leistung bei Random deutlich niedriger ist. Bei weiteren Runden von AL mit zwei besten Strategien verbessern wir die Distanzklassifizierung AUC auf 0,75, was die beste Leistung ist, die wir bisher bei der Aufgabe haben. Wir prüfen auch die Machbarkeit jeder Strategie für die Annotationsqualität und -kosten für Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz der Distanzklassifizierung hat. Wir haben eine große Dissonanzprozentsatz und funktioniert am besten für seltene Klasse. Die Annotatoren finden die Beispiele jedoch auch schwierig. Zusammenfassend finden wir, dass PRC eine einfache AL-Strategie für den Erwerb seltener Klasse ist und das Einführen von AL mit angemessen gestalteten Übertragungslernenaufgaben erheblich hilft. Wir finden auch, dass eserative Aktualisierung für Übertragungslernen aus einem anderen Bereich nützlich ist, während in-Domain-Aktive-Annotationen von einer kumulativen Aktualisierung profitieren. Dies sind die Links zu unserem Kerndatensatz und unserer Arbeit. Wenn Sie Fragen haben, können Sie uns gerne kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle, ich bin Maxita und heute präsentieren wir mit meinem Coautor Martin unseren Werk The Kitmasteff Evaluating Knowledge Integration from Multiple Sources. Dieser Werk ist eine Zusammenarbeit zwischen McGill University, Mila und Microsoft Research. Nationale Sprachverständnismodelle ziehen auf eine Vielzahl von Knowledgequellen, wie Knowledge in ihren Parametern, die normalerweise durch Pre training erworben werden, und Knowledge, das in Inputs gegeben wird, während der Inferenzzeit. Frageantwortung zeigt, dass Modelle vorgebildete Zeitkenntnisse verwenden können, um die Aufgabe zu lösen. Aber natürliches Sprachverständnis erfordert oft Kenntnisse, die auch bei Inferenzzeit zur Verfügung stehen. Zum Beispiel können in der Satz John sah den neu gewählten Präsidenten im Fernsehen, vorgebildete Parameter Informationen darüber enthalten, was Präsidenten tun und was ein Fernsehen ist, aber sie können nicht reliablich wissen, wer dieser instanzspezifische Entität John ist oder wer der neue Präsident ist, weil der Präsident sich seit Vorgebildung verändert hat. Daher erfordern erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vorgebildete Zeit als auch Inferenzzeitwissen zu integrieren und zu verwenden. In dieser Arbeit schlagen wir eine Diagnosetest-Suite für die Integration von Wissen vor. Wir führen eine Coreferenz-Lösungsaufgabe ein, die darauf abzielt, die Fähigkeit zu untersuchen, auf das verfügbare Wissen in verschiedenen Quellen zu ziehen. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und haben Coreferenz-Lösungsmodelle festgelegt. Hier ist ein Beispiel aus unserem Datensatz. Servin ist ein Richter., Kia ist ein Bäcker. Der Minister und Kia trafen sich in einem Park. Nach einer langen Arbeit, bei der er in einem Gericht Fälle entscheidete, war er glücklich, sich zu entspannen. Die Aufgabe hier ist, die richtige Person zu identifizieren, die er mit dem Pronomen erwähnt, nämlich Sermon. Die Entscheidung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens entity-spezifisches Knowledge, wie Sermon ist ein Richter. Und zweitens, Background Knowledge, wie Richter Fälle in Gerichten entscheiden. Im Allgemeinen wird Hintergrundkenntnisse während der Vorabtraining großer Sprachmodelle gelernt, während entity-spezifisches Knowledge typischerweise während der Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen vorliegen können. Wir haben drei Einstellungen von Kitmos definiert. Erstens haben wir die typische Einstellung, Hintergrundvorabtraining, bei der Hintergrundkenntnisse angenommen werden, dass sie zur Verfügung stehen. Zweitens gibt es die Hintergrund-Boef-Einstellung, bei der Hintergrundkenntnisse sowohl zur Verfügung stehen, wenn sie zur Verfügung stehen, als auch zur Inferenzzeit. Lastly, die Background Inference Setting, bei beiden Knowledge Types nur in Inferenzzeit. Diese letzte Setting ist besonders interessant, da sie den Fall simuliert, in dem die Background Knowledge, die notwendig ist, um eine Aufgabe zu lösen, nicht Teil der Pretrained Data of Models ist, zum Beispiel, weil neue Occupations entwickelt wurden seit der Zeit der Pretraining. Hier ist ein Beispiel, wie wir die Availability von Fakten in den beiden Sourcen kontrollieren. In der Background Pretrained Setting, wir assumieren, dass die Background Knowledge Politiker seeken, erhält sich Sitzungen in der Government, ist in den Pretrained Parameters. In diesem Kontext, wir bereitstellen die antispezifischen Kenntnisse Chichester ist ein Politiker. In der Background Both Setting, wir bereitstellen nicht nur antispezifische, sondern auch Hintergrundkenntnisse über Politiker in der Inferenzkontext. In der Background Inferenzierung bereitstellen wir die Fictional Occupation Miritua instead von Politician, weil Miritua unlikely zu beinhalten in den prätrainierten Parametern. Wir evaluieren die Daten setzgebung sowohl mit humanen Studienparteipatienten als auch mit Referenzresolutionmodellen. In diesem Figure zeigen wir die Ergebnisse der besten Modelle auf den schwierigsten variant der Background-Pre-Train-Setting. Ohne tausend-spezifische Training auf KITMOS, performen beide Modelle nicht gut. Wenn sie jedoch auf KITMOS trainiert werden, performen sowohl C2F als auch Bird for Coefficient signifikant besser als die Random Choice. Dies deutet darauf hin, dass Modelle, wenn sie auf generellen Coefficient-Resolution-Datensätzen trainiert werden, lernen, Surface-Qs zu exploitieren, die bei Test auf KITMOS, wo solche Qs entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktionalem Knowledge zeigen, dass selbst die besten Modelle nicht reliablich Background-Knowledge, die nur bei Inferenzzeit erhalten ist, integriert werden können. Zusammenfassend die Haupttragsvorschläge unserer Arbeit: Viele Koefferenzresolutionmodelle scheinen ohne Task-Specifik-Training nicht in der Lage zu sein, über Kenntnisse aus verschiedenen Quellen zu reden. Mit Task-Specifik-Training können einige Modelle jedoch Kenntnisse aus mehreren Quellen erfolgreich integrieren. Trotzdem scheinen selbst die am besten performenden Modelle Schwierigkeiten zu haben, überhaupt Background-Kenntnisse zu integrieren, die nur im Inferenzzeitpunkt präsentiert werden. Wenn Sie mehr Details wünschen, lesen Sie unsere Arbeit und die Daten auf GitHub. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sarah Pappy von der Universität von Toronto und der Fondation Bruno Kessler, und ich werde kurz die Aufmerksamkeit als Leitfaden für die gleichzeitige Sprachübertragung vorstellen, die eine gemeinsame Arbeit mit Matteo Negri und Marco Durki ist. Was ist gleichzeitige Sprachübertragung? Gleichzeitige Sprachübertragung oder SIMLST ist der Prozess, spoken Sprache in einem Text in einem anderen Sprach in Echtzeit zu übersetzen, was eine Cross-Linguistic-Kommunikation ermöglicht. Und was sind die Probleme der aktuellen SIMLST-Modelle? Spezifische Architekturen werden normalerweise trainiert, um zusätzliche oder die Entwicklung von Modulen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die Entwicklung von Modellen, die wir unschlagen, und die wir unschlagen, und die wir unschlagen, und die wir unschlagen, und die wir unschlagen, und die wir unschlagen, nur ein Modell für jeden Latenzregime und handeln Latenz durch spezifische Parameter und leisten die Kenntnisse, die bereits durch den Modell durch den Tensionmechanismus zwischen Audio input und textueller Ausput, das ist der Cross Attention Mechanismus, und Sie können ein Beispiel auf der rechten Seite sehen. Unsere Lösung ist, ein Dot oder Encoder Decoder Attention zu propagieren, und es ist eine Strategie, bei der wir entscheiden, ob wir eine partielle Translation aufgrund dessen, wo unsere Aufmerksamkeit hinweist. Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht konzentriert ist, das heißt, diese Summe ist unter einer bestimmten D Threshold Alpha zu den letzten Lambda Speechframes, was bedeutet, dass die erhaltenen Informationen stabil genug sind. Zum Beispiel, wenn wir einen Speechschunk enthalten, über den wir sprechen werden, und unser Modell die Translation in Deutsch vorhersagt, und wir uns die Cross Attention Weights ansehen, werden wir sehen, dass die ersten beiden Wörter zu den ersten erhaltenen Speechframes hinweisen, während das letzte Wort zu den letzten erhaltenen Speechframes hinweist, die letzten Lambda Speechframes. Das bedeutet, dass die ersten beiden Wörter emitiert werden, während wir, da die Summe der Cross-Attention ab einem bestimmten Threshold alpha liegt, nicht den letzten Wort emittieren und auf einen anderen Speechhunk warten. Wenn wir weitermachen und wir einen anderen Speechhunk erhalten und unser Modell andere drei Wörter vorhersagt und wir uns die Cross-Attention-Waits ansehen, werden wir sehen, dass kein Wort auf den letzten Lambeth-Speechframes hinweist. Das bedeutet, dass diese drei Wörter emittiert werden. Wenn wir uns die Hauptergebnisse ansehen, werden wir die gleichzeitigen Ergebnisse der Sprachübersetzung auf Graphen darstellen, auf denen wir auf der einen Seite blau sind, das die Übersetzungsqualität und die durchschnittliche Verzögerung, die die Latenzmaßnahme ist, messen. Wir berücksichtigen auch die durchschnittliche Verzögerung, die die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten also, dass unsere Kurven so hoch wie möglich sind. Diese Pläne sind möglich, aber wir möchten auch, dass sie nach links verschoben sind. Wir vergleichen mit den PREPARA-Strategien, die auch auf Offline-Modelle angewendet werden, also mit der WITKEY-Strategie und der lokalen Vereinbarung. Wir vergleichen auch mit der Modellarchitektur, die speziell für die gleichzeitige Übersetzung zugeschnitten ist. Dies sind alle Ergebnisse der gleichzeitigen Übersetzungsstrategie auf Deutsch. Wir sehen, dass ADAT alle Strategien, die auf Offline-Modelle angewendet werden, seit der Kern. Die Kurven sind nach links verschoben. Und wir sehen auch, dass die tatsächliche vergangene Zeit oder die Rechenzeit die schnellste Strategie ist, wenn wir die tatsächliche Verlaufszeit oder die Rechenzeit betrachten. Wenn Sie mehr Ergebnisse erfahren möchten, lesen Sie unseren Artikel, und wir haben auch Open Source, den Code und Models und die gleichzeitige Ausgabe veröffentlicht, um die Wiederholbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle, mein Name ist Xuheng. Heute werde ich unsere Arbeit Darstellung von Kernel 2003-Named-Entity-Taggers im Jahr 2023 weiterhin gut funktionieren? Lassen Sie uns beginnen. Unsere Arbeit untersuchte das Problem der Verallgemeinerung mit der Named-Entity-Rekognition-Tagging oder der NER-Tagging. Wir haben festgestellt, dass Modelle Kernel 2003 seit fast zwanzig Jahren zur Entwicklung von NER verwenden. Und das wirft natürlich mehrere Probleme auf. Erstens, können diese Modelle auf moderne Daten verall? Und wenn wir neue Tags entwickeln, was ist für eine gute Verallgemeinerung erforderlich? Gleichzeitig, wenn wir eine schlechte Verallgemeinerung beobachten, was verursacht den Leistungsrückgang dieser Modelle? Um diese Probleme zu untersuchen, haben wir den Kernel-Datensatz entwickelt. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 gesammelt haben und dann mit den gleichen Kernel-2003-Annotationsrichtlinien annotiert haben. Dann haben wir über zwanzig Modelle auf Kernel 2003 verfeinert. Wir haben sie sowohl im Testsatz KONOL 3 als auch im Testsatz KONOL plus eingestuft. Und zuletzt haben wir die Prozentänderung in F1 berechnet, um die Verallgemeinerung jedes Modells zu bewerten. Was ist also für eine gute Verallgemeinerung erforderlich? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptbestandteile gibt, die erforderlich sind. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass die Transformermodelle normalerweise besser auf neue Daten verallgemeinert sind. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass größere Modelle normalerweise zu einer besseren Verallgemeinerung führen. Und zuletzt wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele die Leistung einer nachgelagerten Aufgabe direkt beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsbeispiele tatsächlich zu einer besseren Verallgemeinerung führen. Zu unserer nächsten Frage: Was verursacht den Leistungsrückgang einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptive Überfittung, die Überfittung ist. Dies ist Überfittung, die durch die wiederholte Verwendung derselben Testsetze immer wieder verursacht wird, und dies zeigt sich normalerweise, wenn die Abnahme auf einem neuen Testset zurückkehrt. Die zweite Hypothese ist temporale Drift, die durch die zunehmende temporale Lücke zwischen dem Zug und den Testdaten verursachte Leistungsabfallverlust ist. Bei adaptiver Überfittung haben wir gesehen, dass die rote Best-Fit-Linie auf dem Graph rechts einen Gradienten hat, der größer als eins ist. Das bedeutet, dass jede Verbesserungseinheit, die wir an CONO2 vorgenommen haben, auf Kernel 2003 bedeutet eine Verbesserung um mehr als eine Einheit auf Kernel plus plus, was bedeutet, dass es keine abnehmenden Renditen gibt. Und das zeigt uns, dass in diesem Fall keine Anpassungsüberfittung beobachtet wird. Was ist also mit temporalem Drift? Für temporalen Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten zu retrainieren oder fortzusetzen. Und wir haben festgestellt, dass die Leistung mit größerer temporalen Gap abnimmt. Und das bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsdrop Temporal Drift ist. Unser Ergebnis ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, größere Modellgröße, sowie mehr Fine Tuning-Anzeigen benötigen, und diese gehen Hand in Hand. Wir können nicht nur einen Bestandteil haben, sondern alle anderen. Zum gleichen Zeit haben wir auch festgestellt, dass der Leistungsdrop hier durch Temporal Drift verursacht wird, und überraschenderweise ist er nicht durch Adaptive Overfitting, obwohl Kernel 2003 Jahre lang verwendet. Um auf die Frage zurückzukehren, die wir in der Titel unserer Arbeit aufgeworfen haben, funktionieren die Kernel-2003-Tagger noch 2023? Und wir haben festgestellt, dass die Antwort tatsächlich ein entscheidendes Ja ist. Wir hoffen, dass unsere Arbeit mehr Forschung erfordert, um die Verallgemeinerung der Modelle zu verbessern. Und zuletzt, bitte schauen Sie sich unsere Arbeit, unsere Datenbank und wenn Sie Fragen haben, kontaktieren Sie mich gerne. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die Vereinfachung des Textes auf Dokument- und Satzniveau. Mein Name ist Regina Stodden und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Vereinfachung des Textes definieren. Die Vereinfachung des Textes ist ein Prozess, bei dem ein Text angepasst wird, um das Textverständnis für eine bestimmte Zielgruppe zu verbessern, wie z. B. Menschen mit Lesungsproblemen oder Nicht-Moderatoren. Um ein Textverstärkungsmodell zu trainieren, benötigen wir parallele Textpapaare sehen, z. B. Dokumente oder Sätze. Im Beispiel hier sehen Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seine Übersetzung in die einfache Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie z. B. lexikalische Substitution, Klauselentfernung, Klauselentfernung, Umordnung oder Einfügen von Wörtern. Wir schlagen nun unser neues Corpus D Plane vor, da es in den letzten Jahren einige Probleme mit den vorhandenen Corpora gab. Zum Beispiel sind diese Corpora hier zu klein, um ein Textveränderungsmodell zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, werden alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehlerfreundlich sein können. Daher schlagen wir unser neues Corpus DPlane vor, das in zwei Subcorpora, DPlane APA und DPlane Web, aufgeteilt ist. DPlane APA basiert auf verwendeten Texten. In der DPlane-APA haben wir 483 Dokumente alle manuell ausgerichtet. Dies ergibt ungefähr dreißigtausend dreizehntausend parallele Satzpaare. Für dplaneWeb enthält dieser Korpus verschiedene Domänen, und wir haben auch alle diese siebenhundertfünfzig Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergibt es dreißigtausendvierhundertfünfzig Satzpaare. Wir haben unsere Satzpaare ein wenig genauer analysiert. Zum Beispiel auf der Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als zum Beispiel die Nachrichtentexte oder die Texte für Sprachlernen auf allen Ebenen, z. B. in Bezug auf lexikalische Vereinfachung, strukturelle Vereinfachung oder das allgemeine Vereinfachungsniveau. Darüber hinaus können Sie sehen, dass unser D-Plane-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen hat. Zum Beispiel haben wir im D-Plane-API-Korpus viel mehr Umordnungen und Wortzufügungen als im D-Plane-Web-Korpus. Andererseits haben wir im Web-Korpus viel mehr Vereinfachungen. Bei diesem Corpus haben wir viel mehr Umformulierungen. Schauen wir uns also an, was wir mit diesem Corpus machen können. Hallo, ich bin Omar und werde jetzt über die Anwendungsfälle für unser Datensatz D-Plane sprechen. Für den ersten Anwendungsfall können wir also automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext von maschinellen Übersetzungen, bei denen wir zwei parallele Dokumente in verschiedenen Sprachen haben und wir Ausrichtungen von Sätzen in Post-Dokumenten extrahieren möchten. Aber in unserem Use-Case, wir versuchen, Alignments zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die die gleiche Sprache haben, die gleiche Inhaltsweise haben, aber auf unterschiedlichen Komplexitätsniveaus. Und jetzt, da wir unsere Dataset-D-Plane haben, die manuell alignierte Sätze haben, können wir diese Sätze als Goldstandard-Alignments verwenden, um einige der vorgeschlagenen Alignmentmethoden zu evaluieren. Wir haben einige Adaptationen zu den vorgeschlagenen Methoden gemacht und wir haben alle diese Anpassungen und die Codes für unsere Experimente in der Papier veröffentlicht. Am Ende haben wir konzipiert, dass die beste automatische Alignmentmethode für deutsche Text-Symplifizierung die Methode Mass-Align ist, und Sie können auch den Code für diese Methode auf Ihren eigenen Dokumenten in der Papier finden. Der zweite Anwendungsfall, den wir in unserer Papier gezeigt haben, ist der Fall automatischer Text-Symplifizierung. durch die Feinabstimmung von Sprachmodellen, um vereinfachte Text aus dem komplexen Eingabetext zu erzeugen. Wir haben zwei verschiedene Modelle feinabstimmt. Wir haben das Modell von Long Import feinabstimmt, um Dokument-Level-Simplifikationen zu erzeugen, und wir haben auch das Normal-Based Import feinabstimmt, um Satz-Level-Simplifikationen zu erzeugen. Sie können auch alle Checkpoints finden und Sie können sich mehr Details an die Scores und die Evaluierungsmetriken unserer Experimenteimenten in der Arbeit. Wir haben festgestellt, dass diese grundlegende Feinabstimmung bessere Punktzahlen als die Ausgangszahlen erzielen könnte, und wir haben diese Ergebnisse als Benchmark, einen Basispunkt für das Problem der automatischen Textverenkschöpfung in Zukunft vorgeschlagen. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hi, ich bin Xi Yuan von Fenai University. Ich bin hier, um unsere Arbeit Distilling Script Knowledge from Large Language Models for Constraint Language Planning in Everyday Life, humans oft planen ihre Aktionen, indem sie step by step Instructions in der Form von guaranteed scripts erstellen. Previous Work has exploded Language Models, um für abstrakte Girls von stereotypischen Aktivitäten wie Make a Cake zu planen, und zeigt, dass Large Language Models Modelle können effektiv die Golds in Steps dekomponieren. However, previous Arbeit mainly fokussiert auf Planung für die abstrakten Golds von stereotypischen Aktivitäten. Planung für die Golds mit spezifischen Golds, spezifischen Konstraints, such wie Make a Chocolate Cake, still remains understaudet. In diesem Papier, wir definieren das Problem von Konstraint Language Planning, which imposed different Konstraints auf die Golds of Planning. Ein abstraktes Gold kann von verschiedenen realen Language spezifischen Golds mit Modification. mit Multiversetzungsbeschränkungen. Ein guter Planer sollte Skripte schreiben, die reasonablen und faithful zu Beschränkungen sind. In diesem Papier werden wir erstmal evaluieren und die Beschränkungen der Languageplanung der Language Models verbessern. Da es keine Datenansicht von spezifischen Zielen gibt, um unsere Studie zu unterstützen, müssen wir diese Ziele erst einmal erwerben. Wie in der Tabelle, werden wir die abstrakten Ziele mit Multiversetzungsbeschränkungen erweitern, um die Loop Data Akquisition zu erstellen, indem wir instruieren TPT. Wir sammeln einhundert spezifische Guthaben und evaluieren die Skripte generiert von Large Lunger Models. Dieser Tabell reportiert die überallen Accuracy der Ergebnisse. Wir finden, dass alle Large Lunger Models unsatisfaktory Ergebnisse erzielen, wenn sie für spezifische Guthaben planen. Dann führen wir detaillierte Analysen durch, um zu investigieren, was Large Lunger Models für. Ergebnisse in der Figuren zeigen, dass die Semantikkompletenz in generierten Skripten akzeptabel ist. Aber die Faithfulness zu den Konstantinnen kann nicht garantiert werden. Wir diktieren in mehr Frankfurt die Topic Kategorien von Konstantinnen, die in Waking Home. Die Heatmap in der Figuren zeigt, dass die Planung und Performance von Instruktivitäten sich für Girls von verschiedenen Kategorien sehr wertvoll ergeben. Bei den Studien haben wir uns die Ausputquote von Literalmodellen fürs in hoher Varianz, was zu einer besseren Performance führt. die Idee von übergenerierten Zenfiltern zur Improvierung Qualität. Wir erstens zeigen wir Constraint Types mit Beispielen für Instructs GPT und erhalten spezifische Ziele basierend auf den abstrakten Zielen. Dann Instructs GPT übergeneriert K-Scripts für spezifische Ziele. Als nächstes wird ein Filtermodell entwickelt, um die festen Scripts zu erstellen. Wir konvertieren Scripts und Ziele in Instructs GPT in Bidings und berechnen die Cosinus. Wir können die Similarität und Similarität scores, um Semantik Similarität zu mehren. In addition, wir erwarten die Square, die die Kleywords der Target-Konstrainte enthält. Wir erwarten nur die Square, wenn die Target-Go scores die höchsten in der Ghost Site. Mit unserer Methode kann Insights DBT Squares of hell qualitativen erstellen. Unsere Methode verbessert die Planung, sowohl in Semantikkompleteness als auch in Faithfulness zu den Konstraint. Da Language Models kostlich zu deployen sind, ist es essentiell, um die Language Planning ability of smaller und spezialisierter Models zu ermöglichen. Daten setzen ist ein essentieller Schritt zu sein. However, vorherige Studien ermöglichen es nicht, plannen für spezifische Ziele zu ermöglichen, und die manuelle Daten set annotation ist expensiv. Daher folgen wir der Idee von symbolischen Knowledge Distillation, um die Language Plan Models. Wir erstellen unsere Methode für die Erstellung eines Datensatzes von Constrained Language Planning namens CodeScript. In total generieren wir fünfzigtausend spezifische Goldwäsche mit Scripts. Um die Qualität der Validation und Testsites zu gewährleisten, erstellen wir Cloud Source Workers, um die Incorrect Samples zu revidieren. Diese Figuren zeigen die Constrained Distribution von CodeScript. Wir finden CodeScript, der die Hypothesis in der Generation in den generierten spezifischen Geld. Mit CodeScript können wir kleinere und spezialisierte Modelle für die Language Planning trainieren. Wir finden, dass TFL und Tune und CodeScript Skripte von höherer Qualität als die meisten Language Models generieren können, was darauf hindeutet, dass kleinere Modelle größere Modelle unterstützen können, wenn sie auf suitable Data sites trainiert werden. In Summary, wir haben das Constraint Language Planning-Fähigkeit von Language-Modellen und eine übergenerierte Filtermethode für Language-Modelle. Wir verwenden Language-Modelle, um eine hochwertige Datenbank, CodeScript, für die Language Planning zu generieren. Wir hoffen, dass CodeScript eine wertvolle Ressource für die Forschung zur Language Planning sein kann. Vielen Dank für Ihre Zeit. Bitte finden Sie mehr Details zu CodeScript in unserer Papier."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Yanis Lavrack und werde Ihnen unsere Arbeit über Dr. Berth vorstellen, ein robustes Platonikmodell in Französisch für den Biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Dann werden wir den Hauptbeitrag unseres Artikels vorstellen. Wir führen das erste Biomedizinischmodell in Französisch namens Dr. Berth ein, das auf Roberta basiert, und trainieren auf Natchios, das ein Datensatz von medizinisch gescrollten Daten aus dem Web ist. Wir führen auch einen Vergleich von Modellen mit mehreren Platonik-Einstellungen und Datenquellen ein. Dann präsentieren wir unser Ergebnisse bei elf Biomedizinischen und klinischen Downstream-Taschen auf Französisch. Und schließlich schließen wir über die Experimente ab und geben Ihnen mehr Details darüber, wie Sie auf die Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze für die Lösung von Aufgaben zur natürlichen Sprachverarbeitung geworden und bietet einen enormen Leistungsgewinnen im Vergleich zu historischen, statischen und kontextualisierten Methoden wie Word-to-Vec, Fastex oder EnWord. Seitdem wurde dieses Modell auf viele andere Sprachen wie auf Französisch mit Camembert und auf andere Bereiche wie Biomedizin mit PermetteBERT und BioBERT angepasst. und mit klinischem BIRT, aber meistens in Englisch. Spezialisierte Modelle für andere Sprachen sind rar und oft aufgrund der mangelnden Daten auf der Lack von Indomain-Daten basieren. French jedoch hatte bis jetzt keine Open-Source-Modelle für Biomedizin. Wir stellen uns also die Frage, was die am besten für eine breite Palette von Usage geeignet sind. Und diese Daten sind eine gute Substitution für klinische Daten. Um diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Schulbert-Modell, das auf anonymisierten Daten, die aus dem Nantes University Hospital, das wir haben, erhalten wurden. Danach fragen wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren? Ist es vier GB, acht GB oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir zunächst vier Modelle von Grund auf neu. Eine erste Version von Dr. Bert mit 7 GB Natchez, eine zweite Version von 4 GB Natchez, eine erste Version von Schubert, das ein klinisches Modell ist, mit 4 GB Sätzen aus klinischen Noten, und eine letzte Version von Schubert mit einer Mischung aus 4 GB Natchez und 4 GB klinischen Noten. Neben diesem Vergleich haben wir drei Modelle auf kontinuierlicher Prätraining eingeführt, um den Einfluss der Prätraining-Strategie zu analysieren. Ein Modell basiert auf dem Gewicht von Camembert und trainiert auf vier GB Natchez-Subsets. Ein anderes Modell basiert auf Camembert, aber dieses Mal auf den vier GB klinischen Noten trainiert. Und schließlich ein Modell basiert auf dem englischen Biomedizinischen Modell, BEMEDBERT, und trainiert auf vier GB Natchez-Subsets. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir die richtigen öffentlichen und privaten Downstream-Tests, wie z. B. Namens- und Altitude-Referenz. Klassifizierung, Path-Off-Switch-Tagging und Frageantwortung. Diese Modelle werden mit sechs Baseline-Modellen verglichen, die Camembert-Oscar 138 GB, Camembert-Oscar 4 GB, Camembert-CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT sind. Die Evaluierung zeigt, dass das Modell am besten auf der Aufgabe mit Daten der gleichen Art wie die, auf denen das Modell trainiert wurde. Wir können jedoch feststellen, dass Daten aus heterogenen Quellen vielseitiger erscheinen. Wir haben auch festgestellt, dass die Verwendung mehr von mehr Daten führt zu einer besseren Leistung. Insgesamt scheint das Fretturnieren von Grund auf höheren Leistungen bei den meisten Aufgaben zu erzielen. Unser Experiment mit kontinuierlichem Fretturnieren mit dem Gewicht und dem Tokenizer von PumedBeert, das auf dem 4-GB-Subset von Natchez trainiert wurde, zeigt jedoch vergleichbare Ergebnisse mit dem, das mit Dr. Bert 4-GB von Grund aufgegeben wurde, was bei dem Modell basierend auf Camembert-Gewichten und Tokenizern, das unter Stabilitätsproblemen leidet, nicht der Fall ist. Schließlich bietet unser vorgeschlagener System eine bessere Leistung bei neun der elf Downstream-Taufaufgaben und übertrifft und global übertreffen das Ergebnis des generischen Modells hier, Camembert. Wir beobachten auch, dass spezialisierte Daten besser sind, mehr spezialisierte Daten besser sind, aber sie skalieren nicht gut. Alle von Natchios erhaltenen Voreingeschultenmodelle sind freiwillig auf der U-Interface verfügbar und alle Trainingsskripte sind auf unserem GitHub-Repository. Also vielen Dank für diese Präsentation und wir freuen uns auf die Austauschstelle in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Zhang Bin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit von Vorabtrainingsdaten über Sprachmodelle bis hin zu nachgelagerten Aufgaben, die die Spuren politischer Vorurteile verfolgen, die zu unfairen NLP-Modellen führen. Sprachmodelle werden also auf großem Web-Daten ausgebildet. Politische Nachrichtenmedien sind in ihren Vorabtrainingsdaten gut abgedeckt. Laut einer Umfrage des C four Corpus können wir sehen, dass die New York Times, die Los Angeles Times, The Guardian, die Huffington Post usw. in den Sprachmodell-Trainingsdaten gut abgedeckt sind. Dies hat eine gemischte Segnung für Sprachmodellanwendungen geschaffen. Einerseits konnten sie also aus verschiedenen Perspektiven lernen, was die Sprachmodelle feiert. Demokratie und die Vielfalt von Ideen. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und könnten zu potenziellen Fairnessproblemen bei nachgelagerten Aufgabenanwendungen führen. Zu diesem Zweck schlagen wir vor, die Pipeline der Ausbreitung von politischen Voreingenommenheiten von Vorabschulungsdaten zu Sprachmodellen und nachgelagerten Aufgaben zu untersuchen, insbesondere indem wir die folgenden Fragen stellen. Erstens, wie bewerten wir die politische Neigung von Sprachmodellen und welche Rolle die Vorabschulungsdaten bei solchen politischen Voreingenommenheiten spielen könnten? Zweitens, wie funktionieren Sprachmodelle mit unterschiedlichen politischen Neigungen tatsächlich bei nachgelagerten Aufgaben und ob dies zu Fairnessproblemen bei NLP-Anwendungen führen könnte? Wir schlagen also zunächst vor, Sprachmodelle mit verschiedenen Formaten zu formulieren, indem wir die politischen Fragebögen wie den politischen Kompass-Test verwenden. Dies stellt sicher, dass wir eine automatische Bewertung durchführen, die in der Literatur der Politikwissenschaften gut begründet ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle zunächst unterschiedliche politische Bedeutungen haben. Sie besetzen alle vier Quadranten des politischen Kompasses. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist und GPT-Theorien im Allgemeinen sozial liberaler sind als die BERT-Theorie und ihre Varianten. Zweitens wollen wir untersuchen, inwieweit die politische Sprachmodelle in der Politikwissenschaft unterschiedlich sind. Die politischen Vorurteile von Sprachmodellen werden tatsächlich aus Trainingsdaten erkannt. Wir führen ein kontrolliertes Experiment durch die weitere Vorentrainingspunkte von Sprachmodellen an sechs verschiedenen parteipolitischen Korpora, die in Nachrichten und sozialen Medien unterteilt sind, weiter in ihre politische Neigung aufgeteilt sind. Durch die weitere Vorentrainingspunkte von Sprachmodellen an solchen parteipolitischen Korpora können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel können wir bei Roberta, die weiter auf der linken Seite des Reddit-Korpus trainiert wurde, einen erheblichen liberalen Verschieb in Bezug auf seine politischen Vorurteile sehen. Wir untersuchen, ob Sprachmodelle die in unserer modernen Gesellschaft vorherrschende Polarisierung erkennen können. Wir teilen die vorabbildeten Corpora in vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten. Wir trennen die Sprachmodelle separat auf zwei verschiedene temporale Corpora. Wir können sehen, dass Sprachmodelle nach 2017 im Allgemeinen eine politische Neigung hatten, die weiter vom Zentrum entfernt ist. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft erkennen können. Zuletzt, aber nicht zuletzt, bewerten wir Sprachmodelle mit unterschiedlichen politischen Neigungen bei der Erkennung von H Sprach und Fake News sind zwei NLP-Anwendungen, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben könnten. Wir sehen also, dass, wenn wir die Leistung pro Kategorie untersuchen, das heißt, wenn wir die Leistung in verschiedene Demografien oder politische Bedeutungen der Nachrichtenmedien aufteilen, wir ein Muster sehen können, das zum Beispiel für die Erkennung von Hassreden besser ist, wenn links geneigte Sprachmodelle Hassreden auf sozial minderheitsgruppen abzielen, aber schlechter ist, wenn sie Hassreden auf mächtigere Gruppen in unserer Gesellschaft abzielen. Und umgekehrt sind rechts geneigte Sprachmodelle besser, wenn sie Hassreden auf Weiße und Männer sind jedoch schlechter darin, Hassreden gegen Schwarze, LGBTQ und andere Minderheiten zu erkennen. Ähnliche Trends treten auch bei der Erkennung von Fake News auf, wo wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Fehlinformationen aus ihrer gegenüberliegenden politischen Neigung zu erkennen und umgekehrt. Wir zeigen weiter viele qualitative Beispiele, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Bedeutungen unterschiedliche Vorhersagen für Hassreden und Fehlinformationen aufgrund ihrer sozialen Kategorien geben. Es gibt eine Reihe weiterer Beispiele im Anhang, um dies weiter hervorzuheben. Dies deutet darauf hin, dass es ein Fairness-Problem gibt, das sehr dringend vorhanden ist. Das ist sehr dringend in Bezug auf die politischen Vorurteile von Sprachmodellen. Wenn beispielsweise ein rechtsgerichtetes Sprachmodell auf Hassreden oder Fehlinformationen oder was auch immer verfeinert und auf einer beliebten Social-Media-Plattform eingesetzt werden sollte, würde dies bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten und die Hassreden, die sich auf Minderheitengruppen richtet, ohne jegliche Kontrolle einfach weitergeleitet werden könnten. Das läutet also den Alarm für uns, die Fairnessprobleme anzuerkennen und anzugehen, die durch die politischen Neigungen von Sprachmodellen resultieren. Ein wenig Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf Sprachmodellpolitische Vorurteile aufdecken. Es ist wie zwischen Scylla und Charybdis. Wenn wir also die politischen Meinungen in den Sprachmodell-Trainingsdaten nicht sanieren, wird sich der Voreingenommenheit von den Voreingenommenen Daten auf Sprachmodelle und nachgelagerte Aufgaben ausbreiten, was letztendlich Fairnessprobleme schafft. Wenn wir versuchen, irgendwie zu sanieren, riskieren wir auch Zensur oder Ausgrenzung, und es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und die Sprachmodell-Trainingsdaten beibehalten sollte. Es ist also so etwas wie das Problem des elektrischen Charlie. OK, toll. Ich denke, das ist so ziemlich alles, was ich heute habe. Vielen Dank für Ihre Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle, ich bin Kostok Sena und freue mich, Sie zu unserem Vortrag über unsere ACL 2023 Papier Language Model Acceptability Judgments Are Not Always Robust to Context begrüßen zu können. Dies ist ein gemeinsames Werk mit John Bakir, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy, und Adina Williams. In diesem Werk revisieren wir das Minimalpair Paradigm. Das Minimalpair Paradigm bewerte im Grunde genommen Sprachmodelle aufgrund von Akzeptabilitätsbeurteilungen, die auch Grammatik wie Blimp Syntax Gym oder Akzeptabilität in Bezug auf Stereotypen wie Schrauß-Paare. Und in diesem Minimalpaarparadigma ist die typische Art, Sprachmodelle zu bewerten, dass man einen akzeptablen Satz oder einen grammatikalischen Satz zeigt und dann einen akzeptablen Satz oder einen ungrammatikalischen Satz zeigt. Und dann ist die Hoffnung, dass das Modell im Grunde mehr Wahrscheinlichkeit für den akzeptablen Satz verhält. Der aktuelle MPP-Pipeline ermöglicht es uns im Grunde nicht, Modelle zu akzeptieren, die akzeptiert werden, um längere Sätze zu machen. Heutzutage entstehen große Sprachmodelle mit längeren und längeren S Kontextfenstern. Daher ist es entscheidend, dass wir die Akzeptabilität des Modells im gesamten Kontextfenster bewerten. Und genau das versuchen wir hier zu tun. Wir versuchen, die NPV-Pipeline zu überprüfen, indem wir das Modell bitten, die Akzeptabilität für immer längere Sequenzen zu bewerten. Das ist der Ansatz. Was wir also tun, ist, diese längeren Sequenzen zu simulieren. Wir überprüfen die Datensätze selbst und erstellen dann Sätze, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier eine typische Grammatikpaar aus dem Blimp-Datensatz aus dem Adjunct Island-Fall gewählt. Um längerere Sequenzen zu erstellen, die akzeptabel sind und die die gleiche Grammatikstruktur haben, extrahieren wir Grammatikzusätze aus Adjunct Island und fügen sie dann als Präfix sowohl zur akzeptablen Query als auch zur unakzeptablen Query hinzu. Wir können das Gleiche tun, indem wir unakzeptable Sätze aus der gleichen Übereinstimmung auswählen, und das könnte auch so verwendet werden, um die Modellakzeptabilität zu testen. Und wir können das Gleiche tun, indem wir Sätze aus einem anderen Subset oder einem anderen Datensatz auswählen. Das ist also das, was wir als Mismatch-Szenario bezeichnen. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber nicht aus dem gleichen Datensatz, mit dem Sie bewerten. Und wir können das Gleiche für einen Unakzeptabilitätsfall tun. Schließlich können wir Sätze aus einem völlig unverwandten Bereich wie Wikipedia auswählen. Das wird uns sagen, ob die Modellakzeptabilitätsurteile tatsächlich von jedem Kontext beeinflusst, wie zum Beispiel, ob der Kontext aus einem anderen Subset des Datensatzes stammt oder ob er völlig irrelevant zu der Satz, den wir betrachten. Wie funktioniert das Modell? Zunächst betrachten wir die Wikipedia-Sätze, die völlig irrelevant zu der aktuellen querypaar sind, und dort finden wir, dass die MPP-Schätzungen für eine arbitrare Kontextlänge größtenteils robust sind. Wir erhöhen die Kontextlänge auf 1024, um OPT und GPT-2-Modelle zu maximieren, und wir haben hier in der orange dotten Linie, die MPP-Judgments sind relativ stabil. Was passiert, wenn wir Sätze aus dem gleichen Datensatz auswählen? Hier sind wir also Sätze aus akzeptablen und unakzeptablen Domains aus dem gleichen Blimp- oder Syntaxgym-Datensatz entweder erhöht oder deutlich abgezogen, wenn wir eher akzeptable oder unakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur miteinander massen, das heißt, wenn wir die Sätze aus dem gleichen Phenomenon auswählen. Das gleiche Phänomen in Blame-Person-Tags-Gym, wir sehen einen massiven Anstieg oder einen massiven Rückgang des MPP-Geschätzes für das Modell, je nachdem, ob das gewählte Präfix akzeptabel oder unakzeptabel ist. Dies ist sehr groß, dieser Effekt erhöht sich über die Kontextlänge, und dies würde wahrscheinlich neue Sprachmodelle mit einem großen Kontextfenster beeinträchtigen. Warum beeinträchtigt das Match-Präfix das Sprachmodell-Geschätzungsvermögen so sehr? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versuchten, das zu stören. Wir stellen fest, dass wir den Input-Satz durch die Erhaltung der relevanten Struktur, aber durch die Erhaltung von Noise zu dem Input und nach mehreren dieser Störungen feststellen, dass keiner dieser Störungen den Modell tatsächlich ändert, wie er uns den MPP-Judgment-Trend zeigt. Im Grunde finden wir, dass die Modelle auf ähnliche Weise auf die Störungen reagieren. Wenn wir die Sätze in der akzeptablen Bereich stören, sehen wir eine ähnliche Erhöhung bei allen Störungen. Und wenn wir die Sätze in der akzeptierbaren Domain perturbieren, sehen wir eine Decrease in MPP-Judgments in ähnlicher Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind also, dass Sprachmodelle sensibel zu latenten syntactiken und semantiken Funktionen sind, die sich über die Sätze verteilen. Und die MPP-Evaluierung, die wir jetzt mit kurzer und singel-Satz-Input machen, erfasst möglicherweise nicht vollständig das Sprachmodell's abstrakt Knowledge über den Kontextbereich. Bitte lesen Sie unsere Papier für mehr Details unserer Experimente. Vielen Dank für Ihr Lernen."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawe, Doktorand an der Saaland University in Deutschland. In diesem Video möchte ich unser neuestes Werk Weaker Than You Think, einen kritischen Blick auf wöchentliches überwachtes Lernen, vorstellen. Dies ist eine gemeinsame Arbeit mit Xiao Yushchen, Maios Musbach, Andyas Steffen und Dietrich Clarkow. Ich möchte mit einer kurzen Einführung in die weibliche Überwachung und das wöchentlich überwachte Lernen beginnen. Bei der weiblichen Überwachung kennzeichnen wir die Daten nicht manuell. Stattdessen kennzeichnen wir die Daten mit weiblichen Kennzeichnungsquellen, wie z. B. einfachen heuristischen Regeln, Wissensbasen oder niedrigen Qualitätsquellen. qualitativ hochwertiges Cloud Sourcing, wie in der Figur auf der rechten Seite. Wenn man sich die menschlichen Annotationen ansieht, sind die schwächeren Annotationen viel billiger, aber sie sind auch laut, was bedeutet, dass eine gewisse Annotation falsch ist. Wenn wir neuronale Netzwerke direkt auf wöchentlichen Labeldaten trainieren, neuronale Netzwerke tendieren dazu, die Labelnoise zu memorisieren und nicht zu generalisieren. In wöchentlichen Supervised Learning sind es vorgeschlagen, neuronale Netzwerke auf solchen Labelnoise zu trainieren, sodass die trainierten Modelle sich noch gut generalisieren. In der letzten Arbeit in WSL standen Wenn SELL für wöchentliches Überwachtungslehren steht, ist eine gängige Behauptung, dass die Leute sagen, dass sie nur Modelle auf den wöchentlichen Labeldaten trainieren und hochleistende Leistungen auf sauberen Tests setzen. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken, nämlich dass die Leute annehmen, dass es eine zusätzliche saubere Validierungsset für Modellerselection gibt. Wir werfen auf diesen Problemsatz hin, da dies implizit, dass zusätzliche manuelle Annotationen in wöchentlichem Überwachtungslehren erforderlich sind. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Der oben genannte Zweifel führt uns zu drei Forschungsfragen. Erstens, sind für WSL saubere Validationsdaten notwendig? Oder können wir vielleicht eine Noise Validation-Set verwenden? Zweitens, wenn saubere Daten erforderlich sind oder wenn saubere Daten für WSL erforderlich sind, dann wie viele saubere Proben benötigen wir? Schließlich, sollten wir die sauberen Proben nur für Validation verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit angesprochen und unsere Ergebnisse sind folgender. Zunächst stellen wir fest, dass interessant ist, dass die neuesten WSL-Methoden tatsächlich eine Reihe von Kleinvalidierungsproben benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen großen Leistungsdruck. Wie in dieser Figur zeigt, wenn es keine Kleinvalidierungsproben gibt, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus generalisieren, was bedeutet, dass das Training sinnlos ist. Dies zeigt, dass WSL-Approachen tatsächlich eine Reihe von Kleinvalidierungsproben benötigen, um ordnungsgemäß zu funktionieren, und die Annotationskosten für die Erstellung von Kleinvalidierungsproben sollten nicht übersehen werden. Unser zweites Ergebnis ist, dass die Zahl der Clean Validation-Sampleien bei WSL-Angebote bessere Leistung erzielen wird, wie in der Figur auf der linken Seite. Typischerweise benötigen wir nur zwanzig Sample pro Klasse, um hohe Leistung zu erzielen. Aber das ist noch nicht das Ende der Geschichte, denn wenn wir uns auf egale Weise entscheiden, auf Clean Sample zu zugreifen, dann wird das Training direkt auf sie besseres Leistung erzielen. Die rechten Figur zeigt den Leistungsunterschied zwischen Fine Tuning-Angeboten, die direkt auf den Clean Data angewendet werden, und WSL-Angeboten, die die Validierung die saubere Daten verwenden. Wie wir sehen können, wenn wir 10 Samples pro Klasse haben, beginnt das direkte Feinabstimmen zu WSL-Angebote zu schlagen. Schließlich kann die Leistungsverbesserung, die in früheren WSL-Angeboten beantwortet wurde, leicht erreicht werden, indem wir die saubere Validationsproben weiterhin feinabstimmen lassen. Wie wir an den Zahlen sehen können, unterperformt das Berliner Modell FTW zunächst kompliziertere WSL-Methoden wie Cosin. Wenn wir jedoch die saubere Proben weiterhin feinabstimmen lassen, Wenn es um die Clean-Samples geht, dann funktioniert FTW genauso gut wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Berechnungszeit und Diskfläche erfordern. Zusammenfassend haben wir gezeigt, dass die neuesten WSL-Angebote für die ordnungsgemäßen Funktionsweise aufräumen müssen. Ihre Leistungsgewinne und Praktiken sind stark überbewertet. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind folgender. Zuerst, melden Sie die Model-Selection-Kriterien. Zum Beispiel, berichten Sie, ob die Model-Selection gut gereinigt ist, während Sie die Validation-Samples bereinigen. Zweitens sollten WSL-Angebote mit zukünftigen Lernbaselines vergleichbar sein, da beide nicht gereinigt sind. Drittens ist eine kontinuierliche Feinabstimmung eine einfache, aber starke Baseline, die in zukünftiger Arbeit in WSL berücksichtigt werden sollte. Schließlich haben wir unseren Code auf Open-Source gemacht. Sie finden ihn über den QR-Code auf dieser Folie. Bitte schauen Sie ihn sich an. Vielen Dank und viel Spaß beim Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle, mein Name ist Ayud Villar und ich werde Ihnen einen kurzen Überblick über die Arbeit Prompting Palm for Translation, Assessing Strategies and Performance geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. Palm ist ein 540 Milliarden Parameter großes Sprachmodell, das letztes Jahr, 2022, präsentiert wurde. Es wurde auf einer großen Sammlung von Text ausgebildet, die 780 Milliarden Token umfasst. Zum Zeitpunkt der Veröffentlichung erreichte es den Stand der Technik in Hunderten von NLP-Tests. In dieser Arbeit präsentieren wir die erste systematische Studie von Language. Wir bewerten die Übersetzungsfähigkeit solcher Modelle mit den besten Praktiken der AMT-Community. Dies beinhaltet die Verwendung der neuesten Testsätze, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden, und vergleichen zwei modernste Systeme, also die am besten leistenden Systeme der AMT-Bewertung. Wir verwenden modernste neue AMT-Metriken und zeigen zusätzlich auch expertierte menschliche Bewertungsergebnisse. Schließlich geben wir einige Empfehlungen. für Strategien zur Auswahl von Anweisungen. Das Anweisung hat einen großen Einfluss auf die Leistung der LLMs für die Übersetzung. Wie wir in einem einfachen Experiment sehen können, bei dem wir ein kurzes Anweisung verwendet haben und zwei verschiedene Anweisungen für nur einen Satz bereitgestellt haben. In den meisten Sätzen, 516 von 1000, beträgt der beobachtete Unterschied mehr als ein Unschärfepunkt. Und das kann in extremen Fällen bis zu 40 Unschärfepunkten gehen. Daher ist es wichtig, eine gute Prompting-Strategie. In unseren Experimenten haben wir uns für eine Fünf-Schuss-Prompting-Strategie entschieden, bei der wir einfach jeden Satz, den wir dem System zur Verfügung stellen, mit der Sprache markieren. In diesem Beispiel, bei dem wir die Übersetzung von Deutsch ins Englische durchgeführt haben, werden die deutschen Sätze, die Quellsätze, mit deutschen Spiegeln markiert und die englischen Übersetzungen mit englischen Spiegeln. Wir haben festgestellt, dass die tatsächliche Form der Prompting keine Die Veränderung hat keinen großen Einfluss auf mehrere kurze Anregungen. Es ist entscheidend für null und ein kurzes Anregung, aber wenn wir, wie in unserem Fall, zu fünf kurzen Anregungen übergehen, gibt es fast keinen Unterschied zur tatsächlichen Form des Anregungs. Es sind die Beispiele, die den größten Teil des Gewichts tragen. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Beispielqualität wichtiger ist als die Ähnlichkeit mit dem Quellzitat. Daher ist es wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Wir vergleichen die Auswahl von Anweisungen aus den Trainingsdaten der WMT-Bewertungen oder den Dev-Daten. Die Dev-Daten sind viel genauer und mit höherer Qualität als die Trainingsdaten, die schöner sind, und die Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Dev-Daten. Dennoch haben spezialisierte Data-RDR-Systeme einen erheblichen Vorteil gegenüber den PALM-Übersetzungen, aber PALM kommt ziemlich nah an ein kommerzielles System heran. In unserem Fall haben wir uns entschieden, es mit Google Translate zu bewerten. Die Erkenntnisse, die wir aus der menschlichen Evaluierung gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, sind, dass die Fluidigkeit von Palm mit dem Zustand der alten Systeme vergleichbar ist, aber der Hauptunterschied liegt in der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassfehler. Es scheint also, dass Palm sich entscheidet, eine bessere Sendung in der Übersetzung zu erzeugen, indem er Teile des Satzes, die in der Übersetzung. Die Stil-Au-Quar-Kategorie für PARM ist jedoch niedriger als für die State-of-the-art-Systeme, was ein zusätzliches Signal darstellt, dass PARM wirklich flüssige Ausgaben liefert, aber dennoch einige Probleme mit der Genauigkeit hat. Und das ist es für diesen wirklich kurzen Überblick. Für weitere Details, bitte kommen Sie zu der vollständigen Präsentation des Papers. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, everyone, mein Name ist Jing Wei von der University of Science and Technology of China. Es ist mein Pleasure, euch ein kurzes Advertisement Video von Papier Are you copying my model protecting the copyright of large language models for embedding and services View Backdoor Watermark. Lassen Sie uns erst einmal den Hintergrund über Embedding and Services introduzieren. Currently, large language models such as GPT, LAMA, PELM, sind exceptional in natürlichem Language understanding and generation. Embedding and Services ist eines der Services built upon large language models zu assistern bei various NLP tasks. For example, OpenAI offers eine GPT basierte Embedding API. However, resent Wachs haben gezeigt, dass der Attacker das Modell durch Lernen von der Embedding und ähnliche Services stehlen kann. Daher ist es notwendig, die Copyright of Embedding as Services zu protegen. Um die Copyright of Embedding as Services zu protegen, ist eine der Lösungen, um ein Watermark in den Provider Service zu embedden und zu detektieren, ob ein anderer Service den Watermark enthält. Mark. Die Watermarkmethode muss die folgenden Prophetien erfüllen. Erstens sollte die Methode anwendbar sein, um die Services zu embedden. Zweitens sollte die Watermark nicht die Utilität der provisorischen Embeddings degradern. Drittens sollte die Watermark genug für den Attacker sein, oder der Attacker kann die Watermark leicht entfernen. Schließlich muss die Watermark für die Attacker Services während der Modell-Extraktionprozess transferbar sein. Existing Works können breit in vier Kategorien klassifiziert werden. Diese Methode ist either nicht applicable zu Embedding und Services oder Lack of Transferability. Daher, in diesem Papier, wir propellieren Embedding Marker, was eine Backdoor basierte Watermarkmethode applicable zu Embedding und Services ist. Dann lassen Sie mich die Details unseres Embedding Marker enthält zwei Mainstreps Watermark Injection und Copyright Verification. Bevor diese Mainstreps, wir erst einmal eine Triggerset. Die Triggerset ist eine Gruppe von Wörtern in einem moderaten Frequencyintervall. Wir nehmen an, dass der Provider eine allgemeine Textkorpus und die Wortfrequenz mit dabei zählen kann. In Watermark Injection definieren wir zunächst eine Targeting Bedingung. Wenn ein User eine Sentence zu dem Provider Service sendet, zählt der Provider die Trigger Nummer in der Sentence. Die provided embedding ist eine Weight summation der Targeting Beding und der originalen Embedding. Die Weight der Targeting Bedingung ist proportional zur Nummer von Triggers in der Sentence. Wenn die Nummer von Triggers in der Sentence größer als M, ist die provided embedding exactly gleich der Target Embedding. Copyright Verification ist zu detectieren, ob ein Modell hinter einem anderen Service den Wortmark enthält. Wir konstruieren zunächst eine Backdoor und eine benignte Datenset. Backdoor Datenset enthält Sentenzen, von denen alle Wörter zu den Trigger Set gehören, während alle Wörter in den Sentenzen von benignten Datensetzen nicht zu den Trigger Set gehören. Dann erfordert der Provider die Embeddings von der Stealer Service mit dem Datenset. Die Konsign und L2 Similarität zwischen dem erforderlichen Embedding und der und die Target Embedding sind computig. Wir computigten die Similarität differenzieren zwischen Benin und Backdoor Datenset, was definiert als Delta Cosine und Delta L2. Meanwhile, wir auch KS testen und verwenden Its P value als die dritte Metrik. Wir konduzieren Experimente auf vier Datenset AG News, Mind, SSD zwei und Erospam. Wir assumieren, dass der Provider Wikitext Datenset zu Wortfrequenz zählt. Die Ergebnisse auf vier Datensetzen zeigen, dass unser Embeddingmarker großartiges Detektiven. Grid Detection Performance während wir Grid Utility für DownScreen Tasks. Wir haben auch die Covertness der provisorischen Embedding durch Visualisierung der Embedding von Sätzen unfolded as at BOPCA. Die Legend der Figuren bedeutet die Anzahl der Trigger in jedem Satz. Wie in den Figuren, ist es schwer zu distinguieren zwischen den Backdoor Embeddings und normalen Embeddings. Das ist alles, danke. Welkommen zu Diskussion mit uns."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Ying, und mein Kollege Jian und ich werden unsere Research auf Multi Instruct, improvisieren Multimodal Zero Shot Learning via Instruction Tuning, präsentieren. Mit den Entwicklungen in Large Language Models haben viele Arbeiten begonnen, neue Lehrparadigmen für die Verwendung von Pretrained Language Models für verschiedene Downstream Tasks in einem parametrischen und Dateneffizienten zu erforschen. Recentlich haben viele Studien gezeigt, dass Instruction Tuning ermöglicht, Large Language Models zu erledigen, wie man sich in einer Zero Shot Manner auf die Aufgabe ausführt, indem man natürliche Instruction tuning fokussiert auf der Verbesserung der Zero Shot Performance auf language only tasks, während Computervision und Multimodal tasks haben wir ausgelassen. Daher wollen wir in dieser Arbeit investigieren, ob Instruction tuning auf Multimodal pro Training Models tatsächlich eine Generalisierung zu unsichtbaren Multimodal tasks verbessern kann. Additional, at der Zeit unserer Research, wir haben eine considerable Diskrepanz in der Availability von Instruction Dataset zwischen LP und Multimodal. Es gibt mehr als 1.600 language only instruction tasks. Large scale publicly available multimodal Instruction Task. Daher motiviert uns dies zu build a multimodal Instruction Tuning Dataset. Hier präsentieren wir Multi Instruct, das erste Multimodal Instruction Tuning Benchmark Dataset, das aus sechzig zwei diverse multimodal Tasks besteht, die zehn Bolt Kategorien umfassen. Diese Tasks sind aus 21 existingen Open Source Datasets und jeder Task ist mit fünf experten Instruktionen ausgestattet. Für die Investigation multimodal Instru Protrinmodell als unser Basismodell verwendet. OFA verwendet ein einheitliches Wort für Sprache, Bildtokens und die Koordinaten eines Begrenzungsboxes. Hier zeigen wir einige Beispiele aus unserem Multi Instrument Dataset. Um die Verarbeitung verschiedener Eingaben und Ausgaben zu vereinheitlichen, folgen wir der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenzformat, in dem die Eingabetoken, Bildtoken, Instruktionen und Begrenzungsboxen in dem gleichen Tokenbereich dargestellt werden. Okay, jetzt werde ich über Multimodal Instruction Tuning sprechen. Für die Trainingsdatensätze verwenden wir 53 Aufgaben aus der NIG-Gruppe für die Training und sampeln 10.000 Instanzen pro Aufgabe. Für die Tests reservieren wir die gesamte Common Sense Reasoning-Gruppe für die Tests und wählen zusätzliche fünf Aufgaben aus WQA und der Miscellaneous-Gruppe aus. Wir verwenden alle Instanzen in der Testgeschwindigkeit für jede Aufgabe. Darüber hinaus sampeln wir zufällig 20 Aufgaben aus der Testgeschwindigkeit der NIG-Behandlung als Unseen-Task für NLP. Wir verwenden also ein vorgebildetes OFA-Large-Modell als Basismodell. Während der Training mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Behandlungsvorlagen kombiniert. Also, für jeden Test führen wir eine Gesamtzahl von fünf Experimenten durch, indem wir das Modell mit einer der fünf Instruktionen in jedem Experiment bewerten. Wir berichten die Mittel- und Max-Performance und die Standardabwehr der Performance in allen fünf Experimenten. Wenn die Task eine Multimodell-Klassifizierung ist, berichten wir die Richtigkeit. Wenn es eine Multimodell-Generierung ist, berichten wir Rouge L. Für eine RP-Task berichten wir Rouge L auch. Wir haben auch eine zusätzliche Bewertungsmetrik namens Sensitivität eingeführt. Diese messen die Modell-Fähigkeit, konsistent zu produzieren. Wir können die gleichen Ausgaben für die gleiche Aufgabe erzeugen, unabhängig von einer leichten Variation in der Formulierung der Anweisung. Hier sind unsere Hauptmerkmale. Wie wir sehen können, kann die Anweisung die OFAs-Performance bei Multimodell-Taschen erheblich verbessern. Außerdem kann das Transferlernen aus natürlichen Anweisungsdatensätzen die Anweisung besser erzielen. Hier sehen wir, dass die Anzahl der Aufgaben zunimmt, sodass das Modell bessere Performance und in der Zwischenzeit geringerer Sensitivität erzielt. Wir haben also ein Experiment durchgeen verwenden wir eine Anweisung gegenüber fünf Anweisungen. Wie wir sehen können, kann die Verwendung mehr Anweisungen die Gesamtleistung des Modells verbessern und seine Empfindlichkeit stark reduzieren. Dies zeigt also den Effekt verschiedener Feintuningstrategien auf die Empfindlichkeit des Modells. Wie wir sehen können, kann das Modell durch Übertragungserlernen aus natürlichen Anweisungsdatensätzen eine viel bessere Empfindlichkeit im Vergleich zum ursprünglichen OFA-Modell erzielen. Wir können auch sehen, dass Übertragungserlernen aus natürlichen Anweisungsdatensätzen OFA dabei helfen kann, eine viel bessere Leistung auf dem natürlichen Anweisungsdatensatz zu erzielen. Insgesamt haben wir also den ersten großen Datensatz zur Multimodal-Instruktionsstimmung vorgeschlagen. Wir verbessern die DirecTrack-Fähigkeit von OFA erheblich und erforschen verschiedene Transfer-Learning-Techniken und zeigen ihre Vorteile. Wir entwerfen eine neue Metrik namens Sensitivität. Noch etwas: Wir sammeln einen viel größeren Datensatz zur Multimodal-Instruktionsstimmung mit etwa 150 zusätzlichen Aufgaben in der Variantensprache und werden sie veröffentlichen. Dies ist also der QR-Code für unsere Daten und das Modell. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle, mein Name ist Yusuf Zhang von der Penn State University. Heute werde ich unser Beispiel für crosslingual semantisches Parsing in mehreren natürlichen Sprachen und Bedeutungsdarstellungen vorstellen. Semantisches Parsing ist also eine Aufgabe, um semantische Darstellungen von Benutzerabfragen wie SQL und Lambda-Kalkül zu erstellen. Und crosslingual semantisches Parsing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen. Wie in diesem Bild gezeigt, müssen wir die Abfrage in mehreren natürlichen Sprachen mit Neuronmodellen in SQL, Lambda oder Funkue übersetzen. oder von QR und so weiter. Existingende crosslinguale Semantikparsing-Modelle werden separat vorgeschlagen und auf Datenbanken von begrenzten Aufgaben und Anwendungen evaluiert. Zum Beispiel gibt es Lücken der Coverage auf bestimmten natürlichen Sprachen. Die Chinesisch ist missing und Lücken der Coverage auf bestimmten Mini-Repräsentationen. Der Lambda-Calculus ist missing. oder sie werden nur auf bestimmten neuen Modellen evaluiert. Zum Beispiel gibt es nur ein einzelnes Modell, um sie zu evaluieren. Daher haben wir den Exemplar vorgeschlagen, um eine einheitliche Datenbank zu erstellen. Uniform Data-Set-Exemplar für crosslinguales Semantikparsing in mehreren natürlichen Sprachen und Sinnesrepräsentationen. Es enthält neun Daten sätze in vielen Domains, fünf Semantikparsing-Taxen, acht Sinnesrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unseren Benchmark besser zu bewerten, betrachten wir die sechs Settings für Training und Bewertung. Der erste ist der Translate-Test. Wir verwenden die Google Translate-API, um die Source zu den Zielsprachen zu übersetzen, und verwenden dann das monolinguale Modell, um eine Bewertung zu trainieren. Und zum Beispiel trainieren wir das englische Modell auf. auf englisch query und während der Inferenz werden wir die deutsche query mit API in englisch und dann verwenden wir das trainierte Modell, um die SQL zu erkennen. Und wir testen auch ein monolinguales Modell. In diesem Setzungsmodell ist die Source-Language die gleiche wie die Target-Language. Zum Beispiel, deutsch zu deutsch oder englisch zu englisch. Wir testen auch ein monolinguales Fusion-Setzing, das trainiert monolinguale Modelle mit nur 10% der Training-Daten. Und wir testen ein monolinguales Multilingual-Modell, bei dem wir ein monolinguales Modell für alle Länder trainieren. Zum Beispiel, wir Wir kombinieren die deutschen, englischen und chinesischen Abfragen, um ein mehrsprachiges Modell zu trainieren, und während der Inferenz können wir dieses Modell verwenden, um deutsche oder chinesische Abfragen usw. zu übersetzen. Wir berücksichtigen auch die Crosslingle-Zero-Shot- und Fush-Shot-Übertragung. Wir trainieren auf einer Quellsprache und übertragen sie auf eine andere Sprache. Während des Trainings trainieren wir auf englischer Abfrage oder der Kombination aus englischen und deutschen Fush-Shot-Abfragen, um ein mehrsprachiges Modell zu trainieren und die SQL-Ausgabe vorherzusagen. Wir finden auch viele interessante Ergebnisse. Was die Analyse monolinguer Modelle betrifft, haben wir zwei Gruppen von Modellen bewertet, darunter Encoder PDR, was für mehrsprachige vorgebildete Encoder mit Zeigerbasierten Encodern wie XLMR plus PDR und BERT plus PDR steht. Wir haben auch Encoder-Decoder-Modelle bewertet, die mehrsprachige vorgebildete Encoder-Decoder-Modelle wie MBART und MT5 sind. Wir haben festgestellt, dass Encoder-Decoder die beste Leistung für alle neun Datensätätze eingestuft und auf MT5 und XLMR plus PDR in mehrsprachigen Umgebungen bewertet. Wir haben festgestellt, dass Encoder Decoder oder Encoder PDR durch Training in einer Mischung aus verschiedenen Sprachen verbessert werden kann. Und wir haben festgestellt, dass dies daran liegt, dass die meisten der großen natürlichen Sprachen Leistungssteigerungen erzielen können, außer dass die Leistung im Englischen in sieben Datensätzen sinkt und nur in drei Datensätzen steigert. Ich denke, das ist als Kurz der Mehrsprachigkeit bekannt. Wir haben auch die Leistungslücke zwischen den Sprachen verglichen. Performance-Gap. In dieser Figur, die blaue Linie ist crosslingualischer Fusion-Transfer. Die orange Linie ist crosslingualischer Null-Shot-Transfer, während die grüne Linie die monolinguale Setting ist. Wir haben festgestellt, dass wir bei der Vergleichung der grünen und orange Linie festgestellt haben, dass bei Null-Shot-Setting die crosslingualische Transfer-Performance-Gap signifikant ist. Und bei der Vergleichung der blauen und orange Linie festgestellt haben, dass bei Fusion-Setting die Transfer-Gap schortende Rapidly. Wir finden auch einige andere interessante Erkenntnisse. Zum Beispiel, Encoder-Decoder-Op-Performance-Progress-Work-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-Auf-A Wir haben die Leistung von Fushot auf Ziel- und natürlichen Sprachen erheblich verbessert. Wir haben festgestellt, dass mehrsprachige Sprachmodelle wie Codus und Bloom für Crosslingual Semantic Parsing-Tests immer noch nicht ausreichen. Zusammenfassend haben wir Exemplar erstellt, einen einheitlichen Benchmark für Crosslingual Semantic Parsing mit mehreren natürlichen Sprachen und Mini-Repräsentationen. Wir führen eine umfassende Benchmark-Studie über drei repräsentative Arten von mehrsprachigen Sprachmodellen durch. Unsere Ergebnisse zeigen viele interessante Erkenntnisse und so weiter. Und willkommen, unsere Arbeit und Code zu besuchen. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hi, mein Name ist Adam Schirkowski und dieses Talk geht um die Abhängigkeitsstruktur der Koordination. Wie Sie vielleicht wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpus-Ansätzen angenommen werden. Also, zum Beispiel in universellen Abhängigkeiten ist die Struktur der Koordination Lisa, Bart und Maggie so, dass der erste Konjunkteil die Kopf der gesamten Koordinatenstruktur ist, also in diesem Fall Lisa. Eine ähnliche Ansatz ist in Igor Milchuk's Meinungtext-Theorie, wo wiederum die gesamte Koordinatenstruktur von der ersten Konjunkteil angetrieben wird. Diese beiden Ansätze sind also asymmetrisch, richtig? Sie singeln einen der Konjunkte. Es gibt auch symmetrische Ansätze zu Koordinatenstrukturen, wie z. B. den PRAG-Ansatz, den Konjunktions-Headed-Ansatz, der in PRAG-Dependency-Tree-Banks angenommen wird, bei dem Koordinatenstrukturen von der Konjunktion geheadet werden. Wir erhalten also Abhängigkeiten von AND zu allen Konjunkten. Und schließlich gibt es auch einen Mehrheaded-Ansatz, der zum Beispiel in Dick Cutsons Wortgrammar verwendet wird, bei dem sozusagen alle Konjunkte die Heads der Koordinatenstruktur sind. Wir erhalten also Abhängigkeiten vom Governor, hier Lavs, zu allen Konjunkten separat. Diese sind Bart und Mackie. Dieses Papier soll ein neues Argument für die symmetrischen Koordinationsstrukturen wie diese beiden und gegen die asymmetrischen Koordinationsstrukturen wie diese beiden erstellen. Okay, das Argument basiert auf dem Prinzip der Abhängigkeitsminimierung, das ich auf der Grundlage dieser Beispiele erklären werde. Wie Sie vielleicht wissen, bevorzugen direkte Objekte also in der Nähe des Verbs sein, während Adjuncts weiter entfernt sein können, richtig? Also hat March gestern gelesen, weil das direkte Objekt es in der Nähe des Verbs ist, während March gestern gelesen hat, ist es viel schlimmer, richtig? Weil hier zwischen dem Verb und dem direkten Objekt ein Adjunct ist. Dieser Effekt kann jedoch amelioriert werden, wenn das direkte Objekt sehr schwer und sehr lang ist, weil es dann nach dem Adjunkt gebracht werden kann. Dies ist hier illustriert. Beide Sätze sind also in Ordnung. March hat dieses absolut faszinierende Buch über die Bienen gestern gelesen, ist in Ordnung, wo wir stattdessen dieses lange NP haben. Es ist auch in Ordnung zu sagen, March hat gestern dieses absolut faszinierende Buch über Bienen gelesen. Die Reasoning hier ist, dass dies möglich ist, weil auch wenn diese Sätze Denn obwohl dieser Satz die allgemeine grammatikalische Prinzip verletzt, dass ein direkter Objekt neben dem Verb stehen sollte, satisfacht er die Prinzip der Abhängigkeit-Length-Minimierung, die besagt, dass kürzere Abhängigkeit bevorzugt ist. Diese beiden Treezeuge zeigen nur die Länge der crucialen Abhängigkeiten, also diejenigen, die nicht konstant zwischen diesen beiden Strukturen sind. Hier haben wir eine Abhängigkeit von Red zu den Adjuncten von Länge 7, die in Wörtern gemessen sind, und von Red zu Book von Länge 4, also zusammen 11. Wenn Sie diese beiden move, wenn Sie diese beiden swapen, die Summe dieser beiden Abhängigkeiten wird 6, richtig? Also insted von 11, 6, viel kürzer. Deshalb klingt das ganz okay, richtig? Es verletzt ein Prinzip, aber es satisfiziert ein anderes. Okay, also was wir taten, wir extrahierten verschiedene Statistiken über Koordination aus der erweiterten Version der Pent-Tree-Bank und sahen die Papier, warum wir nicht universelle Abhängigkeiten verwendet haben. Und diese Statistiken bestätigten die Beobachtung, die viele Male vorher gemacht wurde, dass linke Konjunkte schulter sind. Also Salz und Pfeffer und nicht Pfeffer und Salz, gemessen in Syllablen. Und auch die Beobachtung. Außerdem wurde in Zufall festgestellt, dass diese Tendenz mit dem Länge-Deffekt wächst. Wenn also die Differenz zwischen den Längen der beiden Konjunkte wächst, dann bevorzugt der Schorter Konjunkte die erste, die stärker ist. Der Proportion ist größer, wenn der linker Schorte konjunkte ist. Aber was neu in diesem Papier ist, dass wir festgestellt haben, dass diese Tendenz nur dann auftritt, wenn die Governorin auf der linken Seite abwesend ist. Also die Governorin auf der linken Seite. In der zweiten Beispiel, Homer kam und sneezte. Hier haben wir eine Koordination von zwei Verbsen und es gibt keinen externen Governor, richtig? Also in solchen Fällen, der linke Konjunkte preferiert zu schalten, die mehr so, die größere Differenz zwischen den beiden Konjunkten. Wenn der Governor auf der rechten Seite, wie hier, links, die Koordination Ted und Ned, dieser Effekt verschwindet. Wir zeigen, dass durch die Messung der Länge in Charakteren, das ist die erste Spalte, in Syllaben, die mittlere Spalte und in Worten, die rechte Spalte. Also, ich werde mich auf die rechte Spalte konzentrieren. Was wir hier sehen, ist, dass wenn der Governor auf der linken Seite ist, die Tendenz für den linken Konjunkten zu schorteren Wörter steigern mit der absoluten Differenz in Wörtern und das Gleiche beobachtet man, wenn es keinen Governor gibt, wie in der Koordination von Sätzen, aber wenn der Governor auf der rechten Seite ist, verschwindet diese Tendenz. Und wir zeigen in der Papier, wie dies ein Argument gegen asymmetrische Koordinationsstrukturen wie diese beiden und für asymmetrische Strukturen wie diese beiden liefert. Also sehen Sie die Papier für die vollständige Einigung und Argumente und sprechen Sie mit uns über die Poster-Sitzung. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kyoyin und ich werde unser Werk mit dem Titel Wann benötigt die Übersetzung einen kontextuellen, auf Daten basierenden mehrsprachigen Erkundungsprozess vorstellen. Dieses Werk wurde in Zusammenarbeit mit Patrick Fernandes, Emily Liu, Andre FD Martins und Graham Newbig durchgeführt. Viele Übersetzungen hängen also vom Kontext ab. Zum Beispiel, wie würden wir Mol in diesem Satz übersetzen? Nun, wenn der vorherige Satz war, dass die Dinge gefährlich werden könnten, wenn die Minister es herausfinden, dann bezieht sich Mol auf einen Spion. Aber wenn der vorherige Satz war, könnte es etwas Ernstes sein, Doktor? Dann bezieht sich Mol auf ein Geburtsmärke. Je nach Kontext ändert sich also auch die Bedeutung des Wortes und damit auch seine Übersetzung. Es ist jedoch ziemlich schwierig zu bewerten, wie gut Modelle solche Fälle übersetzen können. Erstens hängt nur ein kleiner Teil der Übersetzungen vom Kontext ab, was es für korpussebene Metriken wie Blue unfähig macht, diese Übersetzungen zu erfassen. Einige Leute haben eine gezielte Bewertung kontextsabhängiger Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextsabhängigen Übersetzungen und begrenzte Sprachmengen, da sie in der Regel auf Bereichskenntnisse und menschliche Kuratierung angewiesen sind. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens: Wann benötigt die Übersetzung Kontext? Und zweitens: Wie gut bewältigen Modelle diese Fälle? Um die erste Frage zu beantworten, haben wir damit begonnen, zu messen, wie sehr ein Wort von Kontext während der Übersetzung abhängt. In der vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung von maschinellen Übersetzungsmodellen eingeführt. Dies geschieht, indem man mätet, wie viel Information der Kontext C über das Ziel Y liefert, wenn die Quelle X angegeben ist. Sie können sich CXMI als die Informationen vorstellen, die durch die Bereitstellung von Kontext für das Modell erhalten werden. In dieser Arbeit erweitern wir CXMI auf punktenbasierte CXMI, die die Kontextnutzung auf Satz- oder Wortniveau messen kann. Wir können uns Wörter mit hohem PCXMI als solche vorstellen, die für die Übersetzung einen Kontext erfordern. Jetzt analysieren wir Wörter mit hohem PCXMI, um nach Mustern zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse an Transkripten von TED-Vorträgen durch, die von Englisch auf vierzehn verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zunächst betrachten wir Teile von Sprachtexten mit hohem PCXMI. Dadurch können wir beispielsweise duale Pronomen im Arabischen finden, die relativ hohe PSXMI haben. Dies kann erklärt werden, da Englisch keine dualen Pronomen hat, daher benötigen Sie einen Kontext, um zu bestimmen, ob ein Pronomen bei der Übersetzung ins Arabische dual ist. Ähnlich stellen wir fest, dass bestimmte Sprachen auch einen Kontext benötigen, wenn wir die geeignete Verbform auswählen möchten. Dann betrachten wir Vokabeln, die hohe PSXMI gemessen haben, und das hilft uns, Fälle wie den hier zu identifizieren, bei denen Sie im Chinesischen Kontext benötigen, um richtig zu übersetzen. Und wir finden, dass der Kontext wichtig ist, um die richtige Form zu übersetzen. Und schließlich betrachten wir verschiedene einzelne Token mit hohem P6MI. Und das ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie z. B. die Elipsis-Resolution. Jetzt verwenden wir unsere Ergebnisse aus unserer Analyse, um einen Benchmark für die Dokumenten-Niveau-Übersetzung zu entwerfen. Für jedes der fünf identifizierten Diskursphänomene erstellen wir Tagger, um automatisch Wörter zu identifizieren, die sich auf das Phänomen beziehen, und nennen unseren Tagger den Multilingual Diskurse Aware oder MUDA Tagger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene haben. Dann verwenden wir den MUDA Tagger, indem wir den Tagger auf den parallelen Korpus anwenden, den wir zur Bewertung verwenden möchten, und wenden unsere gewählten Übersetzungsmetriken auf die kontextuell abhängigen Beispiele an, die der MUDA Tagger identifiziert hat. Schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle für maschinelle Übersetzungen auf Dokumentniveau zu bewerten. Zunächst einmal, wenn wir Korpus-Niveau-Metriken verwenden, finden wir bei Blue, dass kontextuell agnostische Modelle die beste Leistung haben, aber wenn wir dann Comet verwenden, leisten kontextuell bewusste Modelle die besten. Und wenn wir WordFMeasure verwenden, haben Modelle mit oder ohne Kontext eine vergleichbare Leistung. Dies zeigt wieder, dass es schwierig ist, das beste Dokumentniveau-Übersetzungssystem zu bestimmen, wenn wir Korpus-Niveau-Metriken verwenden. nun den MUDA-Benchmark, um Modelle zu bewerten, und stellen fest, dass kontextuelle Modelle deutlich genauer sind als Modelle, die keinen Kontext für bestimmte Diskursphänomene wie Formalität und lexikale Kohäsion verwenden. Diese Modelle sind jedoch nicht viel besser als Modelle, die keinen Kontext für andere Phänomene wie Elipses, Pronomen und Verbformen verwenden. Dies deutet darauf hin, dass wir bei der Dokumenten-Niveau-Übersetzung mehr Fortschritte erzielen müssten. Wir verglichen auch verschiedene kommerzielle Systeme, und unser Benchmark zeigt, dass DPL normalerweise genauer ist. Google Translate ist in der Regel genauer als Google Translate für die Übersetzung auf Dokumentniveau. Zusammenfassend lässt sich sagen, dass wir eine datengesteuerte Analyse über vierzehn Sprachpaare durchführen, um zu identifizieren, wann Übersetzungen Kontext erfordern. Dann nutzen wir unsere Ergebnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentniveau zu erstellen, der uns dabei helfen kann, zu identifizieren, welche Discourse-Phänomene-Modelle gut oder schlecht bewältigen können und welche Übersetzungssysteme gut bei der Übersetzung auf Dokumentniveau sind. Vielen Dank für Ihre Aufmerksamkeit. Bis morgen."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich euer Werk Charakterisieren von Designvorurteilen bei Beta-Sets und -Modellen vorstellen. Dieses Werk wurde in Zusammenarbeit mit einigen Leuten der University of Washington und des Allen Institute for AI durchgeführt, nämlich Sebastian Santi, Ronin Lebras, Katarina Reinicke und Martin Sapp. Beginnen wir also damit, uns vorzustellen, dass ihr für eine Zeitung arbeitet und Kommentare unter eurem Nachrichtenartikel durchsucht, um giftigen Inhalt zu entfernen. Ihr könnt euch einer beliebten API wie Perspective API für die Toxizitätserkennung wenden, und das funktioniert wirklich gut, wenn ihr Carl Jones se sind, können Perspective APIs giftige Fälle korrekt erkennen. Aber das ist nicht wirklich der Fall für Dithya Sharma, wo Perspective APIs nicht so empfindlich auf beleidigende Begriffe reagieren, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Designvorurteil, bei dem wir systematische Leistungsunterschiede der Technologie zwischen den Bevölkerungsgruppen sehen. Designvorurteile wie die, die wir gerade gesehen haben, könnten aufgrund der Positionalität der NLP-Forscher und Modellentwickler auftreten. Positionalität sind einfach die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Bereichen, weit verbreitet verwendet wird. Und als Forscher kann Positionalität den Forschungsprozess und seine Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, verändern kann. Und so ist eine Frage, die die Leute stellen könnten: Haben Datensätze und Modelle Positionalität? Und wir versuchen nicht zu sagen, dass Modelle selbst und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von echten Menschen und können daher bestimmte Positionalitäten über andere darstellen. Vorherige Arbeiten haben einige anekdotische Beweise für Positionalität vorgeschlagen, wie z. B. kulturelle Lücken in Modellen und Datensätzen sowie theoretische Definitionen der Modellpositionalität. Diese Arbeiten vergleichen jedoch nicht wirklich Endbenutzer mit den Datensätzen und Modellen selbst. Die Studie der Positionalität von Modellen und Datensätzen ist zunehmend wichtig, da NLP-Tests subjektiver und sozial ausgerichteter werden. Es ist schwierig zu charakterisieren, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind. Um die Datenmenge und die Modellpositionalität zu untersuchen, vergleichen wir die Annotationen mit echten Benutzern mit vorhandenen Datensätzen und Modellen. Wir tun dies über unser Framework NLPositionality. Unser Framework funktioniert in zwei Hauptstufen. Der erste Schritt besteht darin, Datensätze mit verschiedenen Annotatoren neu zu annotieren. Und wir sollten dies tun, indem wir die Demografie der Originaldatensätze anschauen, weil normalerweise nur wenige Annotatoren jede Instanz annoten und weil Demografien selten gesammelt und geteilt werden. Wir entscheiden uns daher dafür, Daten neu zu annotieren, um viele Annotatoren pro Instanz zu erhalten und eine reiche Menge an demografischen Daten zu erhalten. Dann nehmen wir die Annotationen nach demografischen Daten und vergleichen sie mit den Modellen und Datensätzen mithilfe einer Parsons R-Korrelations-Punktzahl. Und so unterscheidet sich unser Framework tatsächlich von der Literatur über Annotator-Diskrepanzen, indem es Endbenutzer mit Modellen und Datensätzen, Vorhersagen und Etiketten vergleicht, anstatt nur die Annotator-Vereinbarung oder die Modellierung von Annotator-Verteilungen zu betrachten. Unser Framework wird größtenteils durch Lab in the Wild aktiviert, eine Online-Crowdsourcing-Plattform für unsere HCI-Mitarbeiter. Und Lab in the Wild ist eine Online-Experimentplatform, auf der wir vielfältige Freiwillige rekrutieren können, im Vergleich zu Plattformen wie MTurk, die größtenteils Teilnehmer aus den USA oder Indien haben. Und Lab in the Wild kann immer noch hochwertige Daten erhalten. Wir veranstalten zwei Aufgaben in Lab in the Wild, eine davon ist die soziale Akzeptabilität. Und so funktioniert das: Die Teilnehmer lesen eine Situation aus dem Datensatz zur sozialen Chemie und schreiben dann, wie sozial akzeptabel eine Situation ist. Danach können sie ihre Antworten auf eine KI und andere vergleichen, um sich mit der Studie zu beschäftigen. Dann haben wir diese Anmerkungen mit der Social Chemistry Delphi in GPT 4 verglichen. Dann haben wir eine sehr ähnliche Einrichtung für die Toxizität- und Hate-Sprache-Erkennungskommission repliziert, bei der sie eine Instanz von DynaHate lesen und bewerten, ob sie denken, dass es sich um eine Instanz von Hate-Sprache handelt. Dann haben wir diese Anmerkungen mit DynaHate, Perspective API, Rewire API, Hate Roberta in GPT 4 verglichen. Unsere Studie hat letztendlich über sechzehntausend Anmerkungen von über tausend Annotatoren aus achtundsiebzig Ländern gesammelt. Jetzt sind wir also besser ausgerüstet, um zu beantworten, mit wem sich die NLP-Datensätze und -Modelle am meisten übereinstimmen. Wir stellen fest, dass es Positionalität in der NLP gibt. Zum Beispiel stellen wir fest, dass Datensätze und Modelle am besten auf englischsprachige Länder abgestimmt sind. Bei der GPD-4-Analyse der sozialen Akzeptabilität stellen wir fest, dass sie am besten auf die Konfuzianer und englischsprachige Länder abgestimmt ist. Wir stellen fest, dass Danahate ebenfalls am besten auf englischsprachige Länder abgestimmt ist. Wir stellen auch fest, dass es am besten auf Menschen abgestimmt ist, die eine Hochschulabschluss haben. Bei der GPD-4-Analyse der sozialen Akzeptabilität stellen wir fest, dass sie am besten auf Menschen mit einer Hochschulabschluss oder einer Graduiertenschulabschluss abgestimmt ist. Und wir stellen das Gleiche für Danahate fest, wo sie am besten auf die Menschen mit einer Hochschulabschluss abgestimmt ist. Wenn Modelle und Datensätze jedoch auf bestimmte Bevölkerungsgruppen abgestimmt sind, bleiben einige unweigerlich zurück. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger auf nicht-binäre Menschen abgestimmt sind als auf ihre Männer- und Frauen-Gegenstücke. Wir finden dies auch in der GPT-Vier-Tag für soziale Akzeptabilität sowie in der DynaHate-Tag-Analyse. Da es also Positionalität in der NLP gibt, was können wir dagegen tun? Wir haben daher einige Empfehlungen dafür. Erstens sollten Sie alle relevanten Designentscheidungen aufzeichnen. Durch den gesamten Forschungsprozess werden relevante Designentscheidungen getroffen. Und die andere ist, NLP-Forschung durch die Linse des Perspektivismus durchzuführen. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von vier bestimmten Gemeinschaften zu erstellen. Ein gutes Beispiel dafür ist die Masakane-Initiative. Wir möchten betonen, dass inklusive NLP nicht nur bedeutet, dass alle Technologien für alle funktionieren. Und damit ist unsere Präsentation abgeschlossen. Wenn Sie mehr erfahren möchten, können Sie sich unser Dashboard für die neuesten Analysergebnisse und unsere Arbeit ansehen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hi, ich werde über unsere Arbeit auf der Lösung indirektes Referierungsformen für Entity-Selection sprechen, in der wir die Alt-Entity-Scoreboard-Art introduzieren. Mein Name ist Javot Hosseini, und dies ist eine gemeinsame Arbeit mit Philipp Radlinsky, Silvia Pareti und Annie Luis. Unser Ziel ist es, die Lage der Nutzer zu verstehen, wenn sie eine Wahl treffen wollen. Betrachten Sie diese alternative Frage. Did Sie meinen, es ist mir leicht oder ich habe ein Gefühl? Hier möchte ein Nutzer zwischen einer dieser beiden Sorgen wählen. Die oberste Sache ist, eine direkte Referenz, zum Beispiel indem man den Namen des Songs Easy on Me oder seine Position, den ersten, sagt. Aber manchmal ist eine indirekte Referenz mehr geeignet, um eine natürlichere Konversation zu haben. Dies könnte passieren, wenn der Name des Songs nicht mehr zu erinnern ist oder die Pronunciationen zu ähnlich zueinander sind und schwer zu disambiguieren. oder wenn der Name eine Präferenz spezifizieren möchte. Hier sind einige Beispiele für indirekte Referenzen. Zum Beispiel die neuere oder die Song, die nicht an etwas, das nicht energiegeladen ist. Dies ist ein wichtiger Problem in conversationalen Systemen und auch für Benchmarking LLMs Entity-Erkenntnisse. Wir sind uns nicht bewusst, dass es eine große Datenbank für die Aufgabe gibt, also haben wir eine Datenbank mit Crowd-Annotation. Unsere Datenbank umfasst drei verschiedene Domains: Musik, Books und Rezepte. Unsere Datenbank-Sammlungmethodologie betont Informalität mit einer Cartoon-Completion-Setup. Die Cartoon hat drei Speechbubels. In der ersten Bubble, Bob sagt, Remember that song we were listening to yesterday? Und mit that, Bob setzt den Dialogkontext. In der zweiten Speechbubble, Alice sagt, do you mean easy on me or I got a feeling? Das ist die alternative Frage. Und in der dritten Speechbubble, Bob verwendet eine indirekte Referenz, um eine dieser Entitäten zu wählen, zum Beispiel den Neuerfond. Wir bereitstellen die ersten und zweiten Speechbubbles automatisch, aber die dritte wird von der Annotator gefüllt. Die erste Speechbubble wird aus einigen manuellen Prompts per Domain gewählt. Die zweite, die alternative Frage, wird als folgt generiert. Wir verwenden immer eine einfache Template. Do Sie A oder B? wo A und B von Wikipedia sampled. Hier sind die verschiedenen Samplingmethoden, die wir verwendet haben. Wenn wir höher in der Liste, die Entitäten werden mehr ähnlich zueinander und es ist normalerweise schwieriger, die Dissambiguation zu machen. Die erste ist einheitlich und random. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen The Return. Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und schließlich, wenn sie ähnliche Infoboxen oder Attributen auf Wikipedia haben. Zum Beispiel, der gleiche Genre oder der gleiche Künstler für ein Song. Wenn wir diese alternative Frage für die Annotatoren. Sie kennen die Namen dieser Entitäten, aber sie wissen nicht unbedingt, was die Entitäten sind. Also, was wir tun, ist, dass wir einige Hintergrundkenntnisse über die beiden Entitäten zeigen. Für Songs, wir zeigen einfach einen Google-Search-Link zu jedem Song und dann bitten die Annotatoren, zu listen zu mindestens einigen von jedem Song und zu lesen über jeden Song. Hier ist zum Beispiel das Google-Search-Result für den Song Easy on Me. Für die Rezepte und Books-Domain zeigen wir einige Background-Text von Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder wieder von Wikipedia so, dass die Annotators wissen, wie sie aussehen. Dann bitten wir die Annotators, eine dieser Entitäten zu picken, zum Beispiel hier die erste, und sie zu beschreiben, indem sie drei bis fünf indirekte Referring-Ausdrücke verwenden. Zum Beispiel die mit der Piano-Musik. Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel die mit die eine ohne Wörter, nicht die eine mit dem 12-jährigen Boy oder der fiktionalen oder kommt aus Aserbaidschan und so weiter. Der Altentities-Corpus hat 6.000 alternative Fragen über drei Domains und 42.000 indirekte Referring-Ausdrücke. Ergebnisse mit T5xLarge-Modell sind unten summarisiert. Wenn das Languagemodell den genauen Backgrundkenntnissen wie die Annotators hat, dann ist die Accuracy wirklich hoch. Es ist um 92 bis 95 Prozent. Aber das ist nicht realistisch. Wenn das Languagemodell Zugang zu einem teilweise überlappenden Hintergrundkenntnis hat, dann liegt die Accuracy zwischen 82 bis 87 Prozent, was realistischer ist. Zum Beispiel, wenn das Languagemodell die Hintergrundkenntnisse retrievert. Wenn das Languagemodell nur zu Entity-Namen hat, dann liegt die Accuracy nur 60 Prozent. Also gibt es noch viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle Domain-Generalisierbar sind. Hier ist ein Link zu unserem Datensatz."}
