{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lendermann e oggi vi darò una breve introduzione alla nostra articolo sulla generalizzazione compositiva senza alberi utilizzando tagging multi-set e permutazioni latenti. Questo è un lavoro con i miei consulenti Alexander Koller e Ivan Titoff. La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire la ricorsione più profonda e le composizioni invisibili di frasi che sono state viste individualmente durante l'addestramento. In contesto della parsing semantico, testare la generalizzazione comp di questa composizione potrebbe apparire così. Come al solito, abbiamo un insieme di affermazioni di addestramento, in questo caso, la ragazza ha dormito e Mary ha saputo che la ragazza aveva dormito. Queste affermazioni sono abbinate a forme logiche che rappresentano aspetti fondamentali del loro significato. In contrasto con la valutazione standard di macchine learning, l'insieme di addestramento non deriva dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una recursione superficiale durante l'addestramento e viene testato su un esempio con una recursione più profonda. I modelli di sequenza a sequenza ingenui hanno difficoltà con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono staccati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono codificate con i colori nell'esempio. Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi sono intesi a catturare il processo compositorio che relaziona le espressioni con le forme logiche. Questo funziona bene, ma gli alberi di solito non vengono forniti e devono essere ottenuti in qualche modo.. Questo può essere complicato e a volte un processo computationalmente costoso. Tipicamente, questo implica una considerabile pre-processing specifico per formalizzare le forme logiche, ad esempio per gestire i simboli variabili. Ottenere trei può anche implicare procedure di induzione grammatica specializzate. In questo articolo, non usiamo trei e introduciamo un modello di sequenza-sequenza neurale che modella direttamente le corrispondenze tra frammenti di input e frammenti di output. Per la prima volta, mostriamo una forte generalizzazione a Diva Recursion senza rilegarsi su alberi. Il nostro approccio predice l'output dall'input in due fasi. Prima, tagliamo ogni token di input con un multi-set di token che appariranno nell'output. Dopo il primo passo, abbiamo tutti i giusti token, ma non sono ordinati. Questo è il motivo per cui, nel secondo passo, utilizziamo un altro modello per predicare una permutazione, per metterla in giusta ordine. Introduciamo un nuovo metodo per predicare una permutazione che non impone costrizioni rigide sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Conceptualmente, il nostro modello di permutazione funziona in modo approssimativamente simile. Si passa da sinistra a destra sull'output e si determina quale multimetro di token mettere in ogni posizione. Per la prima posizione di output, si seleziona semplicemente uno evidenziato in rosso. Poi si passa al successivo multimetro di token per determinare il secondo multimetro di output. Si determina il terzo multimetro di output in modo simile, passando a un altro multimetro di token. Si continua questo processo. fino a quando ogni token della prima fase non viene visitato esattamente una volta. Per darvi un'idea dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark di Koggs. Il nostro modello supera gli altri di gran lunga in generalizzazione a ricorsione più profonda. Alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi. Nel nostro articolo risolviamo alcune interessanti sfide tecniche. Prima di tutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un determinato token non sappiamo da quale multisetter proviene, il che rappresenta una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma la corretta linguisticamente è latente. Abbiamo affrontato questo indossando l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida che trovare la permutazione con il punteggio più alto è npH. La permutazione è difficile. Questo perché è legato al problema del venditore in viaggio. La approssimiamo con una rilassamento continua e amichevole per la GPU che ci consente anche di riprodurre la soluzione e di imparare le permutazioni più plausibili dal punto di vista linguistico. Se desiderate saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, consultate il nostro articolo o visitate il nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò del nostro articolo intitolato Using Natural Language Prompts to Measure Stereotypes in Language Models. Questo lavoro è stato svolto in collaborazione con Essendermouch e Dan Jurowski. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi in grandi modelli linguistici, o LLM. Tuttavia, queste misure hanno vari limiti. Di solito si basano su dataset costruiti a mano che richiedono molto tempo per essere curati. Inoltre, di solito misurano solo stereotipi molto specifici, il che significa che non generalizzano bene ad altre demografie o contesti, o semplicemente catturano aspetti molto generali e ampi. Inoltre, la maggior parte del lavoro in questo settore non tiene conto dell'intersezionalità, ovvero l'idea che le identità sociali multifacetate possano aggravare i pregiudizi e essere unico, a basso costo, di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLM, avvizzati da istruzioni, sono molto bravi a rispondere alle istruzioni e ai suggerimenti. Quindi, possiamo chiedere al modello di generare una persona, che rappresenta una rappresentazione di un individuo immaginato utilizzando un suggerimento come, immagina di essere una donna asiatica, descrivi te stessa. Questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt. Quindi ecco alcuni esempi di generazioni da GPT quattro. Immediatamente, vediamo che, sebbene i risultati non siano evidentemente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni modelli interessanti. La donna asiatica è rappresentata come discreta. La donna del Medio Oriente viene riferita a usare parole come esotico e a una regione affascinante. E entrambe le persone di colore fanno riferimento all'ascendenza, mentre la persona dell'uomo bianco non ha nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima è la generazione di queste personalità. I nostri suggerimenti per generare queste personalità sono stati ispirati da uno studio in cui hanno dato questi suggerimenti a soggetti umani, scoprendo che dando loro agli soggetti umani, sono stati in grado di far emergere anche gli stereotipi razziali. Inoltre, questo consente di fare un confronto diretto tra le nostre personalità generate e le risposte scritte umane. La seconda parte sono le parole contrassegnate, che sono un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, su cui parlerò più avanti. Il vantaggio di questo è che otteniamo stereotipi e modelli molto specifici senza dover fare affidamento su un lexicone specifico. Quindi, il metodo delle parole contrassegnate si basa sul concetto sociolinguistico di contrassegnamento, che afferma che esiste un default non contrassegnato e che qualsiasi gruppo che differisce da tale default è contrassegnato linguisticamente. Quindi, ad esempio, la parola uomo o, scusate, la parola guerriero è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano un guerriero uomo e contrassegnano il termine con donna. E, in termini più ampi, i gruppi dominanti nella società sono contrassegnati sia linguisticamente che socialmente, mentre i gruppi emarginati sono solitamente contrassegnati. Quindi, nel nostro metodo, prima di tutto, indiciamo quali sono i gruppi non segnati e quelli segnati. Poi confrontiamo le persone utilizzando il metodo delle parole combattenti, che consiste fondamentalmente nell'utilizzare rapporti logaritmici ponderati per distinguere le parole principali per ogni gruppo segnato. Quindi, ad esempio, per le persone di donne nere, utilizzerebbe parole combattenti e confronterebbe i rapporti logaritmici sia con le persone bianche che con quelle maschili, perché sono i due gruppi non segnati corrispondenti. Ora, per alcuni risultati. Quindi, per prima cosa, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengonoono molti più stereotipi rispetto a quelle scritte da uomini. Tuttavia, quando esaminiamo la distribuzione delle parole nel lessico, troviamo cose molto diverse. Quindi, mentre le personalità generate hanno tassi molto più elevati delle parole del lessico, quelle scritte da uomini hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate che si trovano nelle personalità generate sono in realtà solo le parole alto e atletico. Quindi, in realtà, solo quelle positive o almeno non negative. E, in effetti, questo lessico non cattura affatto molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, per farlo, ci rivolgeremo ai risultati del nostro metodo delle parole contrassegnate per mostrare come possi queste parole apparentemente positive facilitino gli stereotipi e l'essenzia delle narrazioni. Nella nostra analisi, riveleremo come queste rappresentazioni apparentemente positive riflettano i modelli dannosi. In primo luogo, per i gruppi di Marc, le parole principali includono cose come cultura, tradizione, orgoglio e esotico. E queste parole definiscono questi gruppi solo per la loro relazione con la loro identità e li distinguono come diversi dalla norma bianca. Questo contribuisce a un lungo retaggio di discriminazione e altruismo per questi gruppi. Inoltre, ci sono molti cliché comuni che si riflettono in queste parole, soprattutto per le donne di colore. Quindi, ad esempio, le parole che descrivono le donne latine includono cose come vivace. Come vibrante e curvace, che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come piccola, delicata e setosa, che si collegano a una lunga storia di iper sessualizzazione delle donne asiatiche, viste come molto docili e sottomesse e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resiliente. Questo si collega a un archetipo che le persone hanno chiamato l'archetipo delle donne nere forti. E mentre a prima vista sembra positivo, ci sono stati studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché esercita molta pressione sulle donne. Queste demografie devono essere resilienti e forti contro gli ostacoli sociali. Quindi, anziché lavorare effettivamente per cambiare tali ostacoli, si esercita pressione su quelle persone per superarli, il che porta a risultati molto negativi per la salute di queste persone, tra gli altri danni. In termini più ampi, riteniamo che le parole per ogni gruppo segnato riflettano praticamente solo narrazioni essenzializzanti. Quindi, in base a questi modelli, concludiamo con tre raccomandazioni per i proprietari di modelli. In primo luogo, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare la lente intersezionale per studiare i pregiudizi e i danni, perché sono ci sono molte cose che potrebbero essere trascurate se non lo facciamo. Infine, dovrebbe esserci davvero una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, ad esempio, questi stereotipi positivi, non sappiamo se sia dovuto a una sorta di allineamento di valori eccessivo o forse a altri metodi antistereotipici che stanno portando a questi schemi dannosi. Semplicemente non possiamo fare ipotesi o studiare ulteriormente senza una maggiore trasparenza. Grazie mille per aver ascoltato. Buona giornata all'ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E sono Sarah Finch. Oggi vi parleremo di ABCEval, un nuovo approccio a dimensioni per valutare l'intelligenza artificiale conversazionale. Questo lavoro è stato svolto dal laboratorio di NLP di Emory, guidato dal professor Gino Choi dell'Università di Emory, in collaborazione con Amazon Alexa AI. Quindi, diciamo che avete appena sviluppato un modello di dialogo e volete vedere quanto bene si confronta con lo stato attuale dell'arte. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni è migliore o di valutare le conversazioni in base alla scala di liquidità. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità del dialogo complessivo, ma la qualità del dialogo ha molti aspetti. Pertanto, potrebbe essere utile valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più preciso. Un approccio consiste semplicemente nel chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi di comparazione o di scala Lickert esistenti. Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale. Annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddicersi da soli. Chiamiamo questo approccio annotazione dei comportamenti nella chat, o ABCEval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente. ABCEval è in grado di misurare le percentuali con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABCEval misura il numero di giri in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, contraddice se stesso o il suo partner. Quando il modello si critica da solo o dal suo partner, ha allucinazioni su fatti errati o viola la conoscenza del buonsenso e quando il modello ha successo o non dimostra empatia. Per determinare che tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni umane-bot per modello utilizzando ABCEval. Per confronto, abbiamo valutato anche queste conversazioni utilizzando tre metodi esistenti: valutazioni di Liquid sul livello di turno, valutazioni di Liquid sul livello di dialogo e confronti a coppia a livello di dialogo. Con i metodi di valutazione, abbiamo raccolto valutazioni su otto degli aspetti di dialogo più comunemente misurati, poiché questa è la pratica standard per valutare i modelli di chat in molteplici dimensioni. Dalle nostre analisi dei risultati di valutazione, abbiamo scoperto che le etichette comportamentali ABCEval sono nel complesso più affidabili rispetto alle etichette raccolte con i metodi esistenti, come misurato dall'accordo tra annotatori su cento conversazioni doppiamente etichettate. Inoltre, le etichette ABCEval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare. Ad esempio, si può vedere come la misurazione della proporzione di turni con contraddizioni tra sé e il partner spiega rispettivamente il cinque percento e il dieci percento della qualità della conversazione, mentre i punteggi medi di coerenza delle bevande alcoliche spiegano solo il quattro percento o meno. Infine, abbiamo controllato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare graduale. Si può vedere come la combinazione di tutte le metriche di valutazione ABC spiega oltre il venticinque percento della qualità della conversazione e, se si rimuovono le metriche una alla volta, la maggior parte di esse si risulta nella perdita di una quantità decente di informazioni sulla qualità. La combinazione di metriche di all'alternativa Lickert spiega una qualità molto inferiore e meno di queste metriche contiene informazioni uniche. Queste reliabili, informative, e distinte metriche ABC Eval ci permettono di valutare l'AI conversazionale con una risoluzione più alta rispetto a quella che i metodi precedenti sono in grado di raggiungere. Si può vedere nei risultati del nostro esperimento che diverse sfide rimangono e sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato presentano violazioni di common sense in circa il 20% delle risposte. Infatti, presentano informazioni irrilevanti in circa il 15% delle risposte, e contraddicono la loro capacità di valutazione. L'ABC eVal possono valutare se stessi o il proprio partner circa il dieci percento del tempo. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero vedere una diminuzione dei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è un motivo ancora maggiore per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli. Speriamo che ABC eVal possa essere sfruttato da altri nel campo come un passo significativo in questa direzione e non vediamo l'ora di vedere come l'IA conversazionale avanzerà nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono una candidata al dottorato in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato in ACL 2023 come un lungo articolo intitolato Transfer Learning for Dissonance Detection, affrontando la sfida della classe rara. Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole povere, la dissonanza cognitiva è due convinzioni o azioni inconsistenti, come in questo esempio in cui una persona afferma: So che i sigaretti potrebbero uccidermi, e poi continua dicendo: Ho preso un paio di sigarette. Dopo l'incontro, ho preso un paio di fumo. Questa convinzione e questa azione sono inconsistenti e sono in dissonanza. Inoltre, menzionare che non penso di poter mantenere il mio lavoro senza di loro giustifica il secondo evento e hanno una relazione di consonanza. Anche se la dissonanza è un fenomeno molto comune che si verifica nella decisione quotidiana, è davvero raro trovarla espressa nel linguaggio, tra gli altri tipi di relazioni di discorso. Quindi, perché è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti delle disaccordi tra le persone, a seguire le tendenze e i cambiamenti di credo, valori e atteggiamento nella popolazione. La forte dissonanza cognitiva è anche relativa ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa in linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere i personali stili cognitivi degli individui e aiuta a comprendere meglio i processi decisionali. Per creare una risorsa di dissonanza cognitiva, abbiamo condotto una grande annotazione di relazioni di dissonanza. Abbiamo utilizzato un approccio di dissonanza come visto nel flowchart qui. I tweet sono stati passati utilizzando un parser PDTB e le coppie di unità di discorso sono state annotate in base alle linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo in 3.5% delle coppie annotate. Raccogliendo circa 1.000 esempi di coppie di unità di discorso, abbiamo condotto un addestramento per un classificatore iniziale, trainato solo su 43 esempi di dissonanza. Non a caso, il classificatore non ha funzionato molto meglio del caso. Dato la bassa frequenza di dissonanza e l'assenza di un dataset precedente, ci troviamo di fronte al problema di assoluta rarità. Per alleviare questo problema,, sperimentiamo combinazioni di apprendimento trasversale e apprendimento attivo per annotare in modo da poter raccogliere più campioni di dissonanza in giri di annotazione minori, riducendo i costi complessivi di annotazione migliorando al contempo la rilevazione della dissonanza. Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo pesi da compiti strettamente correlati. Si trasferisce da due compiti diversi. La classificazione delle posizioni di dissonanza indipendente dal tema, un compito che determina se due dichiarazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema. di argomento chiamato debate qui e sulla classificazione binaria delle classi di espansione e comparazione di PDTB, poiché queste due sono strettamente correlate alla concezione di consonanti e dissonanza e le chiamiamo CE qui. Abbiamo scoperto che, trasferendo il risultato di prestazioni a zero breve sul dataset annotato, è già molto migliore del caso con il miglior risultato di AUC 0.62. Inoltre, per affinare iterativamente entrambi i task, abbiamo scoperto che l'affinamento dei task CE seguito da ulteriori affinamenti sul debate produce una prestazioni a zero breve molto migliore. Quindi, questo è il modello che abbiamo utilizzato per avviare l'apprendimento attivo. Successivamente, abbiamo determinato la migliore metodo per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. Il modello accumulato accumulato da tutti i dati raccolti da apprendimento attivo finora, mentre l'iteratore aggiorna il modello con l'ultimo set di dati raccolti. Durante le diverse strategie, abbiamo scoperto che il modello accumulato ha funzionato altrettanto bene o meglio dell'iteratore in tutti i casi. Successivamente, per migliorare il numero di esempi di dissonanza, abbiamo utilizzato una strategia di probabilità di classe rare, PRC, per selezionare i più esempi che sono molto probabili di dissonanza da parte del modello at ogni round di AL. Abbiamo comparato questo con gli altri le altre strategie AL di ultima generazione comuni utilizzate nella comunità. Abbiamo scoperto che la strategia PRC proposta funziona meglio rispetto ad altre strategie di ultima generazione, anche se la differenza è minima. Si noti che le prestazioni sono significativamente inferiori per Random. In ulteriori round di AL con due strategie migliori, abbiamo migliorato la classificazione di distanza AUC a 0,75, che è la migliore prestazione che abbiamo finora sul compito. Abbiamo anche controllato la fattibilità di ciascuna strategia per la qualità e i costi delle annotazioni per gli annotatori. Abbiamo scoperto che PRC ha la percentuale più alta di distanza. La nostra analisi è un'ottima percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, riteniamo che PRC sia una semplice strategia AL per l'acquisizione di classi rare e che avviare AL con compiti di apprendimento di trasferimento appropriatamente progettati può essere utile. Riteniamo anche che l'aggiornamento iterativo è utile per l'apprendimento di trasferimento da un dominio diverso, mentre le annotazioni attive in dominio beneficiano dell'aggiornamento cumulativo. Questi sono i link al nostro dataset di codice e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Makshita e oggi io e il mio coautore Martin presentiamo il nostro lavoro The Kitmas Death, valutando l'integrazione della conoscenza da molteplici fonti. Questo lavoro è una collaborazione tra McGill University, Mila e Microsoft Research. Le modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, generalmente acquisita attraverso pre formazione e la conoscenza data in input al momento di inferenza. Recenti lavori su compiti come la risposta alle domande. Le risposte alla domanda mostrano che i modelli possono utilizzare la conoscenza del tempo pre-trainato per risolvere il problema. Ma la comprensione del linguaggio naturale richiede spesso conoscenze fornite anche durante l'inferenza. Ad esempio, nella frase John ha visto il presidente di recente in TV, i parametri pre-trainati possono contenere informazioni su cosa fanno i presidenti e su cosa sia una TV, ma non possono confermare con sicurezza chi è l'entità specifica di questo caso o chi è il nuovo presidente perché il presidente potrebbe essere cambiato da quando è stato trainato. Pertanto, i modelli di successo per le attività NLU intensive di conoscenza richiedono la capacità di integrare e utilizzare sia le conoscenze di tempo pre-addestrato che quelle di tempo di inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione delle conoscenze. Introduciamo un compito di risoluzione di coreferenza progettato per verificare la capacità di attingere alle conoscenze disponibili in diverse fonti. Valutiamo il dataset con partecipanti allo studio umano e stabiliamo modelli di risoluzione di coreferenza. Ecco un esempio dal nostro dataset. Servin è un giudice., Kia è una panettiera. Termin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro, decidendo casi in un tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronoun, che in questo caso è Sermin. La risoluzione di un pronounato richiede due tipi di informazioni. Prima, conoscenze specifiche dell'entità, come Sermin è un giudice. E secondo, conoscenze di base, come i giudici decidono casi in tribunali. In generale, le conoscenze di base vengono acquisite durante la pre-addestramento dei modelli di linguaggio di grandi dimensioni, mentre le conoscenze specifiche dell'entità vengono tipicamente osservate durante il periodo di inferenza. Variamo la disponibilità di queste due informazioni in modo che possano essere presenti in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di Kitmos. In primo luogo, abbiamo la tipica impostazione, pre-addestramento di base, in cui si assume che le conoscenze di base siano disponibili durante il periodo di preaddestramento. In secondo luogo, c'è la impostazione Both, in cui le conoscenze di base sono disponibili sia durante il periodo di preaddestramento che durante l'addestramento. Ultimamente, il settore di inferenza, dove entrambi i tipi di conoscenza sono disponibili solo all'influenza. Questa ultima impostazione è particolarmente interessante, perché simula il caso in cui la conoscenza necessaria per risolvere un problema non è parte dei dati pre trainati dei modelli, ad esempio perché nuove occupazioni sono sviluppate da quando si è formati. Ecco un esempio di come controlliamo la disponibilità di fatti nelle due fonti. In il settore di inferenza, assume che la conoscenza di base politici cerca sequestri in governo è contenuta nei parametri pre trainati. In questo contesto, forniamo la conoscenza antispecifica Chichester è un politiciano. In questo contesto, forniamo non solo conoscenza antispecifica, ma anche conoscenza di base su politici in un contesto di inferenza. In questo contesto di inferenza, forniamo la definizione di occupazione miritura invece di politiciano perché miritura è improbabile di essere contenuta in parametri pre trainati. Evaluamo il data set sia con i nostri studianti e stabiliamo modelli di risoluzione. In questa figura, mostriamo i risultati dei modelli di miglior performance sui varianti. la variante più difficile del settore pre-train del background. Senza un training specifico su Kitmos, entrambi i modelli non funzionano bene. Quando trainati su Kitmos, tuttavia, sia C2F che Berthro Coref funzionano significativamente meglio della scelta a caso. Questo suggerisce che quando trainati su dati di risoluzione di coefficienti generali, i modelli imparano a sfruttare le cue superficiali, che non sono utili quando testati su Kitmos dove tali cue sono state rimosse. Altri esperimenti con conoscenze di fittizia indicano che anche i modelli che funzionano meglio non possono integrare in modo reliabile le conoscenze di background fornite solo all'inferenza. Per riassumere le principali lezioni del nostro articolo, molti modelli di risoluzione di coerenza sembrano non riuscire a ragionare su conoscenze provenienti da diverse fonti senza formazione specifica per il compito. Tuttavia, con formazione specifica per il compito, alcuni modelli riusciscono a integrare le conoscenze provenienti da più fonti. Tuttavia, anche i modelli che funzionano meglio sembrano avere difficoltà a integrare le conoscenze di base presentate solo al momento dell'inferenza. Se siete interessati a maggiori dettagli, visitate il nostro articolo e il dataset e il codice su GitHub. Grazie per aver ascoltato."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Pappy dell'Università di Toronto e della Fondazione Bruno Kessler, e presenterò brevemente la guida per la traduzione simultanea del testo che è un lavoro con Matteo Negri e Marco Turchi. Cos'è la traduzione simultanea del testo? La traduzione simultanea del testo o simulazione ST è il processo di traduzione del testo in un altro linguaggio in tempo reale, consentendo la comunicazione tra i lingue. E quali sono i problemi dei modelli simulatori attuali? Le architetture specifiche sono solitamente trainate, introducendo additionali moduli da ottimizzare, lunghi e complicati procedure di training, per esempio training coinvolgendo diversi obiettivi di ottimizzazione, e training e maintenendo diversi modelli per raggiungere diversi regimi di latenza, per esempio training un modello con un'avvicinata di una seconda latenza e un altro modello con due secondi di latenza e così via. Quindi qual è la nostra soluzione? Prima di utilizzare i modelli Offline SD senza ritening o adottando architettura specifica per CMLSD. Utilizzare solo un modello. per ogni regime di latenza e gesti la latenza attraverso specifici parametri e utilizza la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output, il meccanismo di cross attenzione, come si può vedere in un esempio sulla destra. La nostra soluzione è quella di proporre un punto o di coderizzare l'attenzione, una strategia per la quale decidiamo se emettere o meno una parziale traduzione basata su dove la tensione si concentra. La tensione non è concentrata, ovvero la somma è sotto una certa threshold alpha verso i lambda speech frames, il che significa che l'informazione ricevuta è sufficiente a stabilizzare. Ad esempio, se riceviamo un speech shank containing I'm going to talk about e il nostro modello predice la traduzione in German e noi esaminiamo i cross attention weights, vediamo che le prime due parole indicano i lambda speech frames ricevuti mentre le ultime parole indicano i lambda speech frames ricevuti. Questo significa che le prime due parole saranno emesse, mentre, poiché la somma della cross attenzione è superiore a una certa frequenza alfa, non emetteremo l'ultima parola e aspetteremo un'altra parola. Se continuiamo e riceviamo un'altra parola e il nostro modello prevedono tre parole e osserviamo i valori di cross attenzione, vediamo che nessuna parola punta all'ultima parola. Questo significa che queste tre parole saranno emesse. Se guardiamo ai risultati principali di questo, tracciamo i risultati della traduzione simultanea su grafici in cui abbiamo il blu da un lato che misura la qualità della traduzione e il ritardo medio che è la misura della latenza. Consideriamo anche il ritardo medio del modello che tiene conto del tempo di calcolo del modello per prevedere l'output. Quindi vogliamo che anche che siano spostati a sinistra. E confrontiamo con le strategie preparate che sono applicate anche a modelli offline, come la strategia WITKE e l'accordo locale. E confrontiamo anche con l'architettura all'avanguardia specificamente adatta alla traduzione simultanea. Questi sono tutti i risultati della strategia di traduzione simultanea in tedesco. E vediamo che ADAT supera tutte le strategie applicate ai modelli offline, poiché sono più adatti a modelli offline. Le curve sono spostate verso sinistra e vediamo anche che se consideriamo il tempo effettivo trascorso o il tempo di computazione, questa è la strategia più rapida. Se desiderate scoprire ulteriori risultati, leggete il nostro articolo e abbiamo anche pubblicato open source, il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Xuheng. Oggi vi presenterò il nostro articolo, i tag di entità nominate del kernel 2003 funzionano ancora bene nel 2023? Iniziamo. Il nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità nominate o il compito NER. Abbiamo osservato che i modelli utilizzano il kernel 2003 per sviluppare NER da quasi vent'anni. E questo naturalmente solleva diversi problemi. In primo luogo, questi modelli possono generalizzarsi ai dati moderni? E quando sviluppiamo nuovi tag, cosa serve per una buona generalizzazione? Allo stesso tempo, se osserviamo una generalizzazione scarsa, cosa causa il calo delle prestazioni di questi modelli? Per indagare su questi problemi, abbiamo sviluppato il dataset Kernel plus plus. Si tratta di un dataset che abbiamo raccolto da Reuters News del 2020 e poi annotato con le stesse linee guida di annotazione Kernel 2003. Abbiamo valutato sia il set di test Kona O3 che il set di test Kona O plus. Ultimo ma non meno importante, abbiamo calcolato il cambiamento percentuale in F1 per valutare la generalizzazione di ogni modello. Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli di trasformazione generalizzano meglio i nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione. E, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di precisione influisce direttamente sulle prestazioni di un compito successivo. Qui abbiamo anche scoperto che più esempi di precisione portano a una migliore generalizzazione. Alla nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfittingfitting causata dal riutilizzo ripetuto dello stesso set di test più e più volte, e questo si manifesta solitamente quando la diminuzione torna su un nuovo set di test. La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra il treno e i dati di test. Per l'overfitting adattivo, abbiamo visto che, dal grafico a destra, la linea di miglior adattamento rossa ha un gradiente maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo apportato al colonna due su Kono 2003 si traduce in un miglioramento di più di unità su Kono 2003, il che significa che non ci sono ritorni diminuenti. E questo ci mostra che in questo caso non si osserva un'overfitting adattiva. Quindi, che dire della deriva temporale? Per la deriva temporale, abbiamo fatto un esperimento per rintrainare o continuare a rintrainare alcuni modelli con dati più recenti. Abbiamo scoperto che le prestazioni si degradano con un'ampia differenza temporale. Questo conferma la nostra ipotesi che la causa principale del crollo di performance è la deriva temporale. La nostra conclusione è che per una buona generalizzazione, abbiamo bisogno di una migliore architettura del modello, di una dimensione più ampia, di esempi di precisione, e questi vanno di pari passo. Non possiamo avere solo un ingrediente, ma scartare gli altri. Allo stesso tempo, abbiamo anche scoperto che il crollo di performance è causato dalla deriva temporale e, sorprendentemente, non è causato da sovrapposizione adattiva, anche se il modello 2003 è stato utilizzato per tutti oltre vent'anni. Quindi, tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, i tagger di Kono 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sì risoundante. Speriamo che il nostro articolo richieda ulteriori ricerche su come migliorare le generalizzazioni dei modelli. Infine, vi prego di consultare il nostro articolo, il nostro dataset, e se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Ciao! Benvenuti alla nostra presentazione di DeepLean, un nuovo corpus per la semplificazione del testo tedesco a livello di documento e di frase. Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo. La semplificazione del testo è il processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo di destinazione specifico, come le persone con problemi di lettura o i non madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie di testo parallele. È possibile utilizzare parole di testo parallele, ad esempio documenti o frasi. Nell'esempio qui, è possibile vedere una coppia di frasi allineate parallelamente di una complessa frase tedesca e la sua traduzione in lingua semplice. Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, come la sostituzione lexica, la deletazione delle clausole, la riordine della deletazione delle clausole o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus D plane, perché negli ultimi anni ci sono stati alcuni problemi con i corpori esistenti. Quindi, per esempio, questi corpora qui sono troppo piccoli per trainare un modello di taxonomia. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere errori nei loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus DPlane, che è diviso in due subcorpora, DPlane APA e DPlane Web. DPlane APA è basato su testi usati. In DPlane APA, allineiamo 483 documenti, tutti manualmente. Ciò dà circa trentamila, tredicimila coppie di frasi parallele. Per DPlaneWeb, questo corpus include diversi domini e allineiamo tutti questi settecentocinquanta documenti, da un lato manualmente e dall'altro con metodi di allineamento automatici. In totale, otteniamo trentamila, quattrocentocinquanta coppie di frasi. Analizziamo un po'di più le nostre coppie di frasi, ad esempio per il tipo di semplificazione. Come si può vedere qui, i testi della Bibbia sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingua, a tutti i livelli, ad esempio, in termini di semplificazione lexica, semplificazione strutturale o di livello generale di semplificazione. Inoltre, si può vedere che il nostro corpus di DPlane presenta una grande varietà di diverse trasformazioni di semplificazione. Quindi, ad esempio, nel corpus di DPlane API abbiamo molte più riordini e edizioni di parole rispetto al corpus web di DPlane. D'altra parte, nel corpus web abbiamo molte più semplific riformulazioni. Vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset DPlane. Quindi, per il primo caso d'uso, possiamo valutare i metodi di allineamento automatici. Negli ultimi anni ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi in post documenti. Ma nel nostro caso, stiamo cercando di estrarre allineamenti tra due documenti paralleli, che hanno la stessa lingua, la stessa content, ma sono a livelli di complessità diversi. Ora che abbiamo il nostro dataset D-plane, che ha sentenze allineate manualmente, possiamo utilizzare queste sentenze come allineamenti standard per valutare alcuni dei metodi di allineamento proposti. Abbiamo fatto alcune adattamenti ai metodi proposti e abbiamo pubblicato tutte queste adattamenti e i codici per condurre i nostri esperimenti in questo articolo. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo di mass align e puoi anche trovare il codice per condurre questo metodo sui tuoi documenti in questo articolo. Il secondo caso di utilizzo che abbiamo mostrato nel nostro articolo è il caso di automatico semplificazione del testo. Tunendo modelli di linguaggio per produrre un testo semplificato da un testo composto. Abbiamo finito due modelli diversi. Abbiamo finito il modello di impart lungo per produrre semplificazioni a livello di documento e abbiamo anche finito il modello di impart normale per produrre semplificazioni a livello di frase. Potete anche trovare tutti i punti di controllo e potete esaminare in maggiori dettagli i risultati e le metriche di valutazione della nostra esperienza. Abbiamo concluso che questo affinamento di base potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base e abbiamo proposto tali risultati come punto di riferimento, un punto di riferimento base per il problema della semplificazione automatica del testo in futuro. Grazie mille per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xi Yuan di Fenai University. Sono qui per introdurre il nostro lavoro distinguendo script knowledge from large language models for constraint language planning. In everyday life, gli esseri umani spesso pianificano le loro azioni seguendo step by step instructions in the form of guaranteed scripts. Previous work has esplorato language models to plan for abstract goals of stereotipical activities, such as make a cake, e mostra che large language models I modelli possono efficacemente decomporre gli obiettivi in steps. Tuttavia, il lavoro previouso si è concentrato sul pianificare gli obiettivi astratti di attività stereotipiche. Pianificare gli obiettivi con specifici obiettivi, specifici costrizioni, come fare una cioccolata, ancora rimane undisturbato. In questo paper, definiamo il problema di costrizione di pianificazione, che impone diverse costrizioni sugli obiettivi di pianificazione. Un obiettivo può essere inheritato da diversi obiettivi di vita specifica con moltiplicazione. Un buon pianista dovrebbe scrivere script che siano ragionevoli e fedeli alle limitazioni. In questo documento, prima di tutto valutiamo e miglioriamo la capacità di pianificare la linguistica di grandi modelli. Senza un data set di specifici goals esiste per supportare il nostro studio, dobbiamo acquisire questi goals prima. Come mostrato in testo, estendiamo gli abstract goals con molte limitazioni per la capacità di acquisizione di dati utilizzando il GPT. Sempliamo centodododici goals e valutiamo gli script generati da modelli di linea. Questo table riporta l'accuratezza dei risultati. Riferiamo che tutti i modelli di linea ottengono risultati insatisfacenti sui risultati di specificità. Poi conduciamo un'analisi dettagliata per investigare i modelli di linea. I risultati in questa figura mostrano che la completenza semantica generata degli script è accettabile. Ma la faithfulness to the constraints cannot be guaranteed. Dagliamo in più freneggiate categorie di constraints definite in Wikihome. La head map in the figure shows that the planning performance of instruct GPD varia considerably for girls of different categories. Previous studies have shown that the output cost of Larry models falls in high variance, leading to bad performance. idea di overgenerated Zen filter per migliorare la qualità di generazione. Prima di tutto, mostriamo i tipi di costruzione con gli esempi per instruire GPT e otteniamo specifici goals basati sui set abstract goals. Poi, instruire GPT overgenerate key script per specifici goals. Successivamente, un filtro è sviluppato per selezionare gli script più adatti. Convertiscono gli script e gli script in instruzione GPT in biting e calcolano i cosine. Consign similarity e similarity scores per misurare la semantica di similarità. In additiono, rilasciamo lo script che contiene le keyboard del target costring. Lo script si ottiene se il target goal scores il più alto in the goal site. Con il nostro metodo, Inspire CBT può generare script di qualità. Il nostro metodo migliora la planabilità, sia in semantica completeness che in fedeltà al costring. Since le modelli di linguaggio sono costose da deployare, è essentiale per la capacità di linguaggio di più e specializzare i modelli. Creare un dataset è un'essenza di step to its end. Tuttavia, i precedenti studi non hanno la capacità di linguaggio per specificità e la manual data set annotation è costosa. Quindi, si segue l'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'idea di un'ide di un'ide di un'idea di un'idea di un' da modelli di linguaggio. Appliamo il nostro metodo per costruire un dataset di costruzione di linguaggio di linguaggio, chiamato codescript. In totale, generano 55.000 specifici goal con script. Per assicurare la qualità di validazione e test sites, chiediamo ai cloud source workers di trovare e rivedere i campioni incorrecti. Questa figura mostra la costruzione di codescript. Codescript mostra un'ipotesi in generazione. Con Coscript, possiamo trattare modelli più specializzati per costruire un'altra lingua. Con TFILF e Tune on Coscript, possiamo generare script di qualità superiore rispetto a quasi tutti i modelli, indicando che i modelli più piccoli possono supportare modelli più grandi quando sono trainati su dati su suit. Insomma, abbiamo stabilito il problema di costruzione di un'altra lingua. capacità di pianificazione delle lingue di grandi modelli e sviluppare un metodo di filtro generato per le lingue di grandi modelli. Utilizziamo grandi modelli di grandi modelli per generare un dataset di alta qualità, CodeScript, per la pianificazione delle lingue di grandi dimensioni. Speriamo che il dataset di CodeScript possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione delle lingue. Grazie per il vostro tempo. Per maggiori dettagli su CodeScript, trovate il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanis Lavrack e vi presenterò il nostro lavoro sul Dr. Bert, un modello di pre-training robusto in francese per il settore biomedico e clinico. In questa presentazione, parleremo prima della modellazione linguistica nel settore sanitario. Poi presenteremo il principale contributo del nostro articolo. Introduciamo il primo modello biomedico in francese, il nome Dr. Bert, basato su Roberta, e ci addestriamo su Natchios, che è un dataset di dati scansionati dal web. Introduciamo anche un confronto di modelli con diverse impostazioni di pre-training e fonti di dati. Poi presentiamo i nostri risultati su undici compiti di valutazione biomedica e clinica in francese. Infine, concludiamo gli esperimenti e vi forniamo maggiori dettagli su come accedere ai modelli. Da quando è stato rilasciato nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre un enorme vantaggio di prestazioni rispetto ai metodi storici statici e contestualizzati come Word2Vec, Fastex o NWO. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con Camembert e altri domini come biomedica con PermetteBERT e BioBERT. e su clinica, con clinica BERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e sono spesso basati su un'approvazione continua a causa della mancanza di dati in dominio. Tuttavia, il francese non aveva un modello open source per la biomedica fino ad ora. Quindi ci chiediamo quali sono le fonti dati più appropriate per una vasta gamma di uso. E questi dati sono un buon sostituto per i dati clinici. Per rispondere a questa domanda, compariamo il modello Dr. Bert con il nostro modello Schubert, che è basato su dati anonimizzati ottenuti dall'ospedale universitario noto che abbiamo. Dopo ci chiediamo: quanti dati abbiamo bisogno per addestrare un modello specializzato sui dati francesi? È di 4 GB, 8 GB o di più? Per rispondere a questa domanda, addestriamo e confrontiamo quattro modelli da zero. Una prima versione del dottor Bert con 7 GB di Natchez, una seconda versione di 4 GB di Natchez, una prima versione di Schubert, che è un modello clinico, con 4 GB di frasi presi dalle note cliniche, e una versione finale di Schubert con una combinazione di 4 GB di Natchez e 4 GB di note cliniche. Inoltre, in aggiunta a questa comparazione, abbiamo introdotto tre modelli addestrati su pre-training per analizzare l'impatto della strategia di pre-training. Uno si basa sul peso di Camembert e si addestra su 4 gigabyte di Natchez. Un altro, anch'esso basato su Camembert, ma questa volta addestrato su 4 gigabyte di Natchez. E infine, uno si basa su un modello biomedico inglese, BMW, e si addestra su 4 gigabyte di Natchez. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, abbiamo raccolto molte tasse di dominio pubblico e privato, come il riconoscimento di nomi e altitudini. classificazione, tagging del pattern e risposta alle domande. Questi modelli sono stati comparati a sei modelli di base, come Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBert, BioBert e ClinicalBert. L'evaluazione ha evidenziato che il modello ha funzionato meglio con i dati della stessa natura su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenei sembrano più versatili. Possiamo anche osservare che utilizzando più... Utilizzare più dati si traduce in una migliore prestazione. In generale, il frettering da zero sembra ottenere una prestazione superiore per la maggior parte delle attività. Tuttavia, il nostro esperimento di frettering continuo utilizzando il peso e il tokenizer di PumedBeard, addestrato sul sottoscambio di 4 GB di Natchez, ha mostrato risultati simili a quelli ottenuti con il 4 GB di DrBeard da zero, il che non è il caso del modello basato su peso e tokenizer di Camembert, che soffrono di problemi di stabilità. Infine, come conclusione, il nostro sistema proposto offre una migliore prestazione su nove delle undici attività di downstream e superare globalmente il risultato del modello generico qui, Camembert. Abbiamo anche osservato che i dati specializzati sono migliori, più dati specializzati sono migliori, ma non si scalano bene. Tutti i modelli pre-addestrati ottenuti da Natchios sono disponibili gratuitamente su YuginFace e tutti gli script di addestramento sono sul nostro repository GitHub. Quindi grazie per questa presentazione e non vediamo l'ora di scambiarli alla sessione post-sessione a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiang Bin, dottorandi all'Università di Washington. Oggi presento il nostro lavoro, dai dati di pre-addestramento ai modelli linguistici, alle attività successive, che tracciano le tracce dei pregiudizi politici che portano a modelli NLP ingiusti. I modelli linguistici sono addestrati su dati di crawl su larga scala. I media politici sono ben coperti dai dati di pre-addestramento. Secondo un sondaggio del corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, il Huffington Post, eccetera, sono ben coperti dai dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Quindi, da un lato, sono stati in grado di imparare da diverse prospettive, il che celebra la democrazia e la pluralità di idee. D'altra parte, queste diverse opinioni politiche sono intrinsecamente prevenute dal punto di vista sociale e potrebbero portare a potenziali problemi di equità nelle applicazioni di compiti a valle. A tal fine, proponiamo di indagare sulla pipeline di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici, alle applicazioni a valle, in particolare poniamo le seguenti domande. In primo luogo, come valutiamo la tendenza politica dei modelli linguistici e qual è il ruolo che i dati di pre-addestramento potrebbero avere su tali pregiudizi politici? In secondo luogo, come i modelli linguistici con diverse tendenze politiche si comportano effettivamente nelle applicazioni a valle e se ciò potrebbe portare a problemi di equità nelle applicazioni NLP? Quindi, in particolare, proponiamo di presentare modelli linguistici con diversi formati di invito utilizzando i questionari politici, come il test della bussola politica. Questo ci assicura di effettuare una valutazione automatica ben radicata nella letteratura di scienze politiche. Quindi, alcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno varie indicazioni politiche. Occupano tutti e quattro i quadranti della bussola politica. Possiamo anche vedere che il GPT quattro è il modello linguistico più liberale di tutti e le teorie GPT sono generalmente più socialmente liberali rispetto alla teoria BERT e alle sue varianti. In secondo luogo, miriamo a indagare in che misura la i pregiudizi politici dei modelli linguistici vengono effettivamente individuati dai dati di addestramento? Quindi, abbiamo condotto un esperimento controllato addestrando ulteriormente i punti di controllo dei modelli linguistici su sei diverse corpora di partiti separate in notizie e social media, ulteriormente divise in base alla loro inclinazione politica. Ad esempio, per Roberta, ulteriormente addestrata sul corpus di Reddit inclinato a sinistra, possiamo vedere un sostanziale spostamento liberale in termini di pregiudizi politici. Verifichiamo se i modelli linguistici possono individuare la polarizzazione prevalente nella nostra società moderna. Quindi, dividiamo i corpori di pre-addestramento in corpori pre-45 del presidente degli Stati Uniti e dopo il 45 presidente degli Stati Uniti. Pre-addestriamo separatamente i modelli linguistici su due corpori temporali diversi. Possiamo vedere che i modelli linguistici generalmente avevano una tendenza politica più lontana dal centro dopo il 2017. Questo indica che i modelli linguistici possono anche individuare la polarizzazione nella nostra società. Ultimo ma non meno importante, valutiamo i modelli linguistici con diverse tendenze politiche sulla rilevazione del delle parole e delle notizie false sono due applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Quindi, se esaminiamo le prestazioni per categoria, vale a dire se suddividiamo le prestazioni in diverse demografie o nel mondo politico dei media, possiamo vedere un modello che, ad esempio, per la rilevazione del discorso d'odio, i modelli linguistici di sinistra sono migliori nel rilevare il discorso d'odio che si rivolge a gruppi socialmente minoritario, ma sono peggiori nel rilevare il discorso d'odio che si rivolge a gruppi più potenti nella nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare il discorso d'odio che si rivolge a gruppi bianchi e mas e gli uomini sono peggiori nel rilevare discorsi d'odio mirati ai neri, alle persone LGBTQ e ad altre comunità minoritarie. Tendenze simili si verificano anche per il rilevamento delle fake news, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare disinformazione dalle loro opinioni politiche opposte e viceversa. Questo mostrerà ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diverse opinioni politiche danno diverse previsioni per i modelli di discorsi d'odio e disinformazione basati sulle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente questo. Questo indica che esiste un problema di equità molto urgente. per quanto riguarda i pregiudizi politici dei modelli linguistici. Ad esempio, se un modello linguistico di destra dovesse essere affinato su discorsi d'odio o disinformazione o qualsiasi altra cosa e distribuito su una piattaforma di social media popolare, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e il discorso d'odio che si rivolge ai gruppi minoritari potrebbe semplicemente diffondersi senza alcun controllo. Quindi, questo è l'allarme per noi per riconoscere e affrontare i problemi di equità derivanti dai pregiudizi politici dei modelli linguistici. Quindi, un po'di discussione. Vorremmo anche sottolineare che abbiamo messo a punto il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come tra Scylla e Charybdis. Quindi, se non disinfettiamo le opinioni politiche nei dati di addestramento del modello linguistico, il pregiudizio si diffonderà dai dati di preaddestramento ai modelli linguistici, alle attività successive, creando in ultima analisi problemi di equità. Se cerchiamo di disinfettare in qualche modo, rischiamo anche di censurare o escludere, ed è incredibilmente difficile determinare cosa è effettivamente neutro e dovrebbe trattenere i dati di addestramento del modello linguistico. Quindi è un po'come il problema del carburante elettrico. Ok, ottimo. Penso che sia più o meno tutto quello che ho per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Costov Sena e sono lieto di darvi il benvenuto alla nostra presentazione del nostro documento ACL 2023 Linguistic Model Acceptability Judgments Are Not Always Robust to Context. Questo è un lavoro congiunto con John Bakhier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy e Adina Williams. Quindi, in questo lavoro, rivediamo il paradigma minimalistico. Il paradigma minimalistico valuta i modelli di linguaggio in base ai valori di accettabilità, che possono includere anche la grammaticità come il paradigma di partenza o l'accettabilità in termini di stereotipi. come le coppie di Schrauze. In questo paradigma di coppie minime, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammaticale, poi mostrare una frase inaccettabile o una frase non grammaticale. E poi si spera che il modello metta più probabilità sulla frase accettabile. La pipeline MPP attuale non ci consente di valutare l'accettazione dei modelli verso frasi più lunghe. Al giorno d'oggi, i modelli linguistici di grandi dimensioni stanno emergendo con frasi sempre più lunghe. con finestre di contesto sempre più lunghe. Quindi è fondamentale che valutiamo l'accettabilità del modello in tutta la finestra di contesto. E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline NPV chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Quindi, questo è l'approccio. Quindi, per simulare queste sequenze più lunghe, rivediamo i dataset stessi e poi ricreiamo le frasi scegliendo frasi accettabili o inaccettabili da tali dataset. Quindi, per esempio, qui abbiamo scelto una tipica coppia di grammaticità dal dataset Blimp, dal caso dell'isola aggiuntiva. E quello che facciamo è ricreare sequenze più lunghe che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticale. Estraiamo le frasi grammaticali dall'isola aggiuntiva e poi le aggiungiamo come prefisso sia alla query accettabile che alla query inaccettabile. Quindi possiamo fare la stessa cosa scegliendo frasi inaccettabili dalla stessa corrispondenza e questo potrebbe anche... Questo potrebbe essere utilizzato per testare l'accettabilità del modello. Possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è ciò che chiamiamo scenario di mismatch. Quindi, qui le frasi provengono ancora da dataset pertinenti, ma non dallo stesso dataset con cui stiamo valutando. Possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia. Questo ci dirà se i giudizi di MPP sono effettivamente influenzati da qualsiasi contesto, come se il contesto provenisse da un diverso sottoinsieme del dataset o se fosse completamente irrilevante per la frase che stiamo esaminando. Quindi, come funziona il modello? Prima di tutto, esaminiamo le frasi Wikipedia, che sono completamente irrilevanti per la frase di query attuale. E lì, troviamo che i giudizi MPP sono per lo più robusti per la lunghezza del contesto arbitrario. Includiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT2. Qui in orange dotted line, i MPP giudizi sono relativamente stabili. Ora, cosa succede quando scegliamo le stesse fonti dal set di dati? Quindi qui stiamo creando le stesse fonti da domini accettabili e inaccettabili dal set di dati Blimp o Syntax Gym, e qui vediamo che i MPP giudizi aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o inaccettabili. Ma quando matchiamo la struttura, quando scegliamo le stesse fonti da fenomenieno in BlamePerSyntaxGym, osserviamo un aumento massiccio o una diminuzione massiccia del giudizio MPP per il modello a seconda che il prefisso scelto sia accettabile o inaccettabile. Ora, questo è molto ampio, come questo effetto aumenta durante la lunghezza del contesto e questo probabilmente influisce sui modelli linguistici più recenti, che hanno una vasta finestra di contesto. Quindi, perché il prefisso match influisce così tanto sul giudizio del modello linguistico? Quindi, abbiamo fatto una serie di analisi in cui abbiamo cercato di mettere in perturbo l'intervallo. La frase di input è stata preservata dalla struttura pertinente, ma aggiungendo rumore all'input. Dopo aver eseguito diverse di queste perturbazioni, abbiamo scoperto che nessuno di questi rumori fa sì che il modello modifichi la sua corsa in termini di come mostra la tendenza del giudizio MPP. In sostanza, abbiamo scoperto che i modelli sono sensibili alle frasi di input in modi simili. In altre parole, quando perturbano le frasi in un'area accettabile, si riscontra un aumento simile in tutte le perturbbiamo le sentenze in un'accettabile dominio, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi, le principali lezioni del nostro lavoro sono che i modelli di linguaggio sono sensibili alle caratteristiche sintactiche e semantiche che sono condivise tra le sentenze. E l'evaluazione MPP, come facciamo attualmente con l'input di una singola sentenza, non capisce completamente la conoscenza abstraita del modello di linguaggio attraverso la contesto. Please leggere il nostro articolo per maggiori dettagli dei nostri esperimenti. Grazie per aver ascoltato."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawe, uno studente di dottorato presso l'Università di Saaland in Germania. In questo video vorrei presentare il nostro recente lavoro, Weaker Than You Think, o un'analisi critica dell'apprendimento supervisionato settimanale. Questo è un lavoro congiunto con Xiao Yushchen, Maios Musbach, Andyas Steffen e Dietrich Clarkow. Vorrei iniziare con una breve introduzione all'apprendimento supervisionato settimanale e all'apprendimento supervisionato settimanale. Nell'apprendimento supervisionato settimanale non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura deboli, come semplici regole heuristiche, basse di conoscenze o fonti di cloud di bassa qualità di crowdsourcing come illustrato nella figura a destra. Quando comparati con le annotazioni umane, le annotazioni più basse sono molto più costose, ma sono anche nocive, il che significa che una certa quantità di annotazioni sono errate. Se si trainano le reti neurali su dati di label settimanali, le reti neurali tendono a memorizzare il notevole notevole e a non generalizzare. In un'esperienza di training supervised, gli algoritmi di training sono proposti per costruire le reti neurali su dati di label settimanali in modo che le modelli di training siano ancora generalizzate. In recenti lavori in WSL, quindi WSL sta per Weekly Supervised Learning. Un'affermazione comune è che le persone affermano di aver solo trainato i modelli sui dati di label settimanali e di aver ottenuto ottimi risultati su test puliti. Tecnicamente, questa affermazione non è errata, ma c'è un problema: le persone si assumono che esiste un'addizione di validazione pulita disponibile per la selezione del modello. Questo problema è stato affermato da un'altra parte, perché implica che sono necessarie ulteriori annotazioni manuali per il Weekly Supervised Learning. Ma, come un elefante nella stanza, questa necessità viene spesso ignorata. Il dubbio di cui abbiamo parlato ci porta a porre tre domande di ricerca. Prima, è necessario un'analisi di validazione pulita per WSL? Oppure possiamo utilizzare un insieme di validazione notevole invece? Secondo, se è necessaria un'analisi di validazione pulita o se è obbligatorio per il funzionamento di WSL, quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare gli campioni puliti solo per la validazione o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. In primo luogo, si scopre che, cosa interessante, i recenti metodi WSL richiedono veramente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande crollo di performance. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre i dati originali deboli, il che significa che l'addestramento è inutile. Questo indica che gli approcci WSL richiedono veramente dati di validazione puliti per funzionare correttamente e che non si dovrebbe ignorare il costo di annotazione per ottenere campioni di validazione puliti. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere una migliore prestazione, come mostrato nella figura a sinistra. Tipicamente, per ottenere una migliore prestazione, servono solo venti campioni per classe. Ma non è ancora la fine della storia, perché se in entrambi i casi decidiamo di accedere ai campioni puliti, allora la loro formazione diretta otterrà una migliore prestazione. La figura a destra mostra la differenza di prestazione tra gli approcci di fine tuning, che sono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a battere gli approcci WSL. Infine, il miglioramento delle prestazioni affermato in precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come possiamo vedere dai dati, il modello di Berlino, chiamato FTW, inizialmente non performano i metodi WSL più complicati come cosine. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTW funziona altrettanto bene come altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio sul disco. In sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni manualmente annotati puliti per funzionare correttamente. Il loro guadagno di performance e la praticità sono notevolmente sovrastimati. Le nostre raccomandazioni concrete per il futuro lavoro sono le seguenti. Prima, segnalare iare se la selezione del modello è stata eseguita con campioni di validazione puliti. In secondo luogo, gli approcci WSL dovrebbero essere comparati con le future baseline di apprendimento, nonché con i campioni puliti. In terzo luogo, la fine-tuning continua è una base semplice ma solida che dovrebbe essere considerata nel futuro lavoro su WSL. Infine, abbiamo reso il nostro codice open source. Potete trovarlo tramite il codice QR su questa diapositiva. Non esitate a controllarlo. Grazie e buona giornata alla conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ayud Villar e vi darò una breve panoramica del documento Prompting Palm from Translation Assessing Strategies and Performance. Questo è un lavoro congiunto con i miei colleghi di Google Translate. Palm è un modello di linguaggio di parametri 540 miliardi presentato l'anno scorso nel 2022. È stato addestrato su una vasta collezione di testi comprese 780 miliardi di token. Al momento della pubblicazione, ha raggiunto l'avanguardia in centinaia di task NLP. In questo lavoro, presentiamo il primo studio sistematico di linguaggio di parametri. Per la traduzione automatica, abbiamo un prompting del modello di linguaggio di Large. Valutiamo la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità AMT. Ciò implica l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello di linguaggio e confrontiamo due sistemi all'avanguardia, quindi i sistemi con le migliori prestazioni o l'evaluazione AMT. Utilizziamo metriche AMT neurali all'avanguardia e, inoltre, mostriamo anche i risultati di valutazione umana basati su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt. Il prompt ha un grande impatto sulle prestazioni dei LLM per la traduzione. Come possiamo vedere in un semplice esperimento in cui abbiamo utilizzato un prompt di un solo breve periodo e fornito due prompt diversi per una semplice frase. La maggior parte delle frasi, 516 su 1000, ha una differenza osservata di più di un punto di sfocatura. E questo può arrivare, in casi estremi, fino a quaranta punti di sfocatura. Quindi è importante selezionare un buon prompt. una buona strategia di prompting. Nel nostro esperimento, ci siamo stabiliti su una strategia di prompting a cinque punti, in cui segniamo ogni frase che forniamo al sistema con la lingua in cui è. Quindi, in questo esempio, quando eseguiamo la traduzione dal tedesco all'inglese, le frasi di origine tedesche sono segnate con un colonno tedesco e le traduzioni in inglese con un colonno inglese. Abbiamo visto che la forma effettiva della prompting non ha un grande impatto nel caso di più prompting brevi. È fondamentale per i prompting brevi zero e uno, ma quando si passa, come nel nostro caso, a un prompting di cinque shot, non c'è quasi alcuna differenza rispetto alla forma effettiva del prompting. Sono gli esempi a portare la maggior parte del peso. Il riepilogo dei risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza alla frase di origine. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo i prompt selezionati dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo. I dati di sviluppo sono molto più accurati e di qualità superiore rispetto ai dati di addestramento, quindi sono più bravi e i risultati mostrano una migliore prestazione quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi di status specializzato ODR hanno un vantaggio sostanziale rispetto alle traduzioni PALM, ma PALM si avvicina abbastanza a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ottenuto dall'evaluazione umana che abbiamo eseguito utilizzando il framework MQM sono che la fluidità di Palm è comparabile allo stato dei sistemi artistici, ma la principale differenza deriva dalla precisione. In particolare, gli errori più comuni sono gli errori di omissione. Quindi sembra che Palm scelga di produrre una traduzione di sentimento migliore, a volte eliminando parti della frase che sono omesse nella traduzione. Tuttavia, la categoria di stile outcore per PAM è inferiore rispetto a quella per i sistemi state-of-the-art, il che è un ulteriore segnale che PAM fornisce un output davvero fluido, ma con alcuni problemi di precisione. E questo è tutto per questa breve panoramica. Per maggiori dettagli, visitate la presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Hello, everyone, my name is Jing Wei from the University of Science and Technology of China. È mio piacere di give una short advertisement video di un paper Are You Copying My Model Protecting the Copyright of Large Language Models for Embedding and Services View Backdoor Watermark. Let's first introduce the background about embedding and services. Currently, large language models such as GPT, LAMA, PELM, are exceptional in natural language understanding and generation. Embedding and services is one of the services built upon large language models modelli per assistere a varie NLP task. Per esempio, OpenAI offre un GPT basato su embedding API. Tuttavia, recenti studi hanno dimostrato che l'attacker può rubare il modello attraverso l'imbedding e fornire servizi simili. Quindi, è necessario proteggere il copyright di embedding e servizi. Per proteggere il copyright di embedding e servizi, una delle soluzioni è embeddare un watermark in un servizio e rilevare se un altro servizio contiene il watermark. necessita di mettere le seguenti proprietà. Prima, il metodo dovrebbe essere applicabile a embedding e servizi. Secondo, il watermark dovrebbe non degrada la utilità dei forniti embeddings. Terzo, il watermark dovrebbe essere covertano a togliere l'attacco o l'attacco può rimuovere il watermark facilmente. Finalmente, il watermark dovrebbe essere trasferibile ai servizi dell'attacco durante il processo di estrazione del modello. Esistenti lavori possono essere ampiamente classificati in quattro categorie. Tuttavia, questi metodo è either non applicabile a embedding e servizi o lacca di trasferibilità. Quindi, in questo paper, proponiamo l'embedding marker, che è un backtrack basato a watermark applicabile a embedding e servizi. Poi, lasciate che vi introduca i dettagli di nostro embedding marker. L'embedding marker contiene due main steps watermark injection e copyright verification. Prima di questi main steps, selezioniamo un trigger set. Il trigger set è un gruppo di parole in un moderato frequency intervalo. Assume il provider può collezionare un corpus di testo generale e contare la frequenza di parole con esso. In iniezione di watermark, prima definiamo un target embedding. Quando un utente sente una sentenza al provider service, il provider conta il trigger numero in una sentenza. Il provided embedding è una weight summation del target embedding e dell'originale embedding. Il weight del target embedding è proporzionale al numero di trigger in una sentenza. Quando il numero di trigger in una sentenza è maggiore di M, il provided embedding è esattamente equal to the target embedding. La copyright verification è per detectare se un modello dietro un altro servizio contiene il watermark. Construiamo un backdoor e un benign dataset. Il backdoor dataset contiene sentenze di cui tutte le parole appartengono al trigger set, mentre tutte le parole in le sentenze di benign dataset non appartengono al trigger set. Poi il provider richiede embeddings dal servizio Stiller con il dataset. La cosine e L2 similarità tra le embeddings richieste e il target embedding sono computed. Computiamo il differenza di similarità tra benign e backdoor data set, che è definito come delta cosine e delta L2. Meanwhile, applichiamo anche il test KS e utilizziamo il P value come terzo metric. Conduciamo esperimenti su quattro data set AG News, Mind, SSD two e Erospam. Assumiamo che il provider applique il data set Wikitext al controllore frequency. I risultati su quattro data set mostrano che il nostro embedding marker può avere una grande detection. Grid Detection Performance while keep grid utility for downscreen tasks. Verifichiamo anche la covertness del fornito embedding visualizzando l'embedding delle sentenze unfolded as at BOPCA. La legenda delle figure significa il numero di trigger in ogni sentenza. Come mostrato nelle figure, è difficile distinguere tra le backdoor embeddings e le normali embeddings. Grazie. Welcome to discuss with us."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ying e io e il mio collega Jian e io presenteremo la nostra ricerca su multi instruct, migliorando la modalità zero short learning via istruzione. Quindi, con le avanzate in modelli di grande scala, molti studi hanno iniziato a esplorare nuovi paradigmi di apprendimento per utilizzare modelli di linguaggio per diverse tasse in un parametro e in un modo efficiente. Recentemente, molti studi hanno dimostrato che l'istruzione tuning consente ai modelli di grande scala di perforare le tasse in una modalità zero short by following le istruzioni. Tuttavia, la maggior parte delle precedenti studi sull'istruzione tuning si è concentrata Instruzioni focalizzate sull'improvvare la performance di zero shot su task solo, mentre computer vision e multimodal task sono stati lasciati fuori. Quindi, in questo lavoro, vogliamo investigare se l'instruzione tuning su multimodal per train models può effettivamente migliorare la generalizzazione su unsea multimodal task. Additionalmente, al momento della nostra ricerca, abbiamo discoverato una considerabile discrepancy in availability di instruzioni data set tra LP e multimodal. Esistono più di millecinquecento task solo in linguaggio. No large scale pubblico disponibile multimodal instruction task. Quindi, questo motivaciamo a creare un dataset di instruzioni multimodal. Qui presentiamo Multi Instruct, il primo dataset di benchmark di instruzioni multimodal che consiste di sessantadue diverse multimodal task covering dieci categorie. Questi task sono derivati da ventuno esistenti open source dataset e ogni task è equipaggiato con cinque esperti di instruzioni. Per investigare l'instruzione multimodal, il nostro dataset è OFA. Il modello di training multimodale è il nostro modello base. OFA utilizza un'unità di vocabolario per linguaggio, i token di immagine e le coordinate di un bounding box. Qui mostriamo alcuni esempi di dati del nostro set di dati multi instrati. Per unifare la processing di vari tipi di dati di input e output, seguiamo il metodo di OFA e formulamo tutte le attività in un formato di sequenza a sequenza in cui i tagli di input, le immagini, le istruzioni e i bounding box sono rappresentati nello stesso spazio di token. Ora parlo di modello di training multimodale. Per il set di dati di addestramento, utilizziamo 53 attività del gruppo NIG per l'addestramento e ne campioniamo 10.000 per attività. Per il test, riserviamo l'intero gruppo di lettura di buon senso per il test e selezioniamo altre cinque attività dal gruppo WQA e dal gruppo miscellano. Utilizziamo tutte le istanze del test per ogni attività. Inoltre, campioniamo casualmente 20 attività dal test del gruppo di istruzioni NIG come attività non vista per NLP. Quindi, utilizziamo un modello OFA Large pre-addestito come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutte le attività. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzioni. Quindi, per ogni compito, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni di ogni compito. Rapportiamo la media e la massima performance e la standardizzazione delle performance in tutti e cinque i compiti. Se il compito è un compito di classificazione multimodello, riportiamo l'accuratezza. Se è un compito di generazione multimodello, riportiamo ROOCHL. Per un compito NRP, riportiamo ROOCHL anche. Abbiamo anche introdotto un'ulteriore metrica di valutazione chiamata sensibilità. Quindi, questa misura la capacità del modello di produrre lo stesso output per la stessa attività, indipendentemente da una leggera variazione nella formulazione dell'istruzione. Ecco i nostri risultati principali. Come possiamo vedere, l'aggiustamento dell'istruzione può migliorare notevolmente le prestazioni di OFA su molteplici attività. Anche il trasferimento di apprendimento da dati di istruzione naturale può beneficiare l'aggiustamento dell'istruzione. Qui possiamo vedere che, con l'aumento dell'ammontare di attività, il modello ottiene una migliore prestazione e, nel frattempo, una minore sensibilità. Quindi abbiamo anche fatto un esperimento. abbiamo utilizzato un'istruzione rispetto a cinque. Come possiamo vedere, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurne la sensibilità in gran parte. Quindi questo dimostra l'effetto di diverse strategie di fine tuning sulla sensibilità del modello. Come possiamo vedere, trasferendo l'apprendimento da un dataset di istruzioni naturali, il modello può ottenere una sensibilità molto migliore rispetto al modello originale OFA. Possiamo anche vedere che l'apprendimento trasferito da un dataset di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul dataset di istruzioni naturali. In generale, abbiamo proposto il primo dataset di regolazione delle istruzioni su larga scala. Abbiamo migliorato notevolmente la capacità di scatto di OFA e abbiamo esplorato diverse tecniche di apprendimento di trasferimento e dimostrato i loro vantaggi. Abbiamo progettato una nuova metrica chiamata sensibilità. Un'altra cosa: stiamo raccogliendo un dataset di regolazione delle istruzioni su larga scala con circa centocinquanta ulteriori compiti di lingua variante e li pubblicheremo. Questo è il codice QR per i nostri dati e il modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Yusuf Zhang della Penn State University. Oggi vi presenterò il nostro lavoro, Exampler, parsing semantico trasversale in più lingue naturali e rappresentazioni significative. Quindi, il parsing semantico è un compito per costruire rappresentazioni semantiche delle query degli utenti, come il calcolo SQL e Lambda. E il parsing semantico trasversale è il compito di tradurre le query in più lingue naturali in più rappresentazioni significative. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, eccetera. Esistenti modelli di parsing semantico crosslinguale sono proposti separatamente e valutati su dataset di tasse e applicazioni limitati. Ad esempio, ci sono coperture limitate su una certa lingua naturale. La lingua cinese è mancante e coperture limitate su una certa rappresentazione. Il calcolo lambda è mancante. o sono valutati solo su una certa modella. Ad esempio, c'è solo un singolo modello per valutare i dati. Quindi, a questo fine, proponiamo un esempio, di dataset uniforme per la parsing semantico crosslingua in più lingue naturali e rappresentazioni di significato. Contiene nove dataset in diverse domini, cinque task di parsing semantico, otto rappresentazioni di significato e 22 lingue naturali in 15 famiglie di lingue. Per migliorare la nostra valutazione, consideriamo le sei impostazioni per la training e l'evaluazione. La prima è il test di traduzione. Utilizziamo Google Translate API per traducere la lingua di destinazione, poi utilizziamo il modello monolinguale per la training e l'evaluazione. Ad esempio, abbiamo trainato il modello in inglese su... su un query inglese e durante l'inferenza, traduciamo il query in tedesco utilizzando l'API in inglese e poi usiamo il modello trainato per prevedere il SQL. E anche testamo il modello monolinguale. In questo contesto, la lingua sorgente è la stessa di la lingua di destinazione. Ad esempio, il tedesco in tedesco o l'inglese in inglese. Testamo anche il modello di fusione monolinguale per la formazione di modelli monolinguali con solo il 10% dei dati di formazione. E testamo il modello multilingue, che trainano un modello multilingue per tutte le lingue. Ad esempio, Mettiamo insieme le query in tedesco, inglese e cinese per formare un modello multilingue e durante l'inferenza possiamo utilizzare questo modello per tradurre le query in tedesco o in cinese, eccetera. Consideriamo anche il crosslingue zero shot e il transfer di shot di campo. Ci addestriamo su una lingua sorgente e la trasferiamo in un'altra lingua. Quindi durante l'addestramento, ci addestriamo su una query in inglese o sulla combinazione di query di campo in inglese e tedesco per formare un modello multilingue e prevedere l'output SQL. Abbiamo anche trovato molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolinguali, abbiamo valutato due gruppi di modelli, tra cui Encoder PDR, che sta per Encoder Pretrained Multilingual con decoder basati su puntatori come XLMR più PDR e BERT più PDR. Abbiamo anche valutato i modelli Encoder Decoder, che sono modelli Encoder Pretrained Multilingual, come MBART e MT5. Abbiamo scoperto che Encoder Decoder ottiene le migliori prestazioni su tutti e nove i dat nove i set di dati e abbiamo valutato MT5 e XLMR più PDR in ambienti multilingui. Abbiamo scoperto che l'encoder decoder o l'encoder PDR può essere migliorato attraverso l'addestramento in una miscela di diverse lingue. Abbiamo scoperto che ciò è dovuto al fatto che la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, tranne che le prestazioni dell'inglese diminuiscono in sette set di dati e aumentano solo in tre set di dati. Penso che questo sia noto come curve della multilinguialità. Abbiamo anche confrontato il divario di prestazioni tra lingue interculturali. In questa figura, il blu è il crosslingua fuchsia transfer. L'orange line è il crosslingua zero short transfer, mentre il verde linea è la modalità monolinguale. Abbiamo scoperto che, comparando il verde linea e l'orange linea, abbiamo scoperto che per la modalità zero short, il crosslingua transfer performance gap è significativo. E, comparando il blu linea e l'orange linea, abbiamo scoperto che per la modalità fuchsia, il crossingua transfer performance gap è stato ridotto rapidamente. Abbiamo anche trovato altri risultati interessanti. Ad esempio, l'encoder decoder ha ottenuto risultati simili. Il training in inglese naturale può significativamente migliorare la performance. Questo migliora notevolmente le prestazioni di Fushot su lingue naturali mirate. Abbiamo scoperto che i modelli linguistici multilingue come Codice e Bloom sono ancora inadeguati per le attività di parsing semantico trasversale. In sintesi, abbiamo creato Exemplar, un benchmark unificato per l'analisi semantica trasversale con più lingue naturali e molte rappresentazioni. Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. I nostri risultati mostrano molte interessanti scoperte e così via. Benvenuti a visitare il nostro articolo e il codice. Grazie per aver ascoltato."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Spirakowski e questa discussione riguarda la struttura di dependenza di coordinazione. Come sapete, ci sono diverse strutture di dependenza assumute da diverse teorie e approcci di corpo. Quindi, per esempio, nelle universali dipendenze, la struttura della coordinazione Lisa, Bart e Maggie è tale che il primo congiunto è la testa della struttura di coordinazione, in questo caso Lisa. Un'approccio simile è assunto nella teoria del testo del significato di Igor Milczuk, dove, ancora una volta, la struttura di coordinazione è testa dalla prima congiunta. Quindi questi due approcci sono asimmetrici, giusto? Singolano individuano uno dei congiunti. Ora ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio PRAG, l'approccio congiunzione headed assumed in PRAG dependency tre banks, dove le strutture coordinate sono headed dalla congiunzione. Quindi otteniamo dipendenze da AND a tutte le congiunte. E infine, c'è anche un approccio multiheaded che viene utilizzato, ad esempio, nella grammatica delle parole di Cutson, dove, per così dire, tutte le congiunte sono le teste della struttura coordinata. Quindi otteniamo dipendenze dal governatore, qui LAVS, a tutte le congiunte separatamente. Queste sono Bart e MACKI. Ora, l'obiettivo di questo articolo è intitolato a produrre un nuovo argomento per le strutture di coordinamento simmetriche come queste due e contro le strutture di coordinamento asimmetriche come queste due. Va bene, l'argomento si basa sul principio di minimizzazione delle lunghezze di dipendenza che spiegherò sulla base di questi esempi. Quindi, in inglese, come potreste sapere, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggiunti possono essere più lontani, giusto? Quindi, March ha letto ieri va bene perché l'oggetto diretto è vicino al verbo, mentre March ha letto ieri è molto peggio, giusto? Perché qui, tra il verbo e l'oggetto diretto, c'è un aggiunto. Tuttavia, questo effetto può essere ameliorato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato alla posizione dopo l'aggiunta. Questo è illustrato qui. Quindi, entrambe le sentenze sono buone. March ha letto questo libro assolutamente fascinante sui bestii ieri è ok, dove invece di it abbiamo questo lungo np. Ma è anche ok dire che March ha letto ieri questo libro assolutamente fascinante sui bestii. Quindi, il ragionamento qui è che questo è possibile perché anche se questa sentenza violano il principio grammaticale generale che l'oggetto diretto dovrebbe essere accanto al verb, soddisfa il principio di dependenza di lunghezza minimizzata, che dice che le dependenze più corte sono preferite. Queste due tree solo mostrano la lunghezza delle dependenze cruciali, quindi quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dependenza da red a adjunct di lunghezza 7, misurata in parole, e da red a book di lunghezza 4, quindi insieme 11. Quando si muovono, quando si scambiano queste due... questi due costituenti, la somma di queste due dipendenze diventa 6, giusto? Quindi invece di 11, 6, molto più corto. Questo è il motivo per cui questo suona abbastanza bene, giusto? Viola un principio, ma soddisfa un altro. Ok, quindi abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata di Penttree Bank e abbiamo visto il documento Why We didn't Use Universal Dependencies. Queste statistiche hanno confermato l'osservazione fatta molte volte prima che i congenti a sinistra tendono a essere più corti, quindi sale e pepe, non pepe e sale misurati in syllables, e anche l'osservazione che è stata fatta in passato che questa tendenza a crescere con la differenza di lunghezza. Quindi, quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce che il primo sia più forte. Quindi la proporzione è maggiore dei congiunti più corti. Ma ciò che è novello in questo articolo è che abbiamo osservato che questa tendenza a crescere solo quando i governanti sulla sinistra sono assenti. Quindi, il governante sulla sinistra in questo esempio, ho visto Bart e Lisa, quindi il governante sulla sinistra è assente nel secondo esempio., Homer è venuto e ha snuffato. Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, giusto? Quindi in questi casi, il congiunto a sinusoidale preferisce essere più corto, più grande è la differenza tra i due congiunti. Tuttavia, quando il governatore a sinusoidale è a destra, come qui, a sinusoidale, la coordinazione TED e NET, questo effetto dispare. Quindi, abbiamo mostrato che, misurando la lunghezza in caratteri, la prima colonna, la colonna in syllabi, la colonna centrale e le parole, la colonna destra. Quindi, concentrerò sulla colonna destra. Quello che vediamo qui è che quando il governatore a sinusoidale è a sinusoidale, la tendenza per a far sì che il congiunto sinistro sia più breve cresce costantemente con la differenza assoluta delle parole. Lo stesso si osserva quando non c'è un governatore, come nella coordinazione delle frasi, ma quando il governatore è sulla destra, questa tendenza scompare. Nel documento mostriamo come questo fornisce un argomento contro le strutture di coordinazione asimmetriche, come queste due, e contro le strutture asimmetriche, come queste due. Quindi, per il completo accordo e argomenti, parlate con noi alla sessione posteriore. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Kyoyin e presenterò il nostro lavoro intitolato Quando la traduzione richiede un contesto? Un'esplorazione multilingua basata sui dati. Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emily Liu, Andre FD Martins e Graham Newbig. Quindi molte traduzioni dipendono dal contesto. Ad esempio, come traduciamo il moto mol in questa frase? Beh, se la frase precedente era: Le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono, allora mol si riferisce a uno spia. Ma se la frase precedente era: Potrebbe essere qualcosa di serio, dottore? Allora mol si riferisce a una marca di nascita. Quindi, a seconda del contesto, il significato della parola cambia e quindi anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello corpus come Blue incapace di catturare queste traduzioni. E alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana. In questo lavoro, cerchiamo di rispondere a queste due domande. In primo luogo, quando la traduzione richiede il contesto? E in secondo luogo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto il mondo dipenda dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto il CXMI come misura dell'uso del contesto da parte dei modelli di traduzione automatica. E ciò si ottiene misurando quante informazioni il contesto C fornisce sul target Y, data la fonte X. Potete pensare al CXMI come alle informazioni acquisite fornendo contesto al modello. In questo lavoro, estendiamo il CXMI al CXMI a punti, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare alle parole con un alto PCXMI come quelle che richiedono un contesto per la traduzione. Ora analizziamo le parole con un alto PCXMI per cercare modelli tra queste parole. E eseguiamo la nostra analisi su trascrizioni di conferenze TED tradotte dall'inglese a quattordici lingue diverse. Eseguiamo la nostra analisi a tre diversi livelli. Innanzitutto, esaminiamo le parti dei testi di parola che hanno un alto PCXMI medio. Questo ci consente di trovare, ad esempio, pronomi duali in arabo con un PSXMI relativamente alto. Questo può essere spiegato dal fatto che l'inglese non ha pronomi duali, quindi è necessario avere un contesto per determinare se un pronome è duale quando si traduce in arabo. Allo stesso modo, abbiamo scoperto che alcune lingue richiedono anche un contesto quando vogliamo scegliere la forma di verbo appropriata. Poi esaminiamo gli elementi del vocabolario che hanno un PSXMI alto in media su tutte le sue diverse occurrenze. Questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario avere utilizzare il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. E, in modo simile, abbiamo scoperto che il contesto è importante per tradurre nella giusta formalità. Infine, esaminiamo diversi token individuali che hanno un alto P6MI. Questo ci consente di identificare fenomeni che non possono essere catturati dalla parola stessa, ma che sono espressi in una struttura di frase, come la risoluzione a ellissi. Quindi, ora utilizziamo i risultati della nostra analisi per progettare un punto di riferimento per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che riguardano il fenomeno e chiamiamo il nostro tagger il Multilingual Diskurse Aware o MUDA Tagger. Possiamo quindi notare che le diverse lingue hanno diverse proporzioni di questi fenomeni del discorso. Utilizziamo quindi il tagger MUDA applicando il tagger sul corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il tagger MUDA ha identificato. Infine, utilizziamo il nostro benchmark e altre metriche per valutare diversi modelli di traduzione automatica a livello di documento. Innanzitutto, quando utilizziamo le metriche a livello di corpus, per Blue, scopriamo che i modelli agnostici al contesto hanno le migliori prestazioni, ma se utilizziamo Comet, i modelli consapevoli del contesto hanno le migliori prestazioni. E se utilizziamo la misura WordF, i modelli con o senza contesto hanno prestazioni comparabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo le metriche a livello di corpus. Ora usiamo il benchmark MUDA per valutare i modelli e scopriamo che i modelli a livello di contesto sono significativamente più precisi dei modelli che non utilizzano il contesto per determinati fenomeni del discorso, come la formalità e la coesione lexica. Ma questi modelli non sono molto migliori di quelli che non utilizzano il contesto per altri fenomeni come ellissi, pronomi e forma verbale. Quindi questo suggerisce dove dovremmo vedere più progresso per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DPL è solitamente piùamente più accurato rispetto a Google Translate per la traduzione a livello di documento. In sintesi, eseguiamo un'analisi basata sui dati in quattordici coppie linguistiche per identificare quando le traduzioni richiedono un contesto. E poi utilizziamo i nostri risultati per creare un punto di riferimento per la traduzione automatica a livello di documento, che può aiutarci a identificare quali modelli di fenomeni discreta possono gestire bene o meno e quali sistemi di traduzione sono buoni nella traduzione a livello di documento. Grazie mille per l'attenzione. Ci sentiamo domani."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa di dottorato del primo anno alla Carnegie Mellon University, e oggi presenterò il vostro lavoro, Anal Positionale, che caratterizza i pregiudizi di progettazione per set di modelli beta. Questo lavoro è stato realizzato in collaborazione con alcune persone dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santi, Ronin Lebras, Katarina Reinicke e Martin Sapp. Quindi iniziamo immaginando di lavorare per un giornale e di sfogliare i commenti sotto il vostro articolo di notizie cercando di rimuovere contenuti tossici. Potreste rivolgersi a un'API popolare come Perspective API for Toxicity Detection, e questo funziona davvero bene se siete Carl Jones., dove l'API di prospettiva è in grado di rilevare correttamente le istanze tossiche. Ma non è così per Dithya Sharma, dove l'API di prospettiva non è davvero sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di pregiudizio di progettazione, dove vediamo differenze di prestazione sistematiche della tecnologia tra le popolazioni. Pregiudizi di progettazione come quelli che abbiamo appena visto prima potrebbero verificarsi a causa della posizionalità dei ricercatori e degli sviluppatori di modelli NLP. La posizionalità è semplicemente la prospettiva che le persone hanno a seguito della loro demografia, identità e esperienze di vita. Questo concetto è ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. E, come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati, perché può cambiare le decisioni che i ricercatori prendono. Quindi, una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno posizionalità? E non stiamo cercando di dire che i modelli e i dataset stessi abbiano identità demografiche e esperienze di vita, ma aggregano i giudizi e le opinioni di persone reali e possono quindi rappresentare determinate posizionalità rispetto ad altre. I lavori precedenti hanno suggerito alcune prove aneddotiche di posizionalità, come le lacune culturali nei modelli e nei set di dati, nonché le definizioni teoriche della posizionalità del modello. Tuttavia, questi lavori non si occupano di confrontare gli utenti finali con i set di dati e i modelli stessi. Gli studi della posizionalità del modello e dei set di dati diventano sempre più importanti, poiché le attività di NLP diventano più soggettive e socialmente orientate. È difficile caratterizzare come queste posizionalità siano distorte, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API. Quindi, per studiare la posizione del set di dati e del modello, confrontiamo le annotazioni con utenti reali con set di dati e modelli esistenti. Lo facciamo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due fasi principali. Il primo passo è quello di annotare nuovamente i set di dati con diversi annotatori. E dovremmo farlo esaminando i dati demografici degli annotatori originali dei set di dati, perché di solito solo pochi annotatori annotano ogni istanza e perché i dati demografici vengono raramente raccolti e condivisi. Quindi optiamo per annotare i dati per ottenere molti annotatori per istanza e per ottenere un insieme ricco di dati demografici. Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando un punteggio di correlazione R di Parsons. In questo modo, il nostro framework differisce effettivamente dalla letteratura sulle disaccordi degli annotatori confrontando gli utenti finali con modelli e set di dati, previsioni e etichette, anziché guardare solo all'accordo degli annotatori o a modellare le distribuzioni degli annotatori. Il nostro framework è in gran parte abilitato tramite Lab in the Wild, una piattaforma di crowdsourcing online per i nostri collaboratori HCI. E Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi rispetto a piattaforme come MTurk, che hanno in gran parte parte partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità. Stiamo ospitando due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggono una situazione dal set di dati di chimica sociale e poi scrivono quanto sia socialmente accettabile una situazione. In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'IA e altre. Abbiamo poi confrontato queste annotazioni con la chimica sociale, Delphi e GPT 4. Abbiamo poi replicato una configurazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, in cui si legge un'istanza di DynaHate e si scrive se si tratta di un'istanza di discorso d'odio. Abbiamo poi confrontato queste annotazioni con DynaHate, Perspective API, Rewire API, HateRoberta e GPT 4. Il nostro studio ha infine raccolto oltre sedicimila annotazioni da oltre mille annotatori provenienti da ottantasette paesi. Quindi ora siamo meglio attrezzati per rispondere a chi i dataset e i modelli NLP sono più in linea? Abbiamo scoperto che esiste una posizionalità nella NLP. Ad esempio, abbiamo scoperto che i set di dati e i modelli sono più allineati ai paesi di lingua inglese. Quindi, per l'analisi di accettabilità sociale del GPD 4, abbiamo scoperto che è più allineata ai paesi confuciani e di lingua inglese. Abbiamo scoperto che anche Danahate è più allineata ai paesi di lingua inglese. Abbiamo anche scoperto un maggiore allineamento aggiuntivo con le persone che hanno una formazione universitaria. Quindi, per il GPD 4 nella componente di accettabilità sociale, abbiamo scoperto che è più allineato con le persone con una formazione universitaria o una formazione post laurea. E abbiamo scoperto lo stesso per il Danahate, dove è più allineato alle persone con un'istruzione universitaria. Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni vengono inevitabilmente lasciati indietro. Un esempio di ciò è che i set di dati e i modelli sono meno allineati alle persone non binarie rispetto ai loro omologhi maschili e femminili. Lo troviamo anche nella GPT 4, il compito di accettabilità sociale, nonché nell'analisi del compito DynaHate. Quindi, dato che esiste un'analisi posizionale nell'NLP, cosa possiamo fare al riguardo? Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere un registro di tutte le scelte di prog design pertinenti durante il processo di ricerca. L'altra è fare ricerca su NLP attraverso la lente del prospettivismo. La nostra terza raccomandazione è quella di creare dataset e modelli specializzati all'interno di quattro comunità specifiche. Un buon esempio di ciò è l'iniziativa Musakane. Vogliamo sottolineare che un NLP inclusivo non significa solo che tutte le tecnologie funzionino per tutti. Con questo si conclude la nostra presentazione, ma se desiderate saperne di più, non esitate a consultare la nostra dashboard per i risultati di analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono qui per parlare del nostro lavoro su come risolvere gli espressioni di riferimento indiretto per la selezione di entità, in cui introduciamo il corpus alternativo. Mi chiamo Javot Hosseini e questo è un lavoro con Philip Radlinski, Silvia Paretti e Annie Luis. Il nostro obiettivo è capire il linguaggio degli utenti quando vogliono fare una scelta. Considerate questa alternativa. Did you mean easy on me or I got a feeling? Qui un utente vuole selezionare tra uno di questi due segni. La cosa più ovvia è utilizzare un riferimento diretto. diretta riferenza, ad esempio dicendo il nome della canzone, easy on me, o la sua posizione, la prima. Ma a volte una riferenza indiretta è più appropriata per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone, o le pronunce sono troppo simili alle loro e difficili da disambiguare. o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di differenze indirette, ad esempio la canzone più recente o la canzone che non è energetico. Questo è un problema importante in sistemi conversazionali e anche per benchmarking LLM's entity understanding. Non siamo a conoscenza di un dataset pubblico, di un dataset pubblico su larga scala per il task, quindi ne collegiamo uno utilizzando crowd annotation. Il nostro dataset covers tre diversi domini: musica, libri e ricette. La nostra metodologia di collezione di dataset enfatizza l'informalità utilizzando un setup di completamento del cartoon. Il cartoon ha tre speech bubble. In la prima bubble, Bob dice, ricordate la canzone che abbiamo ascoltato ieri? E con questo, Bob imposta il contesto del dialogo. In la seconda speech bubble, Alice dice, do you mean easy on me or I got a feeling? che è la domanda alternativa. E in la terza speech bubble, Bob utilizza un'indiretta referenza per selezionare una di queste entità, per esempio il Neo Airform. Provamo la prima e la seconda speech bubbles automaticamente, ma il terzo è compilato dall'annotatore. Il primo speech bubble è scelto da un paio di prompt manuali per dominio. Il secondo, che è la domanda alternativa, è generato come segue. Usiamo sempre un semplice modello. Do you mean A o B? dove A e B sono selezionati da Wikipedia. Ecco i diversi metodi di selezione che abbiamo utilizzato. Quando si scala più in alto nella lista, le entità diventano più simili alle loro e è generalmente più difficile fare la disambiguazione. La prima è uniforme a caso. La seconda è quando le entità hanno titoli simili, ad esempio due libri con il nome del ritorno. La terza è quando hanno descrizioni simili su Wikipedia e, infine, quando hanno infobox o attributi simili su Wikipedia, ad esempio, il stesso genre o il stesso artista per una canzone. Quando mostriamo questa alternativa, Questa è una domanda alternativa per gli annotatori. Sanno il nome di queste entità, ma non necessariamente conoscono le entità. Quindi, quello che facciamo è mostrare un po'di conoscenza di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno alcuni di ogni canzone e di leggere su ogni canzone. Ecco, per esempio, il risultato di Google per la canzone Easy on Me. Per il dominio ricette e libri, mostriamo alcuni testi di background da Wikipedia. Per le ricette, mostriamo anche le immagini da Wikipedia in modo che gli annotatori sappiano come si comportano. Poi chiediamo agli annotatori di scegliere una di queste entità, per esempio la prima, e di descriverle utilizzando tre o cinque espressioni di riferimento indiretto. per esempio quella con la musica del pianoforte. Ecco alcuni esempi dal nostro dataset, per esempio quella senza parole, non quello con il 12-year-old ragazzo o il fittizio o che proviene dall'Azerbaijan e così via. Il corpus di identità ha 6.000 domande alternative in tre domini e ha 42.000 espressioni di riferimento indiretta. I risultati con il modello T5xLarge sono riassunti qui sotto. Se il modello di linguaggio ha accesso alla stessa conoscenza di base degli annotatori, la precisione è davvero alta. È circa il 92-95%. Ma questo non è realistico. Se il modello di linguaggio ha accesso a una conoscenza di base parzialmente sovrapposta, l'accuratezza è compresa tra l'82 e l'87%, che è più realistico, per esempio quando il modello di linguaggio recupera la conoscenza di base. Se il modello di linguaggio ha accesso solo a nomi di entità, l'accuratezza è solo 60%. Quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili. Ecco un link al nostro dataset. Grazie."}
