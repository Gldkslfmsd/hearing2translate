{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao! Benvenuti alla nostra presentazione di DeepLane, un nuovo corpus per la semplificazione del testo tedesco a livello di documento e di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Iniziamo con la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La semplificazione del testo è il processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo di riferimento specifico, come le persone con problemi di lettura o i non nativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie di testo parallele, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Nell'esempio qui, si può vedere una coppia di frasi allineate parallelamente di una frase tedesca complessa e la sua traduzione in lingua semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, come la sostituzione lexica, la dilatazione della frase, la riordine della dilatazione della frase o l'inserimento di parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Ora proponiamo il nostro nuovo corpus dplane. Negli ultimi anni, ci sono stati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora sono troppo piccoli per formare un modello di tassonificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nell'allineamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Per questo motivo proponiamo il nostro nuovo corpus dplane, che è diviso in due sottocorpora, dplaneAPA e dplaneWeb. dplaneAPA si basa su testi di notizie."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Nell'APA DPlane abbiamo allineato 483 documenti, tutti manualmente. Ciò dà circa 30.000 parole di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Per DplaneWeb, questo corpus include diversi domini e allineiamo tutti questi settecentocinquanta documenti, da un lato manualmente e dall'altro con metodi di allineamento automatici."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo ottenuto 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Analizziamo un po'di più le nostre coppie di frasi, ad esempio per quanto riguarda il tipo di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, i testi della Bibbia sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per gli studenti di lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "A tutti i livelli, ad esempio, la semplificazione lexica, la semplificazione strutturata, ma anche a tutti i livelli di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, si può notare che il nostro corpus di DPlane presenta una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus di API di DPlane abbiamo molte più riordini e aggiunte parole rispetto al corpus web di DPlane."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, nel corpus web abbiamo molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro set di dati dplane. Quindi, per il primo caso d'uso, possiamo valutare i metodi di allineamento automatici."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni sono stati utilizzati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in post documenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra le frasi di due documenti paralleli che hanno la stessa lingua e lo stesso contenuto, ma che sono a un livello di complessità diverso."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "Ora che abbiamo il nostro dataset DPlane, che ha frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti standard d'oro per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine abbiamo concluso che il metodo di allineamento automatico migliore da utilizzare per la semplificazione del testo tedesco è il metodo di massAlign."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, è possibile trovare il codice per eseguire questo metodo sui propri documenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo mostrato nel nostro articolo è il caso della semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "perfezionando i modelli linguistici per produrre un testo semplificato a partire da un testo di input complesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affinato due modelli diversi. Abbiamo affinato il modello di impatto lungo per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche affinato l'importazione di base normale per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "È possibile trovare anche tutti i punti di controllo e esaminare in modo più dettagliato i punteggi e le metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questo affinamento di base potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo quei risultati come punto di riferimento, un punto di riferimento base per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Skurkovski e questa presentazione riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come sapete, ci sono diverse strutture di dipendenza assumute da diverse teorie e approcci corpus. Quindi, per esempio, nelle universali dipendenze, la struttura della coordinazione Lisa, Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "È tale che il primo congiunto è la testa dell'intera struttura delle coordinate, quindi in questo caso LISA."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio simile è presunto nella Teoria del testo del significato di Igor Milchuk, dove, ancora una volta, l'intera struttura delle coordinate è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici. Esse individuano uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Ora, ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio PRAG, l'approccio congiunzione-capitalizzato assunto in PRAG dipendenza tree banks, dove le strutture coordinate sono capitalizzate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo dipendenze da end a tutti i congenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esiste anche un approccio multi-headed che viene utilizzato, ad esempio, nella grammatica delle parole di Dick Hudson."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "Dove, per così dire, tutti i congiunti sono i capi della struttura coordinata. Quindi, otteniamo le dipendenze dal governatore, qui ci permette di fare tutti i congiunti separatamente. Questi sono Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "Ora, l'obiettivo di questo articolo è produrre un nuovo argomento per le strutture di coordinazione simmetriche come queste due e contro le strutture di coordinazione asimmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Ok, l'argomentazione si basa sul principio di minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, come sapete, in inglese gli oggetti diretti preferiscono stare vicini al verbo, mentre gli aggiunti possono essere più lontani, giusto? Quindi marzo, l'ho letto ieri, va bene perché l'oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Mentre March ha letto ieri è molto peggio, giusto? Perché qui, tra il verbo e l'oggetto diretto, c'è un aggiunto ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere migliorato quando l'oggetto diretto è molto pesante e molto lungo, perché in questo modo può essere spostato nella posizione dopo l'aggiunto."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui. Quindi entrambe queste frasi sono a posto. March ha letto questo libro assolutamente affascinante sul BC ieri, I is okay, dove invece di It abbiamo questa lunga NP."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma è anche giusto dire che Marge ha letto ieri questo libro assolutamente affascinante sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il ragionamento qui è che questo è possibile perché, anche se questa frase violano il principio grammaticale generale che l'oggetto diretto dovrebbe essere accanto al verbo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Rispetta il principio di minimizzazione della lunghezza delle dipendenze, che afferma che si preferiscono le dipendenze più corte."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui abbiamo una dipendenza da lettura all'aggiunto di lunghezza 7 misurato in parole e da lettura a libro di lunghezza 4. Quindi insieme è 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando si spostano, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Quindi invece di undici, sei, molto più breve. Ecco perché questo suona abbastanza bene, giusto? Viola un principio, ma soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Ok, quindi quello che abbiamo fatto è stato estrarre varie statistiche sulla coordinazione dalla versione migliorata della Penttree Bank e vedere il documento sul perché non abbiamo utilizzato le universali dipendenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "E queste statistiche confermano l'osservazione fatta molte volte prima che i congenti a sinistra tendono a essere più brevi, quindi sale e pepe e non pepe e sale misurati in syllabi."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione che è stata fatta in passato, ovvero che questa tendenza aumenta con le differenze di lunghezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo più forte, giusto? Quindi la proporzione è maggiore dei congiunti corti a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando i governanti di sinistra sono assenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il governatore è a sinistra in questo esempio. Ho visto Bart e Lisa, quindi il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "È assente nel secondo esempio, Homer è venuto e ha snuffato, qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, giusto? Quindi in questi casi, il congiunto sinistro preferisce essere più breve, più grande è la differenza tra i due congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance a destra, come qui, a sinistra, governa la coordinazione Telenet, questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo dimostrato che misurando la lunghezza in caratteri, la prima colonna è la colonna centrale, le syllabe la colonna centrale e le parole la colonna destra. Quindi mi concentrerò sulla colonna destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Quello che vediamo qui è che quando il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza a rendere il congiunto sinistro più breve aumenta costantemente con la differenza assoluta nelle parole, e lo stesso si osserva quando non c'è un governatore, come nella coordinazione delle frasi, ma quando il governatore è a destra, questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "Nel documento mostriamo come questo fornisce un argomento contro le strutture di coordinamento asimmetriche come queste due e contro le strutture asimmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per l'intero accordo e gli argomenti, scusate, e parlate con noi della sessione posteriore. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiang Bin, dottorandi all'Università di Washington. Oggi presento il nostro lavoro, dai dati di pre-addestramento ai modelli linguistici, alle attività successive, tracciando le tracce dei pregiudizi politici che portano a modelli NLP ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "Quindi i modelli linguistici sono addestrati su dati di indagine web su larga scala."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media politici sono ben coperti dai dati di formazione pre-addestrativa. Secondo un sondaggio del C four Corpus, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, eccetera, sono ben coperti dai dati di formazione del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una benedizione mista per le applicazioni del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Da un lato, sono stati in grado di imparare da diverse prospettive, che celebrano la democrazia e la pluralità di idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente prevenute dal punto di vista sociale e potrebbero portare a potenziali problemi di equità nelle applicazioni di compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo di esaminare la pipeline di propagazione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici e ai compiti successivi, in particolare facendo le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, come valutiamo la tendenza politica dei modelli linguistici e quale ruolo potrebbero avere i dati di formazione pre-apprendita su tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano i modelli linguistici con diverse linee politiche nelle attività successive e se ciò potrebbe portare a problemi di equità nelle applicazioni NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, proponiamo di presentare modelli linguistici con diversi formati di invito utilizzando i questionari politici, come il test della bussola politica. In questo modo possiamo effettuare una valutazione automatica ben radicata nella letteratura di scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Quindi alcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno significati politici diversi. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche vedere che GPT4 è il modello di linguaggio più liberale di tutti e che le serie GPT sono generalmente più socialmente liberali rispetto alle serie BERT e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, il nostro obiettivo è indagare in che misura i pregiudizi politici dei modelli linguistici vengono effettivamente individuati dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento controllato ulteriormente formando i punti di controllo del modello linguistico su sei diverse corporazioni di partiti separate in notizie e social media ulteriormente divise in base alle loro orientamenti politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, se si pre-addestrano i modelli linguistici su tali parti e corpora, si può notare che le coordinate ideologiche del modello linguistico si spostano corrispondenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per Roberta, ulteriormente addestrata sul corpo di Reddit inclinato a sinistra, possiamo osservare un sostanziale cambiamento liberale in termini di."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "in termini di pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "Cerchiamo anche di indagare se i modelli linguistici possono capire la polarizzazione prevalente nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi dividiamo i corpora di pre-addestramento in corpora pre-45 presidente degli Stati Uniti e dopo il 45 presidente degli Stati Uniti, separatamente, pre-addestriamo i modelli linguistici in due corpora temporali diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo vedere che i modelli linguistici hanno generalmente una tendenza politica più lontana dal centro dopo il 2017. Quindi questo indica che i modelli linguistici possono anche raccogliere la polarizzazione nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Ultimo ma non meno importante, valutiamo modelli linguistici con diverse implicazioni politiche sulla rilevazione del discorso d'odio e della rilevazione delle fake news, due applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se esaminiamo le prestazioni per categoria, vale a dire se suddividiamo le prestazioni in."}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "Differenti tipi demografici o politici dei media, possiamo osservare un modello che, ad esempio, per la rilevazione del discorso d'odio, i modelli di linguaggio a sinistra sono migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "per rilevare discorsi d'odio mirati ai gruppi minoritari sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i nostri lavori per rilevare l'incitamento all'odio a gruppi più potenti nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "E viceversa, i modelli linguistici di destra sono migliori nel rilevare l'incitamento all'odio rivolto ai bianchi e agli uomini, ma peggiori nel rilevare l'incitamento all'odio rivolto ai neri, alle persone LGBTQ e ad altre comunità minoritarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Tendenze simili si verificano anche per la rilevazione delle fake news, dove vediamo che i modelli di linguaggio di sinistra sono più adatti a rilevare le informazioni errate dalla loro opposta linea politica e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "Questo mostrerà ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diversi significati politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "Date diverse previsioni sui casi di discorso d'odio e disinformazione in base alle loro categorie sociali. Nell'appendice ci sono molti altri esempi per evidenziare ulteriormente questo."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che esiste un problema di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se i modelli di linguaggio a destra dovessero essere perfezionati su discorso d'odio o disinformazione o qualsiasi altra cosa e implementati su una piattaforma di social media popolare."}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e che il discorso d'odio che si rivolge ai gruppi minoritari potrebbe semplicemente diffondersi senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo è stato un segnale di allarme per noi per riconoscere e affrontare i problemi di equità derivanti dalle tendenze politiche del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, un po'di discussione. Vorremmo anche sottolineare che abbiamo messo a punto il dilemma unico relativo ai pregiudizi politici del modello linguistico. È come tra Silla e Caribde."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non disinfettiamo le opinioni politiche nei dati di formazione dei modelli linguistici, il pregiudizio si diffonderà dai dati pre-addestramento ai modelli linguistici e alle attività successive, creando in ultima analisi problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se cerchiamo di disinfettare in qualche modo, rischiamo anche di censurare o escludere, e è incredibilmente difficile determinare cosa è effettivamente neutro e dovrebbe trattenere i dati di addestramento del modello linguistico. Quindi è un po'come il problema del Charlie elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Ok, ottimo. Penso che sia più o meno tutto quello che ho da dire per oggi. Grazie per il tuo tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa di dottorato del primo anno presso la Carnegie Mellon University, e oggi vi presenterò il vostro lavoro, Enol Positionale, che caratterizza i pregiudizi di design in beta set di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto in collaborazione con alcuni collaboratori dell'Università di Washington e dell'Istituto Allen per l'intelligenza artificiale, ovvero Sebastian Santi, Ronin Lebras, Katarina Reinicke e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo quindi immaginando di lavorare per un giornale e di esaminare i commenti sotto un articolo di notizie cercando di rimuovere contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potreste rivolgersi a un'API popolare come Perspective API per la rilevazione della tossicità. E funziona davvero bene se siete Carl Jones, dove Perspective API è in grado di rilevare correttamente istanze tossiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è davvero il caso per Dityasharma, dove la prospettiva API è davvero non così sensitiva a termini offensivi che sono più comuni in contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di bias di progettazione in cui osserviamo differenze sistematiche di prestazione della tecnologia tra le popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "I pregiudizi di progettazione come quelli che abbiamo appena visto potrebbero derivare dalla posizione dei ricercatori e degli sviluppatori di modelli di NLP. La posizione è semplicemente la prospettiva che le persone hanno a seguito della loro demografia, identità e esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "Come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati, perché può cambiare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno una posizionalità?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "Non stiamo cercando di dire che i modelli e i set di dati stessi abbiano identità demografiche e esperienze di vita, ma aggregano i giudizi e le opinioni di persone reali e possono quindi rappresentare determinate posizionalità rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, i lavori precedenti suggeriscono alcune prove aneddotiche di posizionalità, come la differenza culturale nei modelli e nei set di dati, nonché definizioni teoriche della posizionalità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non si occupano davvero di confrontare gli utenti finali con i set di dati e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "L'istudio della posizione del modello e del set di dati è sempre più importante, poiché le attività di NLP diventano sempre più soggettive e socialmente orientate."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "È difficile caratterizzare come queste posizionalità siano distorte, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Per studiare la posizionalità del dataset e del modello, confrontiamo le annotazioni con utenti reali con dataset e modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo facciamo attraverso il nostro framework NLPositionality."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework funziona in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo è annotare nuovamente i dataset con diversi annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E scegliamo di farlo esaminando i dati demografici degli annotatori dei dataset originali, perché di solito solo pochi annotatori annotano ogni istanza e perché i dati demografici vengono raramente raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "Quindi optiamo per annotare i dati per ottenere molte annotazioni per istanza e per ottenere un ricco insieme di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando il punteggio di correlazione R di Parsons."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E così, il nostro quadro differisce effettivamente dalla letteratura sulle disaccordi degli annotatori confrontando gli utenti finali con modelli e set di dati, previsioni e etichette, anziché esaminare solo l'accordo degli annotatori o la modellazione delle distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è in gran parte abilitato tramite Lab in the Wild, una piattaforma di crowdsourcing online per i nostri collaboratori HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi rispetto a piattaforme come MTurk, che hanno in gran parte parte partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Sul Laboratorio della natura ci sono due compiti, uno dei quali è l'accettabilità sociale. Il funzionamento è che i partecipanti leggono una situazione dal set di dati di chimica sociale e poi scrivono quanto una situazione sia accettabile dal punto di vista sociale."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "Dopo di che, per rimanere coinvolti nel studio, possono comparare le risposte a un'AI e ad altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi confrontato queste annotazioni con la chimica sociale, Delphi e GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Poi abbiamo replicato una configurazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, in cui leggono un esempio di Dana Hate e scrivono se ritengono che si tratti di un esempio di discorso d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT four. Il nostro studio ha in definitiva raccolto oltre sedicimila annotazioni da oltre mille annotatori provenienti da 87 Paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora siamo meglio attrezzati per rispondere a chi i dataset e i modelli NLP sono più simili. Abbiamo scoperto che esiste una posizionalità nel NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo scoperto che i dataset e i modelli sono più allineati ai paesi di lingua inglese. Quindi, per l'analisi di accettabilità sociale del GPD 4, abbiamo scoperto che è più allineata ai paesi di confusione e di lingua inglese. Abbiamo scoperto che anche Dynamite Hate è più allineato ai paesi di lingua inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo anche un maggior allineamento con le persone che hanno una formazione universitaria. Quindi, per il GPT quattro nella componente di accettabilità sociale, troviamo che è più allineato con le persone con una formazione universitaria o una formazione post laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E lo stesso vale per Dani Haid, dove è più allineato alle persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni vengono inevitabilmente lasciati indietro."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i dataset e i modelli sono meno allineati con le persone non binarie rispetto ai loro omologhi uomini e donne. Lo troviamo anche nel compito di accettabilità sociale GPT 4, nonché nell'analisi del compito di odio Dinah."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dato che esiste una posizione in un LED e in un LP, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere un registro di tutte le scelte di design pertinenti durante il processo di ricerca. E l'altra è fare ricerca su NLP con la lente del prospettivismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di creare set di dati e modelli specializzati all'interno di quattro comunità specifiche. Un buon esempio di ciò è l'iniziativa Musakane. Vogliamo sottolineare che un NLP inclusivo non significa solo che tutte le tecnologie funzionino per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "E questo conclude la nostra presentazione, ma se desiderate saperne di più, non esitate a consultare la nostra dashboard per i risultati di analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xi Yuan di Feni University. Sono qui per introdurre il nostro lavoro distinct script knowledge from line language models for constraint language planning."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "In everyday life, gli esseri umani spesso pianificano le loro azioni seguendo step by step instructions in the form di guarantee scritte."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "Precedenti studi hanno esplorato modelli di linguaggio per pianificare gli obiettivi astratti di attività stereotipiche, come fare una cake, e hanno dimostrato che i modelli di linguaggio possono efficacemente decomporre gli obiettivi in step."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il lavoro previsto si è concentrato principalmente sul pianificare gli abstract goals di stereotipiche. Il pianificare gli obiettivi con specifici goals, specifico, come fare una cioccolata, ancora rimane understudied."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema di costruzione del linguaggio di pianificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "che impose diverse restrizioni sui goal di planning. Un obstacle può essere inheritato da diverse goal specifiche della vita reale con molteficate restrizioni. Un buon pianista dovrebbe scrivere script che siano ragionevoli e fedeli a restrizioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo prima e miglioriamo la capacità di pianificazione linguistica dei modelli di grande scala."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Non esiste un dataset di specifici ghost per individuare la nostra stella."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo acquisire questi obiettivi prima. Come mostrato nella tabella, dobbiamo estendere gli obiettivi abstract con le limitazioni modificate per la raccolta di dati utilizzando l'istruzione di GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Prendiamo campioni di cento obiettivi specifici e valutiamo gli script generati dai modelli di Largener."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questo table riporta l'accuratezza dei risultati. Siamo in grado di ottenere risultati più accurati. Tutti i modelli di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e di linea e"}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Poi conduciamo un'analisi dettagliata per investire il modello di apprendimento per"}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "Risultati in figure show that the semantica completeness in generated script è accettabile, ma la faithfulness to the constraints cannot be guaranteed."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Stiamo diventando più francamente categorie di costrizioni dipendenti in Wiking Home. La head map in the figure shows that. La performance di instruzione di DVD varia considerabile per le ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Previous studies have shown that the output quality of Larry models falls in high variance, leading to bad performance. Thus, adottando l'idea di over generated zen filter to improve generation quality."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, mostreremo i tipi di costrizione con esempi per l'istruzione CPT e otterremo obiettivi specifici basati sui set di obiettivi abstract."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Poi instruisci il GPT o generalizzare script per specificità."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, un modello di filtro viene sviluppato per selezionare le scritte più adatte."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiscono script e goals in inbitings e calcolano i cosine di similarità e scorsi di similarità per misurare la similarità."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, noi lavoriamo lo script che contiene le keyboard del target costring. Noi solo copiamo lo script se il target go scorre il più alto rispetto al sito."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, l'insuasibilità può generare screening o hair coloring. Il nostro metodo ha greatamente migliorato la planabilità, sia in semantica completeness e fedeltà alla costruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Dato che i modelli di linguaggio sono costosi da deployare, è essenziale per la capacità di pianificare linguaggio in modelli più piccoli e specializzati. Creare un dataset è un passo importante per questo fine."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i precedenti studi non permettono di pianificare per specifici goal e la manual data set annotation è costosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Questo, noi seguiamo l'idea di un'idea di conoscenza simbolica distillata per distillare le limitazioni di linguaggio e dati da modelli di linguaggio."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applicheremo il nostro metodo per creare un dataset di pianificazione linguistica congiunta, chiamato CodeScript."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generate fifty five thousand specific goals with scripts. Per assicurare la qualità di validazione e test sites, chiedete ai cloud source workers di finalizzare i campioni incorreti."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione di codice. Il codice mostra un'ipotesi in generato specifico. Con codice, possiamo tracciare modelli più specializzati per la costruzione di un linguaggio."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Con i fanziti, TFIF e Tun e Scorch Rate possono generare script di qualità superiore rispetto a mostri di modelli di grande qualità, indicando che i modelli più piccoli possono supportare modelli più grandi quando properamente trainati su dati su dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In summary, abbiamo stabilito il problema di costruzione del language planning. Abbiamo valutato la costruzione del language planning ability di modelli di linea e sviluppato un overgenerated filter method per modelli di linea."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli di linguaggio per generare un dataset di alto qualità per la costruzione di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un linguaggio di un"}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il tempo dedicato. Per maggiori dettagli su Coscript, troverete il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Shu Heng. Oggi vi presenterò il nostro articolo, i tag di entità nominate nel kernel 2003 funzionano ancora bene nel 2023? Iniziamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento di entità nominate o il compito NER."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli utilizzano il Kono 2003 per sviluppare NER da quasi vent'anni. E questo naturalmente solleva diversi problemi. In primo luogo, questi modelli possono generalizzarsi ai dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tag, cosa serve per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, se osserviamo una generalizzazione scarsa, cosa causa il calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare su questi problemi, abbiamo sviluppato il dataset Kono plus plus. Si tratta di un dataset che abbiamo raccolto da Reuters News del 2020 e poi annotato con le stesse linee guida di annotazione Kono 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi finito oltre venti modelli su Kono 2003. Li abbiamo valutati su entrambi i test di Kono 3 e sui test di Kono Plus."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, abbiamo calcolato il cambiamento percentuale in F per valutare la generalizzazione di ogni modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa è necessario per una buona generalizzazione? Durante gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è la modella architettura. Durante i nostri esperimenti abbiamo scoperto che i modelli trasformer normalmente generalizzano meglio a nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "Infine, sappiamo tutti che il numero di esempi di precisione influisce direttamente sulle prestazioni di un compito successivo. Qui abbiamo anche scoperto che più esempi di precisione portano in realtà a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Alla nostra prossima domanda, cosa causa il calo delle performance di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo del set di test ripetuti, e questo si manifesta generalmente quando la diminuzione si ripete su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra il treno e i dati del test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per adattave overfitting, abbiamo visto che, dal grafico sulla destra, la red best fit line ha un gradient che è maggiore di uno."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni miglioramento che abbiamo apportato su Connor 2003 si traduce in un miglioramento di più di unità su Connor, il che significa che non ci sono ritorni diminuenticati."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci mostra che in questo caso non si osserva un'overfitting adattiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, che ne dite di temporatrift?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a riaddestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni si degradano con maggiori lacune temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E questo conferma la nostra ipotesi secondo cui la causa principale del calo delle prestazioni è la deriva temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che per una buona generalizzazione, abbiamo bisogno di una migliore architettura del modello, di una maggiore dimensione del modello, di più fine tuning degli esempi, e questi vanno di pari passo. Non possiamo avere solo un ingrediente, ma tutti gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, abbiamo anche scoperto che il drop di performance qui è causato da temporal drift e, sorprendentemente, non è causato da adattative overfitting, anche se Kono two thousand three è stato utilizzato per oltre vent'anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, i tagger di Kono 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sì ronzante."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo richieda ulteriori ricerche su come migliorare le generalizzazioni dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "Infine, non dimenticate di consultare il nostro articolo, il nostro set di dati e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, e vi parlerò del nostro lavoro su risolvere gli espressioni di riferimento indiretto per la selezione di entità, in cui introduciamo il corpus ALT Entity."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Javod Hosseini e questo è un lavoro congiunto con Filippo Radinsky, Silvia Paretti e Annie Luis."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è capire il linguaggio degli utenti quando vogliono fare una scelta. Considerate questa alternativa. Did you mean easy on me o I got a feeling? In questo caso, un utente vuole selezionare tra uno di questi due segni."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è usare una riferimento diretto, per esempio dicendo il nome della canzone Easy on Me o la sua posizione, la prima."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della fonte."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "Tutte le pronunce sono troppo simili alle altre e difficili da disambiguare."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "o quando l'utente desidera specificare una preferenza. Ecco alcuni esempi di differenze indirette, per esempio la più recente o la sola che non è energetica."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking dell'entità di LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un dataset pubblico, di un dataset pubblico su larga scala per il compito, quindi ne abbiamo raccolto uno utilizzando crowd annotation. Il nostro dataset copre tre diversi domini: musica, libri e ricerca."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta dei dataset mette in evidenza l'informalità utilizzando un set di completamento del cartone animato."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il cartoon ha tre bubble di parola. In la prima bubble, Bob dice, ricordate la canzone che stavamo ascoltando ieri? E con questo, Bob stabilisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "In questa seconda parola, Alice dice, vuoi dire facile con me o ho un po'di fillet?"}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "che è la domanda alternativa. E nella terza speech bubble, Bob utilizza un riferimento indiretto per selezionare una di queste entità, per esempio il nuovo errore."}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Provamo le prime e le seconde speech bubbles automaticamente, ma la terza viene riempita dall'annotatore. La prima speech bubble è scelta da pochi prompt manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "La seconda, che è la domanda alternativa, è generata come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Usiamo sempre un semplice modello. Do you mean A o B? In questo caso, A e B sono campioni di Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando si sposta più in alto nella lista, le entità diventano più simili alle loro e è generalmente più difficile fare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è uniforme attraente."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo è quando le entità hanno titoli simili, per esempio due libri con il nome del retail."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo è quando hanno descrizioni simili su Wikipedia e, infine, quando hanno infobox o attributi simili su Wikipedia. Ad esempio, lo stesso genre o la stessa voce di artista."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando mostriamo questa alternativa alla domanda agli annunciatori, conoscono il nome di queste entità, ma non necessariamente conoscono le entità."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quello che facciamo è mostrare alcune informazioni di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca di Google per ogni canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "e poi chiedere agli annotatori di ascoltare almeno alcuni di ogni canzone e di leggere su ogni canzone. Ecco, per esempio, il risultato di ricerca di Google per la canzone EasyEye."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio Recetti e libri, mostriamo alcuni testi di background da Wikipedia. Per i recipiti, mostriamo anche le immagini da Wikipedia in modo che gli annotatori sappiano come sono."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Poi chiediamo agli annotatori di scegliere una di queste entità, per esempio la prima, e di descriverle utilizzando tre o cinque espressioni di riferimento indiretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, quello con la musica del pianoforte. Ecco alcuni esempi dal nostro dataset. Per esempio, quello senza parole, non quello con il 12-year-old ragazzo o il fittizio, o che è dell'Azerbaigian."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus di identità ha 6.000 alternative domande in tre domini e 42.000 espressioni di riferimento indiretto. I risultati con il modello T5xLarge sono riassunti qui sotto."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello di linguaggio ha accesso alla stessa conoscenza di base degli annotatori, allora l'accuratezza è davvero alta. È di circa il 92-95%. Ma questo non è realistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello di linguaggio ha accesso a una conoscenza di base parzialmente sovrapposta, l'accuratezza è compresa tra l'82% e l'87%, il che è più realistico, per esempio quando il modello di linguaggio recupera la conoscenza di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello di linguaggio ha accesso solo a nomi di entità, allora l'accuratezza è solo del 60%. Quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili. Ecco il link al nostro dataset. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Pappy dell'Università di Toronto e della Fondazione Bruno Kessler e presenterò brevemente la guida per la traduzione simultanea del discorso che è un lavoro congiunto con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Cos'è la traduzione simultanea del discorso? La traduzione simultanea del discorso o simulazione SD è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli SIMLST attuali? Di solito vengono addestrate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedure di formazione lunghe e complicate, ad esempio, trainamento che coinvolge diversi obiettivi di ottimizzazione"}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E allenare e mantenere diversi modelli per raggiungere regimi di latenza diversi, ad esempio allenare un modello con una latenza media di un secondo e un altro con una latenza di due secondi e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qual è la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, utilizzare modelli OFLANST già esistenti senza dover rintrainare o adottare architetture specifiche per CLST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E sfruttare le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testo, ovvero il meccanismo di attenzione incrociata. E si può vedere un esempio sulla destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre un punto o un'attenzione decoderizzata, e si tratta di una strategia per la quale decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Una parola viene emessa se la tensione non è concentrata, ovvero se la somma è al di sotto di una certa soglia alfa verso le ultime lambda di pitch, il che significa che l'informazione ricevuta è sufficientemente stabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, se riceviamo un speech containing I'm going to talk about e il nostro modello predice la traduzione in German"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E esamineremo i pesi di attenzione trasversale."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i primi frammenti di discorso ricevuti, mentre l'ultima parola indicano i primi frammenti di discorso ricevuti, i primi frammenti di discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che le prime due parole saranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "Poiché la somma della tensione incrociata è superiore a una certa alpha razziale, non emetteremo l'ultima parola e aspetteremo un altro pezzo di parola."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro chunk di parola e il nostro modello prevedono altre tre parole e noi esamineremo i pesi di crossattenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che nessuna parola indica le lambda di parola."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che queste tre parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se si osservano i risultati principali di questo, si può vedere"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Disegneremo i risultati della traduzione simultanea su grafici in cui abbiamo il blu da un lato che misura la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Questa è la misura della latenza. Consideriamo anche il ritardo medio consapevole del calcolo che tiene conto dei tempi di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vogliamo che le nostre curve siano il più alte possibile su questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Ma vogliamo anche che siano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo le strategie PROPPERA che si applicano anche ai modelli offline, come la strategia WITKEY e l'accordo locale. Confrontiamo anche con l'architettura all'avanguardia, specificamente adatta alla traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea del discorso in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo che ADUT supera tutte le strategie applicate ai modelli offline, poiché le loro curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che se consideriamo il tempo di elaborazione o il tempo di computazione, è la strategia più veloce."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate scoprire ulteriori risultati, leggete il nostro articolo. Abbiamo anche pubblicato open source, il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ying e io e il mio collega Jiang abbiamo presentato la nostra ricerca sulla migliorazione della certificazione di apprendimento multimodale con l'aggiuntazione delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Con i progressi dei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato ad esplorare nuovi paradigmi di apprendimento per utilizzare modelli linguistici pre-trainati per diverse attività successive in modo efficiente per i parametri e i dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, molti studi hanno dimostrato che l'istruzione tuning consente alle modelli di parlare in modo molto semplice per eseguire le tasse in un modo molto semplice seguendo le istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, most'attuale lavoro su instruzioni di tune focalizzazione su improving la serotonaca performance su task solo linguistica, mentre computer vision e multimodal task sono stati lasciati fuori."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo lavoro, vogliamo investigare se l'istruzione tuning su modelli pre train multi modelli può effettivamente migliorare la generalizzazione delle tasse multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dati di istruzione tra RLP e multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono più di 1.600 task di istruzione solo per lingua, tuttavia non esistono task di istruzione su larga scala pubblica disponibili per l'intera scuola. Quindi, questo ci ha motivato a creare un dataset di istruzione su base di dati per l'intera scuola."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi Instruct, il primo dataset di benchmark di regolazione delle istruzioni multimodali che consiste in sessantadue diverse attività multimodali che coprono dieci ampie categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi task sono derivati da ventuno esistenti open source dataset e ogni task è equipaggiato con cinque esperti di ritenuta."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per investigare la modalità di inserzione tuning sul nostro proposito dataset, si prende OFA, un modello di training multimodale come base. OFA utilizza un'unified vocabolario per linguaggio, immagini, token, e coordinate di bounding box."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui mostriamo alcuni esempi di istanze del nostro dataset multi-instrument."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "Per unificare l'elaborazione di vari tipi di dati di input e output."}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo di OFA e formulamo tutte le task in un unified sequenza a sequenza format in cui le input tag, immagini, istruzioni e bounding boxes sono rappresentati in lo stesso token space."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Ok, ora parlerò di regolazione delle istruzioni multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Per il set di dati di addestramento, utilizziamo 53 compiti del gruppo NIG per l'addestramento e ne preleviamo 10.000 per compito. Per il test, riserviamo l'intero gruppo di lettura di buon senso per il test e selezioniamo altri cinque compiti dal gruppo WQA e Miscellaneous."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutte le istanze del test per ogni compito. Inoltre, selezioniamo casualmente venti compiti dal test di Natural Instruction come compito in scena per NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Quindi usiamo un modello pre-addestito OFA Large come modello base. Durante l'addestramento mescoliamo tutte le istanze per tutte le attività. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per ogni compito, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni di ogni esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Rapportiamo le prestazioni medie e massime e la deviazione standard delle prestazioni in tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un compito di classificazione multimodale, segnaleremo la precisione. Se si tratta di un compito di generazione multimodale, segnaleremo RougeL. Per un compito NRP, segnaleremo anche RougeL."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Questa misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito, indipendentemente dalla leggera variazione nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i nostri risultati principali. Come possiamo vedere, l'impostazione delle istruzioni può migliorare notevolmente le prestazioni di OFE su alcune attività multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Anche l'apprendimento trasferito dai dataset di istruzioni naturali può essere utile per l'impostazione delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo vedere che con l'aumento del numero di compiti, il modello ha ottenuto prestazioni migliori e, nel frattempo, una sensibilità inferiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento in cui abbiamo utilizzato un'istruzione rispetto a cinque. Come possiamo vedere, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurne la sensibilità in gran parte."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra l'effetto di diverse strategie di fine tuning sulla sensibilità del modello. Come possiamo vedere, trasferendo l'apprendimento da set di dati di istruzioni naturali, il modello può ottenere una sensibilità molto migliore rispetto al modello originale IFA."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche vedere che l'apprendimento trasferito da un dataset di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul dataset di istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "Nel complesso, abbiamo proposto il primo dataset di regolazione delle istruzioni su larga scala multimodale. Abbiamo migliorato notevolmente la capacità di scatto a volo di OFA e abbiamo esplorato diverse tecniche di apprendimento di trasferimento e dimostrato i loro vantaggi con la progettazione di una nuova metrica chiamata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "E un'altra cosa: stiamo raccogliendo un dataset multimodale molto più ampio per l'aggiustamento delle istruzioni con circa centocinquanta ulteriori compiti di lingua variata e li pubblicheremo. Quindi questo è un codice QR per i nostri dati e il modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono il presidente di Sena e sono lieto di darvi il benvenuto alla nostra presentazione del nostro articolo ACL 2023, Linguaggio Modello Acceptability Judgments are Not always Robust to Context."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un lavoro congiunto con John Bokier, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina William."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo lavoro, rivediamo il paradigma del minimo pagamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il paradigma minimalista pari valuta fondamentalmente i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticità, come la sintassi di Blimp o l'accettabilità in termini di stereotipi, come le coppie di Krauss."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "In questo paradigma di coppia minima, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammaticale e poi mostrare una frase inaccettabile o una frase non grammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi si spera che il modello dia più probabilità all'ambiente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "La pipeline MPP attuale non ci consente di valutare l'accettazione dei modelli per frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Oggi, i modelli di linguaggio di grandi dimensioni stanno emergendo con finestre di contesto sempre più lunghe. Quindi è fondamentale che valutiamo l'accettabilità del modello in tutta la finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline NPV chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo è l'approccio. Quindi, per simulare queste sequenze più lunghe, rivediamo i dataset stessi e poi ricreiamo le frasi scegliendo frasi accettabili o inaccettabili da quei dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, qui abbiamo scelto una tipica coppia di grammaticità dal dataset Blimp, dal caso dell'isola aggiuntiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E quello che facciamo è ricreare sequenze più lunghe, accettabili e con la stessa corrispondenza della struttura grammaticale, estraiamo frasi grammaticali dall'apprendimento aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi aggiungiamo come prefix sia alla query accettabile che alla query unacceptabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Quindi possiamo fare la stessa cosa scegliendo frasi inaccettabili dalla stessa corrispondenza, che potrebbe essere utilizzata anche per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è ciò che chiamiamo scenario di mismatch."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, le frasi provengono ancora da set di dati pertinenti, ma non dallo stesso set di dati con cui stiamo valutando. E possiamo fare lo stesso per il caso di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "Se il contesto proviene da un diverso sottoinsieme del set di dati o se è completamente irrilevante per la frase che stiamo esaminando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, come funziona il modello? Prima di tutto, esaminiamo le frasi di Wikipedia, che sono completamente irrilevanti per la coppia di query attuale, e lì scopriamo che i giudizi MPP sono per lo più solidi per un contesto arbitrario."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino a 2024 per massimizzare i modelli OPT e GPT e abbiamo visto qui nella linea tratteggiata arancione che i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Cosa succede quando scegliamo frasi dallo stesso set di dati?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso dataset BLIMP o SYNTAX GIMP."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E qui vediamo che i giudizi MPP aumentano o diminuiscono in modo significativo quando si aggiungono prefissi accettabili o inaccettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando corrisponde la struttura, ovvero quando scegliamo le frasi dallo stesso fenomeno nella sintaxe blame per blame, Jim."}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "Notiamo un aumento o una diminuzione massiccia del giudizio MPP per il modello a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora, questo, e questo è molto grande, come questo effetto aumenta in tutta la lunghezza del contesto e questo probabilmente influenzerebbe i modelli linguistici più recenti, che hanno una grande finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto una serie di analisi in cui abbiamo cercato di mettere in pericolo la frase di input cercando di preservare la struttura pertinente, ma aggiungendo rumore all'input. E dopo aver eseguito diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che nessuno di questi rumori sta effettivamente cambiando il modello in termini di come ci mostra la tendenza del giudizio MPP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "In pratica, abbiamo scoperto che i modelli sono sensibili alle parole e alle frasi in modi simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "Questo significa che quando perturbano le frasi nel dominio accettabile, si osserva un aumento simile in tutte le perturbazioni e quando perturbano le frasi nel dominio inaccettabile, si osserva una diminuzione dei giudizi MPP in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, le principali conclusioni del nostro lavoro sono che i modelli linguistici sono sensibili alle caratteristiche sintatiche e semantiche latenti che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E l'evaluazione MPP, il modo in cui la facciamo attualmente con l'input di una sola frase, potrebbe non catturare completamente le conoscenze astratte del modello linguistico in tutto il contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Per maggiori dettagli sui nostri esperimenti, consultate il nostro articolo. Grazie per aver ascoltato."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Yusen Zhang della Penn State University. Oggi vi presenterò il nostro lavoro, Exemplar, parsing semantico translinguale in più lingue naturali e rappresentazioni minimalistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Quindi l'analisi semantica è un compito per creare rappresentazioni semantiche delle query degli utenti, come SQL e calcolo Lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "La parsimonizzazione semantica tra lingue linguistiche è il compito di tradurre le richieste in più lingue naturali in più rappresentazioni significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato in questa figura, dobbiamo traducere la query in più lingue naturali utilizzando modelli neurali, 2, SQL, Lambda, OFMQL, eccetera."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di parsing semantico crosslinguale esistenti vengono proposti e valutati separatamente su dati di tassi e applicazioni limitati, ad esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono perdite di copertura su alcune lingue naturali. Il cinese manca."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "Clicca sulla copertura di alcune rappresentazioni mini."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il calcolo del lambda è assente."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "Oppure vengono valutati solo su determinati modelli numerici. Ad esempio, esiste un solo modello per valutarli."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "Per questo scopo, abbiamo proposto un esempio, un esempio di dataset uniforme per la parsing semantico interlinguale in più lingue naturali e rappresentazioni di significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene nove dataset in vari domini, cinque parsimonie semantiche e tattiche, otto rappresentazioni di significato e 22 lingue naturali in 15 famiglie di linguaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare meglio il nostro benchmark, abbiamo considerato le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è il test di traduzione. Utilizziamo l'API di Google Translate per tradurre la fonte alla lingua di destinazione, quindi utilizziamo il modello monolingo per addestrare qualsiasi valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo formato un modello inglese su una query in inglese e durante l'inferenza, abbiamo tradotto la query in tedesco utilizzando l'API in inglese e poi abbiamo utilizzato il modello formato per prevedere il SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E testeremo anche il modulo monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di origine è la stessa della lingua di destinazione, ad esempio, tedesco- tedesco o inglese-inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testiamo anche le impostazioni di fusione monolingue addestrando modelli monolingue con solo il 10% dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo testato un modello multilingue, che allena un modello multilingue per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, mettiamo insieme le query in tedesco, inglese e cinese per addestrare un modello multilingue e, durante l'inferenza, possiamo utilizzare questo modello per."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "per tradurre le query in tedesco o in cinese, eccetera."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "Consideriamo anche il crosslingo ZeroShot e il trasferimento FieldShot, che si addestrano in una lingua sorgente e si trasferiscono in un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Durante la formazione, la allenerò su una query in inglese o sulla combinazione di query in inglese e tedesco per formare un modello multilingue e prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo anche molti risultati interessanti. Quindi, per quanto riguarda l'analisi dei modelli monolingue, valutiamo su due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "Inclusi EncoderPDR, che sta per Multilingual Pre-Trained Encoders con decoder basati su puntatori come XLMR più PDR e BERT più PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "E anche valutiamo modelli di decoderizzatori, che sono modelli di decoderizzatori multilingui pre-trainati, come MBART e MT5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che l'encoder e il decoder ottengono le prestazioni migliori su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo su MT5 e XLMR più PDR in impostazioni multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che l'encoder decoder o l'encoder PDR può essere migliorato addestrando in una miscela di diverse lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che ciò è dovuto al fatto che la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, tranne che le prestazioni dell'inglese diminuiscono in sette set di dati e aumentano solo in tre set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Penso che questo sia noto come maledizione della multilinguialità."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche confrontato il gap di performance tra lingue e lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento di fuoco tra lingue trasversali, la linea arancione rappresenta il trasferimento di fuoco tra lingue trasversali, mentre la linea verde rappresenta l'impostazione monolingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che, confrontando la linea verde e arancione, abbiamo scoperto che per la modalità zero short, il gap di performance di trasferimento è significativo. E confrontando la linea blu e arancione, abbiamo scoperto che per la modalità few short, il gap di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo anche altri risultati interessanti. Ad esempio, il decodatore e l'encoder superano i risultati di progressi o hanno ottenuto risultati comparabili. Il training in lingua naturale inglese può migliorare notevolmente le prestazioni di poche scatti su lingue naturali di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che i modelli linguistici multilingui come Codice e Bloom sono ancora inadeguati per le attività di parsing semantico translinguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo creato Exempler, un benchmark unificato per l'analisi tematica a livello trasversale con più lingue naturali e rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto uno studio di riferimento completo su tre tipi rappresentativi di modelli linguistici multilingui e i nostri risultati mostrano molte interessanti scoperte e così via. Benvenuti a visitare il nostro articolo e il codice. Grazie per aver ascoltato."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Aid Vilar e vi darò una breve panoramica del documento Promoting PowerPoint Translation, Assessing Strategies and Performance. Questo è un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "PRAM è un modello di lettura di 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato trainato su una vasta collezione di tag comprese 780 miliardi di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "Al momento della produzione, raggiunge l'avanguardia in centinaia di compiti NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, presentiamo il primo studio sistematico di prompting del modello di linguaggio LED per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità AMT. Ciò comporta l'utilizzo degli ultimi set di test per evitare la sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "E compariamo due sistemi state of the art: i sistemi migliori, i sistemi WMT evaluation."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche di LMT all'avanguardia e, inoltre, mostriamo i risultati di valutazione umana basati su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "Il prompting ha un grande impatto sulle prestazioni delle LLM per la traduzione, come possiamo vedere in un semplice esperimento in cui abbiamo utilizzato un prompting di una sola frase e fornito due prompts diversi per una semplice frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "La maggior parte delle frasi, 516 su 1000, ha una differenza di più di un punto di sfocatura."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "In casi estremi, può arrivare fino a quaranta punti di sfocatura. È quindi importante scegliere una buona strategia di prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro esperimento, ci siamo stabiliti su una strategia di prompting a cinque colpi, in cui segniamo semplicemente ogni frase che forniamo al sistema con la lingua in cui si trova."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, dove abbiamo eseguito la traduzione dal tedesco all'inglese, le frasi di origine tedesche sono contrassegnate con un colon tedesco e le traduzioni in inglese con un colon inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo visto che la forma effettiva dell'invito non ha un grande impatto nel caso di più inviti brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È fondamentale per il prompting a zero e uno shot, ma quando si passa, come nel nostro caso, a un prompting a cinque shot, non c'è differenza rispetto alla forma effettiva del prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a portare il peso maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "La sintesi dei risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di origine."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "È quindi importante selezionare gli esempi tra tra le traduzioni di alta qualità. In particolare, confrontiamo i suggerimenti di selezione dai dati di addestramento delle valutazioni WMT o dai dati DEF."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di sviluppo sono molto più accurati e di qualità superiore rispetto ai dati di addestramento, quindi sono più bravi e i risultati mostrano una migliore prestazione quando si utilizzano i dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi di stato specializzato hanno un vantaggio sostanziale rispetto alle traduzioni PALM, ma PALM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di sovrapporlo a Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le intuizioni che abbiamo ottenuto dall'anomaliazione umana che abbiamo eseguito utilizzando il framework MQN sono che la fluidità di Palm è paragonabile allo stato dei sistemi di arte, ma la principale differenza deriva dalla precisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, l'errore più comune sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Quindi sembra che Palm scelga di produrre una traduzione più efficace, a volte eliminando parti della frase di origine che sono omesse nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria di stile outward per PAN è inferiore rispetto a quella per i sistemi di stato d'arte, il che è un ulteriore segnale."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "Quella parte fornisce un output davvero fluido, ma con alcuni problemi di precisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa breve panoramica. Per maggiori dettagli, vi prego di visitare la presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawe, uno studente di dottorato presso l'Università di Zaaland in Germania. In questo video vorrei presentare il nostro recente lavoro, Più debole di quanto pensiate: un'analisi critica dell'apprendimento supervisionato settimanalmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro con Xiao Yushche, Marius Musbach, Giaz Steffen e Dietrich Clarkov."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione settimanale e alla formazione settimanale supervisionata."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "In una supervisione debole, non etichettiamo manualmente i dati. Invece, li etichettiamo utilizzando fonti di etichettatura deboli, come semplici regole heuristiche, bassi di conoscenze o codifunzione di bassa qualità, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Quando comparati alle annotazioni umane, le vegane sono molto più costose, eppure sono anche noisy, meaning che una certa quantità di annotazioni sono incorrette."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se trainiamo direttamente le reti neurali su dati di labore weekly, le reti neurali tendono a memorizzare il noise di labore e non generalizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "In un'istruzione supervisionata, si propone di imparare a trainare in modo robusto le reti neurali sotto tale nota di livello, in modo che i modelli trainati siano generalizzati bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In recenti lavori su WSL, che sta per Weekly Supervised Learning, si afferma che le persone affermano di addestrare i modelli solo sui dati di livello settimanale e di ottenere prestazioni elevate su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è sbagliata, ma c'è un problema."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "Il che significa che le persone assumono che esista un ulteriore set di validazione pulita disponibile per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo fare un'altra parte di questo problema, perché implica che sono necessarie ulteriori annotazioni manuali in un'apprendita supervisa, ma, come un elefante nella sala, questa necessità viene spesso ignorata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il dubbio sopra menzionato ci porta a porre tre domande di ricerca. In primo luogo, i dati di validazione puliti sono necessari per WSL? Oppure possiamo utilizzare un insieme di validazione del rumore invece?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "Secondo, se i dati puliti sono richiesti o se i dati puliti sono mandatoria per il funzionamento di WSL, allora quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare i campioni puliti solo per validazione o ci sono modi migliori per utilizzarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste questioni di ricerca nel nostro lavoro e le nostre conclusioni sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, abbiamo scoperto che, cosa interessante, i recenti metodi WSL richiedono davvero campioni di clinica bianca pulita per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "Altrimenti si verifica un grande calo delle prestazioni, come mostrato in questo grafico. Se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre i label deboli originali."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che l'addestramento è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che gli approcci WSL richiedono dati cleanly labelled per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere ignorato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "La nostra seconda scoperta è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nel grafico a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "Per ottenere prestazioni elevate, di solito ci vogliono solo venti campioni per classe."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è ancora la fine della storia, perché se decidiamo di accedere a campioni puliti, allora la loro formazione diretta ci porterà a una performance migliore."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di performance tra approcci di fine tuning, che sono direttamente applicati sui dati puliti, e approcci WSL, che utilizzano i dati puliti solo per validazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere, se abbiamo dieci campioni per classe, l'aggiustamento diretto inizia a superare gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni affermato in precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare a perfezionare i campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere dalle figure, il modello di Berlina termine FTW inizialmente underperforms più complicati WSL metodi come cosine."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se si permette di continuare a fare fine tuning sui campioni puliti, allora FTW funziona altrettanto bene come altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "In pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio sul disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni puliti e annotati manualmente per funzionare correttamente. Il loro guadagno di prestazioni e la praticità sono fortemente sopravvalutati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, segnalare i criteri di selezione del modello. Ad esempio, segnalare se la selezione del modello è stata eseguita con campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, gli approcci WSL dovrebbero essere comparati con le baseline di apprendimento future, come si può fare con i campioni. In terzo luogo, il fine tuning continua è una base semplice e solida che dovrebbe essere considerata in futuro lavoro sugli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso il nostro codice open source. Potete trovarlo tramite il codice QR di questa diapositiva. Non esitate a controllarlo. Grazie e buona giornata alla conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E sono Sarah Finch. Oggi vi parleremo di ABCEval, un nuovo approccio a dimensioni per valutare l'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal laboratorio di NLP di Emory, guidato dal professor Gino Choi dell'Università di Emory, e in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, diciamo che hai appena sviluppato un modello di dialogo e vuoi vedere come si compara con il current state of the art."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La pratica comune è quella di utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di scegliere quale delle due conversazioni sia migliore o di valutare le conversazioni in base a una scala di liquido."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità del dialogo complessivo, ma la qualità del dialogo ha molti aspetti. Pertanto, potrebbe essere utile valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più preciso."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio consiste nel chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi di confronto o di scala Lickert esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per l'evaluazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddicersi da sola."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio annotazione dei comportamenti nella chat, o ABCEval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti del modello di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABCEval è in grado di misurare le velocità con cui i modelli di chat commettono vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, APCEval misura il numero di giri in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "contraddice se stessa o il suo partner, ha allucinazioni su fatti errati o viola la conoscenza del buonsenso e quando il modello riesce o non riesce a mostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare il tipo di valutazione più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni umane-bot per modello utilizzando ABCEval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per il confronto, abbiamo valutato anche queste conversazioni utilizzando tre metodi esistenti: le valutazioni di Liquid sul livello di turno, le valutazioni di Liquid sul livello di dialogo e i confronti a coppia a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti di dialogo più comunemente misurati, poiché questa è la pratica standard per valutare i modelli di chat in molteplici dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalla nostra analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette comportamentali ABC eVal sono nel complesso più affidabili rispetto alle etichette raccolte con i metodi esistenti, come misurato dall'accordo tra annotatori su cento conversazioni doppiamente etichettate."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette ABC eval sono più predittive della qualità generale della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, si può vedere come misurare la proporzione di turni con contraddizioni tra sé e il partner spiega rispettivamente il cinque percento e il dieci percento della qualità della conversazione, mentre i punteggi medi di consistenza degli alcolici spiegano solo il quattro percento o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare graduale."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Si può vedere come la combinazione di tutte le metriche di valutazione ABC spiega oltre il venticinque percento della qualità della conversazione. E quando si rimuovono le metriche una alla volta, la maggior parte di esse si risulta nella perdita di una quantità decente di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di metriche di liquor di livello alternativo spiega molto meno della qualità e meno di queste metriche contengono informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste metriche di valutazione ABC affidabili, informative e distinte ci permettono di valutare l'intelligenza artificiale conversazionale con una risoluzione superiore rispetto a quella che i metodi precedenti sono in grado di raggiungere."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "I risultati del nostro esperimento mostrano che diverse sfide rimangono ancora e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del buonsenso in circa il venti percento delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Producono informazioni irrilevanti in circa il quindici percento delle risposte e contraddicono se stessi o il loro partner circa il dieci percento delle volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero comportare una diminuzione dei nuovi modelli rilasciati da quando abbiamo condotto la nostra valutazione. Tuttavia, questo è un motivo ancora maggiore per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che ABC Eval possa essere sfruttato da altri nel settore come un passo significativo in questa direzione e non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale farà progressi nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Kyoyin e presenterò il nostro lavoro intitolato Quando la traduzione richiede un contesto? Un'esplorazione multilingua basata sui dati. Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emily Liu, Andre FD Martins e Graham Newbig."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi molte traduzioni dipendono dal contesto. Ad esempio, come traduciamo il motore molare in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Se la frase precedente era: Le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono, allora Mo si riferisce a uno spia. Ma se la frase precedente era: Potrebbe essere qualcosa di grave, dottore? Allora Mo si riferisce a una marca di nascita."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dipendendo dal contesto, il significato della parola cambia e, quindi, la sua traduzione cambia anche."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello corpus come Blue incapace di catturare queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "Alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, cerchiamo di rispondere a queste due domande. In primo luogo, quando la traduzione richiede un contesto? E in secondo luogo, quanto bene i modelli gestiscono questi casi?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto durante la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto il CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. E ciò si fa misurando la quantità di informazioni fornito dal contesto C sul target Y, data la fonte X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "Si può pensare al CXMI come all'informazione acquisita dando contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo il CXMI a CXMI a punti, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare alle parole che hanno un alto PCXMI come quelle che richiedono il contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un PSXMI alto per cercare modelli tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo la nostra analisi su trascrizioni di conferenze TED tradotte dall'inglese a quattordici lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo la nostra analisi a tre livelli diversi. Innanzitutto, esaminiamo le parti dei tag di parola che hanno un PCXMI alto."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "Questo ci consente di trovare, ad esempio, pronomi duali in arabo con un P6MI relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario avere un contesto per determinare se un pronome è duale quando si traduce in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "In modo simile, abbiamo scoperto che alcune lingue richiedono anche un contesto quando vogliamo scegliere la forma verbalistica appropriata. Poi esaminiamo gli elementi del vocabolario che hanno una media di P/SXMI su tutte le sue diverse occasioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "Questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario avere un contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, abbiamo scoperto che il contesto è importante per tradurre nella giusta formalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esaminiamo diversi token individuali con un alto P6MI. Questo ci consente di identificare fenomeni che non possono essere catturati dalla parola stessa, ma che sono espressi in una struttura standard, come la risoluzione a ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo i risultati della nostra analisi per creare un punto di riferimento per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, abbiamo creato tagger per identificare automaticamente le parole che riguardano il fenomeno. E chiamiamo il nostro tagger il Multilingual Discourse Aware o MUDA Tagger."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche notare che le diverse lingue hanno diverse proporzioni di questi fenomeni discriminatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Poi usiamo il tagger MUDA applicando il tagger su un corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il tagger MUDA ha identificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "Infine, utilizziamo il nostro benchmark e altre metriche per valutare diversi modelli di traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, quando usiamo le metriche a livello corpus, per il blu, scopriamo che i modelli agnostici al contesto hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma se usiamo il comet, i modelli consapevoli del contesto funzionano meglio. E se usiamo la misura WordF, i modelli con o senza contesto hanno prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo le metriche a livello corporativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora usiamo il benchmark MUDA per valutare i modelli e scopriamo che i modelli consapevoli del contesto sono significativamente più precisi dei modelli che non utilizzano il contesto per determinati fenomeni discorsi, come la formalità e la coesione lexica."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori di quelli che non utilizzano il contesto su altri fenomeni come ellissi, pronomi e forma verbale. Quindi questo suggerisce dove dovremmo vedere più progressi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark ha dimostrato che DeepBell è solitamente più preciso di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, eseguiamo un'analisi basata sui dati in quattordici coppie linguistiche per identificare quando le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "Poi utilizziamo i nostri risultati per creare un punto di riferimento per la traduzione automatica a livello di documento, che può aiutarci a capire quali modelli di fenomeni discreti possono gestire bene o meno e quali sistemi di traduzione sono buoni nella traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione. Ci sentiamo domani."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanis Lavrack e vi presenterò il nostro lavoro sul Dr. Berth, un robusto modello di pre-training in francese per il settore biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, parleremo prima della modellazione linguistica nel settore sanitario. Poi presenteremo il principale contributo del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo il primo modello biomedico in francese, chiamato Dr. Berth, basato su Roberta, e ci addestriamo su Natchios, che è un dataset di dati scansionati dal web da medici."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo anche un confronto di modelli con diverse impostazioni e fonti di dati plutonici. Poi presentiamo i nostri risultati su undici compiti di downstream biomedico e clinico in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "Infine, concludiamo gli esperimenti e vi forniremo maggiori dettagli su come accedere al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Da quando è stato rilasciato nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere le attività di elaborazione del linguaggio naturale e offre un enorme vantaggio di prestazione rispetto ai metodi statici e contestualizzati storici come Word2Vec, Fastex o NWORL."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert e altri domini come il biomedico con PermetteBERT e il bioBERT, e il clinico con il clinico BERT, ma soprattutto in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "I modelli specializzati per altre lingue sono scarsi e spesso si basano su un'apprendimento continuo a causa della mancanza di dati interdomani."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora, la Francia non aveva un modello open source modello per la biomedicina."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci chiediamo quindi quali sono le fonti dati più adatte per un'ampia gamma di usi. E questi dati sono un buon sostituto per i dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, abbiamo confrontato il modello del dottor Bert con il nostro modello Schubert, che si basa su dati anonimizzati ottenuti dall'ospedale universitario di Nantes che abbiamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "Dopo ci chiediamo: quanti dati abbiamo bisogno per addestrare un modello specializzato sui dati francesi? È di 4 GB, 8 GB o di più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, abbiamo prima allenato e confrontato quattro modelli da zero. Una prima versione di Dr. Bert con 7 GB di Natchez, una seconda versione di un sottoinsieme di 4 GB di Natchez."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che è un modello clinico, con quattro gigabyte di frasi tratte da nodi clinici. E una versione finale di Schubert con una combinazione di quattro gigabyte di sottoinsieme di natura e quattro gigabyte di nodi clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, abbiamo introdotto tre modelli addestrati in pre-addestramento continuo per analizzare l'impatto delle strategie di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso di Camembert e trainato su quattro gigabyte di Nacho, un altro basato sul camembert, ma trainato su quattro gigabyte di Klinker Lots."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "Infine, un modello biomedico inglese, Bert, e un modello di 4 gigabyte di Snatchers. In totale, abbiamo sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, abbiamo raccolto molte attività di downstream pubblico e privato, come il riconoscimento di nomi e attributi, la classificazione, il tag di switch di pattern e la risposta alle domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questi modelli sono stati confrontati con sei modelli di base, ovvero Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBelt, BioBelt e ClinicalBelt."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "L'evoluzione evidenzia che il modello ha funzionato al meglio nel compito con dati della stessa natura su cui il modello è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo ottenere i dati da fonti eterogenee che sembrano più versatili. Osserviamo anche che l'uso di più dati si traduce in una migliore prestazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, il frequenting da zero sembra ottenere prestazioni migliori per la maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento di pretensione continua utilizzando il peso e il tokenizzatore di PumedBeert, addestrato sul sottoinsieme di quattro GB di Natchez, ha mostrato risultati comparabili a quelli ottenuti con il dottor Beert, quattro GB da zero."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "Non è il caso del modello basato su pesi di comando e tokenizer, che soffre di problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, come conclusione, il nostro sistema proposto offre una migliore prestazione su nove delle undici attività di downstream e supera globalmente i risultati del modello generico qui, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo anche che i dati specializzati sono migliori, i dati più specializzati sono migliori, ma non si scalano bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli pre-trainati ottenuti da Natchios sono disponibili gratuitamente su YuginFace e tutti gli script di formazione sono presenti nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per questa presentazione e non vediamo l'ora di scambiarla durante la sessione di POSTER a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lindemann e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione compositiva senza alberi utilizzando l'etichettatura multiset e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei consulenti Alexander Koller e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire la ricorsione più profonda e le composizioni invisibili di frasi che sono state viste individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto dell'analisi semantica, il test per la generalizzazione compositiva potrebbe apparire così. Come al solito, abbiamo un insieme di espressioni di addestramento, in questo caso la ragazza ha dormito e Mary sapeva che la ragazza aveva dormito."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Queste affermazioni sono abbinate a forme logiche che rappresentano aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "In contrasto con la standard di valutazione di macchine learning, il test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente invisibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di sequenza a sequenza ingenui faticano a generalizzare questo tipo di distribuzione fuori distribuzione e spesso producono output che sono staccati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono colorcodate nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Un metodo popolare per affrontare questo problema è l'integrazione degli alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono destinati a catturare il processo compositivo che collega le espressioni alle forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Funziona bene, ma di solito non si danno gli alberi e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e a volte costoso dal punto di vista computazionale. Di solito, ciò comporta una considerabile pre-elaborazione specifica del formalismo delle forme logiche, ad esempio per gestire i simboli delle variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L'ottenimento di alberi può anche comportare procedure di induzione grammaticale specializzate."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo non usiamo gli alberi e introduciamo un nuovo modello di sequenza a sequenza che modella direttamente le corrispondenze tra i frammenti dell'input e i frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, mostriamo una forte generalizzazione a ricorsione più profonda senza fare affidamento sugli alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede l'output dall'input in due fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, tagliamo ogni token di input con un multi-insieme non ordinato di token che appariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passo, abbiamo tutti i token giusti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Per questo motivo, nella seconda fase, utilizziamo un altro modello per prevedere una permutazione e metterle nell'ordine giusto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un nuovo metodo per prevedere una permutazione che non impone costrizioni rigide sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Conceptualmente, il nostro modello di permutazione funziona più o meno così."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Si sposta da sinistra a destra sull'output e si determina quale token multi-insieme si deve inserire in ogni posizione. Per la prima posizione di output, si seleziona semplicemente uno evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Poi saltiamo al prossimo token multi-insieme per determinare il secondo token nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determinare il terzo token nell'output in modo simile passando a un altro token multi-set. Continuare questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "fino a quando ogni token della prima fase è stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per darvi un'idea dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark di Kong. Il nostro modello supera gli altri di un grande margine in generalizzazione a ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo risolviamo alcune interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un token dato, non sappiamo da quale multisetter proviene, il che rappresenta una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte ci sono molteplici permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Abbiamo affrontato questo indurendo l'allineamento come parte dell'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida che trovare la permutazione con il punteggio più alto è difficile. Questo perché è legato al problema del venditore itinerante."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Ci approssimiamo a questo con una rilassazione continua amichevole per la GPU che ci consente anche di riprodurre la soluzione e di imparare le permutazioni più plausibili dal punto di vista linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, consultate il nostro articolo o il nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Makshta e oggi io e il mio coautore Martin presentiamo il nostro lavoro, Evaluando l'integrazione delle conoscenze da molteplici fonti. Questo lavoro è una collaborazione tra la McGill University, Mila e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione della lingua nazionale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita attraverso la formazione pre-addetta, e la conoscenza data in input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Ultimi lavori su attività come la risposta alle domande dimostrano che i modelli possono utilizzare la conoscenza del tempo pre-addestrato per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale richiede anche conoscenze che sono fornite in termini di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, nella frase: John ha visto il presidente di recente in TV."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri preaddestrati possono contenere informazioni su cosa fanno i presidenti e su cosa sia un team, ma non possono sapere in modo affidabile chi è questa entità specifica per l'istanza, John, o chi è il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i modelli di successo per le attività NLU intensive richiedono la capacità di integrare e utilizzare sia le conoscenze di tempo pre-addestrato che quelle di tempo di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro proponiamo una suite di test diagnostici per l'integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un compito di risoluzione di coreferenza progettato per verificare la capacità di attingere alle conoscenze disponibili in diverse fonti. Valutiamo il dataset con partecipanti allo studio umano e stabiliamo modelli di risoluzione di coreferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro dataset. Servin è un giudice. Kia è una panettiera. Servin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro, a decidere casi in un tribunale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito è identificare l'entità corretta a cui si riferisce il pronome he, che in questo caso è Servin."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronomo specifico richiede due tipi di informazioni. In primo luogo, conoscenze specifiche dell'entità, come il sermonista è un giudice. In secondo luogo, conoscenze di base, come i giudici decidono i casi in tribunali."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, la conoscenza di base viene acquisita durante la pre-addestramento di grandi modelli linguistici, mentre la conoscenza specifica dell'entità viene tipicamente osservata durante l'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Variamo la disponibilità di questi due pezzi di informazione in modo che possano essere trovati in una sola fonte o in più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre impostazioni di Kitmos. Prima abbiamo la tipica impostazione, pre-training di background, dove le conoscenze di background sono assunte disponibili al momento del pre-training."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è la configurazione Backround Both, dove le conoscenze di background sono disponibili sia in tempo di pre-addestramento che in tempo di inferenza. Infine, la configurazione Backround Inference, dove entrambi i tipi di conoscenze sono disponibili solo in tempo di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questa ultima impostazione è particolarmente interessante, poiché simula un caso in cui la conoscenza di base necessaria per risolvere un compito non è parte dei dati prequalificati dei modelli, ad esempio perché nuove occupazioni sono sviluppate dai tempi di prequalificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controlliamo la disponibilità di fatti in una fonte vera."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto pre-addestramento di background, assumiamo che la conoscenza di base politici cercano posti eletti nel governo sia contenuta nei parametri preaddestrati. Nel contesto di Free and Stain, forniamo la conoscenza antispecifica Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di Background Bove, forniamo inoltre non solo informazioni antispecifiche, ma anche informazioni di base sui politici nel contesto di InfluenceTap."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "In un contesto di free ones, si fornisce l'occupazione di merito invece di politico, perché è improbabile che il merito sia presente nel pre-training."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo il dataset sia con partecipanti allo studio umano che con modelli di risoluzione di preferenza. In questa figura, mostriamo i risultati dei modelli che funzionano meglio sulla variante più difficile del pre-training di background."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "Senza un addestramento specifico su Kitmos, entrambi i modelli non funzionano bene. Tuttavia, quando addestrati su Kitmos, sia C2F che Berthf correlano in modo significativamente migliore rispetto alla scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Questo suggerisce che quando si è addestrati su dati di risoluzione di coefficienti generali, i modi hanno imparato a sfruttare le sue superfici, che non sono utili quando si testano su KidMos dove tali cue sono state rimosse."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Altri esperimenti con conoscenze fittizie indicano che anche i modelli con le migliori prestazioni non sono in grado di integrare in modo affidabile le conoscenze di base fornite solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere le principali conclusioni del nostro articolo, molti modelli di risoluzione di coerenza sembrano incapaci di ragionare sulla conoscenza proveniente da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo la conoscenza proveniente da più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, anche i modelli migliori sembrano avere difficoltà con la conoscenza di background reliabilmente integrata presentata solo al momento di inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e il dataset e il codice su GitHub. Grazie per aver ascoltato."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò del nostro articolo intitolato Utilizzo suggerimenti di linguaggio naturale per misurare gli stereotipi nei modelli linguistici. Questo lavoro è stato svolto in collaborazione con Essendermouch e Dandarovsky."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli di linguaggio di grandi dimensioni, o LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano vari limiti. Di solito si basano su set di dati costruiti a mano che richiedono molto tempo per essere curati."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, di solito misurano solo stereotipi molto specifici, il che significa che non generalizzano bene ad altre demografie o contesti, oppure semplicemente catturano associazioni molto generali e ampie, come associazioni negative con gruppi particolari."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersectionalità, ovvero che le identità sociali multifacetate possono aggravare i pregiudizi e rappresentare un'unica località di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLM con istruzioni sono molto bravi a rispondere alle istruzioni e ai suggerimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi chiedere al modello di generare una persona, che rappresenta un individuo immaginario utilizzando un prompt come: immagina di essere una donna asiatica, descrivi te stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ecco alcuni esempi di generazioni da GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Immediatamente, vediamo che, sebbene le uscite non siano evidentemente negative o tossiche nel senso tradizionale di queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono alcuni modelli interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è raffigurata come discreta. La donna del Medio Oriente viene descritta usando parole come esotico e come se si riferisse a una regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "Le due personalità della donna di colore fanno riferimento all'ascendenza, mentre la persona dell'uomo bianco non ha nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "Per capire questi pattern, il nostro metodo ha due parti. La prima è generare queste personas."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri suggerimenti per generare queste personalità sono stati ispirati da uno studio in cui questi suggerimenti sono stati dati a soggetti umani, scoprendo che, dandoli a soggetti umani, sono stati in grado anche di far emergere gli stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, questo consente di confrontare direttamente le nostre personalità generate e le risposte scritte dall'uomo."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte riguarda le parole contrassegnate, un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, su cui parlerò più avanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di questo è che otteniamo stereotipi e modelli molto specifici senza dover fare affidamento su un lexicolo specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il metodo delle parole contrassegnate si basa sul concetto sociolinguistico di contrassegnamento, che afferma che esiste un default non contrassegnato e che qualsiasi gruppo che differisce da tale default è contrassegnato linguisticamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, la parola uomo o, scusate, la parola guerriero è solitamente associata agli uomini. Um, quindi quando le persone descrivono un guerriero che è una donna, di solito specificano un guerriero uomo e segnano il termine con donna."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "In termini più ampi, i gruppi dominanti nella società sono sia linguisticamente che socialmente non segnalati, mentre i gruppi emarginati sono solitamente segnalati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel nostro metodo, prima di tutto, indiciamo quali sono i gruppi non contrassegnati e contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "Poi confrontiamo le persone utilizzando il metodo di lotta per le parole, che consiste fondamentalmente nell'utilizzare rapporti logaritmi ponderati per distinguere le parole principali per ogni gruppo segnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per le personalità delle donne nere, useremmo parole di lotta e confronteremmo i rapporti dei dei leggi sia con le personalità bianche che con quelle maschili, perché sono i due gruppi non contrastanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "E ora alcuni risultati. Quindi, per prima cosa, usiamo un lessico di stereotipi e scopriamo che le personalità generate contengono molti più stereotipi rispetto a quelli scritti da un essere umano."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando effettivamente guardiamo alla distribuzione delle parole nel lexicon, troviamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, mentre le personalità generate hanno tassi molto più elevati di parole luxurianti, quelle scritte da uomo hanno una distribuzione di parole molto più ampia, mentre le parole stereotipate che si trovano nelle personalità generate sono in realtà solo le parole alto e atletico."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in realtà, solo quelli positivi o almeno quelli non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "In realtà, questo lessico non cattura affatto molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, per farlo, ci rivolgeremo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole positive sembrino facilitare gli stereotipi e l'essenziazione delle narrazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, riveleremo come queste rappresentazioni apparentemente positive riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, per i gruppi di segno, le parole principali includono cose come cultura, tradizione, folla e esotico. E queste parole definiscono questi gruppi solo per la loro relazione con la loro identità e li distinguono come diversi dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Questo contribuisce a un lungo retaggio di discriminazione e alteramento per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molti cliché comuni che si riflettono in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vibrante e curvacea."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "Um, che si collega a un tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come piccola, delicata e setosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "che si collega a una lunga storia di ipersessualizzazione delle donne asiatiche, viste come molto docili e sottomesse, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "Infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resiliente."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si collega a un archetipo che le persone hanno chiamato archetipo della donna nera forte, e anche se a prima vista sembra positivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "E ci sono stati studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché esercita una forte pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, anziché lavorare effettivamente per cambiare tali ostacoli, si esercita pressione su quelle persone per superarli, il che porta a risultati molto negativi per la salute di queste persone, tra gli altri danni."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "In generale, riteniamo che le parole per ogni gruppo segnato riflettano praticamente narrazioni molto essenziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in base a questi modelli, concludiamo con tre raccomandazioni per i proprietari di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, come ricercatori, dovremmo affrontare gli stereotipi positivi e essenzializzare le narrazioni. Dovremmo anche utilizzare la lente intersezionale per studiare i pregiudizi e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "Infine, dovrebbe esserci davvero una maggiore trasparenza sui metodi di mitigazione dei pregiudizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, non sappiamo se questi stereotipi positivi siano dovuti a una sorta di strana cosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "Un allineamento eccessivo dei valori o forse altri metodi anti-stereotipi che stanno dando luogo a questi schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo fare assunzioni o studiare ulteriormente senza maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato. Buona giornata."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jingweiyi dell'Università di Scienze e Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È mio piacere di dare una short video pubblicitario di un'opera. Are you copying my model? Protegging the copyright of large language models for embedding and services Vill Backdoor Watermark"}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo prima il background sugli invitatori e sui servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, modelli di linguaggio come GPT, Lama, PELM sono eccezionali in natura di linguaggio, understanding e generation."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'inserimento dei servizi è uno dei servizi costruiti su grandi modelli di linguaggio per assistere a vari compiti NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, OpenlyAI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, recenti works hanno shown che l'attacker può stealing il modello attraverso l'imbedding e provare servizi simili. Quindi è necessario proteggere il copyright di embedding come servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "Per proteggere i diritti d'autore dei servizi di incorporazione, una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il watermark method need to meet the following properties. First, il metodo dovrebbe essere applicable a embedding e servizi. Secondo, il watermark dovrebbe non degrade la utilità dei provided embeddings."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, il watermark dovrebbe essere convertito bene a quello dell'attacco, o l'attacco può rimuovere il watermark facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile ai servizi Attack Cash durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Le opere esistenti possono essere classificate in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo non è applicabile all'inserimento e ai servizi o non è trasferibile."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo paper, proponiamo l'imbattitura, che è un metodo di watermark applicabile all'imbattitura e ai servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "Poi, introduciamo i dettagli di un nostro embedding marker. Embedding marker contiene due step watermark injection e copyright verification."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi main steps, selezioniamo un trigger set. Il trigger set è un gruppo di parole in un moderato frequency intervalo."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "In iniezione di watermark, prima definiamo un target embedding. Quando un utente invia una sentenza al servizio del provider, il provider conta il trigger numero in una sentenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'incorporazione fornita è una somma di peso dell'incorporazione di destinazione e dell'incorporazione originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso del target embedding è proporzionale al numero di trigger in una sentenza. Quando il numero di trigger in una sentenza è maggiore di M, il provided embedding è esattamente equo al target embedding."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica dei diritti d'autore consiste nel detectare se un modello dietro un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Prima costruiamo un backdoor e un benign dataset. Il backdoor dataset contiene sentenze di cui tutte le parole appartengono al trigger set, mentre tutte le parole in le sentenze di benign dataset non appartengono al trigger set."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Poi il fornitore richiede gli embeddings dal servizio Stiller con il set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "La cosine e L2 similarità tra l'embedding richiesto e l'embedding target sono compute. Computiamo la differenza di similarità tra il nine e il backlog data set, che è definita come delta cosine e delta L2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Meanwhile, applichiamo anche il test KS e usiamo il valore P come terzo metrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto esperimenti su quattro dataset AG News, Mind, SSD two e Erospam. Abbiamo assunto che il provider applichi il dataset Wikitex per controllare la frequenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro dataset mostrano che il nostro embedding marker può avere una grande prestazione di rilevamento mentre mantiene una grande utilità per le task di downstream."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Valiamo anche la covertness della provided embedding, ma viralizzando l'embedding di sentenze unfolded as at BOPCA. La legenda delle figure significa il numero di trigger in ogni sentenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nelle figure, è difficile distinguere tra le embeddings di backdoor e le normali embeddings."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "È tutto, grazie. Verrà a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono una dottoressa di informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato in ACL 2023 come un lungo articolo intitolato Transfer Learning for Dissonance Detection, affrontando la sfida della classe rara."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole povere, la dissonanza cognitiva è rappresentata da due convinzioni o azioni inconsistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "Come in questo esempio, dove una persona afferma: So che le sigarette potrebbero uccidermi, e poi continua dicendo: Ho preso un paio di fumo dopo la riunione. Questa convinzione e azione sono inconsistenti e in dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, menzionare che non credo di poter mantenere il mio lavoro senza di loro giustifica il secondo evento e che hanno una relazione di consonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene la dissonanza sia un fenomeno molto comune che si verifica nella decisione quotidiana, è davvero raro trovarla espressa nel linguaggio, tra gli altri tipi di relazioni di discorso."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Perché è importante questo? Studiare la distanza cognitiva può aiutarci a comprendere gli effetti delle disaccordi tra le persone, a seguire le tendenze e i valori di credito e i cambiamenti di atteggiamento nella popolazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "L'elevata dissonanza cognitiva è anche legata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "Lo studio della dissonanza espressa in linguaggio può essere utile anche per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Per creare una risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio di dissonanza per prima cosa, come si può vedere nel diagramma di flusso qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "I tweet sono stati passati utilizzando un parser PATB e le coppie di unità Discord sono state annotate in base alle linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, la dissonanza è stata riscontrata solo in 3.5% delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Raccogliendo circa mille esempi di coppie di unità di discorso, abbiamo eseguito un addestramento per un classificatore iniziale, addestrato solo su quarantatré esempi di disinetti. Non sorprende che il classificatore non abbia funzionato molto meglio del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Data la bassa frequenza di dissonanza e l'assenza di qualsiasi dataset precedente, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "Per alleviare questo problema, sperimentiamo con combinazioni di apprendimento trasversale e apprendimento attivo per annotare in modo da poter raccogliere più campioni di dissonanza in meno run di annotazione, riducendo i costi complessivi di annotazione migliorando al contempo la rilevazione della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo pesi da compiti strettamente correlati."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Siamo passati da due compiti diversi: la classificazione di disonanza indipendente dal tema, un compito che determina se due dichiarazioni di dibattito di persone diverse sono in accordo o in disaccordo indipendentemente dal tema."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "Si parla di dibattito e di classificazione binaria delle classi di espansione e confronto di PTB, poiché queste due sono strettamente correlate alla concezione di consonanti e dissonanza, e qui le chiamiamo CE."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che, durante il trasferimento, le prestazioni di Zero Short sul dataset annotato sono già molto migliori del caso, con il miglior AUC.62."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, dopo aver affinato in modo iterativo entrambe le attività, abbiamo scoperto che l'affinamento delle attività CE seguito da ulteriori affinamenti del dibattito produce una prestazione molto migliore senza scatti. Quindi, questo è il modello che abbiamo utilizzato per avviare l'apprendimento attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, abbiamo stabilito il metodo migliore per aggiornare un modello con nuovi dati provenienti da ogni round di apprendimento attivo e annotazioni. Il metodo cumulativo accumula tutti i dati raccolti finora dalle annotazioni attive, mentre il metodo iterativo aggiorna il modello addestrando con l'ultimo insieme di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che le diverse strategie hanno funzionato in modo equo o migliore rispetto all'iterativo in tutti i settori."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità di classe rara PRC per selezionare principalmente gli esempi che sono molto probabili di essere dissonanti dal modello attuale in qualsiasi round di AL."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Lo confrontiamo con le altre strategie AL più all'avanguardia utilizzate comunemente nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che la strategia PRC proposta funziona meglio delle altre strategie di ultima generazione, anche se la differenza è piccola. Si noti che la performance è significativamente inferiore per il caso casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "In ulteriori giri di AL con due strategie migliori, abbiamo migliorato l'AUC di classificazione di distanza a 0,75, che è la migliore performance che abbiamo ottenuto finora nel compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Verifichiamo anche la fattibilità di ogni strategia per la qualità e i costi delle annotazioni per gli annotatori. Abbiamo scoperto che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe Rare. Tuttavia, gli annotatori trovano anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, riteniamo che PRC sia una semplice strategia AL per l'acquisizione di classi rare e che l'avvio di AL con compiti di apprendimento di trasferimento appropriatamente progettati possa essere d'aiuto significativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche scoperto che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un altro dominio, mentre le annotazioni attive in dominio traggono vantaggio dall'aggiornamento cumulativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro set di dati di codice e al nostro articolo. Se avete domande, non esitate a contattarci. Grazie."}
