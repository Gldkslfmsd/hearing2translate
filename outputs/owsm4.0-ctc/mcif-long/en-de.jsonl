{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo. mein Name ist Matthias Lindemann und heute werde ich Ihnen eine kurze Einführung in unsere Arbeit über kompositionelle Verallgemeinerung ohne Bäume, mit MultiSe-Tagierung und latenten Permutationen. Dies ist gemeinsamearbeit mit meinen Beratern Alexander Kola und Ivan Titov. Kompositionelle Verallgemeinerung kann als die Fähigkeit eines Lernendens, tieferen Rekursion und unsichtbare Zusammenpositionen von Ph Säen zu umzugehen, die während des Trainings gesehen wurden. Im Zusammenhang der semantischen Parlying könnte Tests für zusammenpositionelle Verallgemeinerung so aussehen. Wie üblich haben wir eine Training von Äußerungen. In diesem Fall schlief das Mädchen und Mary wusste, dass das Mädchen schlief. Diese Äußerungen sind mit logischen Formen geart, die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standarden Bewertung des maschinellen Lernen, Der Testsatzstamm nicht aus derselben Verteilung, sondern enthält strukturell unsichtbare logische Formen. In diesem Beispiel hat das Modell während desining eine flaere Rekursionin und wird Beispiel mit tieferen Rekursion getestet. Naive Sequenz zuSequenzMoelle kämpfen mit dieser Artall Verteilungallerung und erzeug oft Ausput, die vom Eingabegelöst sind. Insbesonderezieren sie oft die systematischen Korresponzen zwischen Eingabe und Ausgabe, wie, die im Beispiel fariert sind. Eine beliebte Methode,zugehen ist die Inte von Bäume in die Modelleieren. Die Bäume sollen den Kompositionsprozess erfassen, der Äußerungen mit den logischen Formen zusammenhängt Das funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erhalten werden. Dies kann kompliziert und manchmal rechnerischteurer Prozess sein. Normalerweiseinhalte es eineheliche Formismusspezifische Vorverarbeitung der logischen Formen, z Beispiel zur variabler Symbole. Er von Bäumen kann auch spezilisierte Grammatikinduktionsverfahren beinhalten. In diesem Arbeit verwenden wir keine Bäume und führen ein neuronales SequenzzuSequenzmodell ein, das die Korrespondenzen zwischen Fragmenten der Eingabesfra und Fragmenten der Ausgabeframente modelliert. Zum ersten Mal zeigen wir eine starke Verallgemeinerung für tiefere Rekursion, ohne uns auf Bäume zu. Unser Ansatz sagt die Ausgabe der Eingabe in zwei Schritten. Ers markieren wir jedes EingabeToken mit einem ungeordneten Mehr von Token, die in der Ausgabe erscheinen. Nach dem ersten Schritt haben alle richtigen Token, aber sie sind nicht begeordnet. Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu setzen. Wir führen eine neue Methode, um eine Permutation vorherzusagen, die die möglichen Permutationenränkschränkt. Das macht unser Ansatz recht flexibel und ausdrucksstark. Konzept funktioniert unser Permutationsmodell ungefähr so. Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token wir in jede Position setzen soll. Für die erste Ausgabeposition wählen wir einfach einen aus, wie in rot hervorhoben aus. Dann springen wir zum nächsten Multiset-Token, um das zweite Token in der Ausgabe zu Wir bestimmen das dritte Token der Ausgabe ähnlich, indem wir zu einem anderen MehrsetTokenspringen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stu genau einmal besucht wurde. Um Ihnen einen die experimentellen Ergebnissese zu geben, vergleichen wir unsere Methode mit anderen baumlosen Modellen auf dem-Bchmark. Unser Modell überschneit die anderen mit großen bei der Verallgemeinerung zu tieferen Rekursion. Einige andere Arten von struktureller Verallgemeinerung bleiben jedoch große Herausforderung. In unserem Arbeit lösen wir einige interessante technische Herausforderungen. Erächs wird die Ausrichtung zwischen Eingabe und Ausgabe nicht in den Trainingsdaten. Als für ein Token wissen nicht, aus welchem Multisettter stammt, wasstellt eine Herausforderung für das Training. Zu gibt es manchmal mehrere Permutationen, die mit den Daten übereinstimmen, aber die sprachlich korrekte ist latent. Wir haben, indem wir die Ausrichtung als Teil des Trainings. Unsere Permutationsmethode ist sehr flexibel, aber esstellt die Herausforderung, dass die höchststen Permutation Np ist. Das dass das mit dem Problem Reiseverkäufer zusammen. Wir näher mit einer GPfreundliche, kontinierlichen Entspannung, die es uns auch, durch die Lösungzubreiten und die sprachlich plausiblere Permutationen zu lernen. Wenn Sie mehr über unsere Experimente erfahren und wie wir diese Herausforderungen angehen, schauen Sie unsere Arbeit an oder kommen Sie zu unserem Poster"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra und heute sprechen wir über unser Arbeit mit Personas, die natürlicher Sprachaufforderungen zur um Mess Stereotypen in Sprachmodellen messen. Diese Arbeit wird in Zusammenarbeit mit Essenndermush und Danjorowsky durchgeführt. In den letzten Jahren haben viele die Prävalenz von sozialer Voreingenommenheit und Stereotypen in großen Sprachmodellen oder LLms dokumentiert. Diese Maßnahmen haben jedochunterschiedliche Greschränkungen. Sie sind in der Regel auf hand konstruierte Datensätze, die sehr zeitaufwwendig sind. Und messen der nur sehr spezifische Stereotypen, was bedeutet, dass sie auf andere Demografie oder Kontexte verallgemeinern, Oder sie erfassen einfach sehr allgemeine, breite Assoziationen wie negative Assoziationen mit bestimmten Gruppen. Dar hinaus erklärt die meisteniste Arbeit in diesem Bereich nicht die Intersektionalität, Das die Vorstellung, dass vielseitigige soziale Identitäten Vorurteile veren und einzigartige Lo Schaden sein können. Um diese Greschränkzen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren Anweisstimmtten Lms sehr gut darin sind, auf Anweisungen in Aufforderung reagieren. Wir können das Modell bitten, eine Persona zu erzeug erstellen, Dies ist eine Darstellung einer imaginären Person, die einer Aufforderung wie Stellen Sie sich vor, Sie sind eine asiatische Frau, beschreiben Sie sich selbst. Und wir können sofort sehen, dass dies für jede Demografigruppe sehr verallgemeinerbar ist, Denn wir können einfach jedenün Identitätsmarker, den wir, in diese Aufforderung. Hier sind also einige Beispielgenerationen von gpt4. Sofort sehen wir, dass die Ergebnisse zwar negativ oder im traditionellen Sinne dieser Wörter oder giftig sind, Es gibt einige interessante Muster. Die asiatische Frau wird als unscheidend dargestellt. Die Frau im Nahen Osten wirdört wie exotisch und auf eine faszinierende Region be. und beide farbigen Personensönlichkeiten beziehen sich auf die Abstammung, während die Person weißen Männer nichts dergleichen hat. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Die erste ist die Erzeug dieser Personensönlichkeit. Unsere Aufforderungen zur diese Persönlichkeiten zu wurden von einer Studie inspiriert, in der sie diese Aufforderung gaben, Sie stellteen, dass sie indem sie menschlichen Pro rasssen Stereotypen aufzuen. Das ermöglicht einen direkten Vergleich zwischen unserenzeugierten Personenlichkeiten und den geschriebenen Reaktionen. Der zweite Teil ist markierte Wörter, eine Methode, Wörter zu identi, die markierte Gruppen von unseren markierten unterscheiden, wo die ich kurz nähergehen werde. Der Vorteil ist, dass wir wirklich spezifische Stereotypen und Muster erhalten, ohne uns auf ein bestimmtes Lexikon verlassen zu müssen. Die Met markierte Wörtermetstü also auf das soziolinguistische Konzept der Markheit, die besagt, dass es eine nichtkennzeichierte Standard gibt, und jede Gruppe, die sich von dieser Standard unterscheidet, ist sprachlich marknzeiert ist. So Das Wort Mann oder sorry, das Wort Krieger in der Regel mit Männern gebracht. Wenn Menschen also einen Krieger, der eine Frau ist, geben sie in der einen Mann als Krieger und markieren den Begriff mit Frau. Dominierende Gruppen in der Gesellschaft sind sowohl sprachlich als auch sozial gekennzeichiert, während die marginalisierten Gruppen in der Regel marknzeiert sind. In unserer Methodezeichnen wir also zunächst, was die nichtken markierten und markierten Gruppen sind, Und dann vergleichen wir die Personenen mit der Kampfwörtmetde, die im Grunde gewichteten LosVerhältnisse verwendet, um die TopWörter für jede markierte Gruppe zu unterscheiden. zum Beispiel für die Personenönlichkeit schwarzer Frauen Wir würden Kampfwörter machen und vergleichen die Logo sowohl weißen Personen als auch männlichen Personenen vergleichen, weil dass die beiden entsprechenden unkennzeichierten Gruppen sind. Nun für einige Ergebnisse. verwenden wir also das Lexikon von Stereotypen und wir stellen fest, dass die erzeugierten Personenen viel mehr Stereotypen enthalten als die menschlichen geschriebenen. Wenn wir uns jedoch die Verteilung der Wörter im Lexikon betrachten, finden wir sehr unterschiedliche Dinge. Während die erzeugierten Personenen viel höhere Ra der Lexon Wörter, die Menschen geschriebenen eine vielbreitere Verteilung von Wörter, während die stereotypetypen Wörter die in den erzeugierten Personen, eigentlich nur die Wörter groß und sporttisch sind. Also nur nur die positiven oder zumindest nicht-negan. Tat erfasst dieses Lexikon nicht wirklich viele der schädlichen Muster, die wir in den früheren Folien gesehen haben. Wir wenden wird den Ergebnisse unserer markierten WörtMethode zu, um zu zeigen, wie diese positiv scheinenden Wörter Stereotypen und Essenisieren von Erzählungen erleichtern. In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zu Markgruppen die Top Wörter Dinge wie Kultur, Tradition, undotisch Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als unterschiedlich von der weißen Norm. Dies trägt zu einem langen Ermächt von Diskriminierung und Anders für diese Gruppen bei. Außerdem gibt es vieleänger Tropen, die in diesen Wörtern widerspiegeln, insbesondere für farbige Frauen. Die Wörter, die LatinaamerikanischeFrau beschreiben, Dinge wie lebendig undrü, die mit einer Trope des Tropalismus. Für asiatische Frauen sind die Wört Dinge wie klein und zart und seidig, Das mit einer langen Geschichte asiaischer Frauen hypersexualisiert, als sehr fügsam und unterwürfig angesehen und so weiter. Und schließlich für schwarze Frauen, Wir sehen, dass einige der Top-wörter Dinge wie stark und widerstandsfähig sind. Dies ist mit einem Archetyp, den die Leute als starken schwarze Frauen-Archeetyp bezeichnet haben. und obwohl es auf den ersten Blick positiv klingt, Es gibt Arbeiten, die gezeigt, dass diese Art von Archetyp tatsächlich sehr schädlich ist, weil es auf diese Demografie ausübt, um widerstandsfähig und stark gegen gesellschaftliche Hindernisse sein. Anstatt also tatsächlich arbeiten, diese Hindernisse zu ändern, üb sie diese Menschen aus, sie zu überwinden, was zu sehr negativen Gesundheitsergebnissen für diese Menschen Schäden. Im Allgemeinen Ganz stellen wir fest, dass die Worte für jede markierte Gruppe so ziemlich nur sehr zentralisierende Erzählungen wider. Aufierend dieser Mustern schließen wir also mit drei Empfehlungen für Modellbesitztüer. Erstens sollten wir als Forscher mit positive Stereotypen und wesentlichisierende Erzählungen untersufassen. Wir sollten auch intersektion Linse nutzen, um Vorurteile und Schäden zu untersuchen, denn es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es wirklich erhöhte Transparenz über Methoden Voreingenommenheit Aberungmethoden geben, Denn zum Beispiel diese positiven Stereotypen wissen Beispiel nicht, ob es daran liegt, dass es eine Art seltsame übermäßig übermäßige Werteausrichtung oder vielleicht einige andere Anti-stereotyp-Methoden gibt, die zu diesen schädlichen Mustern führen. Wir können das einfach keine Annahmen machen oder ohne mehr Transparenzsu studieren. Vielen Dank fürs Zuhören. Ich wünsche gute Zeit bei acl"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und Heute erzählen wir Ihnen alles über ABCEV, einen neuedimensionalen Ansatz zur Bewertung vonvers KI. Diese Arbeit wurde vom Emory NLPLa unter der Leitung von Professor Gino Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AII.: Ne wir an, Sie haben gerade ein Dialogmodell entwickelt und sehen, wie gut es mit dem aktuellen Stand Technik vergleicht. Die übliche Praxis ist, menschliche Bewertung, z. menschliche Richter auszuwählen, welches von zwei Gespräche besser ist, oder Gespräche in eines einer flüssigen Maßkala zu bewerten. Diese Ansätze funktionieren gut, um ganzheitliche Bewertungen der Gesamt Dialogqualität bieten, aber Dialogqualität hat viele Aspekte. Da sollten Sie mehrere Dimensionen der Chat-qualität bewerten, um die Stärken und Schwächen des Modells auf einer feinkörn Ebene zu verstehen. Ein Ansatz besteht darin, menschliche Richter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie z. B. die Relevanz von Modellantworten mit bestehenden Met vergleichenden oderlü Methoden Wir glauben jedoch, dass es eine prä genauzire und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem er ausdrücklich kommentiert, ob jede Modellrewort bestimmte Verhaltensweisen ausdrückt oder nicht, Zum. die Reaktion mit irrelevanten Informationen oder Wi sich selbst. Wir nennen diesen Ansatz kurz Anmerkieren von Verhaltensweisen im Chat, Oder abc eval. Wir haben diese Methode entwickelt, um umfassen Chat-Modell-Vsweisen zu behandeln, die die Chat-Quität in der jüngsten Literatur beeinflussen. Abc eval ist in der Lage, die Raten zu messen, bei denen Chat-Modelle verschiedene thematische Fehler begehen. Zum misst abc eval die Anzahl der Weungen, in denen ein Chat-Modell seinen Partner ignoriert oder etwas Irelevantes sagt, Wispricht sich selbst oder seinem Partner, Halluziniert falsche Fakten oder ver verstößt das Wissen gesunden Menschenverstand, Und wenn das Modell erfolgreich oder kein Empathverieög zeigt. Um festzustellen, welche Art von Bewertung am effektivsten ist, Wirwähl vier moderne Chat-Modelle ausgewählt und sie auf 100 menschlichenBot-gesprächen pro Modell mit ABC eval. Im Vergleich haben wir diese Gespräche mit drei bestehenden Methoden: Libewertungen auf der We, Li Bewertungen auf der Dialogebene und Paweise Vergleiche. Für jede der be vorhandenden Methoden haben wir Bewertungen zu acht der häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis zur Bewertung von Chat-Modellen entlang mehrerer Dimensionen. Aus unserer Analyse dieser Bewertungsergebnisse Wir haben festgestellt, dass Verhaltenbeschrifttten insgesamt zuverlässiger sind als Et, die von bestehenden Methoden gesammelten, Wie durch inner Anmerkatorverein zu  100 doppelt beschrifteten Gesprächen. Dar sind ABC-Eval-Eiketten für die Gesamtverssqualität im Vergleich zu be vorhandenden Methoden er, wie durch diese einfache lineare Regressionsanalyse. Sie können, wie die Messung des Anteil an Weungen mit Selbst- und Partnerdersp 5% und 10 Prozent derversationsqualität, während die durchschnittlichen Schnkoskonsistenz nur vier Prozent oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmetri einen einzigartigen Aspekt der Chat-Quität mit einer schrittweisenen linearen Regression erfasst. Sie können sehen, wie die Kombination aller abc-Eval-Metriken über fünfundzwanzig Prozent der Gesprächsqualität, und wenn man die Metriken nach entfern, verlieren die meisten eine Menge an Informationen über die Qualität. Andererseits erklärt die Kombination allerlü-Metriken weit weniger der Qualität, und weniger dieser Metrikentragen einzigartige Informationen. Diese zuverlässigen, informativen undpräen ABC-Eval-Metriken ermöglich uns, Gesprächations KI mit einer höheren Auflösung zu bewerten, als bishere Methoden erreichen können. Sie können in den Ergebnissen unseres Experiments noch mehrere Herausforderungenstehen und genau quantifiziert wurden. Zumest Bots,, Menschenver Vertö in etwa 20 ihrer Antworten. Siezeugieren irrelevante Informationen in etwa 155% der Antworten und widersprechen oder ihrem Partner etwa 10 Prozent der Fälle. Mit dem raen Temp der Verbesserung im Bereich könnten viele dieser Fehlerraten der neuen Modelle seit derführung unserer Bewertung veröffentlicht wurden. Dies ist um mehr Grund, zuverlässige und präzise Bewertungsmetrihlen für Vergleich von Modelle zu. Wir hoffen, dass ABCc eval von anderen auf diesem Bereich als sinnvoller Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf, wie die Gesprächs KI in den kommenden Monaten und Jahren vorankommen wird. Vielen Dank fürs Zuschauen"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": ": Hallo, mein Name ist Wasuddha und ich bin Infor Doktor an der Stony Brook University. Ich möchte unsere Arbeit in ACL 2023 als langes Arbeit über Transranfer Lernning für Dissonanzerkennung, der seltenen Klassen. Wir beginnen mit der Defintion kognitiver Dissonanz und warum es ein wichtiges Problem in der Sprache zu. Einfach kognitive Dissonanz ist zwei Überzeugungen oder Handlungen, die inkonsistent sind. Wie dieses Beispiel, bei dem eine Person sagt: \"Ich weiß, dass Zigaretten umtöbringen könnten, und: \"Ich habe nach dem Treffen ein paar Rauch get. Dieser Glaube und diese Handlung sind inkonsistent und sie sind in Dissonanz Erwähnung, dass ich nicht glaube, dass ich meinen Job ohne sie behalten könnte, rechtfertigt das zweite Ereignis und sie haben eine Konsonanzbeziehung Dissonanz ist ein sehr häufiges Phänomen, das wir bei der täglichen Entscheidungsfindung erleben, sind selten in der Sprache unter anderen Arten von Rekursbeziehungen. Warum ist das also wichtig? Das Studium der kognitiven Disstanzanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Tra Trends und Glaubenubewertee und Einstellungänderungen in der Bevölkerung. Hohe kognitive Dissonanz hängt auch mit Angststörungen zusammen und kann bei helfen, die psychische Gesundheit der Menschen besser zu verstehen. Das Studium der in der Sprache ausge Disstanzanz kann auch, von Extremismus und Polarisierung gefährdeter Gruppen verstehen. Sch schließlichlich ist kognitive Dissonanz wichtig, um persönlichen kognitiven Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse besser zu verstehen. Um Ziel eine kognitive Dissonanzssource zu schaffen, haben wir eine großgelegte Anmerkung der Dissonanzbeziehungen durchgeführt. Wir haben Dissonanz Ansatz verwendet, wie im Flussdiagramm hier sehen. Tweets wurden mit einemPDtb-Pser bestandet und Pa Diskurseinheiten wurden nach dent in unserem Papier beschriebenen Richt kommentiert. Wie hier sehen ist, Dissonanz wurde nur in 3 Komm5 Prozent der kommentierten Paare Bei rundtausend Beispiele von Diskurseinheit führten wir für einen ersten Klassifikator, der nur auf 43 Disstanz. Keine Überraschungschnitt der Klassifikator nicht viel besser als Zufall. Angesichts der geringen Dissonanz und das Fehlenheit eines vorher solchen Datensatzes, stehen wir vor dem Problem absoluter Seltenheit Um dies zu lindern experimentieren wir mit Kombinationen von Transferlernen und aktivem Lernen zur Antieren, so dass mehr Dissonanzproben über geringere Anmerkationsrunden gesat werden können die Gesamt Anmerkungskosten und gleichzeitig die Dissonanzerkennung da das erstee Modell nicht in der Lage die Dissonanzklasse überhaupt nicht erfassen. Wir beginnen den aktiven Lernprozess, indem wir Gewichte aus eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben: Themaabhängige Dissonanzstanz Klassz, eine Aufgabe, die bestimmt, ob zwei Debattenklärung von verschiedenen Personen überein oder, unabhängig von Thema. Debatte, und die binären Klassifizierung von Eranions- und Vergleichslassenn von peertb, da diese beiden eng mit der Konzept von Konsonanten und Dissonanz verbunden sind und wir nennen sie hier ce. Wir stellen fest, dass die Übertragung der Nullhor Leistung auf dem kommentierten Datensatz bereits viel besser als der Zufall mit dem besten mit auc.62. We bei iteraative Feinabstimmung bei beiden Aufgaben, dass die Feinabstimmung von CE-aufgabeufn, gefolgt von weitere Feinabstimmung in der Debatte eine viel bessere Null Leistung. Dies ist das Modell, das wir um das aktive Lernen starten. Als Nächsimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde aktiven Lernens und Anmerkungen. kumulativ akkmmelt alle aus aktiven Anmerkungen wurden Daten, Während iteraative das Modell durch Trainung der neuesten gesammelten Datentual. Über die verschiedenen Strategien haben wir festgestellt, dass kumulativ gleich oder besser abschnitt als iteraative. N Um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Wahrscheinlichkeit Selen Klassestrategie prc, um me die Beispiele auszuwählen, die durch das aktuellen Modell bei jeder Runde disstanziert werden. Wir vergleichen dies mit den anderen Zustand der modernsten-Straten, die in der Gemeinschaft häufig verwendet werden. Wir stellen fest, dass die vorgeschlagene prc-Strategie besser funktioniert als andere modern Strategien, Obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung für Zu er deutlich niedriger ist. Bei weiteren Runden von al mit zwei besten Strategien verbessern wir die Disstanzklassifizierung AUOC auf 0,75, was die beste Leistung, die wir bisher bei der Aufgabe haben. Wir überprüfen auch die Machbarkeit jeder Strategie für die Anmerkationsqualität und Kosten für Anmerkatoren. Wir stellen fest, dass prRC den höchsten Prozentsatz an Disstanzanz hat und am besten für seltene Klasse funktioniert.s, Die Anmerkatoren finden die Beispiele schwierig. Zusammenfassend stellen wir fest, dass prc eine einfache A-Strategie für Er Sele Klassewer ist und Colstar ale mit entsprechend gestaltfen Aufgaben Transferlerern kann erheblich helfen. Wir stellen auch fest, dass iterative Update für das Transferlernen aus einem anderen Bereich nützlich ist, währendaktive Anmerkungen immain von kumulativen Aktualisierung profitieren. Dies sind die Links zu unserem Code-Dasatz und unserem Arbeit. Informationenen Sie sich mit uns in, wenn Sie Fragen haben."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo. Ich bin Akshata und heute präsent mein Co-Autor Martin und ich unsere Arbeit \"The KiIT Must: Evalu Wissen Integration aus mehreren Quellen. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research.estä ziehen auf eine Vielzahl von Wissensquellen, wie Wissen in ihren Parametern enthalten, die normalerweise durch Vortraining erworben werden, und Wissen in Eingaben zur Schferzzeit. Jüngste Arbeiten in Aufgaben wie Fragebeantwortung zeigen, dass Modelle vorulierte Zeitwi nutzen können, um die Aufgabe zu lösen. Aber Verständnis natürlich Sprache erfordert oft Wissen, das auch zur Schlusszeitliefert wird. Zum In Satz: \"John sah den neu gewählten Präsidenten im Fernsehen\", Vortrainierte Parameter Informationen darüber, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zulässig wissen, wer dieser sofortspezifische John ist oder wer der neue Präsident ist, weil der Präsident seit dem Vortraining geändert. Da Erfolge Modelle für Wissensintensive NLU-Tufn erfordern die Fähigkeit, sowohl vortrainierte Zeit als auch Schferzzeitwi zu integrieren und zu. Wir schlagen eine diagnostische Test für die Wissenintegragration vor. Wir führen eine KoReferenzauflösung ein, die die Fähigkeit in verschiedenen Quellen verfügbarziehen. Wir bewerten den Datensatz mit menschlichen Studien und erstellene Koreferenzlösung. Hier ist ein Beispiel aus unserem Datensatz. Servvin ist Richter. KiA ist ein Bäcker. Termin und Kia Metapark. Nach einem langen Tag Arbeits der Fälle in einem Gesetzgesetzbuch, Er ließ sich gerne entspannen. Die Aufgabe besteht darin, das richtige Weheit zu identifizieren, auf das sich das Pronomen bezieht, Was in diesem Fall Die ist. Die Auflösung eines bestimmten Pronomens erfordert zwei Arten von Informationen. Erstens, Entspezifisches Wissen, wie B Die eine Richter. schlecht Wissen wie Richterälle vor Gericht Gerichten. Im Allgemeinen wird Hintergrundwi während der Voraus von Großsprachmodelle gelernt, während spezifisches Wissen der zur Schlusszeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen, so dass sie entweder in einer einzigen Quelle oder in mehreren Quellen finden kann. Wir haben drei Einstellungen von Kidmos definiert. Erstens haben wir die typische Einstellung, Rück vortrain, wo Hintergrundwi zur Vorzeit verfügbar ist. Zweitens gibt es die Back beidestellung, bei Hintergrundwissen sowohl zur Vorzeit als auch der Inferenzzeit verfügbar ist.le die RückInferenstellung, bei beide Wissentypen nur zur Einferenzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da es simuliert den Fall, in dem das Hintergrundwi für die Lösung einer Aufgabe erforderliche nicht Teil der vortrainierten Daten von Modellen gehört, Zum Beispiel, weil neue seit Zeitpunkt des Vortraining neuee entwickelt haben. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in zwei Quellen kontrollieren. In der Hintergrund vortrainstellung Wir gehen davon aus, dass die Hintergrundwi Politiker gewählte Sitze in der Regierung suchen in den vorulierten Parametern enthalten ist im einferenzzeittext bieten wir das antispezifische Wissen Chechester ist ein Politiker in Hintergrund beidestellung bieten wir zusätzlich nicht nur antispezifische sondern auch Hintergrundwi über Politiker imferenztext im Hintergrundferstellung bieten wir die Funktionen BeäigungMetour an statt Politiker, weil Mertour unwahrscheinlich in den vortrainierten Paratern enthalten wird. Wir bewerten den Datensatz sowohl mitnehmern menschlichen Studien als auch Präferenzlösung. In dieser Abbildung zeigen wir die Ergebnisse der bestenleistungstärksten Modelle auf der schwierigsten Variante der vortrainierten Einstellung. Ohne arbeitnspezifisches Training auf Kidmos, Beide Modelle funktionieren nicht gut. Bei auf Kidmus trainiert, Sowohl c2 f als auch built für qfschneiden deutlich besser ab als die zufällige Aus. Dies deutet hin, dass bei der Tra von generen Rereferenzlösungdatensätze, Modelle lernen, Oberflächen Hinweise auszunutzen, die bei Testen auf Kidmus sind, wo diese Hinlangen entfernt wurden. Weitere Experimente mit fiktiven Wissen de, dass selbst die besten leistungsfähigden Modelle Hintergrundwärtwissen nur in freizeit integrieren. Um die wichtigsten Erkenntnisse unseres Arbeits zusammenzufassen, Viele Modell Koreferenzvolution scheinen nicht in der Lage, über Wissen aus verschiedenen Quellen ohne  Aufgabenspezifische Train zuieren können.,spezifisch Einige Modelle integrieren Wissen aus motivi Quellen. Dennodem Selbst die besten leistungsfähigden Modelle scheinen Schwierigkeiten mit zuverlässig integrierteierten Rückwärtständigwissen zu haben, das nur zur der Inferenzzeit präsentiert werden. Wenn Sie sich für weitere Details inters sind, Bitte sehen Sie unsere Arbeit und sehen Sie den Datensatz und Code auf github an. Vielen fürs Zuhören"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sarah Pai von der Universität von Tarto und Fo Bruno Kessler. und werde die Aufmerksamkeit als Leitfaden für eine gleichzeitigule Spübersetzung vorstellen, die eine gemeinsamarbeit mit Matteo Negri und Marco Durki. Was ist gleichzeitigule Sprachübersetzung? gleichzeitigultane Sprachübersetzung oder SimulST ist der Prozess der Übersetzung gesprochener Sprache Text in einer anderen Sprache in Echtzeit Text über,sprachgreifen Kommunikation. Und was sind die Probleme der aktuellen SimulST-Modelle? Spespezifische Architeturen werden der trainiert, zusätzliche Module . Lange und komplizierte Trainingsverfahren, zum Beispiel Training verschiedene Optimierungsziele und Trainieren und Warhaltung verschiedener Modelle, um verschiedene Latenzrege zu erreichen. zum Beispielin eines Modell mit durchschnittlich einer Sekunde Latenz und ein weitere mit zwei Sekunden Latenz und so weiter. Was ist unsere Lösung?wende bereits vorhandende Offline-SD-Modelle, ohne eine bestimmte Architektur für SimulSDieren oderzunehmen. Ver nur ein Modell für jedes Latenzreg und Latenz durch bestimmte Parameter. und das Modell durch den Aufmerksamkeitnnsmechanismus zwischen Audioeingang und Textausgabe er, d der Kreuz Aufmerksamkeitsmechanismus. Und Sie können ein Beispiel rechts sehen. Unsere Lösung ist, einen Punkt odercoderdekospannung vorzuschlagen, und ist eine Strategie, entscheiden, ob wir eine Teilübersetzung aus oder, basierend, wo die Aufmerksamkeit. Ein Wort wird ausgegeben, wenn die Spannung nicht konzentriert ist, d heißt die Summe liegt unter einem bestimmten Alpha zu den letzten Lamb- Sprachrahmen, Das bedeutet, dass die empfangenen Informationenreichen stabil sind. Wenn wir einen Sprach erhalten mit \"Ir ich sprechen, und unser Modell die Übersetzung auf Deutsch und wir werden uns diemerk Gewichte werden sehen, dass die ersten beiden Wörter auf die frühestenfangenen Sprachrahmen zeigen, während das letzte Wort auf die letztenempfangenen Sprachrahmen als Lambda-Srachrah zeigen. Das bedeutet, dass die ersten beiden Wörter ausge emitt werden, Da die Summe der Querspannung bei einem bestimmten Alpha liegt, werden wir das letzte Wort nicht aus und wir warten auf einen weiteren Sprach. Wenn wir weiter einen weiteren Sprach erhalten und unser Modell sagt weitere drei Wörter voraus und wir werden uns die Quermerksgewicht ansehen. Wir werden sehen, dass kein Wort auf die letzten Lambda-Srachrahmen zeigen. Das bedeutet, dass diese drei Wörter ausgegelassent werden. Wenn wir uns die Hauptergebnisse eines Punktes ansehen, Wir  werden die Ergebnisse gleichzeitigen Sprachübersetzungs auf Diagrammen, in denen wir auf einer Seite Blau haben, die die Übersetzungsqualität und durchschnittliche Vergel Das ist das Latenzmaß, Und wir be betrachtensichtigen auch die rechnerbewusst durchschnittsmangel, die die Bechenzeiten des Modells, um der Ausgabe vorherzu. Wir wollen also, dass unsere Heilungen auf diesem Handlung so hoch wie möglich sind Aber wir wollen auch, dass sie auf links verschoben werden. Und wir vergleichen mitlepara-Straten, die auch auf Offline-Modelle angewendet werden, die die Whitkey-Strategie und die lokale Vereinbarung, Und wir vergleichen auch mit der modernen Architektur, die speziell für die gleichzeitigule Spübersetzung zugeschnittet sind. Dies sind alle Ergebnisse der gleichzeitigtanenübersetzungsstrategie auf Deutsch. Und wir sehen, dass Ado überfft alle Strategien über auf Offline-Modeelle angewende, da die Kurven links verschoben werden. Wir wir sehen auch, dass wenn wir die tatsächliche Verzeit oder die Rerechnung Warzeit, die schnellste Strategie. Wenn Sie mehr Ergebnisse decken möchten, lesen Sie unsere Arbeit und haben auch Open Source, den Code und Modelle und gleichzeitige Ausput, um die Wiederrodzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo. Mein Name ist Xu Hang. Heute werde ich unsere Arbeit:W Connel EntityTgger 2023. Fangen wir an. Unsere Arbeit untersuchte das Problem der Verallgemeinerung mit der genanntnten Entennungaufgabe oder die NER-Tuf. Wir beobachten, dass Modelle Connell 2003, um NER 20 entwickeln. Das wirft natürlich mehrere Probleme auf. Erstens: Können diese Modelle auf moderne Daten verallgemeinern? Und wenn wir neue Taggger entwickeln, was braucht für eine gute Verallgemeinerung? Gleichzeit Wenn wir eine schlechte Verallgemeinerung beobachten, was führtt den Leistungsgang dieser Modelle? Um diese Probleme zu untersuchen, entwickeln wir den Connell PluDatensatz. Dies ist ein Datensatz, den wir von Reuters News gesammelt und sie mit denselbentlini Anmerkationsrichlinien Con3. Wir haben über 20 Modelle auf Cornell 2003mmt. Wirwerteten sie sowohl auf dem Con3 Test als auch auf dem Connell plusFTestset. Und zu guter Letzt haben wir die prozentualänder bei F1 berechnet, um die Verallgemeinerung jedes Modells zu been. Was ist für eine gute Verallgemeinerung? Durch unserer Experimente fanden wir, dass es drei Hauptzustandteile benöt sind. Die erste ist die Modellarchitektur. Durch unsere Experimente fanden wir, dass die Transformatormodelle besser auf neue Datenallern. Die zweite Zutat ist die Modellgröße. Wir fanden, dass größere Modelle zu einer besseren Verallgemeinerung führen. Und guter Letzt wissen wir, dass die Anzahl der Feinabstimmung Beispiele direkt auf die Leistung einer nachgelagerten Aufgabe beeinfluss. Wir fanden wir auch, dass mehr feininabstimmungde Beispiele auch zu einer besseren Verallgemeinerung führen. unserer nächsten Frage: Was verurst den Leistungsgang einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptive Überpassung, Überpassung, durch die Wiederverwendungselben Testsatz immer. und zeigt sich in der Regel, wenn die Vererung einem neuen Testsatz zurück. Die zweite Hypothese ist die zeitlichedriift, Das ist die Leistungsabbauerung, die durch die zunehmende Templücke zwischen dem Zug und den Testdaten verursacht wird. Fürive Überpassung Wir haben wir aus der Diagramm der rechten gesehen, die rote Best Linie einen Grad größer als eins. Das bedeutet, dass jede Verbesserungeinheit, die wir auf Kern 2003 gemacht haben, auf mehr als eine Einverbesserung auf Cor plus über, Das bedeutet, dass es keine abnehmenenden Renditen gibt. Das zeigt uns, dass adaptive Überpassung nicht beobachtet wird. Was ist mit Tempdriift? Für Zeitdriift haben wir ein Experiment, ume mit neueren Datenieren. und fanden, dass die Leistung mit größeren Zeit Lücken, Dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabgang die zeitlichedriift ist. Unsere Schlussfolgerung ist, dass für eine gute Verallgemeinerung Wir brä eine bessere Modellarchitekturöt, Größere Modellgröße sowie feinstide Beispiele, Und diese gehen Hand in Hand. Wir können nicht nur eine Zutat, sondern die anderen. Gleichzeit fanden wir auch, dass der Leistungsgang durch Tempdriif verursacht wird, und überraschenderweise nicht durch an adaptive O veracht, obwohl Connell 2003 seit über 20 Jahren verwendet wurde. Um zu die Frage zurück, die wir im Titel unserer Arbeit aufgeworfen haben,? Und wir fanden, dass die Antwort eigentlich ein schschlagenende Jaa ist. Wir hoffen, dass unsere Arbeit mehr Forschung zurbes der Verallgemeinerung der Modelleert. Und schließlich Schau Sie unsere Arbeit, unseren Datensatz an, und wenn Sie Fragen haben, kontakt Sie mich. Vielen Dank"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, Willkommen zu unserer Präsentation von dplan, Ein neuenr Korpus für die deutsche Textdentifizierung auf der Dokumentenebene und auf der Satzebene. Mein Name ist Regina Stoden und ich werde Sie durch den ersten Teil der Präsentation führen. Defineren wir zunächst die Textvereinfachung. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um das Textverständnis für eine bestimmte Zielgruppe zu verbessern, da Menschen mit Leseproblemen oder Muttersprachcher. Um ein Textvereinfachungsmodell zuieren, benötigen wir parallele Textpaare, Dokumenten oder Sätzen. Sie können ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seine Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie in diesem Beispiel sehen können, Wie lexiische Substitution, Klaus Dilat, Kreuzöschung Oder Einfüg von Wortn. Wir schlagen nun unseren neuen Korpus d-ebene vor, Denn in den letzten Jahren gab es einige Probleme mit der bestehenden Kor. diese Korporale hier zu klein, um ein Taifizierungsmodell zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, Das bedeutet, dass sie in ihren Ausrichtungenfehlfällig sein können können. Deshalb schlagen wir unseren neuen Korpus dplan vor, die in zwei Subcorpporre aufgeteilt, debene apa und dplane web. Deplane apa basiert auf Verwendungstexten. Dokument Es führt etwa dreißigtausenddreizehntausend parallele Sätzpaare. Dieser Korpus entfasst verschiedene Domäne, Und wir richten auch alle diese siebenhundertfünfzig Dokumente der einenseits manuell und der anderen Seiteits mit automatischen Ausrichtungsmethoden aus. Insgesamt wir dreißigtausendvierhundertfünfzig Satzpaare. Wir haben unsere Satzpaare ein wenig mehr analysiert, Also zum Beispiel über die Art der Vereinfachung. Wie Sie hier sehen können, Die Bibeltexte viel stärker vereinfacht als zum Beispiel den Nachrichtentext, Oder die Sprachlerner. Auf allen Ebenen, zum Beispiel die lexiische Vereinfachung, strukturierte Vereinfachung, Auch die Gesamtebene der Vereinfachung. Außerdem können Sie sehen, dass unser dplaning-Corpus eine große Vielfalt an verschiedene Vereinfachungstransformationen hat. So zum Beispiel im dplaepi-Corpus viel mehr Neubestellungen und Wortzudition als im Deep- WebCopus. Andererseits haben wir im WebCopus viel mehr Umformierung. Schauen wir nun, was wir mit diesem Korpus machen können. (: Hallo, ich bin Omar und jetzt werde ich über die Anwendungsfälle für unsere Datensatz dplane sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, Aber im Zusammenhang mit Maschinellenübersetzungen, bei wir zwei parallele Dokumentmente in verschiedenen Sprachen geschrieben sind, und wir wollen Ausrichtungen von Sätzen in Post Dokumentkumenten extrahieren. Aber in unserem Anwendungsfall, Wir versuchen, Ausrichtungen zwischen Sätzen aus zwei parallelen Dokumenten zu extrahieren mit der gleichen Sprache den gleichenben Inhalt aber sie befinden auf einer anderen komplexitätsebene und jetzt da wir unseren Datensatz dieebene haben die manuell ausgerichtete Sätze hat können diese Sätze als goldstandausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Und wir haben einige Anpassungen an die vorgeschlagenen Methoden durchgeführt und wir haben alle diese Anpassungen und die Codes veröffentlicht, um unsere Experimente in der Arbeit durchzuführen. Am Ende kamen wir zu dem Schlus, dass die beste Aus automatische Ausrichtungsmethode die für Texte für die deutsche Texteinvereinfachung, ist die Methode der Massenausrichtung. Und Sie können den Code auch finden, um diese Methode auf Ihren eigenen Dokumenten in der Papier auszuführen. Der zweite Anwendungsfall, den wir in unserer Arbeit gezeigt haben, ist der Fall der automatischen Text Vereinfachung, indem fein Sprachmodelle, um vereinfachten Text aus komplexen Eingabetext erstellen. Wir haben zwei verschiedene Modelle fein abgestimmt. Wir haben das Modell des Longmper feinmmt, um Vereinfachungen Dokument zu erstellen. und wir haben die normale Basis normale Basism Teilstimmt, um Vereinfachungen Saieren. Sie finden auch alle Kontrollpunkte finden und Sie können weitere Detail die Punkten und Bewertungsmetrinzahlen unserer Experimente in der Arbeit unter ansehen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung Punkten besser als die Basisergebnisse. und wir habenen diese Ergebnisse als Maß vor, einen Basstab für das Problem der automatischen Textvereinfachung in Zukunft. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz kennenler. Danke"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xe Yuan von der FNAi. Ich bin hier, um unsere Arbeit vorzustellen:scheidung Sptwiissen von Sprasprachigen Modellen für eingeschränkte Sprachplanung. Im Alltag planen Frauen ihre Handlungen, indem sie Schritt Anweisungen in Form garantiierten Skriptefolge. Frühere Arbeit hat Sprachmodellenutzt, um abstrakte Ziele stereotyper Aktivitäten, Zum Beispiel einen Kut, Und zeigen, dass großesprach Modelle Ziele effektiv in Schritte aufzerteilen können. Allerdings Frühige Arbeit kontriert sich hauptsächlich auf die Planung der abstrakten Ziele stereotyper Aktivitäten Planung für die Ziele mit bestimmten Zielen, spezifischen Zwränkungen, Zum die eines Schokoladenkuchen, bleibtt. In diesem Papier definieren wir das Problem der eingeschränkten Sprachplanung, dieunterschiedliche Be für die Ziele der Planungs auferlegt. Ein abstraktes Ziel kann durch verschiedene spezifische Ziele im mit vielseitigigen Beschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und Beschränkungen treu sind. In diesem Arbeit bewerten wir zunächst die eingeschränkte Sprachplanbarkeit von Langsprachmodellen. Da keine Datenseite bestimmte Ziele gibt, um unser Studium zu, Wir müssen diese Ziele zuerst erben. Wie in der Tabelle zeigt, Wir erweitern die abstrakten Ziele mit vielseitigigen Beschränkungen. Für Menschen bei der Lookdatenquisition mit Instru gpt. Wir Pro hundert spezifische Ziele und bewerten die Sripte aus Le Modellen erzeugen Skripte. Diese Tabelle berichtet die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle Lilang Modelleigende Ergebnisse bei der Planung für bestimmte Zielede Ergebnisse erzielen. Dann führen wir eine detaillierte Analysen durch, um zu untersuchen, warum Lernernmodelle funktionieren. Die Ergebnisse in der Zahl zeigen, dass die semantische Voständigkeit in generierten Skripten akzeptabel, Aber die Treue gegenüber den Beschränkungen kann nicht garantiert werden. Wirfor freistu Thekatgorien von Beschränkungen, Wiub. Die Hitkarte in der Abbildung zeigt, dass die Planungsleistung von Anleitung gps für Mädchene verschiedene Kategorien erheblich vari. Frühere Studien haben gezeigt, dass die Outqualität von Li-Moe in hohen Vararianzfällt, Dies führt zu einer schlechten Leistung. So haben wir die Idee eines übergenerierten Zen-Filters, um die Geneerqualität zu verbessern. Wir zeigen zunächst eingeschränkte Typen mit Beispielen für in cpt und erhaltenspezifisch Ziele basieren der abstrakten Ziele. Dann Instru Gpt Schlüsselkripte für bestimmte Ziele. Als nächstes wird ein Filtermodell entwickelt, um die treen Skripte auszuwählen. Wirwandeln Skripte und Ziele in InstruGpt um und berechnen Kosin Ähnlichkeit und Ähnlichkeitswertse, um semantische Ähnlichkeit zu messen. Wirben das Skript, das die Schlüsselwört der Zielbeschränke enthält. Wir behalten das Skript, wenn das Zielzi höchst an der Zielseite Punkt. Met In gbt kann S von höherer Qualität erzeugen. Unsere Methode verbessert die Planbarkeit sowohlwohl in der Semantik Vollständigkeit als auch in der Tretlichkeit der Beschränkung. Da Sprachesprachige Modellest bereitsatz sind, Es ist wichtigss, um die Sprachplanbarkeit kleinerer und spezialiser Modelle zu Die Erstellen von Datensätzen ist ein wesentlicher Schritt zu Zweck.dings Frühere Studien ermöglich keine Planung für bestimmte Ziele, Und die manuelle Datensatz ist teuer. So folgen wir der Idee der symbolischen Wissensdistillation, um eingeschränkte Sprachplanungsdaten aus Sprachesprachigen Modellen deieren. Wirnden unsere Methode für den Erstellung eines Datensatzes mit eingeschränkter Sprachplanung an,nt als Codescript. Wirzeugieren fünfundfünfzigtausend spezifische Ziele mit Skripten. Um die Qualität der Validierung und der Testseiten zu gewährleisten, Wir bitten Crowd-Source-Arbeler, das Einkommen in falschen Proben zu überarbeiten. Diese Zahl zeigt die eingeschränkte Verteilung von Codescript. Wir, Codescript hohe Plaudismus in den erzeugen spezifischen Ziele. Mit Codescript können wir kleinere, aber spee Modelle für eingeschränkte Spracherachplanung ben. Wir fanden, dass T5co Skripte von Haarqualitäten als die meisten großensprachigen Modelle erzeugen kann, Dasute darauf hin, dass kleinere Modelle größere Modelle unterstützen können, wenn sie richtig auf geeigneten Datenseiten trainiert werden. Wir haben das Problem eingeschränkte Sprachplanung festgestellt. Wir haben eine eingeschränkte Sprachplanungs von Langsprachigen Modellen und entwickeln eine übergenerierte Filtermethode für Langsprache. Wir verwenden große Sprachechmodelle, um einen hochwertigen Quaischen Datensatz, CodeScript, für eingeschränkte Sprachplanung. Wir hoffen, dass CodeSscriptDatensatz eine wertvolle Ressource sein kann, um die Forschung zur Sprachplanung vorreiben. Vielen Dank für Ihre Zeit. Bitte finden Sie Informationen zu Codescript in unserem Arbeit"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Janislavak und ich werde Ihnen unsere Arbeiten über Dr. Bert, ein robustes vorgebildet Modell Französischen für biomedizinische und klinische Bereiche. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Dann werden wir den Hauptbeitrag unseres Artikels vorstellen. Wir haben das erste biomedizinisches Modellös, namens Dr. Bert, der auf Robertaiert, und trainierten Natchos, einem Datensatz von medizinische gekter Daten aus dem Internet. Wir führt eine Vergleich von Modellen mit mehreren kratonischen Einstellungen und Datenquellen. Dann stellen wir unsere Ergebnisse mit 11f biomedizinischen und klinischen nachaufgaben auf Französisch. Schschließen wir die Experimente und geben Ihnen mehr Details, wie man auf diese Modellegreifen. Seit seiner Veröffentlichung 2018 ist BERT einer der effektivsten Ansätze zur natürlichen Spverarbeitungs zu lösen und enorme Leistung im Vergleich zu historischen, statischen und kontextualisierten Methoden wie Wort, fastex oder Nwo Seitdem wurde dieses Modell an viele andere Sprachen angepasst, wie Französisch mit Cammbert und anderen Bereichen wie biomedizinisch mit Perhm Geburt und Biobert und klinisch mit klinischer Geburt, aber me auf Englisch. speziellzilises Modelle für andere Sprachen sind selten und basieren oft auf kontinuier Vortraining aufgrund desfehl. Französen gab kein Open-Source-Modell für Biomedizin und Kinder. Wir stellten uns die Frage, was die geeignen Datenquellen für eine breite Nutzungs? Und diese grau Daten sind eine gute Ersatz für klinische Daten. Um diese Frage zu beantworten, vergen wir Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten bas, aus nichtuniverskrankenhaus in unserem Haus. Danach fragen wir uns: wie viele Daten brauchen wir, um ein spezielllises Modell auf französische Datenieren? Ist es vier Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren wir und vergleichen vier von GrundMode: eine erste Version von Dr. Bert mit sieben Gigabyte Naturos, eine zweite Version von vier Gigaby von Naturos, eine erste Version von Schubert, ein klinisches Modell, mit vier Gigabyte Sätzen aus klinischen Knoten, und eine letzte Version von Schubert mit einer Mischung von vier Gigabyten Naturos und 4 Gigabyte klinischen Knoten. Zusätzlich zu diesem Vergleich haben wir drei Modelle ein, die für kon Vortraining train, um die Auswirkungen der Vortrainingt zu analysieren: Eine basiert auf dem Gewicht von Cammbert und train auf vier Gigabyte Natur, Eine andere auch auf Cammember, aber diesesmal auf den vier Gigabyte an klinischennoten train. Und schließlich eine Bas auf dem englischen biomedizinisches Modell, Bermed Bert, und mit vier Gigabytesna. Insge haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir viele öffentliche und private Downstreamaufgabe, wie Namenerkennung, Klassifizierung, Teil der SprachT und Fragebeantwortung. Diese Modelle werden mit sechs Basissign-Moellen verglichen, CammbertO138 Gigabyte, Cammbert Oscar 4 Gigabyte, Cammbert 6net 4 Gigabyte, Kgel, Biobert und Clinische Birgel. Die Evolution von Highlight dieses Modell funktioniert am besten bei der Aufgabe mit Daten der gleichen Art wie die, auf denen das Modell trainiert wurde. Wir können diese Daten können beobachten, dass Daten aus heterogenen Quellen vielseitiger zu erscheinen. Wir beobachten auch, dass die Nuwendung mehr Daten in bessere Leistung umtragen.gesamt scheinen von Grundtraining bei den meisten Aufgaben höhere zu er erhalten. Unser Experiment zur Kontraining mit das Gewicht und Tokenizer von Genehmigung Birgel auf der 4-Ggabyte- von Naturos, zeigt vergleichbare Ergebnisse Dr. Bir vier Gigabyte von. Das für das Modell bas auf Cammember-W und Tocanizer, die an Stabilitätsproblemen leiden. Schluss: unser richtiges System eine bessere Leistung bei neun relevanten Nichttreamaufgaben und das Ergebnisse des generen Modells, Cammbert. Wir beobachten auch, dass spelisierte Daten besser sind, mehr speziellziae Daten besser, aber es kal nicht gut. Alle Modell von Nachos sind kostenlos verfügbar auf Uface, und alle Schul sind auf unserem GitHub-Repository. Vielen Dank für diese Präsentation und wir freuen uns auf bei der Postitzung in Toronto. ("}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xng Bing, Doktorandand an der Universität von Washington. Heute präsentiere ich unsere Arbeit von Vortraining über Sprachmodellen bis hin zu nachgelagerten Aufgaben, die die Spuren politischer Vorurteilefolgen, die zu unfairen NLB-Modellen führen. Sprachmodelle werden auf großgelegt Web-Craw-dateniert. Politische Nachrichtenmedien sind in ihren Vortrainingdaten gut behandelt. Nach einer Umfrage des c four-Corpus können wir sehen, dass die New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut über Ausbildung Sprachmodelldaten behandeltt sind. Dies hat einen gemischten Segen für Anwendung des Sprachmodell geschaffen. Einerseits konnten sie aus verschiedenen Perspektiven lernen, die die Demokratie und eine Vielzahl von Ideen feieriert. Andererseits sind diese unterschiedlich politischen Meinungen von Natur aus sozial voreingenommen und könnten zu potenziellen Fairness-Promen bei nachgelagerten Aufgabenwendung führen. Zu diesem Zweck schlagen wir vor, die Ver polibreitung von Vortrainings Daten über Sprachmodellen hin zu nachgelagerten Aufgaben untersuchen,bes indem wir folgenden Fragen stellen Wie bewerten wir die politische Abigung von Sprachmodelle und welche Rolledaten auf solche politischen Vorurteile spielen? Zweitens, Wie führen Sprachmodelle mit unterschiedlichen politischen Linieen tatsächlich bei nachgelagerten Aufgaben aus? Und ob dies zu Fairness-Problemen in nlp-Anwendungen führen könnte. schlagen wir zunächst vor, Sprachmodelle mit verschiedenen Aufforderungformaten politischen Fragegen wie z B. den politischen Kompasstest,stellt, wir automatische Bewert in der Politikwissenschaftlichen Literatur. Einige Vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Bedeutung haben. Siesetzen alle vier Quadranten des politischen Kompass. Wir können auch sehen, dass GPT4 das liberalste Sprachmodell von allen ist, und GPT-Srien sind im Allgemeinen sozial liberaler als die Birgeltheorie und ihre Varianten. Zweitens wollen wir untersuchen, inwiemweit die politischen Vorurteile von Sprachmodelle tatsächlich aus den Schuliningsdaten abgeno werden. So könnten wir ein kontrolliertes Experiment durchführen, indem weiterepunkte Sprachmodell auf sechs verschiedene  Partei Korporen, die in Nachrichten und soziale Medien in ihre politische Ne. Durch Vor von Sprachmodelle auf solche partei Korporra können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verändern. Zum Für Roberta, feinsti, weiter im linken Reddit-Corpus, können wir eine erhee liberale Veränderungbung in seine politischen Vorurteile Und wir versucht auch untersuchen, ob Sprachmodelle die Polarisierung, die in unserer modernen Gesellschaft können. Wir teilen wir vortraining corpora in vor 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten, train wir Sprachmodelle auf zwei verschiedene tempo Korporen. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Neigung hatten, die nach sie vom Zentrum entfernt ist. Dies de zeigt darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufgreifen können. Zu guter Letzt bewerten wir Sprachmodelle mit unterschiedlichen politischen Veren auf von Hassreerkenn und Fake News-rknennung auf nlp-Anwendungen, die oft Sprachmodelle beinhalten und sehr erhebliche Auswirkungen haben könnten. Wir sehen also, dass wenn wir die Leistung pro Kategorie untersuchen, d., wenn wir die Leistung in verschiedene Demografischen oder politische Mini Nachrichtenmedien unternnen, können wir ein Muster sehen, dass für der Sprache Hassren link Sprachmodelle besser darin, Hassreden erkennen, sozial Minderheitengruppen ab erkennen, sind schlechter darin, Hassreden, mehr Macht für Gruppen in unserer Gesellschaft abelt. Und umgekehrt Rechte Sprachmodelle sind besser darin, Hassre, auf Weiße und Männer ab, Schlier darin von Hassre auf schwarze LGbtq und andere Minderheitengemeinschaften. Ähnliche Trends finden auch bei Fake- News-rk, bei wir sehen, dass linkige Sprachmodelle besser darin sind, Fehlinformationen aus ihrer gegenübergesetztigen politischen Neigung erkennen und umgekehrt. Wir zeigen weiter viele qualitative Beispiele, um sehen, dass Sprachmodelle mit unterschiedlichen politischen Bedeutungen unterschiedliche Vorhersagen für Hassreuß und Fehlinformationsspiele auf basieren ihrer sozialen Kategorien. Es gibt eine Reihe weiter Beispiele im Anhang, um das weiter hervorzuheben. Dies de zeigt hin, dass es ein Fairness- gibt, das in die politischen Vorurteile von Sprachmodelle ist. Wenn zum Beispiel rechtige Sprachmodelle auf Hassre oder Fehlinformationen oder was auch immersti und auf eine beliebte Social-Media-Plattform einsetzen werden würden, Das würde bedeuten, dass Menschen mit gegengesetzten politischen Meinungen ausgedrängt werden könnten und die Hassre Minderheitengruppen ab, ohne Kontrollelaufen. Das klingt Alarm, Fairness die durch politischen des Sprachzugehen. Also ein wenig Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma Bezug den politischen Vorurteilen Sprachemodell aufdecken. Es ist wie zwischen Sycylla und Charbdis. Wenn wir also politische Meinungen in Sprachmodell desinfiisieren, würde sich die Voreingenommenheit von Vortrainingsdaten zu Sprachmodellen hin zu nachgelagerten Aufgaben ausbreiten und was letztlich Fairness-Thememen schaffen. Wenn wir versuchen, irgendwie zu desinisieren, Wir würden auch Zensur oder Ausschluss riskieren, und es ist unglaublich schwer bestimmen, was tatsächlich neutral ist und Sprachmotrainingsdaten beibehalten sollte. Es ist also wie das elektrische Charlielie-Problem. Okay toll. ich denke, das ist so ziemlich alles, was ich für TED habe.ün für heute. Vielen für Ihre Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo. Ich bin Ko Sina und ich freue mich Sie zu unserem Vortrag über unsere ACL23 begrüen. Akzeptanz sind nicht immer Kontext. Dies ist eine gemeinsame Arbeit mit John Boqui, Aaron Muler, Kanishka Mishra, Karen Fs, Roger Levy und Atina Williams. In dieser Arbeit sehen wir das Minipaare-digma. Das Minipaar Paradigma bewertet Sprachmodelle auf Akzeptitäturteil, die auch Grammatikal wie Bmp, Syntax Jim, oder Akzeptanz in Stereotypen, wie Crowdpaar. In diesem minimalpaar-paradigma ist die typische Sprachmodelle, dass man einen akzeptable Satz oder einen grammatikalischen Satz, und einen inakzeptable Satz oder einen ungrammatikalischen Satz, und die Hoffnung, dass das Modellr den akzeptable Satzrscheint. Die aktuelle MPP-Pipeline erlaubt nicht, die Akzeptanz des Modelle gegenüber längeren Sätze zu bewerten. Heutzutage kommen großesprach Modelle mit länger immer längeren Kontextfenster, Daher ist es entscheiden, dass wir die Akzeptanz des Modells im gesamten Kontextfenster bewerten. Und das was wir hier tun. Wir versuchen, die MPV-Pipeline einmal zu übergreifen, indem wir das Modell auffordern, die Akzeptanz bei längeren Sequenzen zu bewerten. Das ist der Ansatz. Wir diese längeren Sequenzen zuen, Wir die Datensätze selbst, und dann erstellen wir Sätze, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. Wir haben wir zum Beispiel ein typisches Paar grammatikalischer T aus Bblim-Datensatz, vom Adjun In. Wir um längere Sequenzen erstellen und die akzeptabel sind und die gleiche der grammatikalischen Struktur, hieren grammatikalische Sätze aus der Adjun Insel undügen sie als Präix sowohl die akzeptable Abfrage als auch die inakzeptable Abfrage. Wir können Gleich tun, indem wir inakzeptable Sätze aus derselben Überstimmung auswählen. und das könnte auch verwendet, um die Akzeptbarkeit des Modells zu testen. Und können dasselbe tun, indem wir Sätze aus einer anderen Untermenge oder einem anderen Datensatz. Das nennen wir das FegleichendeSzenario. Hier kommen die Sätze immer noch von relevanten Datensätzen, aber siestamm nicht aus demselben Datensatz, mit dem wir bewerten. Und können dasselbe für Fall Unakzeptanz. Schließ können wir Sätze aus einem völlig nichtwandten Bereich wie Wikipedia. wird, ob die Akzeptanzsurteil des Modells tatsächlich von irgendeine Kontext beeinflusst werden, z.. ob der Kontext aus einer anderen Teilmenge des Datensatzes kommt oder ob es irrelevant für den aktuell dem Satz, den wir betrachten. Wie funktioniert das Modell? Zuers schauen wir uns die Wikipedia- Sätze an, die für das aktuelle Abfragepaar relevant sind. und stellen wir, dass die MPP- me für willliebliche Kontextlänge sind. Wir erhö die Kontextlänge bis zu 2024, um OPT und GPT2-Modelle maximieren. und wir sahen hier in der orangefaren Punkt Linie, dass MPP- sind relativ stabil. Was passiert nun, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen wir also oder erstellen Sätze aus akzeptablen und inakzeptablen Domänen aus demselben Bblim-Personsteuerm-Dasatz, Und dort sehen wir, dass die pp-Uteile entweder deutlich steigen oder sinken, wenn Sie entweder akzeptable Präfixe oder inakzeptable Präfixe hinzufügen. Aber wenn wir der Struktur überstimmen, d heißt, wenn wir die Sätze aus denselben Phänomenen im Schul Person Steuerwählen, sehen einen massiven Anstieg oder massiven Rückgang des pp-ur für das Modell, je nachdem, ob der gewähltte Präfix akzeptabel oder inakzeptabel ist. das ist sehr groß. Dieser Effekt nimmt während der Kontextlänge, und das würde wahrscheinlich neuere Sprachmodelle, mit große Kontextfenster haben. Warum also beeinflusskt daseinstimmung- Präfix das Urteil des Sprachmodell so aus? Wir haben wir eine Reihe von Analysen durchgeführt, bei denen wir versuchten, den Eingabessatz zu stören, indem wir versuchten, die relevante Struktur zu bewahren, indem Lärm. Und nachdem wir mehrere dieser Störungen durchgeführt haben, Wir stellen fest, dass keines dieser Geräusche das Modell seinen Kurs in den MP- Urur zeigt. Im stellen wir, dass die Modelle emp auf die Stör Sätze auf ähnlich sind. wenn wir die Sätze im akzeptable Bereich stören, sehen wir einen ähnliche Annahme aller Störungen. Und wenn wir die Sätze im nicht akzept Zuhm stören, sehen wir einen Rück der MPP-ur in ähnlicher Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind also darin, dass Sprachmodelle empfind auf latente syntaktische und semantische Merkmale sind, die in den Sätzen geteilt werden. Und die -wert, Die Art und Weise, wie wir es derzeit mit kurzen und einzigen Satzeneingabe tun, Kan kann das abstrakte Wissen der Sprachechmodelle im gesamten Kontextfenster. Bitte lesen Sie unsere Arbeit für weitere Deheiten zu unsere Experimente. Vielen Dank fürs Zuhören"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawei, Doktorandand an der Staland University in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit vorstellen \" Schwächer als Sie denken, Ein kritischer Blick auf wöchentliche Überler. Dies ist gemeinsame Arbeit mit Xiachen, Maos Mosbach und Gi Steffen und Dierich Klaow. Ich möchte mit einer kurzen Einführung in die schwache Überwachung und wötlich überwachtes Lernen beginnen. Bei schwachen Überwachung bekenzeichnen wir die Daten nicht manuell. Stattdessen beschriftzeichnen wir die Daten mit schwachen Kenschriftzeichnungsquellen, wie einfache heliche Regeln, Wissensbasis oder Loqual Clod-sourcing, Wie Sie in deren Ab rechts verlicht. Im Vergleich zu menschlichen Anmerkungen, sind die schwächeren Notungen viel billiger, Denn sind auch laut, was bedeutet, dass eine bestimmte Menge der Anmerkungen falsch sind. Wenn wir neuronale Netze auf wöchentliche Kenschriftzeichnungdaten trainieren, neigen die neuronalen Netze, das Etikettengeräuschen aus medig lernen und nicht zu verallgemeinern. Bei wöchenttlich überwachtem Lernen werden Trainingssalgorithmen vorgeschlagen, neuroner Netze unter solchen Labelttengeräuschen robust trainieren, so dass die trainierten Modelle immer noch gut verallgemeinern werden In den jüngsten Arbeiten in wl so Wl für wöchentlich überwachtes Lernen Eine häufige Behauptung, dass Leute sagen, dass sie einzige trainmodelle auf wöchentlichen Arbeitsdaten sind und hohe sauberen Testsätzen. Technisch gesehen ist diese Behauptung nicht falsch, Aber es gibt einen Haken, Das die Leute davon ausgehen, dass es ein zusätzliches sauberes Validierungsset, Oder für Modellauswahl. Wir haben diese Problemeinstellung, Dies bedeutet, dass derö zusätzliche manuelle Anmerkungenöchenlichen erforderlich, Aber wie ein Elefant im Raum, Diese Notwendigkeit wird oft übersehen. Der oben erwähnte Ad wird, drei Forschungsfragen zu stellen. Ers: Sind saubere Validierungsdaten für wsl notwendiger? Oder können wir stattdessen einläes Validierungsatz verwenden? Zweitens, Wenn saubere Daten erforderlich sind oder wenn saubere Daten wsl sind, Wie viele saubere Proben brauchen wir dann? Endlich, Sollten wir nur die sauberen Proben nur zur Validierung verwenden, Oder es gibt bessere Möglichkeiten, sie zu nutzen. Wir haben diese Forschungsfragen in unserer Arbeit an angesprochen und unsere Ergebnisse sind wie folgt: Zuächs stellen wir, dass jüsten WSL-Methoden saubere Validierungben richtig funktionieren. Ansonfall gibt es einen stark Leistungsgang. Wie dieser Zahl zeigt: Wenn es keine sauberen Validierungproben gibt, können die Trendmodelle nicht über die ursprünglichen schwachiketten hinaus verallgemeinern, Das bedeutet, dass die Training sinnlos ist. Diesute darauf hin, dass Wsl-Ansätze tatsächlich er, dass sauber beschriftete Daten richtig funktionieren, und die Anmerkungskosten für die Erschaung von sauberer Validierungsproben nicht übersehen werden sollten. Un zweites Erkenntnis ist, dass die Erhöhung der Anzahl der sauberen Validierungproben WSL-Ansätzen, bessere Leistung zuelen, wie der links. Normalerweise ben brauchen wir nur 20 Proben pro Klasse, um hohe Leistung zuelen. Aber das ist nicht das Ende der Geschichte, denn wenn wir so entscheiden, auf saubere Proben zugreifen, Dann wird diee Train sogar eine bessere Leistung. Die rote Zahl zeigt den Leistungsunterschied zwischen Fein-Tning-Ansätzen, die direkt unter sauberen Daten angewendet werden, und Wl-Ansätzen, die die sauberen Daten nur zur Validierung verwenden. Wie wir sehen können, Wenn wir zehn Proben pro Klasse haben, beginnt die Di direkte Feinabstimmungning, wsl-Ansätze zu erreichen. Schließlich kann diesvers bei früheren wsl-Ansätzen bespruchten Leistungs leicht erreicht werden, indem es Feinabing auf sauberen Validierungsproben fort ermöglicht. Wie wir aus den Zahlen sehen können, Das ValMo, das ftw bezeichnet, zunächst kompliziertere WSL-Methoden wie Kosinin. Wenn wir jedoch Fantuuny unter sauberen Proben fortzusetzen, Dann funktioniert ftw genauso gut wie andere Methoden. In Praxis gibt es also keinen Grund, komplexere WSL-Methoden auszuwählen, die mehr Berechnungzeit und den Festplatraum erfordern. Zusammenfassend haben wir gezeigt, dassüngsten Wl-Ansätze saubere, manuell kommentierte Proben benfordern, damit sie richtig funktionieren. Ihr Leistungsgewinn und Prakabilität werden stark überschätzt. Unsere konkreten Empfehlungen für die zuünftige Arbeit sind wie folgt: Mellden Sie die Modellauswahlkritterien. Mellden Sie zum, ob die Modellauswahl mit sauberen Validierungsproben durchgeführt wird. Zweitens sollten WSL-Ansätze mit wenig kurzen Landlinien verglichen werden, z Arbeit auf sauen Proben. Drittens: kontinuierliche Feinabstimmungierung ist eine einfache, aber starke Basislinie, die bei der zukünftigen Arbeit in wsl berücksichtig t werden sollte. Schließlich haben wir unseren Open Source Code. Sie können es über den QR-Code auf dieser Folie finden. Bitte sehen Sie es sich gerne an. Vielen Dank und genießen Sie die Konferenz"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. mein Name ist Al Villaard und ich werde einen kurzen Überblick über die Arbeit geben, das PAM von der Übersetzung, Strategien und Leistung bet. Das ist gemeinsamearbeit mit meinen Kollegen von Google Translate. PAM ist ein 540 Milliarden Parameter Sprache, das letztes Jahr 2022präent. Es bas auf einer großen Sammlung von Texten train, die siebenhundertachtzig Milliarden Token um. Zum Zeitpunkt der Veröffentlichung erreicht sie neues in Hunderten von NLP-Tufn. In dieser Arbeit ent wir eine frei systematische Studie der Auf Großsprachigen Modell für die Maschinelleübersetzung. Wir bewerten die Übersetzungskapazität solcher Modelle mit denw Prakti dert-mein. Dahalte der neuesten Testsätze, um eine Überschneidung der Testdaten mit den Trainsdaten des Sprachmodells zu vermeiden. und wir vergleichen zwei moderne Systeme, die bestenleistungsfähigsten Systeme oder der Wmt-Ewertation. Wir verwenden moderne neuronalemt-Metriken und zeigen außerdem auch experttenbasierte menschliche Ewertungsergebnisse. Schließlich stellen wir einige Empfehlungen für Aufforderungauswahlstrategien. Die Aufforderung hat einen großen Einfluss auf die Leistung der LLms für die Übersetzung. Wie wir in einem einfachen Experiment sehen können, bei dem wir eine kurze Aufforderung verwenden und zwei verschiedene Aufforderung für einen Satz. Die Mehrheit der Sätze, 516 von 1000, ist der beobachtet Unterschied mehr als einem verschwoär Punkt. Und kann in extreme Fällen bis zu 40 verschwoär Punkt. Da ist es wichtig, eine gute Aufforderungt auszuwählen In unseren Experimenten haben wir uns eine Fünfige Auf-trate, bei der wir den Satz mark, den wir dem System mit der Sprache. In diesem Beispiel, wo wir Übersetzung von Deutsch ins Englischführen, deutschen die Quellesätze mit deutscher Spaal und die englischen Übersetzungen mit englischer Spalte. Wir sahen, dass die tatsächliche Form des Drucken keinen großen Einfluss im Fall mehreren kurz Ausgabe hat. Es ist entscheiden für Null- und Ein kurz, aber wenn wir wie in unserem Fall Fakten Aus, gibt es fast keinen Unterschied in der tatsächlichen Form der Aufforderung. Es sind die Beispiele, die den größten des Gewichts tragen. Die Zusammenfassung unserer experimentellenergebnisse ist, dass die Beispielqualität wichtiger ist als die Ähnlichkeit mit dem Quellsatz. Da ist es wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen.bes vergleichen die Auswahl von Aufforderungen aus den Traingdaten der wmt-Ewertungen oder den Entwicklungdaten. Die Entwicklungdaten sind viel erstellter und mit höherer Qualität als die traindaten, die mehr und die Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Entwicklungdaten. Dennoch haben speziae moderne Systeme einenhe Vorteil gegenüber die Palm-Übersetzungen Aber Palm kommt ziemlich na einem kommerziellen System. in unserem Fall haben wir entschieden, mit Google Translate zu auszuieren. Die Erkenntnisse, die wir aus der E-Mail-Nierung haben, die wir mit dem npm-Framework durchführen haben, ist, dass die Geläufigkeit von Palm mit dem Zustand der Kunstsysteme vergleichbar ist, der Hauptunterschied ist von der Genauigkeit. Der häufigsten Fehler sind Auslassfehler. Es scheint also, dass Palm eine bessere klingendeübersetzungieren, Manchmal, indem sie Teile des Quesatzs fallen, die in der Übersetzung sind Allerdings Die Stil außenrie für pan ist niedriger als für die modernen Systeme, was ein zusätzliches Signal ist, dass parm wirklich fließende Ausgabe bietet, aber immerno mit einigen der Genauigkeit. Und das war 's für diesen kurzen Überblick. Für weitere Details kommen Sie zur vollständigen Präsentation der Arbeit. Vielen Dank. ("}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo. Mein Name ist Jin Wei Y von der Universität of Science and Technology China. Es ist mir eine Freude, ein kurzes Werbungvideo unserer Arbeit zu geben: Koieren Sie mein Modell? Schutz das Urheberrechts von Großsprach Modelle für Einbettung und Diensten. Hintertür Wasserzeichen. Lassen Sie uns zunächst den Hintergrund über Einbettung-Anzedienste. Derzeit, Groß Sprachemodelle wie gbt, Lama, Palm sind außeröhnlich im Verständnis undzeugerierung vonen Spracheöhnlich. EinbetddingAdienste sind einer der Dienste, die auf großensprachn Modellen aufbauen, um verschiedene nlp-aufgabeufn unterstützen. Openad bietet eine g-basierte Einbettung-API. Re haben gezeigt, dass der Angreifer das Modell indem aus Einbettter stehlen und ähnliche Diensteten. Da ist es notwendig, das Urheberrecht von Einbettung als Dienste zu schützen. Um das Urheberrecht von EinbettungAdienst zu schützen, ist der Lösung, ein Wasserzeichen in den Anbieters einzutten und festzustellen, ob ein Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen: Erstens Die Methode auf EinbettungAdienst angewendet werden. Zweitens sollte das Wasserzeichen den Nu der bereitgestellten Einbetungen nicht abern. Drittens sollte das Wasserzeichen den Angreifer umt sein, oder der Angreifer kann das Wasserzeichen leicht entfernen. Schlich muss das Wasserzeichen während des Modellexprozess An übertragenbar werden. Bestehende Werke können in vier Kategorien eint werden. Diese Methode gilt entweder nicht auf Einbettung als Dienste oderfehl an Übertragbarkeit. Daschlagen wir in diesem Arbeit einen Einbettungsmarker vor, eine Hintertürbasierte Wasserzeichenmethode, die für Einbettung als Dienstebar ist. Lassen Sie mich die Details unseres Einbettungsmarkers vorstellen. Der Einbettungsmarker enthält zwei Hauptschritte: Wasserzeichenktion und Urheberrechtsverifiation. Vor diesen Hauptschritten, Wir wählen zunächst einen Auslösersatz aus. Der Auslösersatz ist eine Gruppe von Wörtern in einem moderatemäßign Frequenzintervall. Wir gehen davon aus, dass der Anbieter通用文本编,并用它计算单词频率。 在水注入, 我们首先定义目标嵌入。 当用户向提供商服务发送句子时, 提供商计算句子中的触发器号。 提供的嵌入是目标嵌入 und der ursprünglichen Einbettung. Das Gewicht der Zieleinbettungs ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als m ist, Die bereitgegebene Einbettung ist genau gleich der Zielinbettung. Die Urhepyberrechtsüberprüfung besteht,zustellen, ob ein Modell hinter einem anderen Dienst das Wortzeichen enthält. Wir konstruieren wir zunächst eine Hintertür und einen gutartigen Datensatz. Der HintertürDasatz enthält Sätze, von denen alle Wörter zum Trilösggersatz gehören, Während alle Wörter in den Sätzen eines gutartigen Datensatzs nicht zum Triggersatz gehören. Dann verfordt der Anbieter Einbetungen vom Steelerdienst mit dem Datensatz. Diehnlichkeit Kosin und L2hnlichkeit zwischen dem angeforderten Einbettung und dem Zielinbettung werden berechnet. Wir berechnen den Ähnlichkeitsunterschied zwischen Be- und Hintertürsatz, der als DeltaCosinus und Delta L2 definiert. In Zwischenzeitnden wir auch den K-Test an und verwenden seinen p-Wert als dritte Kenrik. Wir führen Experimente an vier Datensätzen: A Nachrichts, Mind, SSD2 und Apam. Wir gehen aus, dass der Anbieter Wikitextextdatensatz anwendet, um die Wortfrequenz zu zählen. Die Ergebnisse von vier Datensätzen zeigen, dass unser Einbettungsmarker eine gute Erkleistung haben und Nutzen für Downschiraufgaben. Wirprüfen auch die Ver der bereitgestellten Einbettung, indem wir die Einbettung von Sätzen auf vier Datensätze über PCA visual. Die Legende der Zahlen bedeutet die Anzahl der Auslöser in jedem Satz. Wie in den Zahlen gezeigt, ist schwer, zwischen den Faktoren Einbetungen und normalen Einbetungen zu unterscheiden. Das war 's, danke. Wir kommen um mit uns zu diskieren"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo. Mein Name ist Ian und mein Kollege Jion und ich werden unsere Forschung über Multi-Instruct vorstellen, Lernen durch. Mit den Fortschritten der großen Sprachmodelle begannen viele Arbeite, neue Lernparadigmen, protrain Sprachechmodelle für verschiedene nachgelagerte Aufgaben auf parameter und dateneffiziiziente. In Kürzlich haben viele Studien gezeigt, dass die Anweissabstimmung große Sprachmodellen ermöglicht, Aufgaben auf serll, indem sie natürlichen Anweisungen be. Die meisten früheren Arbeiten zur Anleitungsstistimmung konzentrieren sich jedoch auf die Verbesserung der Zellscht Leistung auf sprach Aufgaben, während Computer Vision und Multimoddale Aufgaben ausgelassen wurden. wollen In dieser Arbeit untersuchen, ob Anrichsab oder multimodale Protrain-Modelle die Verallgemeinerung auf unsichtbare multimoddale Aufgaben verbessern können. Zu haben zum unserer Forschung eine erhee Diskrepanz in der Verfügbarkeit von Anweissdatensätze zwischen LP und Multimodal. Es gibt mehr als 1600 Anleitungungsaufgaben. Es gibt keine großgelegt öffentlich zugängliche Mulmoddale Anrichsaufgabe. Deshalb motivierte uns, einen multimodalen Andatensatz erstellen Hier präs stellenentieren wir Multi-insstruct, den erste multimodalDasatz, der aus 62 verschiedenen multimodal Aufgaben, die 10 Brekategorien abfasst. Diese Aufgabenstamm aus 21 be vorhandenden Open-Source-Datensätze und jede Aufgabe ist mit fünf ausschriften Anweisungen ausgestattet Für Untersuchung multimod Anweissab unserem vorgeschlagenen Datensatz nehmen wir OFA, ein einheitliche MultimodMode als unser Basismodell. OFA verwendet einen einheitlichen Voschatz für Sprache, BildToken und Koordinaten eines Begrenzfeld. Hier zeigen wir einige Beispiel Beispiele aus unserem Multi-Instra-Datensatz. Um die Verarbeitung von verschiedene Eingabe- und Ausgabedatentyp zu vereinen, Wir folgen der Metho von offa und formulieren alle Aufgaben in einem einheitlichen Format Sequenz-Sequenzformat, in dem die Eingabetext, Bilder, Anweis und Begrenzfeldchen im selben Token-raum dargestellt werden. Okay, jetzt werde ich über die multimoddale Anweisungsstimmung. Für den Trainingsdatensatz verwenden wir 53 Aufgaben von NIGrup zum Training und 10.000 Instanzen pro Aufgabe. Für Testsreservieren wir die gesamte Les gesund Menschenver für Testen und wählen zusätzliche fünf Aufgaben von Wiki und der  Gruppe. Wir verwenden alle Instanzen in der Testgeschwindigkeit jede Aufgabe. Zu nehmenieren wir zufällig 20 Aufgaben aus der Testgeschwindigkeit der natürlichen Anweisung als Aufgabe für NLP. Wir verwenden ein vortrainierte OFA- Modell als Basismodell. Während Trainings machen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer der fünf Anweisungsvorlagen kombiniert. Während jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment bewerten. Wir merichten die mittle- und maxim- Leistung und die Standardabweichung der Leistung in allen fünf Experimenten. Wenn die Aufgabe um eine multimod Klassifizierungsaufgabe handelt, melden wir Genauigkeit. Wenn es sich um eine multimodaufgabe, belden wir Rouot L. Für eine LP- Aufgabeuf melden wir auch Rouot L. Wir haben auch eine zusätzliche Bewertungsmetri die Sensifindbilität eingeführt. So misst die Fähigkeit des Modells, kon die gleichen Aus für dieselbe Aufgabeieren, unabhängig von der geringen Varation der Wortlauierung der Anweisung. Hier sind unser Hauptergebnis. Wie wir sehen können, Außerdem können das Transferlernen aus natürlichen Anweisungsdatensätze der Anweissabstimmung zugute kommen. Hier können wir sehen, dass das der der Aufgaben das Modell eine bessere Leistung und in der Zwischenzeit geringere SenEmpfindität. Wir haben auch ein Experiment durchgeführt. Wir haben eine Anweisung gegen fünf Anweisungen. Wie wir sehen, kann die mehr Anweisungen die Gesamtleistung des Modells verbessern und die SenEmpfindbilität stark reduern. Das Dies zeigt die Wirkung verschiedener Fronttuning-Straten auf die Senfindität des Modell. Wie wir sehen können, durch Transferlernen aus natürlichen Anweisungsdatensätze, vielbilität im Vergleich zum ursprünglichen OFA-Modellität. Wir können auch sehen, dass Transferlernen aus natürlichen Anweisungsdaten OFA helfen kann, viel bessere Leistung auf Nitrostrusatz. Insgesamschlagen wir den erste groß Mulmod An vorgeschlagen. Wir verbessern die Datenfähigkeit von OFA und erforschen verschiedene Transfer Lern und zeigen dass ihre Vorteile. Wirentwerfen eine neue Metnzahl namens Sensibilität. No: Wir sammeln eine viel größere multimod Ansstimmungsätze mit etwa 150 zusätzliche Varariansprachaufgaben und wir werden sie freiöffentlichen. Dies ist also ein QR-Code für unsere Daten und Modell. Vielen."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. mein Name ist Yuen John von der Penn State University. Heute werde ich unsere Arbeit, Exempr Crosssprachiges Semantisches Parlysing in mehreren natürlichen Sprachen und Hauptdarstellungen. Semantisches Parlysing ist also eine Aufgabe, semantische Darstellungen von Benutzeranfragen wie SQl und Lambda-Cocul. Und sprachgreifenige Semantische Parlysing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungdarstellungen übersetzen. Wie in dieser Figur gezeigt, Wir müssen die Abfrage in mehrere natürlich Sprachen mit neuronenmodellen, zwei Sql, Lambda oder funql usw.. Bestehende sprachige semantische ParlysingMoe werden auf einem Datensatz von begrenzter Aufgaben und Anwendungen getrengeschlagen und bewertet. zum Es gibt Abdeckung über eine bestimmte natürliche Sprache Das Chinesisch fehlt und es gibt Abdeckungs auf bestimmten Mini-darstellungen Der Lambda-Kkul fehlt Oder sie werden nur auf einem bestimmten neuronalenmodell bewertet. Es gibt zum Beispiel nur ein einziges Modell, um sie zu bewertt. Zu diesem Zweck haben wir Exempr vorgeschlagen, haben einen einheitlichen Datensatzexempr fürgrensprachige SemiPsing in mehreren natürlichen Sprachen und Bedeutungdarstellungen. Es enthält neun Datensätze in VirusDomänen, Fünf SeparendeTn, acht Darstellungen und 22 natürlichsprachen in 15 Sprachfamilien. Um unseren Maßstab besser zu bewerten, betrachten wir die sechs Einstellungen für Training und Bewertung. Die erste ist Translate Test. Wir verwenden Google Translate API, um Quelle in die Zielsprache zu übersetzen, Dann monosprachige Modell umieren und Bewertung trainieren. Wir trainieren wir ein englisches Modell auf englischen Abfrage und während der Schlussfolgerung übersetzen wir die deutsche Abfrage mit API in Englisch und verwenden dann das trainierte Modell, um die SqL vorherzusagen. Wir testen auch das monosprachiges Modell. In dieser Umgebungstellung ist die Quellsprache gleiche wie die Zielsprache, z Beispiel Deutsch bis Deutsch oder Englisch zu Englisch. Wir testen auch die monosprachige Fusionstellung, indem wir zweisprachige Modelle mit nur zehn Prozent der Trainingsdaten trainieren. Und wir test mehrsprachiges Modell, bei dem wir ein mehrsprachiges Modell für alle Sprachen trainieren. Wir stellen die deutschen, englisch und chinesische Abfragen zusammen, um ein mehrsprachiges Modell trainieren und während der Schlussfolgerung können wir dieses Modell verwenden um deutsche Abfragen oder chinesische Abfrage usw. Und wir be betrachten auchgrensprachige Nullhor und Nuüberfer. Wir trainieren auf eine Quellsprache und übertragen in eine andere Sprache. Während des Trainings trainieren wir es auf englischen Abfrage oder die Kombination von englischen und deutschenr FucharAbfragen, um ein mehrsprachiges Modell trainieren und den SQL- vorhersagen. Wir wir finden auch viele interessante Ergebnisse. In die Analyse von monosprachigen Modelle, Wir haben zwei Gruppen von Modellent, Einschließlich Encoder pdr, Das steht für mehrsprachige protrainiertecodider mit Zeigerbasierten Decodern, wie x element r plus pdr, bert plus pdr. Und wir haben auch Encoder-decodermodelldellewertet, Das sind mehrsprachige protrainierte Encoderdecodermodelle, Wie mbart und mt fünf. Wir habenen festgestellt, dass der Encoder-decoder die beste Leistung bei allen neun Datensätzen erzi. Wir fanden fest, dass Encoderdecoder oder Encoder-PDdr durch das Training in einer Mischung aus verschiedenen Sprachen verbessert werden. Und wir haben fest daran liegt, dass die meisten wichtigsten natürlich Sprachen Leistungsgewinn erelen können, Nur dass die englische Leistung in sieben Datensätzen sinkt und nur in drei Datensätzen gewinnt. Ich denke, die ist als Kurs der Mullingualität bekannt. Wir haben auch diegrensprachige Leistungslücke verglichen. In dieser Figur ist die blaue Linie einegrensprachige Fe-überfer. Die orangefarbene Linie istgrensprachige Null-t-überfer, Während die grüne Linie in der monosprachigen Einstellung, Wir haben festgestellt, dass durch den Vergleich der grünen und orangefarbenen Linie fand dass die Nullinstellung diesprachige Transfer signifikant, Und durch den Vergleich von blauer und orangefarbener Linie, Wir haben festgestellt, dass die Transferlücke schnell verkürzt wird. Wir finden auch einige andere interessante Ergebnisse. Dercoderdecoder überschnei die Pro-arbeitbei oder vergleichbare Ergebnisse.ing unserer englischen Natursprache kann die Leistung weniger Aufnahmen auf Ziel und natürlichen Sprachen er. Wir fanden, dass mehrsprachige Sprachmodelle wie Codedas und blau immer noch unreichen fürsprachgreifen SemantischePsinget sind. Um Zusammenfassen haben wir Exar, einen einheitlichen Maßchstab fürsprachgreifenige Semantische Parsing mit mehreren natürlichen Sprachen und Darstellungen. Wir führten eine umfassende Maßchstudie über drei repräsentative von mehrsprachigen Sprachmodellen. Und unser Ergebnisse zeigen viele interessante Ergebnisse usw. Und willkommen, unsere Arbeit und unseren Code besuchen. Vielen fürs Zuhören"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Skurkoski und diesem Vortrag geht es um die Abhängigkeitsstruktur der Koordination. Wie Sie wissen, gibt es verschiedene Abhängigkeitsstrukturen von verschiedene Theorien und KorpusAnsätze angenommen. Beispiel in universellen Abhängigkeiten die Struktur der Koordinatekoordination Lisa, Bart und Maggie so, dass das erste Konjunkt der Kopf der gesamten Koordinatenstruktur ist, also in diesem Fall Lisa. Ein ähnlicher Ansatz wird in Igord Milchuk Bedeutungs Texttheoriegenommen, wo die gesamte Koordinatenstruktur vom ersten Konjunkt geleitet wird. Diese beiden Ansätze sind also asymmetrisch, Sien einen der Konjunkte aus. Es gibt auch symmetrische Ansätze an Koordinatenstrukturen wie. den Prag-Ansatz, der Konjunktionpf Ansatz, in prag-Abhängigkeitbaumen angenommen wird, wo Koordinatenstrukturen von der Konjunktion geleitet werden. Wir erhalten also Abhängigkeiten von Ende zu allen Konjunkte. Und schließlich gibt es auch einen MehrköpfigenAnsatz, der zum Beispiel in De Katsons Wortort Gramatik verwendet, wo sagen, alle Konjunkte Kopfter der Koordinatenstruktur sind. so erhalten also Abhängigkeiten vom Gouverneur, hier Liebe, zu allen Konjunkten getrent. das sind. Das Ziel dieses Papiers ist es, ein neuartiges Argument für die symmetrischen Strukturen der Koordinationsen wie diese beiden und gegen die asymmetrischen Strukturen der Koordinationsstrukturen wie diese beiden. Okay, Das Argument basiert auf dem Prinzip der Abhängigkeits undlängengeminimierung, die ich auf der Grundlage dieser Beispiele erklärt werde. Also Englisch, wie wie Sie vielleicht wissen, Direkte Objekte ziehen es vor, na dem Verb sein, während Adjunkte weiter entfernt sein, oder? also März hat es gestern gelesen ist in Ordnung, weil das direkte Objekt ist dem Verb ist, Während Mä gestern ist viel schlimmer, oder? denn hier zwischen dem Verb und dem direkten Objekt gibt es gestern ein Adjuntiv. Dieser Effekt kann jedoch verbessert werden, wenn Wenn das direkte Objekt sehr schwer und sehr lang ist, Denn dann kann es in die Position nach dem Adjun verschoben werden. Dies ist hier anillustrlicht. diese Sätze sind in Ordnung Marsch hat dieses absolut faszinierende Buch über die Bi gelesen ist okay haben statt dieses lange und p. Aber es ist auch in Ordnung zu sagen, dass mar gestern dieses absolut faszinierende Buch über Bienen gelesen. Der Grund hier ist, dass dies möglich ist, denn obwohl dieser Satz gegen das allgemeine grammatikalische Prinzipsatz verstößt, dass direkte Objekte neben dem Verbliegen sollten, Es besfriedigt das Prinzip der Miniierung der Abhängigkeitslängem, Das besagt, dass kürzer Kürzere Abhängigkeiten bevorzugt. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also die, die zwischen diesen beiden Strukturen nicht konstant sind. Hier haben wir also die Abhängigkeit von Rot zum Adjun von Länge sieben in Wort gemessen und von rott zu Buch von Länge vier. also zusammen ist es elf. wenn Sie sich bewegt, wenn Sie diese beiden Bestandteile tauscht, Die Summe dieser beiden Abhängigkeiten sechs. Anstellet 11, sechs, viel kürzer. deshalb klingt das ganz okay. Es verstößt ein Prinzip, aber esfriedigt ein anderes. Okay. Wir haben verschiedene Statistiken über Koordination aus der verbeten Version der Pentreebank und die Papier, warum wir keine Universitätsababhängigigkeiten verwenden. Und diese Statistiken bestätigten dieo schon oftgestellt wurde Be, dass linke Konjunkte in der Regel kürzer sind. Also Salz und Pfeffer, nicht Pfe und Salz in Silben. Und auch die Beobachtung, die Vorufgestellt wurde, dass diese Tendenz mit Läunterschied wächst. Wenn also der Unterschied zwischen den Länge der beiden Konjunkte wächst, Die  kürzere Konjunkt ziehen es vor der erste stärker sein, Richtig? Der Anteil ist größer für die linken kurzen Konjunkteen. Aber in diesem Arbeit neuartig ist ist, wir beobachtet haben, dass diese Tendenz nur statttrittet, wenn Gouverneure auf der linken oder abwesend sind, Richtig? Der Gouverneur ist also link in diesem Beispiel links. Ich habe Bart und Lisa. also ist der Gouverneur, ist auf der linken. Es ist im zweiten Beispiel. Homer kam und nieest. Hier haben wir die Koordinnation von zwei Verben und es gibt keinen extern externen Gouverneur, oder? also in solchen Fällen, Das linke Konjunkt zieht es vor, kürzer zu sein, des mehr Je größer der Unterschied zwischen den beiden Konjunkten. Allerdings, Wenn der Gouverneur auf der rechten Seite ist, wie hier links die Koordination T und Netz,wind Wir haben wir gezeigt, dass Durch die Messung der Länge in Zeichen gibt es die erste Spalte in Silben, die mittlere Spalte und in Worten die rechte Spalte. Ichzentriere ich mich also auf die rechte. Was wir hier sehen, ist, dass, wenn der Gouverneur auf der linken Seite ist, Die Tendenz, dass die linke Konjunkt kürzer ist, wächst ste mit dem absoluten Unterschied der Wörtern. Und dasselbee gilt beobachtet, wenn es keinen Gouverneur gibt, wie in der Koordination von Sätzen, Aber wenn der Gouverneur auf der rechten Seite ist, verschwindet diese Tendenz. Und wir zeigen in der Papier, wie dies Bietet ein Argument gegen asymmetrische der Koordinationsstrukturen wie diese beiden und fal die asymmetrischen Strukturen als diese beiden. Sehen Sie also das Papier für vollständigen Übereinstimmung und Argumente, sorry, und sprechen Sie mit uns über die Post-sitzung. danke"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kyio Yin und ich werde unsere Arbeit mit dem Titel \"Wnnnert Transsetzung Kontext, eine datengesteuerte Mehrsprachige Erforschung. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernage, Emiliu, Andre F.D Martins und Graham Newbiig durchgeführt. Viele Übersetzungen hängen also vom Kontext ab. Wie würden wir zum Mol in diesem Satz übersetzen? Wenn der vorheriger Satz: \" Dinge gefährlich werden, wenn die Minister herausfinden, dannzieht \"Mo auf einen Spion. Aber wenn der vorherige Satz: \"Könte es etwas Erns sein, Doktor?\" Dann bezieht \"Mo auf ein Geburtszeichen. Je nach Kontextändert sich die Bedeutung des Wortes und ändert die Übersetzung. Es bewerten, wie gut Modelle solche Fälle übertragen können, ist jedoch ziemlich.stens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, wasch Korpusebeneken wie blau nicht Lage, diese Übersetzungen erfassen. Und einige Leute haben eine gezielte Bewertung von kontextabhängigen Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachen, da sie in der Regel auf Domänenwi und die menschliche Kuration angewiesen. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens: Wann erfordert die Übersetzung Kontext? Und zweitens, wie guthab Modelle mit diesen Fällen? Um die erste Frage zu beantworten, begann wir zunächst zu messen, wie viel ein Wort von der Kontext der Übersetzung abhängt. In der vorherigen Arbeit haben wir CxI als Maß für Kontextnutzung durchmaschinenübersetzungsmodelle eingeführt. Dies wird indem Messung, wie viele Informationen der Kontext C über das Ziel y anges der Quelle xliefert. Sie können Cxmi als die Information vorstellen, die durch Kontext Modellhält. In dieser Arbeit erweitern wir cxmi auf Punkt y cxmi, die Kontextnutz auf Satz oder auf der Worterebene messen kann. Wir können uns Wörter vorstellen mit hohe p6mi haben als vorstellen, die Kontext für Übersetzung erfordern. Jetzt analysieren wir Wörter mit ho p6mi durch, um nach Mustern zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse von Abschriften von TED-Talks durch, die aus Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuers schauen wir uns Teil Sprach-Tags mit hohe PxMI. Da können wir zum Beispiel Du Pronomen Arabisch, mit relativ hohe PxI haben. Dies kann erklärt, weil Englisch keine doppel Pronomen hat, also braucht man Kontext, um festzustellen, ob ein Pronotiv beim Über ins Arabische übersetzt ist. ähnlich finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die entsprechende Verbform ausen wollen. Wir schauen wir uns Voschatz an, mit hohen P6Ischnitt über alle verschiedenen Vor Ereignis. Und hilft uns, Fälle wie den hier zu identifizieren, bei Sie auf Chinesisch Kontextöt, um pop Substantive zu übersetzen, um sicherzustellen, dass Sie die gleiche Übersetzung innerhalb des Dokuments verwenden. Und ähnlich stellen wir fest, dass der Kontext unterstützt wird, um in der richtigen Formalität zu übertragen können. Und schließlich schauen wir uns verschiedene einzelne Token an, die hohe pxmi haben, Und können wir uns Phänomene zu identifizieren, die nicht wirklich vom Wort selbst erfasst werden können, sondern in der Standardstruktur ausgedrückt wird, wie die Ellipseauflösung. Jetzt verwenden wir nun unsere Ergebnisse aus unserer Analyse, um einen Maßchstab für die Übersetzung von Dokumentenebene zu ent erstellen. Für jedes der fünf DisPhäomene, die wir identifiziert haben, haben wir Taggger, um automatisch Wörterzieren, die mit dem Phänomen beziehen, Und wir nennen unseren Taggger den multisprachigen Diskursbewusst oder Muda-Tgger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anporte dieser Diskursphäomene haben. Wir verwenden dann den Muda-Tagger, indem wir den Taggger auf einen parallelen Korpus anwenden, den wir zur Bewertung verwenden möchten, Und wir wenden unsere Übersetzungsmetritriken auf die kontextabhängigen Beispiele an, die der Mda-Tgger identifiziert hat. Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle zu bewerten Auf der Maschinenübersetzung Dokumentebene. Zunächst einmal, wenn wir Korpusebeneken verwenden, Blau stellen wir fest, dass agnostische Modelle die beste Leistung haben, Aber wenn wir dann Kometen verwenden,schneiden kontextbewusste Modelle am besten ab. Und wenn wir Wort f verwenden, haben Modelle mit oder ohne Kontext eine vergleichbare Leistung. Dies zeigt wiederum, dass es schwierig ist, die besten Übersetzungssystem auf Dokumentebene zu bestimmen, wenn wir allein Corpus Metrikenpus verwenden. Jetzt verwenden wir den Mouda-Bchmark zur um Modelle zu bewerten, Und wir stellen fest, dass kontextabhängige Modelle deutlich genauer sind als Modelle, die keinen Kontext für bestimmte diskkur Phänomene verwenden, Formalität und lexiischer Zusammenhalt. Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext zu anderen Phänomenen wie Ellipse, Pronomen und Verbform verwenden. Das deute also darauf hin, dass wir mehr Fortschritte für die Übersetzung Dokumentenebene sehen müsste. Wir habenlichen auch verschiedene kommerzielle Systeme verglichen, und unser Maßchmark zeigt, dass dbel in der Regel genauer ist als Google Translate für die Übersetzung von Dokumentenebene. Zusammenzufassend führen wir eine datengesteuerte Analyse in vierzehn Sprachpaare durch, umzustellen, wann Übersetzungen Kontext erfordern. Und dann verwenden wir unsere Verffinungen, um einen Maßchstab für die Maschinelleübersetzung von Dokumentenebene zu erstellen, die uns helfen kann zuzustellen, welche diskreten Phänomenale gut gehabgehen können oder nicht, und welche Übersetzungssysteme gut in der Übersetzung von Dokumentebene sind. Vielen Dank für Ihre Aufmerksamkeit, bis sehen in Toronto"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": ": Hallo zusammen. Ich bin Jenny, Doktorand an der Carnegie Mel University. und heutestelle Ihre Arbeit NL Positionality, DesignVurteile und Betasätze Modelle. Diese Arbeit wurde in Zusammenarbeit mit einigen Leuten an der Universität of Washington und dem Allen Institute for AII, nämlich Sebastian Santi, Rone Labrasse, Katharina Reinika und Martin Saapp. Stellen Sie wir vor, dass Sie für eine Zeitung arbeiten und Kommentare unter Ihrem Nachrichtenartikel durchen und giftige Inhalte zu entfernen. Vielleicht könnten sich einer beliebten API wie Perspektive API zur Toxizitäterkennung. Und das funktioniert sehr gut, wenn Sie Carl Jones, wo Perspektive API, giftige Fälle erkennen, aber das ist bei Ditha Sharma nicht der Fall, wo Perspektive API nicht so empfindlich gegenüber beleidigende Begriffe, die in indischen Kontexten häufig sind. Dies ist ein Beispiel für eine DesignVgenommenheit, bei wir systematische Leistungsunterschiede der Technologie zwischen Bevölkerungen sehen. Design Vorurteile wie die wir zuvor gesehen haben, könnten aufgrund der Positionalität der NLP-Forscher und Modellentwickler er. Positionalität ist einfach die Perspektive, die Menschen aufgrund ihrer demografischen, Identität und Lebenserfahrungennehmen. Das ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Bereichen. Als Forscher kann Positionalität den Forschungsprozess, die Ergebnisse und Ergebnisse beeinflussen, weil sie die Entscheidungen, die Forscher treffen. Eine Frage, die die Leute stellen könnten, also: Hab Datensätze und Modelle Positionalität? Und wir versuchen nicht zu sagen, dass Modelle selbst und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie Urteile und Meinungen echter Menschen und können so bestimmte Positionalitäten gegenüber anderen darstellen. Prior Arbeit hat einige anekdotische Beweise für Positionalität, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen der Modellpositionalität. All Diese Arbeitenfassen nicht, Endnutzer mit den Datensätzen und Modellen selbst. Und das Untersuchung der Position von Modell- und Datensätzeposition ist  immer wichtiger, da nlp-Tufs subjektiver und sozial orientiert werden. Und es ist eine Herausforderung zuakterisieren, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter Apis versteckt. Um Positionalität von Datensatz- und Modellposition zu studieren, Wir vergleichen die Anmerkungen mit echten Nunutzern mit bestehenden Datensätzen und Modellen. Wir tun dies durch unseren Rahmenmework nl Positionalität. Unser Rahmen funktioniert in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit verschiedenen Anmerkatoren, und wir entscheiden dies indem Betra der Demografie von ursprünglichen Datensätzenotatoren, denn in der kommen nur wenige Anmerkatoren jede Instanz und weil Demografie selten gesammelt und geteilt werden. Deshalb so entscheiden wir uns für, die Daten neu zu kommentieren, um viele Anmerkatoren zu erhalten und eine reichhalte demografischer Daten zu erhalten. Wir nehmen dann die Anmerkungen nach demografisch und vergleichen sie mit den Modellen und Datensätzen mit R- Korrelationswert vergleich. Und so unterscheidet unser Rahmen von der Anmerk Meinungsverschiedenheitenli, indem wir Endnutzer mit Modellen und Datensätzen Vorhersagen und Etiketten vergleichen, im Gegensatz nur der Anmerkatorvereinstimmung oder Modellierung Annotatorverteilungen. Unser Framework wird weitgehend über lab in the Wild aktiviert, Eine Online-Crowdsourcing-Plattform, ehemaliger Hci-Kollabor. Und lab in the Wild ist eine Online-Experimentationsplattform, mit der wir Frei im Vergleich zu den Plattformen wie mturk, die größtenteild Teilnehmer aus den USA oder Indien haben. Außerdem außerdem hinaus kann Labor in the Wild immer noch in der, hochtativwertige Daten zu erhalten. Wir veranstalten zwei Aufgaben Labor in Wild, Eine ist soziale Akzeptanz. und funktioniert, dass die Teilnehmer eine Situation aus sozialen Chemie lesen und schreiben, wie sozial akzeptabel eine Situation ist. Dan Um an der Studie bleiben, können sie ihre Reaktionen auf einer KI und anderen vergleichen. Wirg diese Anmerkungen mit Sozial Chemie, Delphi und Gpd vier verg. Wirli haben wir eine sehr ähnliche Einrichtung für dieennung von Toxizitäts- und Hassre, bei der sie eine Instanz aus Dyna Hass lesen und schreiben, ob sie es für Fall für Hassreden halten. Dann verglichen wir diese Anmerkungen mit Dyna Hass Perspektive API, Rewite API, Hass Roberta und gpt four. Unsere Studie und das Ende über sechzehntausend Anmerkungen von über.000 Anmentatoren aus 87 Ländern. Wir sind wir besser ger beantworten, mit wem NLP-Datensätze und Modelle am meisten übereinstimmen? Wir finden, dass in NLP. Wir finden, dass Datensätze und Modelle am meisten auf englischsprachigen Ländernstimmen. Bei diese soziale Akzeptanz stellen fest, dass es am meisten auf konfuzianischen und englischsprachigen Ländernmmt. Wir stellen fest, dass Dynah Hass am meisten auf englischsprachigen Ländern. Wir finden auch die zusätzliche Einrichtung mit Menschen, die eine Hoch-Ausbildung haben. Für gpd4 in der Aufgabe der sozialen Akzeptanz stellen wir, dass sie am meisten auf Menschen mit einer Hoch-Ausbildung oder Graduiertenbildungrichtet ist. Und wir finden Gleich für Danny Haight, wo es am meisten auf Menschen mit einer Hoch-Ausbildungrichtet ist. Wenn Modelle und Datensätze auf bestimmte Popsgruppen ausgestimmt sind, bleiben einige unweigerlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger auf nicht-binäre Personen im Vergleich zu den Männern und Frauen. Wir finden die in GPD Akzeptanz sowie die Analy DinerHas. An es Ä Ana NP gibt, was können wir dagegen tun? Wir haben einige Empfehlungen. Die erste ist, gesamten Aufzeichnung alle relevanten Designentscheidungen während des gesamten Forschungsprozesseshalten. die andere ist NLP- Forschung mit der Linse des Perspektivismus. Unsere dritte Empfehlung ist, speziellziaer Datensätze und Modelle innerhalb vier bestimmten Gemeinschaften erstellen. und ein gutes Beispiel dafür ist die Muakanne-Initiative. Ich wir möchten betonen, dassklusive NLP nicht nur alle Technologien für alle funktioniert. Und ist unsere Präsentation been, aber wenn Sie mehr erfahren möchten, schauen Sie sich unser Dashboard für die aktuelltualen Analyseergebnisse und unsere Arbeit. Vielen."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo und ich werde über unsere Arbeit zur Lösung indireterscheidung Ausdrücke für die Entitätauswahl, in der wir den Alt-Eitätpus einführen. Mein Name ist Javad Hosseini und das ist eine gemeinsame Arbeit mit Philipp Raddinsky, Sylvia Parity und Annie Louis. Unser Ziel ist es, die Sprache der Benutzers zu verstehen, wenn sie eine Wahl treffen wollen. Ichtra diese alternative Frage: Meinten Sie \"Ey auf mir oder \"Ich habe ein Gefühl? Hier möchte ein Benutzer zwischen einem dieser beiden Lieds auswählen. Am offensichtlichste ist, eine direkte Referenz verwenden, zum Beispiel indem er den Namen des Lied \"Easy on mich\" oder seine Position, die erste. Aber manchmal ist eine indirekte Referenz angemessener, ein natürlicheres Gespräch zu führen. Dies könnte passieren, wenn sich der Benutzer nicht an den Namen des Liedes erinnern kann. Oder die Ausspracheen sind ein zu ähnlich und schwer zu gleichscheiden. Oder wenn der Benutzer eine Präliebenz angeben möchte. Hier sind einige Beispiele für indirekte Unterschiede, Zum Beispiel das neuere oder das Zeichen, das nicht energisch ist. Dies ist ein wichtiges Problem in Gesprächssystemen und auch für die Benchmark des Entitätsverständsses vonm. Wir sind eines öffentlichen Datensatzes nicht  bekannt, Einer öffentliche Datensatz für eine Aufgabe, so sammeln wir eine mit Mass-Amerkungen. Unser Datensatz umfasst drei verschiedene Bereiche ab: Musik, Bücher und Rezepte. Unsere Datenfassungmethodik betont Informalität mit einem Cartoontig. Der Kartoon hat drei Sprachblasen. In der ersten Blase sagt Bob: \"Einnern du dich an das Lied, das wir gesternhört haben?\" Und damit stellt Bob den Dialogkontext. In der zweiten Sprachblase sagt Alice: \"Wst du leicht mir?\" oder \"Ich habe unser Gefühl?\" Das ist die alternative Frage. und in der dritten Sprachblase Bob eine indirekte Referenz, um eine dieser Entheiten auszuwählen, zum Beispiel die neuere. Wir stellen die erste und zweite Sprachblasen automatisch an, aber die dritte wird vom Anmerkator ausgefüllt. Die erste Sprachblase wird aus einigen manuellen Aufforderungen pro Domän ausgewählt. Die zweite, die die alternative Frage, wird wie folgt erzeug Wir verwenden immer eine einfache Vorlage meinen Sie a oder b? Wo a und b aus Wikipedia werden? Hier sind die verschiedenen Stbenmethoden, die wir verwendet haben. Wenn wir in der Liste höher bewegen, Die Entheitenen sind ähnlicher, und ist in der schwieriger, die Ungleichgation vorzustellen. Die erste ist einheitlicher Trend. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen, sie zurückgeben. Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und schließlich wenn sie ähnliche Infofelder oder Attribute auf Wikipedia haben, zum Beispiel das gleiche Genre oder derselbe Künstler für Lied. Wenn wir diese alternative Frage dentern zeigen, kennen sie den Namen dieser Einheiten, aber sie wissen nicht unbedingt von die Einheiten. Wir zeigen ein Hintergrundkenntnis über die beiden Einheiten. Für Lieder zeigen wir einfach einen Google-SuchLink zu jedem Lied, und bitten die Anmerkatoren, mindestens einen jedes Liedeshören und über jedes Lied zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für das Ling easy und mich. Für die Rezepte und Bücher zeigen wir einige Hintergrundtext. Für Rezepte zeigen wir ihre Bilder erneut von Wikipedia, damit die Anmerkatoren wissen, wie sie aussehen. Dann habenten wir die Anmerkatoren, eine dieser Entheiten auszuwählen, zum Beispiel hier die erste, Und beschreiben sie mit drei bis fünf indirektenweisenden Ausdrücken, Zum Beispiel die mit der Klaviermusik. Hier sind einige Beispiele aus unserem Datensatz. zum Beispiel die ohne Worte, nicht die mit dem 12-jährigen Jungen oder dem fiktive, oder aus Aserbaidschan und so weiter. Der AlterdentitätCopus hat .000 alternative Fragen in drei Bereichen, und hat 42.000 indirekte Ausdrücke Die Ergebnisse mit T5x Modell werden unten zusammengefasst. Wenn das Sprachmodell Zugang zu genau demselben Hintergrundkenntnis hat wie die Anmerkatoren, dann ist die Genauigkeit sehr hoch. Es liegt etwa 92 bis 955%. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugang zu einem teilweise überladen Hintergrundwi hat, dannträgt die Genauigkeit zwischen 82 und 7%, was realistischer ist. Zum Beispiel, wenn das Sprachmodell das Hintergrundwi abt. Wenn das Sprachmodell auf Einnamen hat, dannträgt die Genauigkeit nur 600%. Es gibt viel Platz für Verbesserung. Wir haben auch gezeigt, dass die Modelle domän verallgemeinerbar sind. Hier ist ein Link zu unserem Datensatz. Vielen fürs Zuschauen"}
