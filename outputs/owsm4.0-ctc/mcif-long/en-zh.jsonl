{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "大家, 我叫马thias Lind德mann, 今天我将大家简介绍关于构概括 使用多集标记和潜在排列。 这是与我的顾问亚 Alexander Kol拉和伊van Ti托夫。 构图概括可以理解为学习者处理更深次归训练单。 在语义解析背景下, 构图概括的测试这样。 像往常一样,我们有组的话语, 在这种情况下,女孩睡了,玛丽知道女孩睡了。 这些话语与逻辑形式对, 代表它们意义义的核心方面。 与标准的机器学习评估相反, 测试集并不来自相同的分布, 包含结构上看不见的逻辑形式。 在这个例子中,模型在训练过程中递归, 例如,更深层的递归。 天真的序列序列模型与这种分布概括, 经常产生与输入离的输出。 特别是,它们通常无法复制输入和输出之间的系统对应, 比如示例中颜色编码的。 解决这个问题方法是将树木集合入模型中。 树旨在捕捉将话语与逻辑形式相关的 这很好, 但是树通常没有给,需要以某种方式获得。 这可能复杂,有时计算昂贵的过程。 通常,这涉及辑公式预处理, 例如处理可变量符号。 获得树也可以涉及专门的语法感入程序。 在这篇论文中,我们不使用树,而是引入一个神经序列到序列模型,直接模拟输入片段和输出片段之间的对应。 我们第一次更深层次递归而依赖。 我们的方法预测输入的输出为两步骤。 首先,我们将每个输入令牌输无订序的多组令牌出现在输。 第一步后, 我们有所有正确的令牌,但它们没有订序。 这就是为什么在第二步, 我们使用另一个模型来预测排列 将入正确的顺序。 我们引入一种新的方法来预测排列, 不会对可能的排列严格的限制。 这使得我们的方法非常灵活和表现。 从概念上,我们的排列模型大这样工作。 我们输出从左到右,并确定放在每个位置放多集令牌。 对于第一个输出位置, 我们只需选择一个红色突出显示的。 然后我们跳到下一个多集令牌,以确定输出中的第二个令牌 我们输出第三令牌 跳到另一个多集令牌。 我们继续这个过程, 直到第一阶段的每个令牌都一次。 为了你们实验结果, 将的方法与考格基准上的其他无树模型进行比较。 我们的模型概括到更深回归。 其他类型的结构概括仍然很挑战性。 在我们的论文中,我们解决了几个有趣的技术挑战。 首先,输入和输出之间的对齐训练数据。 因此,对于给定的令牌, 我们不知道它来自哪多集, 这给训练带来挑战。 此外,有时有多个列与数据一致, 但语言正确的是延迟。 我们导对作为训练的一部分来。 我们的排列方法非常灵活, 但带来挑战 找到分最高的排列是硬。 这是因为与旅行推销员问题有关。 我们用GP友 持续放松, 让我们通过解决方案, 学习语言更合理的排列。 如果你想了解更多关于我们的实验 以及如何应对这些挑战, 请看看我们的论文或来到我们的海报"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "嗨,我是米拉,今天我们将讨论我们的标记人物的论文, 使用自然语言提示来测量语言模型中的刻板印象。 这项工作与埃s德mush和丹jerovsky合作完成的。 近年来,许多人记录了大型语言模型社会偏见和刻板印象或LLm流行。 然而,这些措施量有各种局。 它们通常依赖手数据耗时。 它们通常只测量非常具体的刻板印象, 这意味着它们不会与其他人口或背景, 或者它们只是捕捉非常普遍广泛的联, 比如与特定群体的负面联系。 此外,这个领域的大部分工作并不解释交叉性,即多方面的社会身份可以复合偏见,成为独特的伤害。 为了克服这些局限性,我们依依赖这些新的指令调Lm非常擅长响提示中的说明令。 所以我们可以要求模型生成一个角色, 这是想象中的使用提示,比如想象你是一个亚洲女人,描述你自己。 我们可以立即看到这对任何人口统计都是非常概括, 因为我们可以将我们想要的任何身份标记为这个提示中。 所以这里GPT4的例子例代。 我们立即发现, 虽然输出传统意义负面, 但有一些有趣的模式。 亚洲女性被描绘为不谦, 中东女性使用异国情调词, 指一个迷人的区域, 有色人种女性角色都提到祖先,而白人男性角色没有这种东西。 为了捕捉这些模式,我们的方法有两部分。 第一个是生成这些角色。 我们生成这些角色的提示受到一项研究的启发,他们向人类受这些提示,发现通过将其给人类对象, 他们能够展示种族刻板印象。  我们生 和人反应直接比较。 第二部分是标记文字词, 识别分标记群体和标记, 详述。 好处是,我们获得非常具体的刻板印象和模式,而不必依赖任何特定的词汇。 因此,标记单词方法借鉴了社会语言标记社会概念,它指有一个没有标记的默认值,任何不同于默认的群体都是语言标记。 例如,男人这个词,或对不起,战士这个词通常与男性有关在一起。 所以当人们描述一个女人战士女人时,他们通常会指定一个男性为战士,并以女人标记这个术语。 社会中主导地位群体在语言上和社会上都没有标记,而边缘化群体通常是标记. In unserer Methodezeichnen wir also zunächst, was die nichtken markierten und markierten Gruppen sind, Und dann vergleichen wir die Personenen mit der Kampfwörtmetde, die im Grunde gewichteten Logos-Verhältnisse verwendet, um die TopWörter für jede markierte Gruppe zu unterscheiden. zum Beispiel für die Personenönlichkeit schwarzer Frauen, Wir würden Kampfwörter machen und vergleichen die Lo与白人和男性角色比,因为这是两个相应的未标记组。 现在一些结果。首先,我们使用刻板印象的词典,我们发现生成角色包含印象比人写面更多的刻板印象。 然而,当我们实际观察词典中词分布时, 我们发现了非常不同的东西。 所以生成角色比词词高, 人写广泛分布, 而生成中的刻词 只是高和运动。 所以只有正或至少非消极。 事实上,这个词汇并没有真正捕捉到我们在早期幻灯片中看到的许多有害模式。 因此,为了此,我们将转向我们标记单词方法的结果,以展示这些看起来积极单词如何促进刻板印象和本质叙事。 在我们的分析中,我们揭示了这些看似积极的描述是如何反映有害的模式。 首先,对于 Markgruppen die Top Wörter Dinge wie Kultur, Tradition,tisch Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als unterschiedlich von der weißen Norm. Dies trägt zu einem langen Ermächt von Diskriminierung und Anders für diese Gruppen bei. Außerdem hinaus gibt es viele gemeinsamer Tropen, die in diesen Wörtern widerspiegeln, insbesondere für有色人种女性。例如, 描述拉丁裔女人的单词包括活力和弯曲的东西, 这与热带主义的比喻联系。 对于亚洲女性来说,这些词是小、精致和丝绸的东西, 这与亚洲女性过性化的悠历史,被视为非常温顺和顺从等等。 最后,对于黑人女性来说, 我们看到一些顶词是强大和弹性的东西。 这与人们称之为强大的黑人女性原型的原,虽然乍一看听起来是积极, 有工作表明,这种原型实际上非常有害,因为它给这些人口施了很大的压力,和对御社会障碍。 因此,与其真正努力改变这些障碍,而是给这些人克服它们, 这导致这些人和其他伤害非常负面的健康。 更广泛地说,我们发现每个标记组的单词几乎只是反映了非常基本的叙述。 因此,根据这些模式,我们以为模型所有者的三个建议。 首先,作为研究人员,我们应该解决积极的刻板印象和基本质叙事。 我们还应该使用交叉镜头来研究偏见和伤害,因为如果我们不这样做,有很多事情可能会被忽视。 最后,增加对偏见缓解方法的透明度, 因为例如,就像这些积极的刻板印象一样,我们不知道这是否是因为有某种奇怪的过度过度的价值对齐正在进行, 或者一些其他印象导致这些有害模式板。 我们真的任何假设没有更多的透明度假设或研究。 非常感谢你的倾听。 CL开心"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是J姆斯· Fin奇。 我是莎拉· Fin奇。 今天我们将告诉 ABCEV, 评估对话人工智能方法。 这项工作是由 EmoryLP实验室 由埃默里大学的吉诺 Cho伊教授, 与亚马逊 Alexa AI合作。 :假设你刚刚开发了一个对话模型,你想看看它与当前的艺术状态相比。 常见的做法是使用人类评估, 比如要求人类评选择两种对话中哪更好, 或者给液体尺度评对话评。 这些方法很好提供整体对话质量的整体评估, 但是对话质量有许多方面。 因此,您你可能需要评估聊天质量的维度,以了解在细度层面上模型的优势度和弱点。 一种方法是简单地要求人类评评估几个对话质量的维, 比如使用现有的比较或液体尺度的方法来模型的相关。 然而,我们认为维度对话评估有一个更精确和可靠的策略。 我们的方法试图通过明确注释每个模型反应应是否表达某些行为来减少人类评估主观性, 比如用无关紧信息做出或。 我们简而言称之为这种方法注释行为, 或者abc eval。 我们开发了这种方法来全面涵盖聊天模型,献建议在最近文影响聊天质量聊天行为。 ABCc eval能够测量聊天模型犯各种主题错误的速度率。 例如,abc eval测量聊天模型忽略其伴档或说一些无关紧要的话的弯数量, 与自己或其伴档矛盾, 幻觉不正确的事实或违反常识知识, 以及模型成功或未表现同理心。 为了确定什么样的评估最有效的, 我们选择了四个最先进的聊天模型,并使用ab模型一百个人机器人对话。 比较,我们还使用三种现有的方法评估这些对话: 转级别的评级, 对话级上的酒评级, 和对话级别对比较。 对于现有的种方法, 我们收集对对话八个最常见测量的八个方面评估,因为这是沿着多维度评估聊天模型的标准做法。 从我们对这些评估结果的分析来看, 我们发现abcB行为标签总体上比现有方法收集的标签更可靠, 正如 100个双标签的对话注释来量。 此, ABCEval标签与现有方法的指标整体对话质量, 这个简单的线性回归分析显示。 例如,你可以看到测与自我和伴矛盾比例 如何解释对话质量的和, 而平均酒体一致性分数只解释4%或更少。 最后,我们检查了每个评估指标是否使用一步线性回归捕捉聊天质量的一个独特方面。 您可以看到所有abceval指标的组合如何解释了25%的对话质量2, 当你一次删除指标时, 大部分导致丢关于质量的信息。 另一方面,所有转级液体指标的组合 解释了质量, 指标包含独特的信息。 这些可靠、信息和独特的AEval指标 使我们能够比以前的方法实现的更高的分辨率来。 你可以在实验的结果中看到,几个挑战仍然,被精确量化。 例如,我们测试的机器人大约20%的反应中有常识违。 它们大约15%的反应中产生无关相关信息, 1的时间。 随着领域改进, 这些错误率自评估以来的新模型下降。 然而,这比较可靠和精确的评估指标比较 我们希望abc eval能够被该领域其他人利用作为朝这个方向的意义步, 我们期待着看到ai工智能在未来几个月和几年进步。 感谢收看"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "大家,我叫瓦苏udha, 我是斯通尼布鲁克大学的计算机科学博士学位候选人。 我想介绍ACL23 作为一篇篇文 失谐检测学习, 解决罕见阶级挑战。 我们首先定义认知失调 以及为什么语言研究的一个重要问题。 简单,认知失调是两种不一致的, 比如这个例子, 一个人说, “我知道香烟会杀死我” 然后接着说,“我在会议后喝了几烟。 这种信念和行动是不一致的,它们处于不谐 进一步提到我认为我可以没有他们保留我的工作证明第二次事件是正当的,他们有共谐关系 虽然不谐是非常常见的我们在日常决策中经历非常常见,但它们很少在其他类型话语关系中表达。 那么为什么这关系? 研究认知失谐可以帮助我们理解分歧的影响, 轨趋势、信仰、价值观和态度变化。 高认知失调也与焦虑障碍有关,可以助于更好地理解人们的心理健康。 研究语言中表达的不谐也可以对理解弱势的极端主义和两极分化有益。 最后,认知失谐对理解个人的个人认知风格很重要,并助我们更好地理解决策过程 为了创造认知失谐资源的目标,我们进行了对失谐关系的的大注释。 我们使用方法,正如流程图中看到的。 推文使用PDtb解析器通过,对话语单位根据我们论文中描述的指导方则注释。 正如在这里看到的, 失谐只在.5%的注释对3.5% 在收集了大约 1000个话语单位对例子, 我们初始分类,  43个距离例子。 毫不奇怪,分类符表现比偶好。 鉴到距离谐 数据, 我们面临绝对罕见的问题 为了减解这一点,我们转移学习和主动学习的组合来注释,以便更多的在较小的注释收集更多的谐样本,降低整体注释成本,同时提高失谐检测 由于最初模型根本无法捕捉失谐类,。 我们从密切相关的任务转移权量来开始学习过程。 我们从两个不同的任务: 主题独立不谐立态分类, 决定不同的人两个辩论分, 。 叫做辩论, 和Ptb扩展和比较类的二制分类,因为两者与辅音和失谐的概念密切相关,我们在这里称之为cee。 我们发现在在注释数据集上转移零短性能已经比机会好得多,auuc点62。 此,在任务的迭代细调, 我们发现CE任务的精调,然后辩论进一步微调 产生更好的零性能。 因此,这是我们用来共同启动活学习的模型。 接下来, 我们确定更新每轮活动学习和注释新数据的模型。 累积迄累到从主动注释中收集的所有数据, 而迭代通过最新数据集来更新模型。 在不同的策略中,我们发现累积表现比迭代表现相平等或。 接下来,为了提高失调示例的数量,我们使用罕见类级策略的概率prc概率主要选择在任何很当前模型不谐的例子。 我们将其与在社区中的最先进a策略。 我们发现提议的prc策略比其他最先进策略,  虽然差异很小。 请注意随机表现低。 在有两个最佳策略, 我们提高距离分类AUC到.75,这是我们迄目前为止任务的最好的佳表现。 我们还检查每个注释注释质量和成本的可行性。 我们发现prRC失谐最高,最适合罕见班。 然而, 注释员也发现这些例子例困难。 总结之,我们发现prc是一个简单的罕见类级收购的策略, 与适当设计的转移学习任务可以很大帮助。 我们还发现迭代更新对于不同领域转移学习有用,而域内主动注释受益于累积更新。 这些是我们我们的代码数据集和论文的链接。如果您有任何问题,请随时与我们联系。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好, 我是Ma谢塔, 今天我和我的合著者马丁和我正在展示我们的作品 KiIT: 评估知识整合。 这项工作是麦吉尔大学、米拉和微软研究的合作。 国家语言理解模型借各种知识来源, 例如参数中知识, 通常通过预训练获得的, 以及推断时间输入给的知识。 问答等任务的工作显示,模型可以使用预训练的时间知识来解决任务。 但是自然语言理解通常需要知识推断时间提供知识。 比如,在 “约翰在电视上看到新选的总统, 预训练的参数可以包含关于总统 电视的信息, 但他们不能靠知道这个实实体约翰, 或者新总统是谁, 因为总统从预训练例可能改变。 因此, 知识密集型NLU任务的成功模型需要整合和使用预训练的时间和推断时间知识。 我们提出了一个知识集成诊断套。 我们引入了一个共参考分辨率任务, 旨在探利用取不同来源可知识的能力。 我们与人类研究参与者评估数据集,并建立共同参考分辨率模型。 数据集的一个例子。 塞文是一名法官。基a是面包师文和基a元公园。 在工作决定法律法规中的案件后, 他很乐意放松。 这里的任务是确定他指的代词的正确实体, 在这种情况下,是仆文。 给定代词的解需要两种类型的信息。首先, 实体知识,如仆是教堂。 错误知识比如法官在法法庭决定案件。 一般来说,背景知识在大语言模型预培训中学到的, 而实体知识通常在推断时间观察。 我们改变这两信息的可用性,以在一个单个来源或多个来源发现 我们定义了 KiDmos的三个设置。 首先,我们有典型的设置: 预训练, 后知识在预训练时。 第二,有一个后两者设置, 知识在预训练前时间和推断时间。 最后,后扰入设置, 两种知识类型只在推断时间获得。 最后一个设置特别有趣, 因为它模拟了解决任务所需的背景知识不是模型预训练数据的一部分, 例如,因为自预训练以来新的职业发展。 有一个说明我们如何控制两个来源中事实可用性的一个例子。 在后预训练环境中, 我们假设知识政治家在政府选席位的知识包含在预先训练的参数中。 在涉时间下, 我们提供反具体知识奇切斯特是政治家 在后两者设置中,我们外提供反具体关于干涉时间背景下政治家背景知识。 在背景劣环境中,我们提供特征职业Mtour而不是替政治家, 因为Mtour不太可能包含在预训练的参数中。 我们与人类研究参与者评估并建立偏辨率模型。 在这个图中,我们显示在背景预训练中最困难的变体中最佳模型结果。 在没有ki没有任务训练, 两种模型都表现好。 然而,在在kidmus上训练时, C2f和 forqf比随机选择好。 这表明,在通用引解率数据集训练时, 模型学会了利用表面线索, 在在kimus上除这些线索除测试时没有用。 虚构知识的实验表明,即使是最表现最好的模型也只有在自由寸的时间内靠集成知识。 为了总结我们论文的主要收获, 许多协进化模型似乎无法没有任务训练靠不同的推合知识推理。 然而, 有了任务训练, 一些模型成功地将从动机来源集成知识。 尽管, 即使是最表现最好的模型似乎断可靠集成后知识困难。 如果你对更多细节感兴趣, 请参看我们的论文并查看github上的数据集和代码。 感谢收听"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "大家,我是来自多伦多大学和 Brun诺·凯斯ler拉。 我将简要介绍作为同时语翻译论文指南, 与马te奥·内格里和Mar可 Duki。 是同时语翻译? 同时语翻译,或SST是实时将口语成另一种语言成成文本的过程, 使跨叉语言沟通。 目前前的SST模型问题? 特定架构通常训练引入额外模块优化。 长而复杂的训练过程, 比如,涉及不同的优化目标, 训练和维护几个模型 来达到不同的延迟方。 例如,训练一个平均一秒延迟模型, 另一个两秒延迟等等。 那么我们的解决方案是什么? 首先,使用已经存在有的离线ST模型 不训练或采用特定模拟SD。 只使用一个模型每个延迟系统模型, 通过特定参数处理延迟, 利用模型知识通过音频输入和语本输出之间的力机制, 交叉注意力机制。 你可以右边看到一个例子。 我们的解决方案是提出或编码注意力, 决定 基于注意力指翻译 如果张力不集中,发出, 也就是说这个总和于一定α 最后的la语音框, 这意味着接收的信息足够的稳定。 例如,如果我们收到包含“我要谈, 我们的模型预测德语翻译, 我们将看看交叉注意力 重量, 我们将看到前两个单词指向最早收到的语音框, 而最后一个词指向最后接的语音为lambda语音帧。 这意味着前两个词将发出, 而由于交叉注意力的总和在一定α, 我们不会发出最后一个词, 然后等待另一个语音。 如果我们继续收到另一个语音, 我们的模型预测三个单词,我们将看交叉注意力权量。 我们会看到没有单词指向最后一个lambda语音帧。 这意味着这三个单词将被省。 如果我们看看点的主要结果, 我们将,一边有蓝色测量翻译质量和平均后 延迟测量, 我们还考虑计算意识平均缺,解释模型的计算时间来预测输出。 所以我们希望我们的在这个图节上尽可能。 但我们也希望它们在左边移动。 我们比较策略也适用于离线模型,wi键策略和本地协议。 我们与专门针对同时语翻译定的先架构比较。 这些都是德语的同时语翻译策略的所有结果。 我们看到do优越了所有应用于离线模型的所有策略, 因为曲线左。 我们也看到,如果我们考虑实际延逝时间 或计算时间, 适应是最快的策略。 如果你想发现更多的结果, 阅读我们的论文, 还发布了开源代码和模型和同时输输,以促进我们工作的可复制性。谢谢的关注"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。 我叫舒恒。 今天我要介绍我们的论文康奈3命名实体标记在23年? 让我们开始吧。 我们的论文了使用命名为实体识别任务 或者 NER task we observe that models have been using ConO 2003 to develop NER for almost 20 years and this naturally raises several problems firstly, can these models generalize to modern data and when we develop new taggers what is needed for good generalization? 同时,如果我们观察概括, 是什么导致这些模型的性能下降? 为了调查这些问题, 我们开发了康奈++ 数据集。 这是从20 年从路透新闻收集的数据, 然后用同康奈2003注释指南注释。 然后我们在康奈3了20多个模型, 康nel3测试集和康nel+测试集上评估它们。 最后但同样重要的是,我们计算了f 1的百比变化以评估每个模型的概括。 那么概括呢? 通过我们的实验, 我们发现有三主要成分。 第一个是模型架构。 通过我们的实验,我们发现变换器模型通常更好对新数据。 第二因素是模型尺小。 我们发现通常较大的模型 会更好的概括。 最后但同样重要的是,我们都知道 微调示例的数量 直接影响下游任务的表现能。 在这里我们也发现更多精调示例 也会导致更好的概括。 下一个问题: 是什么导致模型的性能下降? 我们有两个假设。 第一个是适应性过合, 重复使用同一个测试的, 通常在新测试集的减回表现。 第二个假设是时间漂移, 这是列车和测试数据之间增加的时间差距的性。 对于适应性过度, 我们从右边的图表中看到了 红色最合线具有梯度大于 1度。 这意味着我们在2003每改进 都转为++单位改进, 这意味着回报减少。 这适应性过度没有观察。 那么时间时漂移呢? 对于临时漂移, 我们做了一个实验 重新训练或预 最新的数据, 我们发现性随着时间差距下降, 这证实了我们的假设,即性能下降的主要原因是时间漂移。 我们的结论是,为了好的概括, 我们需要一个更好的模型架构, 更大的模型尺小,以及更微调的例子, 这些手进进行。 我们不能仅仅一种成分, 而是其他。 同时,我们也发现这里的性能下降是由时间漂移引起的, 令人惊讶的是, 它不是适应性合引起, 尽管康诺2003已经使用20年。 回到我们在论文标题中提出的问题, 我们发现答案实际上是一个响亮的肯定。 我们希望我们的论文需要更多关于如何改进模型的概括的研究。 最后,请一定要查看我们的论文,我们的数据集,如果您有任何问题,请随时联系我。 非常感谢"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "大家, 欢迎来到我们d平面的演示, 在文档级别和句子级别上文的新语库。 我叫regina娜·施 sto登,我将指导导您完成演示的第一部分。 让我们首先定义文本简化。 文本简化是适应文本的过程以提高对特定目标组文本理解的过程, 例如阅读问题或非母语的人。 要训练文本简化模型,我们需要平行对文本,例如文档或句子。 在这里的例子中,您可以看到复杂的德句子的平行对齐句子,并翻译成普通语言。 为了简化这个句子,不同的技术是,正如你在在这个例子中看到的, 比如词汇替换, 字句扩迟, 交叉删除重新排序, 或者插入字词插入。 我们现在提出我们的新的语料d平面, 因为近年来,现有有一些问题。 例如,这里的这些太小了,无法训练分类模型。 近年来提出的另外三个模型都自动对齐, 这意味着它们可以在在它们对齐中过误差。 因此,我们提出我们新的语料d平面, 它分成两个子公司,d平面 apa和d平we。 d平 apa是基于使用文本。 它导致大约三万一万三千平行句子对。 这个语库包括不同的域, 我们还将所有这些七50个文档, 一方手动,另一方面与自动对齐方法。 总共三万四百五十句对。 我们更多分析了我们的句子对, 例如,关于简化的类型。 正如你在这里看到的, 圣经文本比新闻文本, 或者语言学习者文本。 在所有层面上,关于词汇简化、结构化简化,以及整体简化体水平。 此外,您可以看到我们的dplan语料库有的不同简化转换。 例如,在dplanepi语料库中,我们有重新排和单词比D平面语。 另一方面,在网络语料库中, 更多的措辞。 现在让我们看看用这个语料做。 :你好,我是奥马, 现在我将讨论数据集D平的用例。 对于第一个用例, 我们可以评估自动对齐方法。 近年来,有很多对齐方法, 但是在机器翻译的背景下, 我们有两个行用不同语言写的行文,我们想在两个文档中提取句子对齐。 但是在我们的用例中, 我们试图 to extract alignments between sentences of two parallel documents having the same language having the same content but they are on a different complexity levels and now as we have our data set d plane which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper at the end we concluded that the best alignment用于德国文本简化文本最佳对齐自动对齐方法是大对齐的方法。 您也可以找到代码在论文自己的文档上运行此方法。 我们在论文中展示的第二个用例是自动文本简化 通过微调语言模型 复杂输入文本简化文本。 我们微调了两种不同的模型。 我们调了长帝国的模型 来产生文档级别简化, 我们还微调了正常基础m部分以产生句子级简化。 您也可以找到所有的检查点,你可以在更多我们论文实验的分数和评估指标。 我们得出结论,这种基本的微调 可以可以比基线分数分。 我们这些结果作为基  自动文本简化的问题的基准。 非常感谢的关注,我们希望在会议上见到你们所有人。 谢谢"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "大家, 我是来自芬奈大学的San。 在这里介绍我们的工作: 脚知识与语言模型分 语言规划。 在日常生活中, 女性经常通过脚一步指令 以前的工作利用了语言模型来规划刻板活动的抽象目标, 比如做蛋糕, 并表明大语言模型可以有效地将目标分解成步骤。 然而, 以前的工作主要侧于规划刻板型活动的抽象目标。 规具有特定目标、特定限制目标, 比如做巧克力蛋糕, 仍然未开始。 在这篇论文中,我们定义了受约语言规划的问题, 它对规划目标施加了不同的限制。 抽象的目标可以由不同的现实生活中特定目标具有多面约。 一个好的规划者应该阅读写合理和忠实约的脚本。 在在这篇论文中,我们首先评估和改进生活语言模型受限制语言规划性。 因为由于没有特定目标的数据来启我们的研究, 我们必须先获得这些目标。 正如表中所显示的, 我们多来展象目标。 循数据收购。 我们采100个特定的目标,并评估从L模型中生成的。 这个表报告了结果的整体准确性。 我们发现所有线模型都在规特定目标取得不满意的结果。 然后,我们进行详细分析,以调查学习模型。 图中的结果显示,生成脚本中的语义性是可以接受的,但约束的忠诚性保证。 我们我们将深入维基定义更级的主题的约束类别。 图中的热图显示,指令gpd的计划性能对于不同类别的目标差异很大差异。 以前的研究表明,拉模型的输出质量在高差异, 导致性能。 因此,我们采用了过度生成z过滤器的想法来提高生成质量。 我们首先指示cpt的例子例约, 并根据抽象目标获得具体目标。 然后,指示令Gpt过生特定目标关键脚本。 接下来,开发过滤器模型来选择实脚本。 我们将脚本和目标转换为指Gpt嵌入, 并计算正弦相似性和相似性分数以衡量语义相似性。 此外,我们避免包含目标约关键词的脚本。 如果目标目标在目标最高,我们保留脚。 使用我们的方法, InstaGbt可以生成更高质量的屏幕。 我们的方法大大提高了语义、完整性和忠性重要。 由于长语言模型部署, 使较小和专业模型的语言规性。 创建数据集是其目的的重要步骤。 然而, 以前的研究不允许规特定目标, 手动数据集注释很昂贵。 因此,我们遵循象征性知识蒸馏的想法,来模型提取受约的语言规划数据集。 我们应用我们构建约语言规划的数据集, 命为代码script。 我们用脚本生成五万五千个特定目标。 为了确保验证和测试网站点的质量, 我们要求众包工作人员 最终不错误样本修收入。 这个图显示了代码cript的分布。 我们发现 Codecript显示生成特定目标高的。 使用 Codescriptpt,我们可以专门的 来限制语言规划。 我们发现率可以生比大多数大语言模型发质量脚本, 表明在合适的数据上训练时,模型支持更大的模型。 我们建立了受约的语言规划问题。 我们开发了大型语言模型的受语言规划能力, 并开发过生语言模型度方法。 我们使用大语言模型 来生成高质量的平方形数据集CoScript 用于限制语言规划。 我们希望CoScri数据集可以成为 推进语言规划研究的资源。 感谢您时间。请在我们的论文中找到更多有关codescriptpt"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "大家, 我是Jnis斯拉瓦克, 我将向你们介绍关于伯T博士的,  生物医学和临床领域。 在在这个演讲中,我们首先讨论医疗保健的语言建模。 然后我们将介绍我们文章的主要贡献。 我们引了第一个生物医学模型 名伯特博士, 基于Ro伯塔, Nios, 网络医学取数据的。 我们还多个临设置和数据源模型比较。 然后我们法11个生物医学和临床下游任务。 最后,我们实验, 你们更多如何访问这些模型。 自 2008 年发布 , BERRT已经成为解决自然语言处理任务最的方法之一, 历史、静态和语、或 从那以后,这种模型改到许多其他语言, 比如法语,卡门bert, 其他领域, 比如生物医学,许可出生和生物出生, 临床临床出生, 但主要是英语。 其他语言的专门模型稀少, 通常基于由于 然而,法国没有任何生物医学和儿童源。 问问题 广泛使用最合适的数据? 这些关键数据是临床数据的替代。 为了回答这个问题, 我们比较伯特博士和舒伯特模型, 基于我们家的非大学医院获得的匿名数据。 之后,我们问自己, 我们需要多少数据来训练法国数据模型? 是 4千兆字节,8千字节还是更多? 为了回答这个问题, 我们先从头开始模型四个: 伯特博士7兆字节的自然, 第二4千字节自然, 希伯特, 临床模型, 句临床点子, 最后舒伯特, 4千字节自然和 4千兆字节的临床节点。 除了比较, 我们还引入了三个对预训练训练的模型 来分析预训练策略的影响。 一个基于卡门伯的重量 4千兆字节的自然, 另一个也是基于卡门ember, 但这次在4千字节的临床节点。 最后,一个基于英国生物医学模型,BmedBT, 4千B节的na。 总我们有七个模型。 为了评估七个模型, 我们收集了公共和私人下任务, 比如识别, 分类,语音标和问答。 这些模型与六个基设计模型, 卡门bertOcar138G兆B,卡门bertO卡4GB节,卡门bert 6net4G兆B, 鸟、生物鸟和临床鸟。 光的进化任务表现  与模型训练的。 然而,我们可以数据 观察异来源的数据 看起来更多功能。 我们我们也观察使用更多的数据 转为更好的性能。 总的来说,从头开始自由训练 似乎 在大部分任务获得更高的性能。 然而,我们消费者训练实验 使用许鸟的重量和标4千B节子实验 显示获得4兆B节。 基于卡门贝和令器模型 稳定性。 最后,作为结论, 我们的适系统九个不流任务性能, 全球超过生成模型的卡门ember。 我们我们也观察 专业数据更好, 更多的专业数据更好, 但扩展。 从Nos获得的所有预训练模型免费在Youface上免费, 的培训在我们的GHub存储库上。 谢谢这次演讲, 我们期待多伦的海报会议行动。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": ":大家,我是华盛顿大学的博士生。 今天我将展示我们的工作 从预训练前数据到语言模型到下游任务, 跟踪偏见导致不公平NLB模型偏见。 所以语言模型是在大规模网络爬行数据训练。 政治新闻媒体在预训练数据中报道。 对c 4语料的调查,我们可以看到《纽约时报、洛杉矶时报、《卫报》、赫芬顿邮报等在语言模型培训数据中很好报道。 这为语言模型应用创造了混合的祝福。 所以一方面,他们能够从不同的角度学习,庆祝民主和多思想。 另一方面,这些不同的政治观点本质上是社会偏见的,可能会导致下游任务应用中潜在公平问题。 为此,我们提议调查传播从预训练数据到语言模型到下游任务道, 特别是问以下问题:首先,我们如何评估语言模型的政治倾向, 预数据在这些政治偏见扮什么作用? 其次,具有不同政治线向的语言模型是如何在下游任务上表现,这是否会导致nlp应用中公平问题。 因此具体来说,我们首先提议使用问不同提示格式提,比如政治指南针测试, 这确保我们政治科学文献进行自动评估。 因此一些初步结果表明, 首先,语言模型有政治倾向。 它们占据了政治指南四个四限。 我们我们也可以看到GPT4是最自由的语言模型,GPT系列通常比鸟特理论及其差异体更社会自由。 其次,我们目标是调查语言模型的政治偏见在从训练数据中获得。 因此,我们可以通过预训练模型对六个不同的党派群体进行,这些分成新闻和社交媒体进一步分为政治倾向。 通过党派群体, 我们可以看到语言模式的意识形态协标也相应变化。 例如,对于罗伯塔精,左倾红dit语, 政治偏见 我们还试图调查语言模型是否能发现我们现代社会普遍的两极分化。 我们将训练前人分成美国第45任总统之后, 在美国第45任总统之后, 我们训练语言模型在两种不同的时间语言模型。我们可以看到语言模型通常政治向2017离中心远向。 因此表明语言模型也可以我们社会两极分化。 所以最后但同样重要的是,我们评估仇恨言论检测和虚假新闻检测模型nlp通常涉及语言模型,可能产生非常重要的影响。 所以我们看到,如果我们调查每个类别表现,也就是说,如果我们把表现分成不同的人口统计或政治新闻媒体,我们可以可以看到一种模式, 例如,对于仇恨言论检测,左翼语言模型更擅长检测针对社会少数民族群体的仇恨, 然而,更擅长于检测仇恨言论我们社会中群体更多的权力。 反之亦然,右翼语言模型更擅长检测针对白人和男性的仇恨。 然而,在检测针对黑人性btq和其他少数民族社区仇趋势虚假新闻检测, 我们我们看到左翼语言模型擅相反政治倾向测错误信息, 反之亦然。 我们进一步展示了许多定性例子, 发现具有不同政治意义义的语言 确实基于类别仇恨言论和错误信息预测。 在附录中还有有很多例子来进一步强调这一点。 这表明语言模式的政治偏见非常紧迫。 例如,如果右线语言模型仇恨言论或错误信息微, 并部到流行的社交媒体平台上, 意味着政治观点的人 可能被边缘化, 针对少数群体的仇恨言论 控制獗。 承认和解决语言模式政治含的公平问题。 讨论。 我们我们也想强调我们揭露了关于语言模型政治偏见的独特困境。就像西illa和卡bdis之间。 因此如果我们不语言模型训练中消政治意见,这种偏见将从预训练前数据传播到语言模型到下游任务,最终造成公平问题。 如果我们真的试图以某种方式消毒, 我们也会冒审查或排斥的, 很难确定什么实际上是中立的, 应该保留语言单训练数据。 所以有点像电查理问题。 好的,太好了,我想的全部。 今天五。谢谢你抽时间"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。 我是科塞纳, 很高兴欢迎大家我们的ACL20论文的。 语言模型可接受性判断并不总是。 这是与约翰 Bohir尔, Aaron穆勒,Kish卡 Mishra, Karen弗特斯,罗杰 Levivy和阿蒂娜 William斯。 在这个工作中,我们重新审小对范。 小对范式基本上评估语言模型可接受性判断, 这也可以包括语法, 比如,语法,  或者, 比如人。 在这个小对范式中, 评估语言模型的典型方式 显示一个可接受的句子或语法句子, 然后显示一个不可接受的句子  或者非语法句子, 然后希望模型接受的句子更多的概率。 目前的 MPP管道基本上不允许我们评估模型接受对更长句子。 如今,大型语言模型长的上下文窗口, 因此整个上下文窗口评估模型接受性。 这就是我们做的。 我们尝试重新审MP管, 要求模型长的序列评估可接受性。 这就是方法。 所以我们所做的是模拟这些较长序列, 我们重新审视数据集本身, 然后我们通过选择这些数据集可接受或不可接受的句子重句。 例如,在这里,我们选择从Bblim数据集型容型语法t。 我们所做的是,为了重创建更长的序列,并且可接受的,并且具有语法结构相同匹配, 我们从容岛中提取语法句子然后将其添加作为前缀到可接受的查询和不可接受的查询前缀。 因此我们可以通过从同匹配中选择不可接受的句子来来做同样的事情, 这也可以用来测试模型的可接受性。 我们也可以通过从不同的子集或不同 这就是我们称之为不匹配场景。 ,句子仍然来自相关的数据集, 但并不来自评估的同数据集。 我们可以对于不可接受案例。 最后,我们可以选择完全不相关的领域选择句子, 比如维基百科。 因此这将告诉我们模型的可接受性判断是否实际上受到任何上下文影响, 比如上下文是否来自数据集的不同子集,或者它是否与当前我们看到的的句子完全无关。 那么模型是如何做到的? 首先我们看看维基百科的句子 ,与当前的查询对完全无关, 然后发现 MPP 判断任意上下长度。 我们把上下文长度 1024 年  OP和 GPT2 模型, 橙色点线 MPP 判断相对稳定。 现在,当我们从相同一个数据集中选择句子时会发生什么? 因此在这里,我们选择从相同blim税数据接受句子。 在那里我们看到您添加可接受的前缀或不可接受的前缀时。 但是当我们匹配结构配时,也就是说当我们责人税收健房同现象选择句子时, 我们看到pp判断大幅增加或大幅下降,这取决于选定的前缀是否是可接受还是不可接受的。 现在 这非常大,就像这种效果在整个上下文长度增加, 这可能会影响新的具有上下文窗口语言模型。 那么为什么匹配前缀影响语言模型的判断呢? 所以我们做了一系列分析,我们试图通过试图保存相关结构来扰输入句子, 但是 噪音。 在做了几个扰后, 我们发现这些噪音使模型改变评趋。 基本上 , 我们发现模型对句子敏感。 也就是说 , 当我们可接受区域句子时 , 我们看到所有扰增加 , 当我们接受批准领域句子 , 我们 P评下降。 因此我们工作的关键收获是语言模型对句子共享的潜在语法和语义特征敏感, 评估, 目前用短和单句子输入, 可能不会在整个下文窗中语言模型抽象知识。 请阅读我们的论文了解有关我们实验。 谢谢的聆听"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是达维,德国萨兰大学的博士生。 在在这个视频中,我想介绍我们最近的作品 比你想象的弱, 每周监督学习的关键。 这是与夏生的工作, Ma穆sbach和Gya Stefan和Diish克拉ko。 我想从简单要介绍弱监督和每弱监督学习开始。 在弱监督中,我们不手动标记数据。 相反,我们使用弱标记源来标记数据标签, 如简单的直理规则, 知识基础或局质量云源, 正如您右边的图中说明。 与人类注释相比, 较弱的注释要便宜得多, 然而它们也很吵杂,这意味着一定数量的注释是不正确的。 如果我们直接在每周标签数据上直接训练神经网络, 神经网络倾向于记住标签噪音,不会推括。 在每周监督学习中,训练算法 在这种标签噪音下强力训练神经网络, 以训练模型仍然概括。 在最近WSL的工作中,Wl代表每周优学习, 常见的说法是人们说他们只训练模型是每周标签数据,并干净测试集达到高性能。 从技术格上讲,这一说法不是错误的, 但是有一个问题, 那就是人们确实假设有一个额外的干净验证集,或模型选择。 我们对问题设置, 但这意味着每周支持学习手注释, 但是像房间里的大象一样, 这种必要性经常被忽视。 上述的do用要求我们问三个研究问题。 第一,清验证数据对于Wl必要? 或者我们可以使用噪杂验证集吗? 其次,如果需要清洁数据,或者如果干净数据wsl工作, 那么我们需要多少干净的样本? 最后, 我们应该只使用干净的样本来验证, 或者有更好的方法来利用它们? 我们在工作中解决了这些研究问题, 我们的结果如下: 首先,我们发现有趣的是, 最近的WSL方法确实需要清验证样本才能正常工作。 否则性能下降。 在这个图显示, 如果没有清验证样本, 那么趋势模型不能到原始的弱标签, 这意味着训练是毫无意义的。 这表明WSL方法实际上需要干净标记的数据正常工作, 获得取清验证样本的注释成本不应该忽视。 我们的第二发现是,增加清洁验证样本的数量 将帮助WSL方法 更好的性, 正如左边的图。 通常,每只需要20个样本 才能达到高性能。 但这不是故事结束, 因为如果我们哪方式决定获干净洁样本, 那么直接它们甚至会达到更好的性能。 红色图显示了微调方法性能差异,直接应用于清洁数据下, 和WSL方法, 使用清数据来验证。 正如我们所看到的, 如果我们每班有十个样本, 直接微调开始击Wsl方法。 最后,以前Wsl方法中声的性能改可以通过允许在清验证样本下继续细调来实现。 正如我们从数字中看到的,称ftw的模型最初更复杂的WSL方法。 然而,如果我们允许在干净的样本下继续F图, 那么FTW与其他方法一样。 因此,在实际上中,没有理由选择更复杂的WSL方法,这些需要更多的计算时间和磁盘空间。 总结一下,我们展示最近的WSL方法 需要干净的手动注释的样本 它们正常工作。 它们性能增和实用性严重高估了。 未来工作的具体建议如下: 首先,报告模型选择标准。 例如,报告模型选择清洁验证样本。 第二,WSL方法应与短着陆基线进行, 假设清洁样本。 第三,持续微调是简单但强基线,应该在Wsl未来工作中考虑。 最后,我们有开源代码。 您可以通过这张幻灯片上的q代码找到它。 请随时查。 谢谢并享受会议"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫艾德· Villa拉德, 我将简述提PAM翻译, 评估策略和性。 这是与谷歌翻译的同事联合合作。PAM是400亿参数的语言模型去年22年。 它在大量文包括7八亿代币组成文训练。 在出版的时候,它在数百个nlp任务中实现最先进。 在这项工作中,我们对大语言模型提示系统研究。 我们使用It社区的最佳实践 IMT社区的最佳实践。 这包括使用最新的测试集来避免数据与语言模型的训练数据重叠。 我们比较两个最先进系统, 系统WMT评估系统。 我们使用最先进的神经MT指标, 此还显示基于的的人评估结果。 最后,我们提示选择策略提供了一些建议。 提示对 LLM翻译的性有了很大的影响。 正如我们简单的实验中看到的, 我们使用一个简短提示, 并为句子提供两个不同的提示。 大多数句子,, 观察的差异超过一个个模糊点。 在极端情况下,达 40个模糊点。 所以选择一个好的提示策略很重要 在我们的实验中,我们决定了一个五提示策略, 标系统句 的语言句。 在这个例子中, 我们从德语翻译成英语, 句源句标用德语列号 英语有英语列。 我们看到打印的实际形式在几个短印的情况下没有大影响。 对于零和一短提示至关重要, 但是当我们像我们的例子事实简提示时, 打示的实际形式几乎没有区别。 这是承载大部分重量的例子。 我们实验结果的总结是,示例质量比与源句的相似之性更重要。 因此,从高质量翻译中选择示例很重要的。特别是,我们比较WmT评估的训练数据示或dev数据示。de数据创建,比训练数据,,结果在使用开发v数据时显示更好的性能。 然而,专门的先先进系统在pal翻译有实质优势, 但是palm非常接近商业系统。 在我们的例子,我们选择与谷歌o翻译。 我们使用npn框架进行的电子邮件中获得的见解,P的流与艺术系统状态比, 但主要的区别来自准确性。 最常见的错误是遗漏错误。所以pal掌选择产生更好的的声音翻译, 有时通过丢翻译中的源子部分来。 然而 潘的风式外类别于先进系统,这是一个额外信号panm提供非常流利的输出,但仍然有一些准确性的问题 这个简短的概述。 更多细节,请论文的完整演。 非常感谢"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家。 我叫来自中国科学科技大学的金we。 很高兴我们的论文一个简短广告视频。 你在复制我的模型? 保护大语言嵌入和的版权。 后水标。 让我们先介绍关于嵌入广告服务的背景。 目前, 大语言模型如gbt,  La,pal在自然语言理解和生成方面是的。 嵌入广告服务是建立在大语言模型以协各种nlp任务的服务之一。 例如,open api提供基于Gpt的嵌入 API。 然而,现工作表明, 攻击者可以通过从嵌入学习取模型 并提供类似的服务窃模型。 因此,必要保护嵌入作为服务的版权。 为了保护嵌入广告服务的版权, 解决方案是将入提供商服务中入, 并检测服务是否包含水印。 水标方法需要符合以下属性: 首先,该方法应适用于嵌入广告服务。 第二,水标应降低提供嵌入器的效用。 第三,水印应对攻击者, 攻击者可以轻移除水印。 最后,水需要 在模型提取过程中 到攻击者。 现有的工作可以广泛分类为四类。 然而,这种方法要么不适用于嵌入服务,要么缺乏传输性。 因此,在这篇论文中,我们提出了嵌入标记, 是一种基于后门水方法适用于嵌入服务的。 然后让我介绍我们嵌入标记的细节。 嵌入标记包含两个主要步骤: 水印注入和版权验证。 在这些主要步骤之前, 我们首先选择一个触发器集。 触发器集是中度频率隔中的单词。 我们假设提供商可以收集通用文本编并用它计算单词频率。 在水印注入中,我们首先定义目标嵌入。 当用户向提供商服务发送句子时, 提供商计算句子中的触发器号。 提供的嵌入是目标嵌入和原始嵌入的权量总和。 目标嵌入的重量与句子中的触发器数量成。 当句子中的触发器数量大于m时, 提供的嵌入正等于目标嵌入。 版权验证是检测另一个服务背后的模型是否包含字标。 我们首先构建一个后门和良性数据集。 后门数据集包含句所有单词都属于触发器集的句, 而良性数据集句子中的所有单词都不属于触发器集。 然后提供商请求从钢服务数据入。 嵌入和目标嵌入之间的和L2相似之。 我们计算性和后门数据集之间的差异, 定义为三角正弦和三角L2。 与此同时,我们还应用ks测试,并使用其p值作为第三个度标。 我们对四个数据集进行实验:新闻,M、sSD2和Apam。 我们假设提供商应用维基文本数据集来计单词频率。 四个数据集的结果显示,我们的嵌入标记可以具有检测性能, 同时下屏幕任务效。 我们还入 通过可CA四个数据集的句子来验提供嵌入性。 数字的传说表示每个句中的触发的数量。 如数字中所显示, 很难区分因嵌入和正常嵌入。 就此这样,谢谢。 我们将来和我们讨论"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "大家。 我叫英恩, 我的同事吉扬我将介绍关于多教的研究, 通过教令多模型序学习。 随着大型语言模型的进步, 许多工作开始探索新的学习, 使用语言模型 不同的下训练。 最近,许多研究表明,指令调谐使大型语言模型能够指以序行图的方式执行任务。 然而,以前大多数关于指令调谐的工作集中重于提高语言任务的提高序列性能, 计算机视觉和多模任务遗。 因此,在这项工作中, 我们想要调查调或多模编训练模型是否可以提高对看不见多模任务的概。 此外,在研究的时候, 我们发现了P模教学数据集的可用性存在差异。 超过一千600 个午餐教学任务。 然而,没有大公开的多模教学任务。 因此,这促使我们建立一个多模式指令调数据集 我们展示 MultiInstruct, 第一个多模指令调基准数据集, 由62个不同的多模任务 包含 10个类别。 这些任务来自21个现有的开源数据集, 每个任务都配备了五个指令 对于数据多模型指令调, 我们使用OFA统一的多模型模型作为基础模型。 OFA使用统一语言、图像令牌和边界框的协标。 我们展示多sta数据集的一些示例实例。 为了统一各种输入和输出数据类型的处理, 我们遵循Ofa循方法,并以统一的序列二序列格式制所有任务, 输入文本、 图像、指令和边界框在相同的令牌空间中表示。 好的,现在我要谈谈多模态指令调。 对于训练数据集, 我们NG组的53个任务训练, 每个任务万个实例。 为了测试,我们保留整个常识读组进行测试, 维QI和杂组个任务。 我们每个测试速度实例。 此外,我们随机自然指令测试速度20个任务 NLP。 我们使用预训练的OFA大型模型作为基础模型。 在训练中,我们所有任务实例。 每个实例都随机与五个指令模板中机组。 在每个任务, 我们我们使用每个实验中五个指令来进行实验。 我们报告所有值五和标准差。 如果任务是多模型分类任务, 我们报告准确性。 如果是多模生成任务, 我们报告根otL。 对于P任务,我们也报告根otL。 我们还引入了额的称敏度。 测量模型同相同输出, 无论指令变化变化。 这是我们的主要结果。正如我们所看到的, 此外,自然指令数据集的转移输学习可以有造指令调谐。 在这里,我们可以看到,随着任务量增加, 模型达到更好的性能, 同时降低敏感度。 我们也做了一个实验。 我们使用一个指令和五个指令。 可以看到,使用更多的指令可以提高模型的总体性能, 降低敏感度。 这显示了模型不同前调策略对的影响。 正如我们看到的,通过从自然指令数据集的转移学习, 与原始的OFA模型相比,实现更好的敏度。 我们还可以看到从自然指令数据集的转移学习可以帮助OFA氮基指令数据集更好的性能。 所以总的来说,我们提出了第一个大型规模多模指令调数据集。 我们显提高了OFA的能力, 我们探索了不同的转移学习技术, 并显示它们好处。 我们设计了一个叫做敏度指。 一件事, 收集一个大的多模指令调数据集, 大约150个额外的变体语言任务, 我们将发布它们。 所以这是我们我们的数据和模型的Q代码。 谢谢"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。我叫宾夕法尼亚州立大学的尤 John。 今天我将介绍我们的工作,例:多种自然语言和主要表示语义。 所以语义解析是用户查询构建语义表示任务,如sql和lambda微积分。跨语言语义解析是将多种自然语言查询翻译化为多个意义表示的任务。 正如图所显示的, 我们需要将使用神经模型语言,sql、lambda或funql等成语言。 现有的跨语言语义解析模型在有限任务和应用的数据集上单独提出和评估。例如, 某些自然语言的覆盖。 中文缺失了,某些迷你表示的覆盖漏。 lambda微积分缺失了, 或者它们只在某个神经模型上评估。 例如,只有一个单一模型来评估它们。 所以到此,我们提出了ex示例r, 我们提供了一个统集为多种自然语言跨半解析和意思义表示提供统一数据集例。 它包含病毒域的九个数据集, 五个半解析文, 八个表示 和22种语言 15个语言家庭语言。 为了更好评估我们的基准, 我们考虑六个训练和评估设置。 第一个是翻译测试。 我们使用谷歌翻译 API 将源转成目标语言, 然后使用单语言模型来训练和评估。 例如,我们在英语查询训练英语模型, 在推断过程中,我们使用api翻译成英语, 然后使用训练的模型来预测SqL。 我们还测试单语言模型。 在这个设置中,源语言与目标语言相同, 例如德语到德语或英语到英语。 我们还测试通过训练模型 只有10%的训练数据测试。 我们测试了多语言模型, 我们训练所有语言一种多语言模型。 我们把德语、英语和中文查询放在一起 来训练一个多语言模型 在推断过程中, 我们可以使用这个模型 翻译德语查询或中文查询等等。 还考虑跨语言零短和零短转移。 一种源语言进行训练 转到另一种语言。 所以在训练中, 我们英语查询 或者英语和德Fsho查询的组合 来训练多语言模型 并预测SQL输出。 我们还发现了许多有趣的结果。 关于单语言模型的分析, 我们评估了两组模型, 包括编码器pdr, 它代表多具有基于指针解码器的多训练编码, 比如xr pluspdr, bert加pdr。 我们还评估了编码器解码器模型, 这是多语言训练编码器解码器模型, 比如mbart和mt 5。 我们发现编码器解码器在所有九个数据集上获得最佳性能。 我们发现编码器解码器或编码器PDdr可以通过各种语言混合训练来改进。 我们发现这是因为大多数主要的自然语言可以获得性能增益, 除了英语性能在七个数据集中下降,只在三个数据集增加。 被称为多语言曲尔。 我们还比较了跨语言性能差距。 在这个数字中,蓝线是跨语转移。 橙色线是跨语言零短转移, 而绿线是单语设置中。 我们发现通过比较绿色和橙色线,我们发现零短设置,跨转移性能差距是显, 通过比较蓝色和橙色线, 我们发现通过很少短设置,转移差距迅速缩短。 我们还发现了一些其他有趣的发现。例如 编码器解码器优专业工作或实现了可比较的结果。 训练我们的英语自然语言可以显提高在目标和自然语言上数镜的表现。 我们发现多语言语言模型, 比如编和蓝色 跨语言语解析课程。 总结之一下,我们建立了ar, 跨语言语解析 多种自然语言和表示。 我们对三种代表语言语言模型型的广泛基准研究。 我们的结果显示了许多有趣的发现等等。 欢迎你参观我们的论文和代码。 谢谢倾听"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "大家,我叫萨dam S里kovsky,这次演讲是协调的依赖结构。 知道,不同的结构 不同的理论和语方法。 例如,在通依赖中, 协丽莎,巴特和 Maggie 第一个是整个坐标结构的, 所以在这种情况下,L莎。 伊gor米尔楚uk的意思义文本理论中假设, ,整个坐标结构由第一个连指导。 两种方法是不对称的,对吧 他们分出一个连。 现在坐标结构对称方法, 比如ra格方法, 拉格依赖树库中假设的, 协标结构由连指导。 所以我们从端到所有连的依赖。 最后,还有一个多头方法, 在卡森的文字词语法中使用,说所有连都是坐标结构的主。所以我们从州长的依赖,这里爱, 到分所有。这些是按。 现在这篇论文的目的是为协调的对称结构新的论点,比如这两个,与这两个者协调不对称结构。 好吧, 这个论点是基于解释依赖长度最小化的原则,这些例子的基础解释。 所以在英语,可能知道, 直对象更喜欢接近动词,而接容词可能更远,对吗?所以三月读昨天很好,因为直接对象接近动词, 而三月读昨天糟糕得多,对吗?因为在动词和直接对象之间,昨天有一个辅容词昨天。 然而,这种效果可能会 当直接对象非常重和很长时, 因为它可以移动到附属词之后后的位置。 在这里说明。所以两句子都很好。三月读了关于蜜蜂的迷书。 某种程度,而不是它我们有这个长和p。 但是说三月昨天读一本关于蜜蜂的绝对迷人书。 所以这里的理由是这是可能的,因为即使这句话子违反了语法原则即直接对象应该在动词旁边的, 它满足了依赖长度最小化的原则, 它说短 短的依赖项。 所以所以这两棵树只显示关键依赖项的长度,所以这两个结构中不恒定的。 所以在这里我们有从红色到长度七的附属量, 从红色到长度4的书。所以加是11。 当你移动时,当你交换这两个组成分时,这两个依赖的总和变成6,? 所以而不是11,6短。 听起来不错,? 它违反了一个原则, 但满足另一个。 。 我们银行增版本, 看看为什么我们不使用大学依赖。 这些统计数据证实了以前多次的观察,左连往往较短, 所以盐和胡椒,而不是胡椒和盐音节量。 还有顺便的观察,这种趋向随着长度差异增长。 所以当两个连长度之间的差异增加时, 较短的体更喜欢第一个更强, 对吧? 所以比左短大。 但是在这篇论文中新, 我们观察到这种趋势只发生在左边州长缺席时, 对吗?所以州长左边。我看到巴特和丽莎。所以是州长,他在左边。 它第二个例子中。荷马来了打喷嚏。这里我们有两个动词的协调,没有外部外部州长,对吗?所以在这种情况下, 左词更喜欢更短, 两个词之间的差异越大。 然而, 当治长在右边时,就像在这里,左边管理协调T和网, 所以我们表明 通过测量字符中的长度时,音节中的第一,中间列,文字词是右列。 所以我将专注于右边。 我们在这里看到的是,当州长在左边时, 左词较短的趋势随着单词的绝对差异增长。 没有州长时,,就像句子的协调一样, 但是当州长在右边时,这种趋势消失。 我们在论文中展示了这是 提供反对我们不对称协调结构论点,将不对称结构为这两个者。 所以看论文完全同意和争点,对不起,和我们谈谈海报会议。谢谢你"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫凯io恩,我将介绍我们的标题为翻译何时需要下文:一个数据驱动的多语言探索。 这项工作与pat特rick克· Fer南、艾meu、安德烈 FD. Martins和ram姆· Newbi格合作完成的。 所以许多翻译都取决于上下文。 例如,如何在这个句子中翻译mo呢? 如果前的句子是, 如果部长发现,情况变得危险, 那么“Mo”指的是间谍。 但如果前的句子是 “严重吗,医生?” 然后Mo”指出生号。 所以根据下, 词的的意思义变化, 因此翻译也会改变。 然而,评估模型翻译这样的案例困难。 首先,因为只有一小部分翻译依赖上下文, 这使得蓝色这样的语库级指标无法捕捉这些翻译。 有些人建议对依赖上下文翻译的针对评估, 但是这些资源只支持有限类型的依赖上下文翻译和有限的语言, 因为它们通常依赖于域知识和人类策划。 这项工作中,我们试图回答这两个问题。 首先,翻译什么时候需要上下文? 其次,模型处理这些案例? 为了回答第一个问题,我们从测量词依赖上下文。 在之前的工作中,我们将CXMI 作为机器翻译模型下使用的测量。 这是通过测量上下文C提供目标y 。 你可以把CXI 成模型给上下文获得的信息。 在这项工作中,我们将cxmi 扩展到点yCxmi, 它可以测量在句子级或单词级别上下文使用。 我们可以把具有高p6mi高的单词 成需要翻译上的。 现在我们分析高p6mi高的单词来寻找这些单词之间的模式。 我们对英语TED演讲本进行分析 从英语翻译成14种不同语言。 我们三个不同的层面进行分析。 首先,我们看看高 P6MI。 这我们找到 在阿拉伯语代词相对pXI高。 这可以解释因为英语没有双重代词, 所以需要上下文来确定代词翻译成阿拉伯语时代词是双重。 同样,我们发现当我们想选择合适当的动词形式时,需要下。 然后我们看6不同的汇。 这有助于我们识别像这里例, 在中文中,你需要上下文来翻译流行名词,以确保你在在文档中使用相同的翻译。 同样,我们发现上下文支持以正确的形式翻译。 最后,我们看了不同的具有高pxmi高的单代牌。 这允许我们识别不能真正被单词本身捕捉的现象,但在语例结构中表达的, 比如椭圆分辨率。 所以现在我们从分析中使用发现来设计文档级别翻译设计基准。 对于我们确定的五个语现象中的, 我们创建了标记来自动识别与现象相关的单词, 我们称我们的标为多语言话语意识或穆标。 然后我们还注意,不同的语言这些话语现象有不同比例。 然后我们使用标, 将标签应用于语想要用于评估的平行语料库上, 我们我们将指标应用于穆标gger确定的上文依赖示。 最后 我们使用我们的基准以及和其他指标来评估不同的模型 在文档级别机器翻译上。 首先,当我们使用语料库级指标时, 所以对于蓝色,我们发现组不可知论模型最好的性能, 但是如果我们使用彗号,上下文意识模型表现最好。 如果我们使用单词f测量,那么或没有上下文的模型具有比较的性能。 这再次表明,如果我们仅独使用语库级别指标,很难确定最佳文档级翻译换系统。 现在我们使用穆uda基准来评估模型, 我们发现上下文模型比不为某些差异语现象使用下文的模型更准确得多, 形式和词汇凝聚力。 但是这些模型并不比其他现象上文模型,比如椭圆、代词和动词形式好得多。 这表明我们需要看到文档级别翻译更多进展的地方。 我们还比较了不同的商业系统,我们的基准显示dpal通常比谷歌翻译文档级别翻译更准确。 总结之一下,我们对14语言对进行数据驱动的分析,以确定翻译何时需要上下文。 然后我们使用我们的提为文档级别机器翻译建立一个基准, 这可以帮助我们确定哪些差异现象模型可以处理好,以及哪些翻译系统擅长文档级别翻译。 非常感谢您的关注。在多罗多见"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": ":大家好。 我是珍妮, 我是卡内基米利大学一年年级博士生。 今天我将介绍你们的工作位置位ality, 描述设计偏和测试模型。 这项工作与华盛顿大学的一些 艾伦人工研究所的, 也就是塞巴斯tian·桑蒂,罗ine Lebra斯,  Katarina Re尼卡和 Martin丁 Sa普 让我们先想象你在为报纸工作, 新闻文章评论 试图去除有毒内容。 可能会转向一个流行的 API, 比如毒性检测。如果你是卡尔琼斯, 潜AP能够正确检测有毒例子例, 但对于迪th亚沙ma, a API对在印度环境更常见的攻击性术语敏感。 这是一个设计偏见的例子, 技术系统性性能差异。 像我们刚才看到的设计偏见可能由于 NLP研究人员和模型开发者的。 定位性只是人们人口、身份和生活经历的观点。 在临性研究中概念, 特别是在女权主义和同性恋学术领域。 作为一名研究人员, 定位性可以影响研究过程 以及结果和结果,因为它可以改变研究人员做出的决定。 所以人们可能会问的一个问题是,数据集和模型有位置性吗? 我们不是想说模型本身和数据集本身具有人口身份和生活经历,但它们确实聚合了真实人的判断和意见,从而代表比他位置。 所以工作提出了一些具有位置位性的轶证据, 比如模型和数据集的文化差距, 以及模型定位的理论定义。 然而, 这些工作不将最终用户与数据集和模型本身比较。 研究模型随着nLlp任务变得更加主观和社会导向越来越重要。 描述这些位置是如何扭曲的挑战性的,因为并不是所有的决策都是记录,许多模型都隐藏在api背后。 因此,为了研究数据集和模型位置性, 我们实际上将注释与用户与现有数据集和模型进行注释。 我们通过我们的框架nl位置性来做到这一点。 我们的框架有两个主要步骤工作。 第一步是与不同的注释器重新集进行比较,我们选择通过看原始数据集的注释的人口统计来这样做,因为通常只有少几个注释器注释每个实例,因为人口统计数据很少收集和共享。 因此,我们选择重新注释数据,以例获得许多注释,并获得一组丰富的人口统计数据。 然后我们按人口统计注释,并与使用皮森的r相关性分数比较模型和数据集进行比较。 因此,我们的框架实际上与注释文献通过比较最终用户与模型、数据集、预测和标签进行比较释差异,而不仅仅是看注释器协议或建模注释符分布。 我们的框架主要是通过野外实验室启用的, 在线众包平台,前Hci合作者。 野外实验室是一个在线实验平台,与mk等平台,志者,主要有来自美国或印度的参与者。 此外,野外实验室仍然能够获得高质量的数据。 我们在野外实验室举办两项任务, 其中是社会可接受性, 是参与者将从社会化学数据集读取情况, 然后他们写情况社会接受。 ,为了继续参与研究, 他们可以将与人工智能反应。 然后我们将这些注释与社会化学Dphi和Gptd 4进行比较。 然后我们复制了一个非常为毒性和仇恨言检测任务一个非常相似的,他们将从dyna仇恨中读取的一个实例并写他们是否认为这是仇恨言论的例子。 然后我们我们将这些注释与dyna仇恨透视api, 重新线api、仇恨 Roberta和gptt 4。 我们的研究聚集累了 来自87个国家一千多个注释者个注释。 现在我们能力回答, NLP数据集和模型一致? 我们发现 NLP存在位置。 例如,我们发现数据集和模型最与英语国家一致。 所以对于GPD4社会可接受性分析,我们发现它最与儒家和英语国家。 我们发现戴纳仇恨也最与英语国家。 我们还发现与受过大学教育的人额外的对。因此对于g社会可接受性任务中中的,我们发现它最与受大学教育或研究生院教育的人。 丹尼·海德发现了, 最与受大学教育的人。 然而, 当模型和数据集与特定的人一致时, 有些不可避免地落。 一个例子是,集和模型与男性男女数据非二进人一致。 GPD4社会可接受性任务中发现了, 以及餐仇恨任务分析中。 所以,考虑到L有医生,我们能做些? 我们有几个建议。 第一个是在整个研究过程中记录所有相关设计选择, 另一个是用潜的视角进行nL研究。 我们的第三个建议是在四个特定社区内建立专门的数据集和模型  这是一个很好的例子是 Muakan尼倡议。 我们想强调包容性NLP不仅仅是让所有技术为每个人工作。 我们的演讲此结束, 但是如果你想了解更多, 请随时查看我们的仪表板了解最最新的分析结果和论文。 谢谢"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "大家,我将讨论解决间接差异表达式的工作, 介绍入alt实体语。 我叫J瓦d Hosseini, 与菲hillip普拉din斯sky,西尔维亚帕蒂和 Annie Louiss合作。 我们的目标是想要做出选择时理解的语言。 考虑另一个问题: 你的意思轻, 还是“我我有感觉? 在这里,用户想要这两首歌中选择。 最明显的是使用直接引用, 例如,说“易”” 或者它的位置第一个。 但是有时间接引用更适合进行更自然的对话。 这当用户不记住歌曲的名字称时,发生。 或者发音彼此太相似,很难清。 或者当用户想指定偏好时时。 这里有一些间接差异的一些例子, 例如更新或不活力的标志。 这是对话系统中的一个重要问题,也是Lm实体理解基准重要问题。 我们不知道公共数据集, 任务大规模的公共数据集, 所以我们使用人群注释收集。 我们的数据集涵盖了三个不同的领域: 音乐,书籍和食谱。 我们的数据集收集方法 使用卡通完成设置来强调非正式。 漫画通有三个语音泡。 在第一个泡泡中,鲍勃说: “还记得我们昨天听的那首歌吗?” 了这个,鲍勃设置定了对话下。 在第二个演讲泡泡中,爱丽丝说:“你是的意思对我 还是“我的感觉? 这是另一个问题。 在第三个语音泡泡中, 鲍勃使用间接引用来选择其中这些实体,例如新的。 我们自动域提供第一个和第二个语音泡泡,但第三个由注释符填写。 第一个语音泡泡从每个域的几个手动提示中选择的。 第二个,这是另一个问题, 生如下的。 我们总是使用一个简单的 模板,你是说a还是b? A和b是维基百科的样?使用的不同的采样方法。 当我们列表中更高时, 实体加相似, 通常更很难分。 第一个是统一趋势。 第二是当实体有相似的标题时, 例如,两本他们返回。 第三个是在当他们在维基百科上有类似的描述, 最后,当他们在维基百科上类似的信息框或属性, 例如同类型或同一个艺术家。 当我们向m者展示这个问题时,他们知道这些实体的名字称, 但他们不一定知道这些实体。 所以我们关于这两个实体的背景知识。 对于歌曲,我们简单每显示谷歌搜索链接, 然后要求注释者听每首歌, 阅读每首歌。 例如,歌曲易我的谷歌搜索结果。 对于食谱和书籍域,我们显示了一些背景文本百。 对于食谱,我们从维基百科再次显示他们的图像,以便注释员知道它们是什么样子。 然后我们要求注释员选择其中一个实体,例如,这里第一个, 并使用三到五个间接引用表达式来描述它们, 例如,有钢琴音乐。 我们数据集的一些例子。 比如,没有词的, 而不是12岁的男孩, 虚构, 或者来自阿塞拜疆等等。 ity语三个领域有 6000个替问题, 有 4千个间接指表达 T5x大模型的结果下面总结。 如果语言模型能够注释员相同的背景知识, 那么准确度非常高。 大约是92到95%。 但这并不现实。 如果语言模型能够获得一些部分重叠的背景知识, 那么准确度在82到87%, 这更现实。 比如,当语言模型获取背景知识时。 如果语言模型只访问实体名称, 那么准确度只有60%, 所以有很多改进空间。 还展示模型是领域推的。 这里我们数据集的链接。 谢谢收看"}
