{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "en", "output": "Hi! Welcome to our presentation of Deplane, a new corpus for German text identification on the document level and on the sentence level."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "en", "output": "My name is Regina Stoden and I will guide you through the first part of the presentation. Let's first define text simplification.."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "en", "output": "ramification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "en", "output": "train a textification model, we require parallel pairs of text, for example, of documents or sentences."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "en", "output": "the example here, you can see a parallel aligned sentence pair of a complex German sentence and it's today translation into plain language.."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "en", "output": "simplify the sentence different techniques are possible as you can see in the example such as lexical substitution clause dilation crosseletion reordering or insertion of bootss"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "en", "output": "now propose our new corpus d plane because in the recent years there were some problems with existing corporalra so for example these corporal here are too small to train a taxonification model on"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "en", "output": "other three models which are proposed in recent years are all automatically aligned, which means they can be over error prone in their alignment."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "en", "output": "we propose our new corpus D planee which is split into two subcorpora, Dplane APA and Dplane web. D planee APA is based on use texts."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "en", "output": "In Depla APA, we aligned 483 documents all manually. It results in roughly 30,000 13,000 parallel sentence pairs."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "en", "output": "deepplane web. this corpus includes different domains and we also align all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "en", "output": "total we result in 30 thousand four50 sentence pairs."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "en", "output": "analyzed our sentence pairs a little bit more so for example on the type ofifications"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "en", "output": "you can see here the Bible texts are much stronger simplified than for example the news text or the language learner texts"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "en", "output": "all level regarding for example lexical simplificaification, structured simplificaification also overall level of simplificaification."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "en", "output": "you can see that our deep planeing corpus has a high variety of different simplification transformations so for example in the deep plane api corpus we have much more reorderings and root additions than we have in the deep plane web corpus"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "en", "output": "the other hand in the web corpus we have much more rephrasings"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "en", "output": "So let's now see what we can do with this corpus.: Hello, I am Omar, and now I will talk about the use cases for our dataset dLAN. So for the first use case, we can evaluate automatic alignment methods."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "en", "output": "In the recent years, there has been a lot of alignment methods, but in the context of machine translations."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "en", "output": "where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documentsments."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "en", "output": "But, in our use case, we are trying to extract alignments between sentences of two parallel documents, having the same language, having the same content, but they are on a different complexity level."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "en", "output": "And now as we have our dataset deepplan, which have manually aligned sentences, we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "en", "output": "we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "en", "output": "the end we concluded that the best alignment automatic alignment method to use for text for German text simplification is the method of mass alignment."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "en", "output": "And you can also find the code to run this method on your own documents in the paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "en", "output": "The second use case that we showed in our paper is a case of automatic text simplification."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "en", "output": "by fine-tuning language models to produce simplified that text from the complex input texts"}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "en", "output": "We have fine-tuned two different models. We have fine-tuned the model of long part to produce document level simplifications."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "en", "output": "And we also fine-tuned the normal base the normal base in part to produce sentence level simplifications."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "en", "output": "can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "en", "output": "We concluded that this basic fine- tuning could produce or could get scores better than the baseline scores."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "en", "output": "And we propose those results as a benchmark, a base benchmark for the problem of automatic text simplification in the future."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "en", "output": "you so much for your attention and we hope to meet all of you during the conference thank you"}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "en", "output": "Hi, my name is Adam Skirkovsky and this talk is about the dependency structure of coordination."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "en", "output": "As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies are the structure of the coordinate coordination Lisa, Bart and Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "en", "output": "is such that the first conjunct is the head of the whole coordinate structure so in this case lisa"}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "en", "output": "approaches assumed in igor milchuk's meaning text theory where again the whole coordinate structure is headed by the first contract so these two approaches are asymmetric right they they single out one of the conjuncts"}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "en", "output": "there are also symmetric approaches to coordinate structures such as the prag approach, the conjunction headed approach, assumed in plugg dependency tree banks, where coordinate structures are headed by the conjunction."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "en", "output": "So we get dependencies from end to all the conjuncts."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "en", "output": "And finally, there's also a multi-headed approach that's used for example in Dekatson's word grammar."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "en", "output": "where so say all conducts are heads of the coordinate structure so we get dependencies from the governor here loves to all conducts separately these are buttons making"}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "en", "output": "the aim this paper is to produce a novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these two"}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "en", "output": "Okay, the argument is based on the principle of dependency length minimization that I will explain on the basis of these examples."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "en", "output": "So in English as might as you might know our direct objects prefer to be close to the verb while adjuncts may be further away right so March read it yesterday is fine because the direct object it is close to the verb"}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "en", "output": "March read yesterday it is much worse right because here between the verb and the direct object there is an adjunct yesterday."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "en", "output": "this effect may be ameliorated when when the direct object is very heavy and very long because then it can be moved to the position after the adjunct"}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "en", "output": "is illustrated here. So both these sentences are fine. March read this absolutely fascinating book about the beast yesterday I is okay in a way instead of it we have this long andp."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "en", "output": "it's also okay to say March read yesterday this absolutely fascinating book about bees"}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "en", "output": "the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "en", "output": "It satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "en", "output": "So these two trees only show the length of the crucial dependencies, so the ones that are not constant among these two structures."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "en", "output": "So here we have the dependency from red to the adjunct of length 7 measured in words and from red to book of length 4. so together it's 11.."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "en", "output": "you move when you swap are these two constituents the sum of these two dependencies becomes six right so instead of 11 6 much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one"}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "en", "output": "Okay, so what we did, we extracted various statistics about coordination from the enhanced version of the pentry bank and see the paper why we didn't use university dependencies."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "en", "output": "these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper not pepper and salts measured in syllables"}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "en", "output": "And also the observation that was made in passing that this tendency grows with length the France."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "en", "output": "So when the difference between the length of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is bigger of the left short conjuncts"}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "en", "output": "what's novel in in this paper is we that we observed that this tendency only occurs when the governors on the left are absent"}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "en", "output": "So the governor is on the left in this example, I saw Baton Lisa, so' the governor he's on the left."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "en", "output": "absent in the second example Homer came and sneezed here we have coordination of two verbs and there's no outside external governor right so in such cases the left conjunct prefers to be shorter the more so the the bigger the difference between the two conjun"}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "en", "output": "However, when the governance is on the right as here, left governs the coordination tail and net, this effect disappears."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "en", "output": "we showed that by measuring length in characters there's the first column in syllables the middle column and in words the right column so I'll concentrate on the right one"}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "en", "output": "What we see here is that when the governor is on the left,"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "en", "output": "tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears"}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "en", "output": "we show in the paper how this provides an argument against us asymmetric structures of coordination as these two andfold the symmetric structures as these two"}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "en", "output": "see the paper for the full agreement and arguments sorry and talk to us about in the poster session thank you"}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "en", "output": "Hi I'm Shahang B PhD student in the University of Washington Today I'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair NLB models"}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "en", "output": "language models are trained on large-scale web crawl data."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "en", "output": "news media are well covered in their pre-training data according to a survey of the C4 corpus we can see that New York Times Los Angeles Times The Guardian Huffington Post etc are well covered in language model training data"}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "en", "output": "This has created a mixed blessing for language model applications.."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "en", "output": "on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "en", "output": "this end we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks specifically by asking the following questions."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "en", "output": "first, how do we evaluate the political meaning of language models and what role doesing data might have on such political biases?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "en", "output": "Secondly, how do language models with different plutolinis actually perform on downstream tasks and whether that might result in fairness issues in NLP applications?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "en", "output": "So specifically we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test. This ensures us to do automatic evaluation well grounded in political science literature."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "en", "output": "So some preliminary results demonstrate that first language models do have varying political leanings. They occupy all four quadrants on the political compass."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "en", "output": "can also see that GPT4 is the most liberal language model of them all and GPT series are generally more socially liberal than BER series and its variants."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "en", "output": "secondly we aim to investigate to to which extent the political biases of language models are actually picked up from training data."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "en", "output": "we could conduct a controlled experiment by further pre-training language model checkpoints on six different partisan corpora separated into news and social media further divided into their political leanings"}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "en", "output": "further pre-training language models on such parties and corpora we can see that the ideological coordinates of the language model also correspondingly shifts"}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "en", "output": "For example, for Roberta, further fine-tned further trained on the left leaning reddit corpus, we can see a substantial liberal shift in terms of its."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "en", "output": "In terms of its political biases."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "en", "output": "And we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "en", "output": "we divide pre-training corpora into pre-45th president of the united states and after 45th president of the united states we separately pre-train language models on the two different temporal corpora"}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "en", "output": "can see that language models generally had a political leaning that is further away from the center after 2017. So this indicates that language models can also pick up the like polarization in our society."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "en", "output": "So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "en", "output": "So, we see that if we investigate the per category performance, that is to say, if we separate the performance into."}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "en", "output": "different demographics or political medium news media, we can see a pattern that, for example, for hate speech detection, left-ling language models are better."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "en", "output": "At detecting hate speech targeting socially minority groups."}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "en", "output": "However, are worse at detecting hate speech targeting more powerful groups in our society."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "en", "output": "vice versa, rightling language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ+ and other minority communities.."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "en", "output": "trends also happen for fake news detection where we see that left-ling language models are better at detecting misinformation from their opposite politicallini and vice versa"}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "en", "output": "this in we further show many qualitative examples to see that language models with different political meanings,"}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "en", "output": "give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in Appendix to further highlight that."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "en", "output": "This indicates that there is a fairness issue that is very pressing regarding the political biases of language models."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "en", "output": "For example, if a rightling language models were to be fine-tuned on hate speech or misinformation or whatever and deploy to a popular social media platform,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "en", "output": "This would mean that people with opposite political opinions might be marginalized, and the hate speech targeting minority groups might just run rampant without any control."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "en", "output": "this has sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political meanings"}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "en", "output": "a little bit of discussion we would also like to highlight that we expose the unique dilemma regarding language model political biases it's like between syilla and charybdis"}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "en", "output": "So, if we do not sanitize political opinions in language model training data, the bias would propagate from pre-training data to language models to downstream tasks, ultimately creating fairness issues."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "en", "output": "If we do try to sanitite somehow, we would also risk censorship or exclusion, and it's incredibly hard to determine what is actually neutral and should be retaining language monottaining data. So it's kind of like the electric trolie problem."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "en", "output": "great. I think that's pretty much all I have for today. F5 for today. Thank you for your time."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "en", "output": "Hi everyone, I'm Jenny, a first-year PhD student at Carnegie Mellon University, and today I'll be presenting her work Anal positionality, characteracterizing design biases and data sets models."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "en", "output": "This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santi, Ronan Labrasse, Katarina Reinika, and Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "en", "output": "So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "en", "output": "You might turn towards a popular API like Perspective API for toxicxicity De detection, and this works really well if you're Carl Jones, wherespective API is able to detect correctly toxic instances."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "en", "output": "But that's not really the case for Aditya Sharma, where prospective A API is really not as sensitive to offensive terms that are more common in Indian contexts."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "en", "output": "This is an example of a design bias where we see systematic performance differences of technology between populations."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "en", "output": "Design biases like the one that we just saw before might encourage you to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "en", "output": "This is a concept widely used in critical studies, specifically in feminist and queer academic spaces."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "en", "output": "And as a researcher, positionality can influence the research process and its outcomes and results, because it can change the decisions that researchers make."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "en", "output": "And so one question that people might ask is, do dataset and models have positionality?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "en", "output": "And we're not trying to say that models in cells and datasets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "en", "output": "So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps in models and data sets, as well as theoretical definitions of model positionality."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "en", "output": "However, these works really don't look at comparing end users with the datasets and models themselves."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "en", "output": "And studying model and dataset positionality is increasingly important as NLP tests become more subjective and socially oriented."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "en", "output": "And it's challenging to characterize how these positionalities are skewed, because not all decisions are documented, and many models are hidden behind APIs."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "en", "output": "So to study dataset and model positionality, we actually compare the annotations with real users with existing dataset and models."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "en", "output": "do this through our framework NL positionality."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "en", "output": "framework works in two main steps."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "en", "output": "The first step is to re-annotate datasets with diverse annotators."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "en", "output": "And we ought to do this over looking at the demographics of original dataset annotators, because usually only a few annotators annotate each instance, and because demographics are rarely collected and shared."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "en", "output": "And so we opt to re-annotate data to get many annotates, for instance, and to get a rich set of demographic data."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "en", "output": "We then take the annotations by demographic and compare them to the models and dataset using comparisonar's R correlation score."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "en", "output": "And thus our framework actually differs from annotator disagreement literature by comparing end users with models and data sets, predictions and labels, as opposed to looking at just annotator agreement or modeling annotator distributions."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "en", "output": "framer is largely enabled through Lab in the wild, an online crowdsourcing platform former HCI collaborator."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "en", "output": "And Lab in the Wild is an online experimentation platform where we can recruit diverse volunteers compared to the platforms like MTERk, which largely have participants from the US or India. And further, Lab in the Wild still is able to get highqual data."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "en", "output": "We host two tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset, and then they'll write how socially acceptable a situation is."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "en", "output": "Afterwards, to stay engaged in the city, they can compare their responses to an AI and others."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "en", "output": "We then compared these annotations with social chemistry, Delphi and GPT4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "en", "output": "then replicate a very similar setup for the toxicity and hate speech detection task where they'll read an instance from Dinah hatete and write whether they think it's instance of hate speech."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "en", "output": "We then compared these annotations with Dynah Hate, Perspective API, Rewire API, Hate Roberta and GPT4. Our study and the end amassed over 16,0000 annotations from over 1,000 annotators from 87 countries."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "en", "output": "now we're better equipped to answer who do NLP dataset and models align with the most. We find that there is positionality in NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "en", "output": "For example, we find that dataset and models are most aligned to English-speaking countries. So for the GPD4 social acceptability analysis, we find that it's most aligned to Confucian and English-speaking countries. We find that dyna hate is also most aligned to English-speaking countries."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "en", "output": "We also find most additional alignment with people who have a college education. So for GPD4 in the social Acceptability task, we find that it's most aligned to people with a college education or graduate school education."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "en", "output": "we find the same for Diny Haight, where it's most aligned to people with the college education."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "en", "output": "However, when models and datasets are aligned to specific populations, some are inevitably left behind."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "en", "output": "An example of this is that dataset and models are less aligned to non-binary people compared to the men and women counterparts. We find this in the GPG4 social Acceptability task as well as the Diny hatete task analysis as well."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "en", "output": "So given that there is position in LD in LP what can we do about it?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "en", "output": "So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process, and the other is to do NLP research with the lens of perspectivism."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "en", "output": "Our third recommendation is to build specialized dataset and models within four specific communities and a good example of this is the Masakanne initiative. I mean we want to emphasize that inclusive NLP isn't just making you know all technologies work for everyone."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "en", "output": "so that concludes our presentation but if you'd like to learn more feel free to check out our dashboard for the most updated analysis results and our paper Thank you"}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "en", "output": "Hi, I'm X Yuan from Faii University. I'm here to introduce our work: Distinguiing Script Knowledge from Light Language Models for Constrained Language Planning."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "en", "output": "In everyday life, who must often plan their actions by following step-by-step instructions in the form of guaranteed scripts."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "en", "output": "Previous world has exploreed language models to plan for abstract goals of stereotypical activities, such as make a cake and showed that large language models can effectively decompose goals into steps."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "en", "output": "However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planing for the goals with specific goals, specific constraints, such as make a chocolate cake, still remains understated."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "en", "output": "In this paper, we define the problem of constrained language planning."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "en", "output": "Which impose different constraints on the goals of planning, an abstract goal can be inherited by different reallife specific goals with multiface constraints. A good planner should write scripts that are reasonable and faithful to constraints."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "en", "output": "In this paper, we first evaluate and improve the constrained language planning ability of life language models."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "en", "output": "no data outside of specific goals exists to spot our star day."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "en", "output": "have to acquire these goals first as shown in the table, we extend the abstract goals with multifaceted constraints for human in the loop data acquisition use instruct Gpt."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "en", "output": "We sample hundreds specific goals and evaluate the scripts generated fromlogical models."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "en", "output": "This table reports the overall accuracy of the results. We find that all Lilong models achieve unsatisfactory results on planning for specific goals."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "en", "output": "Then we conduct detailed analysis to investigate what learning models for."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "en", "output": "Results in the figure show that the semanal completeness in generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "en", "output": "We dig into more finegradd topic categories of constraints defined in Wi home. The heat map in the figure shows that the planning performance of instructiv varies considerably for girls of different categories."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "en", "output": "Previous studies have shown that the output quality of live models falls in high variance, leading to bad performance. Thus, we adopted the idea of overgenerated the filter to improve generation quality."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "en", "output": "We first show constrained types with examples for instruct CPT and obtain specific goals based on the set abstract goals."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "en", "output": "Ins instruct the GPT over general key scripts for specific goals."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "en", "output": "Next, a filter model is derived to select the physical scripts."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "en", "output": "We convert scripts and girls into instruct GPT embeddings and calculate cosine similarity as similarity scores to meth semantic similarity."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "en", "output": "In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal site."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "en", "output": "With our method, in instructibility can generate screws of higher quality. Our method greatly improves the planability both in semantics, completeness and faithfulness to the constraint."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "en", "output": "Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. C Creating dataset is an essential step to"}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "en", "output": "However, previous studies do not enable planning for specific goals, and the manual data dataset annotation is expensive."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "en", "output": "Thus, we follow the idea of symbolic knowledge distillation to distill constrained language planning dataset from life language models."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "en", "output": "We will apply our method for building a dataset of contrained language planning named as CodeScri."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "en", "output": "In total, we generated fifty five thousand specific goals with scripts to ensure the quality of validation and test sites. We ask crowd-sourced workers to finally revise the income in incorrect samples."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "en", "output": "This figure shows the consstraint distribution of codeSscript. We find Coscript shows the high pluralism in the generated specific goals. With Coscript, we can treat smaller but specialized models for constrained language planning."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "en", "output": "With size, t five finetu on score rate can generate scripts of hair qualities and most largelevel models, indicating that smaller models can suppress larger models when properly trained on suitable data sites."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "en", "output": "In summary, we established the constrained language planning problem. We developed a constrained language planning ability of large language models and develop an overgenerated filter method for large language models."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "en", "output": "We use large language models to generate a high qualityity square dataset, Codecri, for constrained language planning. Wehop CodeSscript dataset can be a valuable resource to advance research on language planning."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "en", "output": "thanks for your time. Please find more details of Codecri in our paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "en", "output": "hello everyone my name is Shu H today I'm going to present our paper do Connell 2003 named entity Tagggers still work well in 2023 let's get started"}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "en", "output": "Our paper investigated the problem of generalization using the named entity recognition task or the NER task"}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "en", "output": "We observe that models have been using ConONO 2003 to develop NER for almost 20 years. and this naturally raises several problems. Firstly, can these models generalize to modern data?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "en", "output": "And when we develop new taggger, what is needed for good generalization?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "en", "output": "At the same time, if we do observe poor generalization, what causes the performance drop of these models?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "en", "output": "To investigate these problems, we developed the Connell++ dataset. This is a data set that we collected from Reuters News from 2020 and then annotated them with the same Connell 2003 annotation guidelines."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "en", "output": "then fine-tuned over 20 models on Conal 2003. We evaluated them on both the Con O3 test set and the Cono plus first test set"}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "en", "output": "And last but not least, we calculated the percentage change in F1 to assess the generalization of each model."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "en", "output": "So, what is needed for good generalization? Through our experiments, we found that there are three main ingredients that are needed."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "en", "output": "The first one is the model architecture. Through our experiments, we found that the transformer models normally generalize better to new data"}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "en", "output": "The second ingredient is the model size. We found that usually larger models lead to better generalization."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "en", "output": "And last but not least, we all know that the number of fine-tuning examples directly affect the performance of a downstream task. Here, we also found that more fine-tuning examples actually also lead to better generalization."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "en", "output": "our next question, what causes the performance drop of some models"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "en", "output": "We have two hypotheses. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again, and this is usually manifested as the diminishing returns on a new test set."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "en", "output": "The second hypothesis is temporal drift, which is the performance degradation that is caused by the increasing temporal gap between the train and the test data."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "en", "output": "For dative overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than 1."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "en", "output": "This means that every unit of improvement that we made on Colo 2003 translates to more than one unit improvement on Colo++, which means that there is no diminishing returns."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "en", "output": "And this shows us that adaptive overfitting in this case is not observed."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "en", "output": "So what about temperature of that?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "en", "output": "For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "en", "output": "And this confirms our hypothesis that the main cause of the performance drop is temporal drift."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "en", "output": "Our conclusion is that for good generalization, we would need a better model architecture, larger model size as well as more fine-tuning examples. and these goals hand in hand. We can't just have one ingredient but throughout the others."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "en", "output": "At the same time, we also found that the performance drop here is caused by temporal drift, and kind of surprisingly, it is not caused by adaptive fitting, even though Connell 2003 has been used for over 20 years."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "en", "output": "So going back to the question that we raised in the tide of our paper, do Carnal 2003 taggers still work in 2023? And we found that the answer is actually a resounding yes."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "en", "output": "We hope our paper calls for more research on how to improve generalizations of the models."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "en", "output": "And lastly, please make sure to check out our paper, our dataset and if you have any questions, feel free to contact me. Thank you so much."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "en", "output": "hi and i'm going to talk about our work on resolving indirect differential expressions for entity selection in which we introduce the alt entity corpus"}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "en", "output": "My name is Javad Hosseini and this is a joint work with Philipp Radlinsky, Sylvia Parity, and Annie Greece."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "en", "output": "goal is to understand users' language when they want to make a choice and consider this alternative question: did you mean easy on me orI got a feeling? Here a user wants to select between one of these two songs."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "en", "output": "most obvious thing is to use a direct reference for example by saying the name of the song is on me or its position the first one"}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "en", "output": "But sometimes an indirect reference is more appropriate to have a more natural conversation. this could happen when the user cannot remember the name of the song."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "en", "output": "the pronunciations are too similar to each other and hard to disambiguate"}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "en", "output": "or when the user wants to specify a preference. Here are some examples in direct differences, for example the newer one or the sign that's not energetic."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "en", "output": "is an important problem in conversational systems and also for benchmarking llm's entity understanding"}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "en", "output": "are not aware of a public data set a large scale public data set for a task so we collect one using crowd annotation our data set covers three different domains music books and reception"}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "en", "output": "data set collection methodology emphasizes informality using a cartoon completion set"}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "en", "output": "cartoon has three speech bubbles. in the first bubble Bob says remember that song we were listening to yesterday and with that Bob sets the dialogue context"}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "en", "output": "this in the second speech bubble Alice saysDo you mean easy on me or I got a feeling"}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "en", "output": "is the alternative quest and in the third speech bubble Bob uses an indirect reference to select one of these entities for example the newer friend"}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "en", "output": "provide the first and second speech bubbles automatically but the third one is filled in by the annotator the first speech bubble is chosen from a few manual prompts per domain"}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "en", "output": "The second one, which is the alternative question, is generated as follows."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "en", "output": "always use a simple template do you mean a or B where a and B are samples from Wikipedia"}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "en", "output": "Here are the different sampling methods we've used when we move higher in the list the entities become more similar to each other and it's usually harder to make the disambiguation"}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "en", "output": "first one is uniformre"}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "en", "output": "The second one is when the entities have similar titles, for example two books with the name the retail."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "en", "output": "third one is when they have similar descriptions on wikipedia and finally when they have similar info voices or attributes on wikipedia for example the same genre or the same artist for example"}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "en", "output": "we show this alternative question to the amers they know the name of these entities but they don't necessarily know about the entity"}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "en", "output": "so what we do is that we show some background knowledge about the two entities for songs we simply show a Google search link to each song"}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "en", "output": "then ask the annotators to listen to at least some of each song and read about each song here's for example the Google search result for the song easy answer"}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "en", "output": "for the recipes and books domain we show some background text from Wikipedia for recipes we additionally show their images again from Wikipedia so that the annotators know how they look like"}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "en", "output": "Then we ask the annotators to pick one of these entities, for example, here the first one, and describe them using three to five indirect referring expressions."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "en", "output": "example the one with the piano music here are some examples from our data set for example the one without words not the one with the 12 year 12 year old boy or the fictional one or comes from Azerbaijan and so on"}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "en", "output": "alternatives corpus has 6 000 alternative questions across three domains and it has 42 000 indirect referring expressions results with t5x large model are summarized below"}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "en", "output": "If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high. It's around 92 to 955%. But this is not realistic."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "en", "output": "If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87 percent, which is more realistic, for example, when the language model retrieves the background knowledge."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "en", "output": "if the language model has access only to entity names then the accuracy is only 6 percent so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our data set thanks for"}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "en", "output": "Hi, I'm Sarah Papppy from the University of Trento and Foa scene Bruno Kessler and I will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with Matteo Negri and Marco Duchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "en", "output": "What is simultaneous speech translation? simultaneous speech translation or simSD is the process of translating spoken language into text in another language in real time, enabling cross language communication."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "en", "output": "what are the problems of the current SimST models? Specific architectures are usually trained, introducing additional modules to be optimized."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "en", "output": "Long and complicated training procedures, for example, training involving different optimization objectives."}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "en", "output": "And training and maintaining several models to reach different latency regimes, for example, training a model with an average of one second latency and another one with two seconds latency, and so on."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "en", "output": "So what is our solution?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "en", "output": "First to use already existing offline SD models without retraining or adopting specific architecture for SSD. Use only one model for every latency regime and handle latency through specific parameters."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "en", "output": "And leverages the knowledge already acquired by the model through the tension mechanism between audio input and textual output, that is the crosstention mechanism and you can see an example on the right."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "en", "output": "Our solution is to propose a dot or encoder decoral attention, and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "en", "output": "A word is emitted if the tension is not concentrated, that is this sum is below a certain threshold alpha towards last lambda speech frames, meaning that the received information is enough stable."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "en", "output": "for example if if we receive a speech chunk containing \"I'm going to talk about\" and our model predicts the translation in German."}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "en", "output": "And we will look at the cross attention weight."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "en", "output": "We'll see that the first two words points to the earliest received speech frames while the last word points to the last received speech frames as lambda speech frames."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "en", "output": "This means that the first two words will be emitted."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "en", "output": "while since the sum of the cross tension is above a certain threshold alpha we will not emit the last word and we wait for another speech chunk"}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "en", "output": "If we go on and we receive another speech chunk and our model predicts over three words and we will look at the cross attention weights."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "en", "output": "We will see that no words points to the last Lamb speech frames."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "en", "output": "This means that these three words will be emitted."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "en", "output": "If you look at the main result of a dot."}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "en", "output": "We plot the simultaneous page translation results on on graphs in which we have blue on one side that measure the translation quality and average lagging."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "en", "output": "that is the latency measure. and we also consider the computational aware average lacking that accounts for the model's computational time to predict the output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "en", "output": "So we want our cures to be as high as possible on this plot."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "en", "output": "But also we want that they are shifted on the left."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "en", "output": "And we compare with plepara strategies that are also applied to offline models that are the withK strategy and the local agreement. and we compare also with the state-of-the-art architecture specifically tailored for simultaneous speech translation."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "en", "output": "These are all the results of the simultaneous speed translation strategy on German."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "en", "output": "And we see that a doubt outperforms all the strategies applied to offline models, since the curves are shifted over the left."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "en", "output": "And we also see that if we consider the actual elapsed time or the computational wear time, and that is the fastest strategy."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "en", "output": "If you want to discover more results, read our paper and we also released open source, the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "en", "output": "Hello everyone. My name is Ian and my colleague Jion and I will be presenting our research on multi-Instruct, improving multimodal socialial learning via instruction tuning."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "en", "output": "So with the advances in large language models, many works started to explore new learning paradigms of reusing pretrain language models for different downstream tasks in a parameter and dataffi way."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "en", "output": "Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a ser shortt manner by following natural instructions."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "en", "output": "However, most previous works on instruction tuning focused on improving the serialhel performance on language-only tasks, while computer vision and multimodal tasks have been left out."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "en", "output": "Therefore, in this work, we want to investigate whether instruction tuning on multimodal proteintrain models can actually improve generalization to unseen multimodal tasks"}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "en", "output": "Additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction dataset between RP and multimodal."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "en", "output": "exists more than 1600 lunch-only instruction tasks however there is no largescale publicly available multimodal instruction task therefore this motivate us to build a multimodal instruction tuning data set"}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "en", "output": "Here we present Multi-insstruct, the first multimodal instruction tuning benchmark data set that consists of 62 diverse multimodal tasks covering 10 board categories."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "en", "output": "tasks are derived from 21 existing open source dataset and each task is equipped with five Expir written instructions."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "en", "output": "investigating multimodal instruction tuning are our proposed dataset we take ofFA a unified multimodal training model as our base model ofFA use a unified vocabulary for language image tokens and the coordinates of a bounding box"}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "en", "output": "Here we show some example instances from our multi-instra dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "en", "output": "unify the processing of a various input and output data types."}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "en", "output": "We follow the method from OFA and formulate all the tasks in a unified sequence to sequence format in which the input text, images, instruction and bounding boxes are represented in the same token space."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "en", "output": "Okay, now I'm going talk about multimodal instruction tuning."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "en", "output": "So for the training dataset, we use 53 tasks from N group for training and we sample 10,000 instance per task. For testing, we reserve the entire common sense reading group for testing and we select additional five tasks from WiQ and the miscellaneous group."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "en", "output": "We use all the instance in the test speed for each task. In addition, we randomly sample 20 tasks from the test speed of natural instruction as on same task for NRP"}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "en", "output": "So we use a pretrained OFA large model as a base model. During training, we mix all the instance for all the tasks. Each instance is randomly combined with one of its 5 instruction templates."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "en", "output": "So during test for each task, we conduct the total of 5 experiments by evaluating the model using both of the 5 instructions in each experiment."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "en", "output": "We report the mean and max performance and the standard deviation of the performance across all 5 experiments."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "en", "output": "If the task is a multimodal classification task, we report accuracy. If it's a multimodal generation task, we report rootjL. For an RP task we report RujL as well."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "en", "output": "We also introduced an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "en", "output": "Here is our main results. As we can see, instruction tuning can significantly improve OFE's performance on same multimodal tasks"}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "en", "output": "Also transfer learning from natural instruction dataset can benefit instruction tuning."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "en", "output": "Here we can see as the amount of tasks increase, the model achieve better performance and in the meantime, lower sensitivity."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "en", "output": "So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instruction can improve the model's overall performance and reduce its sensitivity a lot."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "en", "output": "So this shows the effect of different front-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction dataset, the model can achieve much better sensitivity compared to the original IFA model."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "en", "output": "We also can see transfer learning from Nitro instruction data set can help OFA to achieve much better performance on the NitroE In instructct data set."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "en", "output": "Overall, we proposed the first large-scale multimodal instruction tuning data set. WithFA continually improve the neural capability of OFA and we explore different transfer learning techniques and show there are benefits. We design a new metric called sensitivity."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "en", "output": "So one more thing we are collecting a much larger multimodal instruction tuning data sets with around 150 additional variant language tasks and we will release them so this is a QR code for our data and model thank you"}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "en", "output": "Hi, everyone. I'm Koovsna, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "en", "output": "This is a joint work with John Baqui, Aaron Muller, Kanishka Mishra, Karen Fs, Roger Levy and Atina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "en", "output": "So in this work, we revisit the minimal pair paradigm."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "en", "output": "So the minimal  pairtopara basically evaluates language models on top of acceptability judgments, which can also include grammaticality like blimp, syntax gym or acceptability in terms of stereotypes such as crowds pairs."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "en", "output": "And in this minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "en", "output": "And then the hope is that the model basically puts more probability to the acceptable settlement."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "en", "output": "The current MPP pipeline basically doesn't allow us to evaluate models's acceptance towards longer sentences."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "en", "output": "These days, large language models are coming up with longer and longer context spin. So it's crucial that we evaluate the model's acceptability on throughout the context window."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "en", "output": "And that is what we are trying to do here. We are trying to revisit the NPP pipeline by asking the model to evaluate acceptability on longer and longer sequences."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "en", "output": "So that is the approach. So what we do is that to simulate these longer sequences, we revisit the dataset themselves and then we recreate sentences by choosing like acceptable or unacceptable sentences from those data sets."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "en", "output": "So for example, here we have chosen like a typical pair ofmaticity from the BbliIM data set from the adjunct island case."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "en", "output": "And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure, we extract grammatical sentences from adjun pilot"}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "en", "output": "And then we add as a prefix to both the acceptable query and the unacceptable query."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "en", "output": "So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also like be used to test the model's acceptability."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "en", "output": "And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "en", "output": "So here, the sentences are still coming from relevant data sets, but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "en", "output": "Finally, we can choose sentences from a completely unrelated domain such as Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "en", "output": "So this will tell us like whether the model's acceptability judgments are actually impacted by any context."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "en", "output": "Like whether the context is coming from a different subset of the data set or whether it's like completely irrelevant to the current -- to the sentence that we are looking at."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "en", "output": "So how does the model do? So first, we look at the Wikipedia sentences which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "en", "output": "We increased the context length toward up to 2024 for to max out OPT and GPT2 models. and we saw here in the orange dotted line, the MPP judgments are relatively stable."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "en", "output": "Now what happens when we choose sentences from the same data set?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "en", "output": "So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BlimIM syntax gymIM data set"}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "en", "output": "And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "en", "output": "But when we match the structure, that is when we choose the sentences from the same phenomena in blame person taxgen,"}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "en", "output": "We see a massive increase or a massive decrease of the MPP judgment for the model depending on whether the chosen prefix is acceptable or unacceptable."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "en", "output": "Now this -- and this is very large, like this effect increases throughout the context length, and this would probably affect like newer language models which has large context window."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "en", "output": "So why does the match prefix affect the language model judgment so much?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "en", "output": "So we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "en", "output": "We find that none of these noises are actually making the model like change its course in terms of how it shows us then payP judgment trend."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "en", "output": "Basically, we find that the models are sensitive to the per of sentences in similar ways."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "en", "output": "That is when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations. And when we perturb the sentences in the acceptable approval domain, we see decrease in MPP judgments in similar fashion."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "en", "output": "So the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "en", "output": "And the MPP evaluation, the way that we do it currently with short and single sentence input, may not fully capture the language models' abstract knowledge throughout the context window."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "en", "output": "Please read our paper for more details of our experiments. Thank you for listening."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "en", "output": "Hello everyone. My name is Just John from the Penn State University. Today I'm going present our work, Exemplar: cross-lingual Semantic parsing in multiple natural languages and manual representations."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "en", "output": "semantic processing is a task to build semantic representations of user queries such as ZQL and Lambda calculus."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "en", "output": "cross-lingual semantic pars is the task to translate queries in multiple natural languages into multiple meaning representations."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "en", "output": "shown in its figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or funQL and etc."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "en", "output": "Exist cross-lingual semantic parsing models are separately proposed and evaluated on the set of limited tosses and applications. For instance,"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "en", "output": "There are links of um coverage on certain natural language the Chinese is missing, and."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "en", "output": "Lakecus of coverage on certain many representations."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "en", "output": "The Lamb calculus is missing."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "en", "output": "they're only evaluated on certain neural model for example there's only one single model to evaluate the"}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "en", "output": "So to this end, we proposed Ex exampler, but provide a uniform dataset exampler for cross-lingual semiperssing in multiple natural languages and meaning representations."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "en", "output": "contains 90 sets in virus domains, 5 seman parts in taxes, 8 million representations, and 22 natural languages in 15 language families."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "en", "output": "to better evaluate our benchmark, we consider the six settings for training and evaluation."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "en", "output": "first one is translate test we'll use Google translate API to translate source to the target language then use monolingual model to train any evaluations"}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "en", "output": "And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "en", "output": "we'll also test monolingual model."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "en", "output": "this setting the source language is the same as target language, for example German to German or English to English."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "en", "output": "also test monolingual future setting by training modellingual models with only 10 percent of training data"}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "en", "output": "And which has modeling a multilingual model, which we train one multilingual model for all languages."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "en", "output": "For example, we put the German, English, Chinese queries together to train a multilingual model and during inference, we can use this model too."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "en", "output": "Um to translate German queries or Chinese query or etc."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "en", "output": "And we also consider cross-lingual zeroshot and zero-shot transfer. We train on one source language and transfer to another language."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "en", "output": "during training, we train it on English query or the combination of English and German few short queries to train a multilingual model and predict the SQL output."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "en", "output": "we also find many interesting results. So regarding analyze of monolingual models, we evaluate on two groups of models."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "en", "output": "including encoderPDdR which stands for multilingual pertrained encoders with pointer-based decoders such as X elementr plus pdr and bird plus pdr"}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "en", "output": "And we also evaluate encoder decoder models, which is multilingual pertrain encoder decoder models, such as B and Mt5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "en", "output": "found that encoder decoder obtains the best performance on all nine datasets."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "en", "output": "we evaluate our Mmt5 and example xlmr plusPDdr our multilingual settings"}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "en", "output": "that, encoder decoder or encoder PDR can be improved by training in a mixture of various languages."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "en", "output": "we found it is because most of the major natural languages can obtain performance gain except that English performance drops in seven data sets and only gains in three data sets"}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "en", "output": "I think this is known as Kurds of multilinguality."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "en", "output": "We also compare the cross-lingual performance gap."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "en", "output": "In this figure, the blue line is cross-lingual Fu transfer. the orange line is cross-lingual zero-she transfer, while the green line is the monolingual setting."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "en", "output": "found that by comparing the green and orange line we found the for zero short setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that for few short setting the transfer gap is shortened rapidly"}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "en", "output": "also find some other interesting findings. For example, encoder decoder outperforms proW work or achieved comparable results. Pertruing on English natural language can significantly boost the performance of futuresho on target natural languages."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "en", "output": "we found multilingual language models such as coders and blue are still indequate for cross-lingual semi-personsing classes."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "en", "output": "sum up we' build exampler, a unified benchmark for cross-angle semantic parsing with multiple natural languages and many representations."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "en", "output": "conduct a comprehensive benchmark study on three representative of types of multilingual language models and our result shows many interesting findings and Etc and welcome to visit our paper and code thanks for listening"}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "en", "output": "everyone my name is al Villaad and I will make even a short overview of the paper printinging palm from translation assessing strategies and performance this is joint work with my colleagues from Google Translate"}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "en", "output": "is a 540 billion parameters large language model presented last year in 2022. it's trained on a large collection of texts comprising 780 billion tokens"}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "en", "output": "duma for the kitchen it achieves state of the art in hundreds of Nlp tasks"}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "en", "output": "this work we present a first systematic study of large language model prompting for machine translation"}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "en", "output": "We evaluate the transition capacity of such models using the best practices of the IMT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "en", "output": "we compare to state-of-the-art systems so the best performance systems or the WMT evaluations"}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "en", "output": "We use state-of-the-art neuralMT metrics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "en", "output": "the prompting has a big influence on the performance of the of lnms for translation as we can see in a simple experiment where we use one short prompting and provided two different prompts for for different sentences"}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "en", "output": "majority of sentences, 516 out of 1000, the difference observed is of more than one blurred points."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "en", "output": "this can go in extreme cases up to 40 blur points. So it's important to select a good prompting strategy."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "en", "output": "our experiments with solution for a fiveshot prompting strategy where we just mark its its sentence that we provide the to the system with the language it's in"}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "en", "output": "In this example here where we perform translation from German into English, the German sentences the source sentences are marked with German colon and the English translations with English colon."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "en", "output": "saw that the actual form of the printing doesn't have a big influence in in the case of several short printing"}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "en", "output": "It's crucial for zero and one- shott prompting, and when we go, as in our case to fact- shott prompting, there is nearly no difference to the actual form of the prompting.."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "en", "output": "s the examples that carry most of the of the weight"}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "en", "output": "The summary of our experimental results is that the example quality is more important than the similarity to the source sentence."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "en", "output": "it's important to select the examples from high quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations or the dev data."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "en", "output": "dev data is much more created and with higher quality that the train data that it's more nice and the results so a better performance when using the the dev data"}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "en", "output": "nevertheless specialized state-of-the-art systems have a substantial advantage over the pan translations but one comes pretty close to a commercial system in our case we chose to avoid with Google Translate"}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "en", "output": "The insights that we gain from the emailation that we perform using the MQN framework is that the fluency of palm is comparable to state of the-art systems, but the main difference comes from the accuracy."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "en", "output": "In particular, the most common errors are omission errors."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "en", "output": "So it seems that Palm chooses to produce a better sounding translation sometimes by dropping parts of the source sentence that aremitted in translations."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "en", "output": "However, the style outward category for pan is lower than for the state-of-the-art systems, which is an additional signal."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "en", "output": "that parm provides really fluent output, but still with some problems of accuracy."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "en", "output": "that's it for this really short overview for more details please come my to the full presentation of the paper thank you very much"}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "en", "output": "Hello I am Dawei a PhD student at silent University in Germany in this video I would like to present our recent work biggerer than you think a critical look at weekly surprise Lening"}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "en", "output": "is joint work with Sha my muba and gear Stefan and ditishklakov"}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "en", "output": "like to begin with a brief introduction to week supervision and weekly supervised learning"}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "en", "output": "weak supervision we do not manually label the data. Instead we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or locality code sourcing, as illustrated in the figure and right."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "en", "output": "compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "en", "output": "we directly train neural networks on weakly label data, the neural networks tend to memorize the label noise and do not generalize."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "en", "output": "weakly supervised learning training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalized well"}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "en", "output": "recent works in wSL so wSL stands for weekly support learning a common claim is that people say that they only train models on the weekly label data and achieve high performance uncle clean test sets"}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "en", "output": "Tech this claim is not wrong, but there's a catch."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "en", "output": "is that people do assume that there' an additional clean validation set or wellwol form model selection"}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "en", "output": "stopped on this problem setting but this implies that additional manual annotations are required in weekly support learning but like an elephant in the room this necessity is often overlooked"}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "en", "output": "aforementioned adopt us to ask three research questions. First, is clean validation data necessary for WSL? or can we maybe use a noisy validation set instead?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "en", "output": "if clean data is required or if clean data is mandatory for WSL to work then how many clean samples do we need finally should we only use the clean samples for validation or there are better ways to utilize them"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "en", "output": "addressed these research questions in our work and our findings are as follows."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "en", "output": "First, we find that interestingly recent WSL methods indeed require clean wideation samples to work properly."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "en", "output": "Otherwise there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trend models cannot generalize beyond the original weak labels."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "en", "output": "that of training is pointless."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "en", "output": "indicates that WsSL approaches actually require cleanly labeled data to work properly and the annotation cost for obtaining clean validation samples should not be overlooked"}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "en", "output": "second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "en", "output": "we only need 20 samples per class to attain high performances"}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "en", "output": "But thats not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "en", "output": "red figure shows the performance difference between fine-tuning approaches which are directly applied on the clean data and WSL approaches which use the clean data for validation onlying"}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "en", "output": "we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "en", "output": "Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "en", "output": "we can see from the figures the validna model termed ftw initially underperforms more complicated WSL methods like cosine"}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "en", "output": ", if we allow to continue fantuni on the clean samples, then Tw performs equally well as other methodsrs"}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "en", "output": "in practice, there's no reason to choose more complex WSL methods which require more computation time and disk space."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "en", "output": "summarize we showed that recent wSL approaches require clean manually annotated samples for them to work properly their performance gain and practicality are heavily overestimated"}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "en", "output": "concrete recommendations for future work hours follows."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "en", "output": "First, report the model selection criteria. For example, report if the model section is done while clean validation samples."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "en", "output": "Second, WSL approaches should be compared with few short landing baselines as suppose work on concrete samples Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL"}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "en", "output": "we have open source our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoying the conference you."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "en", "output": "Hello, I'm James Finch And I'm Sarah Finch. And today we'll tell you all about ABC Eval, a new dimensional approach to evaluating conversational AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "en", "output": "This work was done by the Emory NLP Lab, led by Professor Gino Choi at Emory University, and in collaboration with Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "en", "output": "let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "en", "output": "The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a liquor scale."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "en", "output": "These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grainined level."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "en", "output": "approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or liquor scale methods."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "en", "output": "However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "en", "output": "approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "en", "output": "We call this approach annotating behaviors in chat, or ABCEval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "en", "output": "ABC eval is capable of measuring the rates at which chat models will commit various thematic errors."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "en", "output": "For example, ABCEval measures the number of turns in which a chat model ignores its partner or says something irrelevant."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "en", "output": "contradicts itself or its partner hallucinates incorrect facts or violates common sense knowledge and when the model succeeds or fails to show empathy"}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "en", "output": "determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using ABC eval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "en", "output": "For comparison, we also evaluated these conversations using three existing methods: liquor ratings on the turn level, liquor ratings on the dialogue level, and dialogue level pairwise comparisons."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "en", "output": "each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "en", "output": "our analyses of these evaluation results, we found that ABC behavior labels are overall more reliable than labels collected by existing methods, as measured by inner annotator agreement on a 100 doubly labeled conversations."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "en", "output": "In addition, ABCEval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "en", "output": "example you can see how measuring the proportion of turns with self and partner contradictions explains five percent and ten percent of conversation quality respectively while the average liquor consistency scores explain only four percent or less"}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "en", "output": "Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "en", "output": "can see how the combination of all ABC Eval metrics explains over 25% of conversation quality. And as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "en", "output": "On the other hand, the combination of all turn-level liquor metrics explains far less of the quality, and fewer of these metrics carry unique information."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "en", "output": "reliable, informative and distinct ABC Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "en", "output": "can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "en", "output": "produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% percent of the time."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "en", "output": "the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models"}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "en", "output": "hope ABC Eval can be leveraged by others in the field as a meaningful step in this direction, and we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "en", "output": "hello my name is Kyyo Yin and I will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with Patrick Fernage Emiliu Andre FD Martins and Graham Newbiig"}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "en", "output": "So a lot of translations depend on context. For example, how would we translate mole in this sentence?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "en", "output": "if the previous sentence wash could start to get dangerous if the ministers find out, then more refers to a spy. But if the previous sentence wasCould it be anything serious, doctor? then more refers to a birthmark."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "en", "output": "So depending on context, the meaning of the word changes and therefore its translation changes as well."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "en", "output": "However, evaluating how well models can contrast cases like this is pretty hard. Firstly, because only a small portion of translations depend on context, which makes corpus level metrics like blue unable to capture these translations."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "en", "output": "And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages, since they usually rely on domain knowledge and human curation."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "en", "output": "In this work, we try to answer these two questions. First, when does translation require context? and second, how well do models handle these cases?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "en", "output": "To answer the first question, we started by measuring how much work depends on context during translation."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "en", "output": "In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y given the source X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "en", "output": "can think of CXMI as the information gained from giving context to the model."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "en", "output": "In this work, we extend CXMI to point-wise CXMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high PA6MI as ones that require context for translation."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "en", "output": "Now we analyze words with high piecexMI to look for patterns between these words."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "en", "output": "we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "en", "output": "We perform our analysis at three different levels. First we look at part of speech tags that have high means pxMI."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "en", "output": "this allows us to find for example, dual pronouns in Arabic that have relatively high p6MI. and this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "en", "output": "similarly we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high pxMI averaged over all of its different occurrences."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "en", "output": "And this helps us identify cases like the one here where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "en", "output": "And similarly, we find that context is supported to chas it in the right formality."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "en", "output": "finally, we look at different individual tokens that have high p6MI. and this allows us to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in the sentence structure, such as ellipsis resolution."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "en", "output": "So now we use our findings from our analysis to design a benchmark for document novel translation."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "en", "output": "For each of the five discourse phenomena we identified, we created taggers to automatically identify words that pertain to the phenomenon. and we call our tagger the multilingual discourse aware or muda tagger."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "en", "output": "We can then also note that different languages have different proportions of these discreive phenomena."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "en", "output": "then use the M tager by applying the tagger on the parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context dependent examples that the M tagger has identified."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "en", "output": "And finally, we use our benchmark as well as other metrics to evaluate different models on the document level machine translation."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "en", "output": "First of all, when we use corpus level metrics, so for blue, we find that Conic's agnostic models have the best performance."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "en", "output": "then if we use commentt, context aware models perform best. And if we use wordf measure, then models with or without context have comparable performance."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "en", "output": "This again demonstrates that it is difficult to determine the best document level translation system if we use corpus level metrics alone."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "en", "output": "we use the MUDA benchmark to evaluate models and we find that context aware models are significantly more accurate than models that do not use context for certain discourse phenomena, such as formality and lexical cohesion."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "en", "output": "But these models are not much better than models that do not use context on other phenomena like ellipses, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document level translation."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "en", "output": "also compared different commercial systems and our benchmark shows that DeP is usually more accurate than Google Translate for document level translation."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "en", "output": "To summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "en", "output": "And then we use our refins to build a benchmark for document level machine translation, which can help us identify which disc discourse phenomenon models can handle well or not and which translation systems are good at document level translation."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "en", "output": "Thank you so much for your attention. See you in Trado."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "en", "output": "hi I am Yanislavak and I will present you our works on Dr. Bert a robust pretrain model in French for biomedical and clinical domains"}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "en", "output": "In this presentation, we first talk about language modeling in Herke. Then we will present the main contribution of our article."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "en", "output": "We introduced the first biomedical model in French named Dr. Bert, which is based on Roberta, and trained on Naos, which is a data set of medical crawled data from the web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "en", "output": "We also introduce a comparison of models with multiple protononic settings and data sources. Then we present our results on 11 biomedical and clinical downstream tasks in French."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "en", "output": "And finally we conclude about the experiments and give you more details about how to access to the models"}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "en", "output": "Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offer huge performance gain compared to historical, static and contextualized methods such as wordto ve, fast text or andword."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "en", "output": "Since then, this model has been adapted to many other languages, like in French, with Cammbert, and other domains like biomedical, with Permed Bert and Biobert, and on with clinical birth, but mostly in English."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "en", "output": "Specialized models for other languages are scarce and are often based on continuous pre-training due to the lack of in-domain data."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "en", "output": "However, French did not have any opensource model for biomelicon until now."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "en", "output": "So we ask ourselves a questions about what is the most appropriate data sources for a wide range of usage, and those crude data are good substitution for clinical data."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "en", "output": "To answer this question, we compared Dr. Bert with our Schubert model, which is based on anonymized data obtained from the non-geneerity hospital at our house."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "en", "output": "Afterward, we ask ourselves, how much data do we need to train a specialized model on French data? Is it four gigabyte, at gigabyte or more?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "en", "output": "To answer this question, we first train and compare four from scratch model: a first version ofD. Bert with seven gigabytes of nachos,\" a second version of four gigabytes of set of nachos."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "en", "output": "A first version of Schubert, which is a clinical model, with four gigabytes of sentences taken from clinical nodes, and a final version of Schubert, with a mix of four gigabytes of set of natures and four gigabytes of clinical nodes."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "en", "output": "In addition to this comparison, we introduced three models trained on contra pre-training to analyze the impact of pre-training strategies."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "en", "output": "One based on the weight of Cammbert and trained on four gigabytes of set of nachls; another also based on Cammbert, but trained this time on the four gigabyte of Kcliner knots."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "en", "output": "And finally, one base on English biomedical model, Bermed Bert, and train on four gigabytes of set of snatches. In total, we have seven models."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "en", "output": "To evaluate our seven models, we gather multiple public and private downstream tasks such as name and entity recognition, classification, part of speech tagging and question answering."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "en", "output": "These models are compared to six B design models, which are Cammbert OscarOS 18 gigabytes, Cammbert Oscar four gigabyte, Cammbert cinet four gigabyte,lomet Bert, Biobert and Clin BERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "en", "output": "The evolution of highlights, that model perform best on the task with data of the same nature as those on which the model has been trained."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "en", "output": "However, we can obtain that data we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translates into better performance."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "en", "output": "Inall, from scratch free training seem to obtain higher performance on most of the task."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "en", "output": "However, our experiment on control pretening using the weight and tokenizer of permit Bir trained on the four-Ggabyte subset of naturals, showed comparable results to those obtained with Dr. Bert four gigabyte from scratch."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "en", "output": "Which is not the case for the model based on Cammbert whites and tokenizer, which suffer from stability issues."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "en", "output": "Finally has a conclusion our proper system offer better performance on nine of the 11 downstream tasks and surpass globally the result of the generic model here camembert"}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "en", "output": "We also observe that specialized data is better, more specialized data is better, but it doesn't scale well."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "en", "output": "all the pre-train model obtained from nachos are freely available and on you face and all the training script are on our githubHub repository"}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "en", "output": "so thank you for this presentation and we are looking forward to actions at the poster session in Toronto"}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "en", "output": "Hi, my name is Matthias Lindemann, and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "en", "output": "is joint work with my advisors Alexander Kola and Ivan Tittov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "en", "output": "compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "en", "output": "In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, the girl slept, and Mary knew that the girl slept."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "en", "output": "These utterances are paired with logical forms that represent core aspects of their meaning."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "en", "output": "In contrast to standard machine learning evaluation, the test set does not come from the same distribution, but contains structurally unseen logical forms."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "en", "output": "In this example, the model has seen shallow recursion during training and is tested on example with deeper recursion."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "en", "output": "naive sequence-to-sequence models struggle with this kind of outof distribution generalization and often produce outputs that are detached from the input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "en", "output": "In particular, they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the examples."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "en", "output": "The popular method to address this is to integrate trees into the models."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "en", "output": "The trees are intended to capture the compositional process that relates utterances with the logical forms."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "en", "output": "This works well, but trees are usually not given and need to be obtained somehow."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "en", "output": "This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "en", "output": "Obtaining trees may also involve specialized grammar induction procedures."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "en", "output": "In this paper, we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "en", "output": "For the first time, we show strong generalization to deeper recursion without relying on trees."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "en", "output": "approach predicts the output from the input in two steps."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "en", "output": "First, we tag each input token with an unordered multiset of tokens that will appear in the output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "en", "output": "After the first step, we have all the right tokens but they're not ordered."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "en", "output": "That's why in the second step, we use another model to predict a permutation to put them into the right order."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "en", "output": "We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "en", "output": "Conceptually, our permutation model works roughly like this."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "en", "output": "We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one as highlighted in red."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "en", "output": "Then we jump to the next multi-set token to determine the second token in the output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "en", "output": "We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "en", "output": "Until every token from the first stage has been visited exactly once."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "en", "output": "To give you a teaser of the experimental results, here we compare our method with other treeless models on theCOGs benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "en", "output": "Some other kinds of structural generalization remain very challenging, though."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "en", "output": "In our paper, we solve a couple of interesting technical challenges."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "en", "output": "First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we dont know which multi-setter it came from, which poses a challenge for training."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "en", "output": "In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We addressed this by inducing the alignment as part of the training."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "en", "output": "Our permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is Np-harked. That's because this is related to the traveling salesman problem."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "en", "output": "We approximate this with a GPU-friendly, continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "en", "output": "you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster"}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "en", "output": "Hello everyone, I'm Akshata and today my co-author Martin and I are presenting our work, the Kit Master: Evaluating Knowledge Integration from Multiple Source. This work is a collaboration between McGill University, Mila and Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "en", "output": "Lang understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired by a pre-training and knowledge given in inputs at inference time."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "en", "output": "works in tasks like question answering show that models can use pre-trained time knowledge to solve the task"}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "en", "output": "but natural language understanding often requires knowledge that is also supplied at inference time"}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "en", "output": "For example, in the sentence, \"John saw the newly elected president on TV."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "en", "output": "Pre-train parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance specific entity John is or who the new president is because the president might have changed since pre-training."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "en", "output": "Therefore, successful models for knowledge-intenive NLU tasks require the ability to integrate and use both pre-trained time and inference time knowledge."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "en", "output": "this work we propose a diagnostic test suite for knowledge integration."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "en", "output": "introduce a core reference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the data set with human study participants and establish core reference resolution models"}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "en", "output": "Here is an example from our dataset. Servin is a judge. Kia is a baker. Termin and Kia met at a park. After a long day at work deciding cases in a law code, he was happy to relax."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "en", "output": "task here is to identify the correct entity that the pronoun he refers to which in this case is sermon"}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "en", "output": "The resolution of a given pronoun requires two types of information: first, entity specific knowledge such as servile is a judge, and second, bad knowledge such as judges decide cases in law courts."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "en", "output": "generallyly background knowledge is learned during the pre-training of large language models while entitypec specific knowledge is typically observed at inference time"}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "en", "output": "vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources"}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "en", "output": "have defined three settings of kitmos first with have the typical setting background pre-train where backward knowledge is assumed to be available at free train time"}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "en", "output": "second there's the background both setting where backward knowledge is available both at pre-train time and inference time lastly the back one in experience setting with both knowledge types available only at inference time"}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "en", "output": "last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-train data of models for example because new occupations have developed since the time of pre-train"}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "en", "output": "'s an example of how we control the availability of facts in the two sources"}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "en", "output": "the background pre-trained setting we assume that the background knowledge politicians seek elected seats in government is contained in the pre-trained parameters in the interferencetime context we provide the anti-specific knowledge chechester is a politician"}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "en", "output": "the background both setting, we additionally provide not only anti-specific but also background knowledge about politicians in the interference tab context."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "en", "output": "the background free setting we provide the fictional occupation merit tour instead of politician because merit tour is unlikely to be contained in the pre-t20peri region"}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "en", "output": "evaluate the data set both with human study participants and establish preference resolution models in this figure we show the results of the best performing models on the most difficult variant of the background pre-trained setting"}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "en", "output": "our task specific training on Kidmus, both models do not perform well. When trained on Kidmus, however, both C2F and built forQF perform significantly better than the random choice."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "en", "output": "This suggests that when trained on generic reference resolution datasets mod learn to exploit surface cues which are not useful when testing on Kidmus where such queues have been removed."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "en", "output": "additional experiments where fictional knowledge indicated even the best performing models cannot reliably integrate backward knowledge quite only at in interference time"}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "en", "output": "summarize the main takeaways of our paper many coreference evolution models appear unable to reason over knowledge from different sources without taskpecific training however with taskpecific training some models successfully integrate knowledge from multiple sources"}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "en", "output": "Still, even the best performing models seem to have difficulties with reliably integrated backward knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the data set in code on githubt. Thanks for listening"}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "en", "output": "Hi I'm Myra and today I'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with Essenndermush and Danjorovsky"}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "en", "output": "recent years many have documented the prevalence of social bias and stereotypes in large language models or LLms."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "en", "output": "these measures have various limitations they usually rely on hand-constructed data sets that are very time consuming to curate"}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "en", "output": "they also usually only measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts, or they simply capture very general broad associations like negative associations with particular groups."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "en", "output": "most work in this space doesn't account for intersectionality which is the notion that multifaceted social identities can compound biases and be unique loci of harms"}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "en", "output": "overcome these limitations we rely on the property that these newer instruction tuned Lms are very good at responding to instructions and prompts"}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "en", "output": "So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt to like imagine you are an Asian woman. Des describe yourself."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "en", "output": "we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "en", "output": "So here are some example generations from GPT4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "en", "output": "Immediately we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words,"}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "en", "output": "There are some interesting patterns."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "en", "output": "Asian woman is depicted as unassuming. The Middle Eastern woman is referred to using words like exotic and like referring to a mesmerizing region."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "en", "output": "And both of the women of color personas make references to ancestry, while the white man persona has nothing of the sort."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "en", "output": "capture these patterns, our method has two parts. The first one is generating these personas."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "en", "output": "Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects they also were able to surface racial stereotypes."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "en", "output": "also this enables direct comparison between our generated personas and the human written responses."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "en", "output": "second part is marked words, which is a method to identify the words that distinguish marked groups from our marked ones, which I'll elaborate on shortly."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "en", "output": "benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon.."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "en", "output": "So the marked words method draws upon the sociolinguistic concept of markedness, which states that there is an unmarked default and any group that differs from that default is linguistically marked."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "en", "output": "for instance, the word man or sorry, the word warrior is usually associated with men. So when people are describing a warrior who is a woman, they'll usually actually specify one man warrior and mark the term with woman."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "en", "output": "And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "en", "output": "So in our method, we first designate what the unmarked and marked groups are."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "en", "output": "then we compare the personas using the fighting words method, which is basically using weighted log odds ratios to distinguish the top words for each marked group."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "en", "output": "So for instance, for the personas of black women, we would do fighting words and compare the law godss ratios against both white personas and man personas because those are the two corresponding unmarked groups."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "en", "output": "Now for some results. So first we use the lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human written ones."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "en", "output": ", when we actually look at the distribution of the words in the lexicon, we find very different things."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "en", "output": "So while the generated personas have much higher rates of the Luxon words, the human written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words tall and athletic."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "en", "output": "So really just only the positive or at least non-negative ones."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "en", "output": "And in fact, the lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we'll turn to the results from our marked words method to show how these positive seeming words facilitate stereotypes and essentializing narratives."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "en", "output": "our analysis, we review how these seemingly positive portrayals reflect harmful patterns."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "en", "output": "for mark groups, the top words include things like culture, tradition, proud and exotic. And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "en", "output": "contributes to a long legacy of discrimination and othering for these groups."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "en", "output": "Furthermore, there's a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina woman include things like vibrant and curvaceous."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "en", "output": "Um, which connect to a trope of tropicalism. For Asian women, the words are things like petit and delicate and silky."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "en", "output": "connects to a long history of Asian women being hypersexualized, seen as very docile and submissive and so on."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "en", "output": "finally, for black women, we see that some of the top words are things like strong and resilient.."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "en", "output": "connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "en", "output": "There's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "en", "output": "rather than actually working towards changing those obstacles it puts pressure on those people to overcome them which leads to very negative health outcomes for these people among other harms"}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "en", "output": "broadly we find that the words for each marked group pretty much just reflect very essentializing narratives"}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "en", "output": "So based on these patterns, we conclude with three recommendations for model owners."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "en", "output": "first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that"}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "en", "output": "And finally, there should really be increased transparency about bias mitigation methods."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "en", "output": "for instance, like these positive stereotypes, we don't know if it's because there is some sort of like weird."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "en", "output": "overly excessive value alignment going on, or maybe some other like anti- stereoeotyping methods that are resulting in these pernicious patterns."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "en", "output": "just really can't make any assumptions or really study that further without more transparency"}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "en", "output": "you so much for listening um have a good time at Ace"}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "en", "output": "Hello everyone, my name is Jing Wei Y from the University of Science and Technology of China."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "en", "output": "It's my pleasure to give a short advertisement video of our paper. Are you copying my model, protecting the copyright of large language models for embedding and services? Vill back the watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "en", "output": "Let's first introduce the background about embedding services."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "en", "output": "Currently, large language models such as GPT, Lama,  PM are exceptional in natural language understanding and generation."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "en", "output": "Embedding as services is one of the services built upon large language models to assist various NLP tasks"}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "en", "output": "For example, OpenI offers aGbt-based embedding API."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "en", "output": "However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services, therefore, it is necessary to protect the copyright of embedding as services."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "en", "output": "To protect the copyright of embedding services, one of the solutions is to embed a watermark in the provider service and detect whether another service contained the watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "en", "output": "watermark method need to meet the following properties: first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embedding"}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "en", "output": "Third, the watermark should be convertt enough to the attacker, or the attacker can remove the watermark easily."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "en", "output": "Finally, the watermark need to be transferable to the attacker' services during the model extraction process."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "en", "output": "Existing works can be broadly classified into four categories."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "en", "output": "However, this method either not applicable to embedding as services or lack of transferability."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "en", "output": "Therefore, in this paper we propose embedding marker which is a backdoor-based watermark method applicable to embedding services"}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "en", "output": "Then, let me introduce the details of our embedding marker. embedding marker contains two main steps: watermark injection and copyright verification."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "en", "output": "Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "en", "output": "We assume the provider can collect a general text coverspus and count the word frequency with it."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "en", "output": "watermark injection, we first define a targeting bedding When a user send a sentence to the provider service, the provider counts the trigger number in the sentence."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "en", "output": "The provided embedding is a weight summation of the target embedding under the original embedding."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "en", "output": "The weight of the target embedding is proportional to the number of triggers in the sentence. when the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "en", "output": "copyright verification is to detect whether or model behind another service contains the watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "en", "output": "We first construct a backdoor and a benign data set. Backdoor dataset contains sentences of which all words belong to the trigger set, while all words in the sentences of benign dataset do not belong to the trigger set."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "en", "output": "the provider requests embeddings from the stiller service with the dataset"}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "en", "output": "The cosine and l2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between beniggh and the backdoor data set, which is defined as delta cosine and the delta l2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "en", "output": "Meanwhile, we also apply KS test and use its p-value as the third matrix."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "en", "output": "We conduct experiments on four dataset AG news, mind, SSD two and A spam. We assume the provider of liewikitext dataset to count word frequency."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "en", "output": "The results on four dataset show that our embedding marker can have great detection performance while keep great utility for downstream tasks."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "en", "output": "We also validate the coverness of the provided embedding by visualizing the embedding of sentences unfolded at BPCca. The legend of the figures means the number of triggers in each sentence."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "en", "output": "As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "en", "output": "That's all thank you. We'll come to discuss with us."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "en", "output": "Hello, my name is Vaudha and I am a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper Transfer learning for dissonance detection addressing the rare class challenge."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "en", "output": "We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply but cognitive dissonance is two beliefs or actions that are inconsistent."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "en", "output": "as this example where a person states I know that cigarettes could kill me and then goes on to say I grabbed a couple of smokes after the meeting. This belief and action are inconsistent and they are in dissonance."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "en", "output": "mentioning that I don't think I could keep my job without them justifies the second occurrence and they have a consonance relationship."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "en", "output": "dissonance is a very common phenomenon we experienced in daily decision making they are really rare to find expressed in language among other kinds of discourse relations."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "en", "output": "So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values and attitude changes in population."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "en", "output": "High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "en", "output": "Stu dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "en", "output": "Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "en", "output": "the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance first approach as seen in the flowchart here."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "en", "output": "were passed using a PDTV parser and pairs of discourse units were annotated according to the guidelines that are described in our paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "en", "output": "can be seen here dissonance was only found in 3.5 percent of the annotated pairs"}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "en", "output": "collecting around 1000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of distance. To no surprise, the classifier performed not much better than chance."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "en", "output": "the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "en", "output": "alleviate this we experiment over combinations of transfer learning and active learning to annotate such that more dissonance samples can be collected over lesser annotation rounds lowering the overall annotation costs while improving dissonance detection"}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "en", "output": "the initial modeler was not able to capture the dissonance class at all we start the active learning process by transferring weights from closely related tasks"}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "en", "output": "transfer from two different tasks: topic independent dissonance sta classification, a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "en", "output": "called debate here and on binary classification of expansion and comparison classes of PB, since these two are closely related to the conception of consonants and dissonance, and we call themCE here."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "en", "output": "find that on transferring the zero short performance on the annotated data set is already much better than chance with the best with Auc 0.62"}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "en", "output": "Fur on iteratively finetuing on both tasks, we find that fine tuning of CE tasks followed by further fine-tuning on debate yields a much better zero shot performance. Thus this is the model that we use to coal-start the active learningning."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "en", "output": "Next we determine the best method to update a model with new data from each round of active learning and annotations. cumulative accumulates all the data collected from active annotations so far, whereas iterative updates the model by training on the latest set of data collected"}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "en", "output": "Over the different strategies we found that cumulative performed equal or better than iterative across the boards"}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "en", "output": "Next to improve the number of dissonance examples we use a probability of rare class strategy PRC to select mostly the examples that are highly likely to be dissonant by the current model at any round of AL"}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "en", "output": "compare this to the other state of the more state of the art A strategies that are commonly used in the community."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "en", "output": "find that the proposedprc strategy works better than other straight state-of-the-art strategies although the difference is small note that the performance is significantly lower for randoms"}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "en", "output": "further rounds of AL with two best strategies, we improve distance classification, AUC to 0.75, which is the best performance that we have on the task so far."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "en", "output": "also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "en", "output": "In summary, we find that PRC is a simple A strategy for rare class acquisition and cold-starting ale with appropriately designed transfer learning tasks can help significantly."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "en", "output": "also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update"}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "en", "output": "These are the links to our code data set and our paper. Fe free to get in touch with us if you have any questions. Thank you."}
