{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "en", "output": "Hello, my name is the Safari, and I will present our paperFsho tabular Data En enrichment using finetued Transformers Arch architectures. Thus scientists analyze data and mainly focus on the manipulating the data existing features, but sometimes these features are limited. Feature generation using another data source may add substantial information. Our research goal is automatic tabular data enrichment using external sources free text. Assume we have a tabular data set and a knowledge base. We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base free text our framework fest is exactly this automatic process so let's see an example in data sets fed into fest in this example the data set is university data set when it when its goal is to classify universities into low ranked universities and highran universities as knowledge base we use Wikipedia the first phase of first is entity linking when each entity in this example the university name is linked to an entity within the knowledge base and ex the text of the entities of the knowledge base is extracted and added to the data set in this example the text is the Wikipedia page abstract. Now we need to generate or extract features from the retrieved text so we need to we need an feature extraction phase which include text analysis and this is the main novelity of this paper and I will deep dive into it in the next slides after the feature extraction phase there is a feature generation phase when we use the extracted features to generate a small number of new features first generate a features in the number of classes of the original data set in this example, the original data set has two classes so first generate two new features. But if the dataset has five classes, first generate five new features. Each feature represents the likelihood for each class. To analyze the text, we use the current state of the outof-text analysis, which are transformer-based language models as bear GPT X and lets and etc it is but it is not likely that we can train language model using the input data sets so a naive approach will be a target task fine-tuning so in the feature extraction phase we can download pertrain language model fine-une the language model over the target data set in this example to fine-tune the language model to classify to classify text into classes, abstract into classes low or high, receive the language model output, which is the likelihood for each class, and use as new features the problem with this approach is dataset may have few distinct entities text in our experiment almost half of the data sets contain less than 400 sample and the smallest data set contain 35 sample in his init training set so to fine-tune a language model over this data set will be ineffective but we can use prior knowledge about pre-analyzed data sets because fast is we apply fast over a multiple data set we can use the n minus 1 data sets to gather information about the n minus 1 data sets and use use this information when we analyze the ends data set what we what we suggest is to add to add another fine-tuning phase a preliminary multitask fine-tuning phase when you fineune the language model over n minus one data sets and and and then we execute another fine- tuning phase which is a target task fine tuning when you find when we fineune the language model over the nth target data set the state of the art in multitask multitask fine tuning called dnn in dnn tdnn maintain a heads in the number of tasks in the training set so if in this example there are four tasks in the training set so dnn maintain four heads as you can see at the image and it samples a random batch from a from the training set and if the random batch belongs to a for example single and sentence classification tasks it's execute forward and backward pass through the first head and if the random batch belongs to pairwise ranking a task its attitude for and backward pass through the last head in our scenario a tableau dataset vary the number of classes so there are many tasks a empty DNN maintain number of classes heads output layers and additional additionally empty DN needs to initially as a new heads for a new data set with a new task our approach called task reformulation fine-tuning is we in our approach task reform fine tuning instead of maintaining multiple heads we reformulate each data set into a sentence per classification problem which is two classes tasks so let's see an example here is the our input data set which consists of entities features text and classes and we reformulate a the task from a classifying the text into low and high to classify the the text the abstract and the class into true or false or in other words we train the language model to classify a abstract and class a a to to try to abstracting class if the abstract belong to the class or not so the label vector in z's case stays always which consists always with two classes and this is the algorithm for our fine or formulated fine tuning approach so let's see the full framework uh data set fed into fast and then a fast execute into the linking phase it extract the text from the knowledge base which in this example is the abstract of the Wikipedia page then it reformulated the task into a pair sentence per classification tasks apply the language model to the new task and the output likelihood for each class. note that the language model is already fine- tuned over n minus one dataset using a preliminary multitask fine tuning. Then we use the output vector of the language model as a newly generated feature in the number of classes to evaluate um our framework we use a seventeen tabular classification data set which defines size features, balance domain and initial performance and as knowledge waste we use Wikipedia we design our experiment as leave one out a evaluation when we train fast over 16 data sets and apply it to the 17th data set we also each we also split each data set into a for faults and apply a fork false a cross-validation then we generate the new feature and evaluate them using five evaluation classifiers we use in our experiment based bird based architecture here are the results for our experiment you can see that we compare our is our framework to target a data set fine-tuning target task fine-tuning and a mtd n preliminary fine tuning and um our reformulated uh fine tuning achieve a the best uh result the best performance while m t d n n e um achieve two percent uh improvement over the um the target data set fine tuning our approach achieves six percent improvement when we look uh on the small uh data set we can see that the performance and of mtdnn decreases and the improvement of the pre preliminary multitask fine tuning phase decreases to one point five um percent but our performance increased to 11 percent compared to the target task fine- tuning along. for summing fast enables fuel shot enrichment from 35 samples in our experiment. It uses one architecture for all tasks data sets, and it keeps the head of of the model. but it adds reformulation phase its augment the train set and it needs a target value with semantic meaning, so we can fed it into the language model and use it in the sentence per classification problem. Thank you"}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "en", "output": "- Hi everyone. Today I'm going to present our research work learningarning to reasonason deductly, Metro problem solving as complex region extraction. I'm All from Biance AI Lab and this is a joint work with Che from the University of Texas at Austin and Wedu from SUDD. First I'd like to talk about our motivation for reasoning. So here we're showing examples where multi-step reasoning is helpful. So this figure is taken from the pound paper where they perform prompting to solve the method problem in a future learning scenario. So on the left hand side, we can see if we give some samples with just question and answers, we may not be able to obtain the correct answers. But if we give some more reasoning description the model is able to predict the reason description and also make a correct prediction here so it is good to have interpretable multi-step reasoning as output and we also think method problem is a straightforward application to evaluate such reasoning abilities so here in our problem setup given the questions we need to solve this question and obtain the numerical answers so in our data sets we are also given the mathematical expression which leads to the to this particular answer as well so certain assumptions also apply as in previous work we assume the precision of quantities are known and we only consider basic operators such as addition subtractions multiplication division and exponential furthermore complicated operators can be actually decomposed into these basic operators so previous work in method problem solving actually can categorize into sequence to sequence and sequence to tree model so traditional sequence to sequence model convert the expression to a specific sequence for generation and it is pretty easy to implement and it can generalize to many different complicated problem but the drawback of the performance is actually gen generally not better than the structure model and it is lack of the interpretability for prediction but actually this direction is still quite popular because of the transformer model so in treebased models we actually structure these expressions in the tree form and follow a pre-order traversal in tree generations so here we keep generating the operators until we reach the leaves which are the quantities so here the good thing is that it actually gives us this binary tree structure and it is um but but but actually it is quite counterintivity because we generate the operator first and then at the end we generate the quantities and the second thing is that it also contains some repetitive computations so here if we look at this expression a times 3 plus 3 is actually generated twice but in fact we should reuse the results so in our proposed approach we want to solve those problems in a step-by step and interpretable manners so for example here in the second step we can obtain this divisors which is 27 and we can also refer back to the original questions to find the relevant contents and in these steps we obtain the divisors so and then at this third step we actually get the quotient all right and after these three steps we can actually reuse the results from the second step and then gets the results of the fourth step and then finally we can obtain the dividends so here we actually generate the whole expression directly rather than generating a single operators or quantities so this makes the process more accurate. So in our deductive system we first starts with a bunch of quantities presented in the questions and also including some constants as our initial initial state. So the expression is represented by eij where we perform operator fromqi to qj and such expression is actually directed so we also have subtraction reverse here to represent the opposite direction this is quite similar to relation extraction so in a formal deductive system at the time stept we apply the operator between theqi and qj pair and then we obtain this new expressions we add it to the to the next states to become a new quantity so this slides actually visualize the evolution of the states where we keep adding expression to the current states so in our model implementations we first use a pre-traination model can which can be birds or rawhoods and then we encode the sentence and then we obtain these quantity representations so once we get the quantity representations we can start to do inference here we show an example of q1 to to obtain the representation for q1 divided by q2 and then times q3 first we get the pair representation which is basically just the concatenation between q1 and q2 and then we apply a feed forward network which is parameterized by the operator and then finally we obtain the expression representation q1 divided by q2 but in fret in practice in the inference stage we might be able to get the incorrect incorrect expression as well so here all the possible expression is equals to three times the number of operators so the nice thing here is that we can easily add constraints to control this search this search space for example if this expression is not allowed we can simply remove this expression in our search space so in the second step we do the same thing but the only difference is that we the the only difference is one more quantities so this quantity come from the previous calculated expression so finally we can obtain this final expression q3 times q4 and we can also see the number of all the possible expression is different from the previous step so such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced so the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step and here we also use this tau to represent when we should terminate the this generation process and here the space is different from sequence to sequence because the space is different at each times that while in traditional sequence to sequence model it is the number of vocabulary and it also allows to impose certain constraint from prior from prior knowledge so we conduct experiments on the commonly used method problem data setsmwps method3k math qaA and swam and here we briefly shows the results compared with the previous best approaches so our best performing weapon is Roberta deductive reason and in fact we do not use beam search in contrast obvious approaches using beam search all right so the best approaches are often a treebased model so overall our reasoner is able to select significantly output from this tree-based model but we can see the absolute number on math qaA or swam are not really high so we further investigate the results on swam and this data set is challenging because the author tried to manually adding something to confuse the nlp model like such as adding available information and extra quantities so in our prediction we find some of the intermediate values are actually negatives for example in these questions we are asking how many apples does Jake have but we have some extra information like 17 fewer pitchachees and Stephen have eight pitchachees which is totallylevant so our model makes some prediction like this which is producing negative values and we observe these two represent these two expression actually have similar scores so we can actually limit this search space by removing like those results are negatives so that we can make the make the answer correct so um we further find such constraint actually improves uh quite a lot for for for some model for example for birds we improved seven point and then for the roberta based model we actually improved two points so better language model has a better language understanding abilities so that the number here is higher for roberta and lower for for birds and we also try to analyze the difficulty behind this behind all this data set we assume the number of unused quantity can be regarded as relevant information here so uh here we can see that uh we we have the lump the percentage of samples we unus quantities and the swamp data set has the largest portion and here we also show the overall performance for those samples without unused quantities so the overall performance is actually higher than the the performance is actually higher than the overall performance but with those samples that with unused quantity is actually way worse than the com uh way worse than the uh the overall performance formWps we don't we we don't really have uh too many death cases so I just uh ignore this part so uh finally we want to show the interpretability through a crash and presentation example so here our model actually makes a wrong prediction at the at the first step so we can actually correlate the this expression with the sentence here all right so we think this sentence might be misleading the model to uh an incorrect predictions so here printing another 35 makes the model makes the model think it should be an addition operators so we try to revise the sentence to be something like the number of pear trees are5 fewer than the apple trees so we make it to convey more accurate semantics such that the model is able to make the prediction correct so this study shows how the interpretable predictions help us understand the model behavior so to conclude our work so first our model is actually pretty efficient and we are able to provide interpretable solvings procedure and we can easily incorporate some prior knowledge as constraint which can help improve the performance. And the last thing is that the underlying mechanism does not only apply to network problem solving tasks, but also other tasks that involve multi-step reasoning. But we also have certain limitations. If we have a large number of operators or constants or constants, the memory consumption could be pretty high. And the second thing is that as mentioned because the probability distribution is unbalanced between at it at different time steps so it's also pretty challenging to apply beam searches beam search strategy so this is the end of the talk and questions are welcome thank you you"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "en", "output": "Hi, my name is Antoine, and I'm from Maastricht University. I will be presenting my John work with Jerry, which is about a new data set for statutory article retrieval. Legal issues are an integral part of many people's life, but the majority of citizens have little to no knowledge about their rights and fundamental legal processes as a result many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or worst exploited all work aims to bridge the gap between people and the law by developing effective retrieval system for statutory articles such a system could provide a free professional legal help service for unskilled humans before diving into the main contribution of this work let's first describe the problem of statutory article retrieval given a simple question on a legal matter such as what do i risk if i violate professional confidentiality a model is required to retrieve all relevant statutory articles from a large body of legislation this information retrieval task comes with its own set of challenges first it deals with two types of language common natural language for the questions and complex illegal language for the statutes this difference in language distributions makes it harder for a system to retrieve relevant candidates as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes besides statutory law is not a stack of independent article that can be treated as a complete source of information on their own like news or recipes, for example. Instead, it's a structure collection of legal provision that have a whole meaning only when considered in the overall context, that is together with the supplementary information from their neighboring articles, the fields and subfields they belong to, and their place in the structure of the law. Lastly, statutory articles are in small paragraph which usually is the typical retrieval unit in most retrieval works here they are long documents that may be up to 6 000 words the recent advances in nlp have sparked huge interest in many legal tasks such as legal judgment prediction or automatic contact contract review but statutory article retrieval has remained mainly in touch due to the lack of large and high quality labeled data sets in this work we present a new French native citizen-centric dataset to study whether retrieval model can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval or Belgian statutory article retrieval data set consist of more than 1100 legal questions posed by Belgian citizens these questions cover a wide range of topics from family housing money to work and social security each of them has been labeled by experienced jurist with references to relevant articles from a corpus of more than twenty thousand six600 legal articles from Belgian codes of law let's now talk about how we collected this data set first we started by compiling a large corpus of legal articles we considered 32 publicly available Belgian codes and extracted all their articles as well as the corresponding section headings then we gathered legal questions with references to relevant statutes to do so we partner with the Belgian law firm that receives each year around 400s email from Belgian citizens who ask for advice on a personal legal issue we were lucky enough to get access to their websites where their team of experienced jurists addresses Belgian most common legal issues we collected thousands of questions annotated with categories subcategories and legal references to relevant statutes lastly we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered the remaining references were matched and converted to the corresponding article IDs from our corpus. We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the IDs of the relevant articles from a large corpus of twenty two thousand six hundred and thirty three statutory articles, in addition, each question comes with a main category and a concatenation of subcategories, and each articles comes with a concatenation of their subsequent heading in the structure of the law. This extra information is not used in the present work but might be of interest for future research on legal information retrieval or legal text classification let's look at some characteristic of our data sets the question are between 5 and 44 words long with a median of 40 words the article are much longer with a median lengths of 77 words with 142 of them exceeding 1000 words the lengthiest one being up to five thousand seven hundred90 words as previously mentioned the question covered a wide range of topics with around 85 percent of them being either about family housing money or justice while the remaining 15 percent concern either social security foreigners or work the article are also very diverse as they come from 32 different Belgian codes that cover a large number of illegal topics here's the total number of articles collected from each of these Belgian codes out of the twenty 26633 articles only 1612 are referred to as relevant to at least one question in the data sets and around 80 percent of these cited articles come from either the civil code judicial codes criminal investigation code or penal codes meanwhile 18 out of 32 codes have less than five article mentioned as relevant to at least one question which can be explained by the fact that this code focus less on individuals and their concerns overall the median number of citation for these cited articles is two and less than 25 percent of them are cited more than five times using our data sets we benchmark several retrieval approaches including lexical and dense architecture given a query in an article a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article we experiment with the standard TFIDf and  bm25 ranking functions The main problem with these approaches is that they can only retrieve article that contain keywords present in the query to overcome this limitation we experiment with a neural-based architecture that can capture semantic relationship between queries and article we use a b encoder model that maps queries and articles into dense vector representations and calculate a  relevantlevance score between a query article pair by the similarity of their embeddings these embeddings typically result from a pooling operation on the output of a word embedding model first we study the effectiveness of Siamesebiancoders in a zero shott evaluation setup meaning that pre-trained word embedding models are applied out of the box without any additional fine-tuning we experiment with context independent text encoder namely word to vec and fast text and context dependent embedding models namely Roberta and more specifically camembert which is a French roberta model additionally we train our own camembert based modelbiancors on all data sets note that for training we experiment with the two flavors of thebiancoder architecture Siamese which uses a unique word embedding model model that maps the query and article together in a shared dense vector space and to tower which uses two independent word embedding models that encode the queryion article separately into different embedding spaces. We experiment with mean max and CLls pooling as well as dot product and cosine for computing similarities. Here are the result of a baseline on the test sets with the lexical methods above, the Siamesebian encoders evaluated in a zero shot setup in the middle and the fine-tunedbian encoders below overall the fine-tuned biancoders significantly outperform all the other baselines the two tower model improves over its siaamese variant on recall at 100 but perform similarly on the other metrics although bm25 underperformed the trainedbiancoder significantly its performance indicate that it's still a strong baseline for domains specific retrieval regarding the zerosho evaluation of Siamesebiancoder we find that directly using the embeddings of a pre-trained camembert model without optimizing for the information retrieval task gives poor results which is consistent with previous findings furthermore we observe that the wordto-ve bird-basedbian encoder significantly outperformed the fast text and bird-based model, suggesting that maybe pre-trained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box. Although promising, these results suggest ample opportunity for improvement compared to a skill level expert who can eventually retrieve all relevant article to any question and thus get perfect scores. Let's conclude by discussing two limitations of all data sets. first, the corpus of article is limited to those collected from the 32 considered Belgian codes, which does not cover the entire Belgian law as articles from decrees directives and ordinances are missing during the data set construction all references to these uncollected articles are ignored which causes some question to end up with only a fraction of the initial number of relevant articles this information loss implies that the answer contained in the remaining relevant articles might be incomplete, although it still completely appropriate. Second, we should note that not all legal questions can be answered with statutes alone, for instance, the question, \"Can I evict my tenants if they make too much noise?\" might not have a detailed answer within statutory law that quantifies a specific noised threshold at which eviction is low instead the landlord should probably rely more on case law and find precedents similar to their current situation for example the tenant makes two parties a week until 2 a.m hence some questions are better suited than others to the statutory article retrieval task and the domain of the less suitable ones remains to be determined. We hope that all work sparks interest in developing practical and reliable statutory article retrieval models that can help improve access to justice for all. You can check out our paper dotset encode at the following links. Thank you"}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "en", "output": "Hello we are happy to present our work on vowels a taskindepend benchmark meant for testing vision and language models with specific linguistic phenomena Why did we do the trouble in setting up this benchmark? Well during the last years we have seen an explosion of transformer based vision and language models pre-trained on large amounts of image text pairyers each one of these models pushes state of the art on vision and language tasks such as visual question answering visual common sense reasoning image retrieval phrase grounding so we got a message: the accuracies on these task-s specific benchmarks are increasing steadily, but do we know what the models have actually learned? What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match and a low score for this one. Do vision and language models focus on the right thing, or do they focus on biases as shown by previous work? To shed more light on this aspect, we propose a more task agnostic direction and introduce vowels that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities we target existence plurality counting spatial relations actions and entity coference but how do we test whether the vision and models have captured this phenomena by foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators and on counting by us in previous work foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore and we do these phrase alterations by focusing on six specific pieces such as existence plurality counting spatial relations actions and entity coreference where each piece can consist of one or more instruments, in case we found more than one interesting way to createFOIL instances. For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action and one in which actants are swapped. Counting and correference also are pieces that have more than one instrument. And we create these foils by making sure that they fail to describe the image, that they are grammatical and otherwise valid sentences. This is not easy to do because a foiled caption may be less likely than the original caption. For example though it's not impossible it is statistically less likely for plants to cut a man than a man to cut plants and large vision and language models could pick up on this therefore to obtain valid foils we must take action first we make use of strong language models to propose foils second we use natural language inference or short Nli to filter out foils that could be still describing the image since when constructing foils we need to ensure that they fail to describe the image to test this automatically we apply natural language inference with the following rationale we consider an image to be the premise and its caption its entailed hypothesis in addition we consider the caption to be the premise and the foil is its hypothesis if an Nli model predicts the foil to contradict or to be neutral with respect to the caption we take this as an indicator of a valid foil if an Nli predicts the foil to be entailed by the caption it cannot be a good foil since by transitivity it will give a truthful description of the image and we filter these foils out but this procedure is not perfect it is just an indicator for valid foils therefore as a third measure for generating valid foils we employ human annotators to validate the data used in valse so after filtering and human evaluation we have as many test instances as described in this table note that valse does not deliver any training data but only test data since it is a zero shott testing benchmark only. It is designed to leverage the existing capabilities of vision and language models after pre-training. Fine-tuning would only enable models to exploit artifacts or statistical biases in the data. And we all know that these models like to cheat and take shortcuts. And as we said, we are interested in assessing what capabilities the vision and language models have after pre-training we experiment with five vision and language models on vowels namely with clip alex Mert wilbert wilbert 12 in 1 and visual bird two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils perhaps more relevant for this video we will showcase our more permissive metric the pairwise accuracy which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair for more metrics and results on them do check out our paper the results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics iss that the best zero shot performance is achieved by wilbert 12 in one followed by wilbert Alexex Mert clip and finally visual bird it's notable how instruments centered on individual objects like existence and noun phrases are almost solved by wilbert 12 in one highlighting that models are capable of identifying named objects and their presence in images however none of the remaining pieces can be reliably solved in our adversarial foiling settings. We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects or counting them in an image the relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image they also have trouble distinguishing actions and identifying their participants even if supported by plausibility biases as we see in the actions piece from the reference piece we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models as a sanity check and because it's an interesting experiment we also benchmark two text only models Gpt1 and Gpt2 to assess whether falses is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption no image here and predicting the entry with the lowest perplexity if the perplexity is higher for the foil we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases and it's interesting to see that in some cases the textonly GPT models have captured the plausibility of the world better than the vision and language models so to sum up valse is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities. Our experiments show that vision and language models identify named objects in their presence in images well as shown by the existence piece but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators. We would really like to encourage the community to use vowels for measuring progress towards language grounding with vision and language models. And even more, vowels could be used as an indirect assessment of data sets, as models could be evaluated before and after training or fine-tuning to see whether a dataset helps models improve on any of the aspects tested by VAs. If you're interested, do check out the ValAs data on GitHub and if you have any questions, do not hesitate to contact us"}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "en", "output": "Hello, my name is Kamisura from the University of Tokyo. I'll be presenting a paper entitled \"O En Sum: A largerge Scale Desset for Automatic List Notduration viamit Lo Sumization.\" I will explain in this order first, I will introduce the automatic Li notation that we are working on in this research. ReaseNote is a technical document that summarizes the changes distributed with each release of a software product. The image shows the release notes for version two point six point four of the Bujs library. These notes play an important role in open source development but they are time consuming to prepare manually therefore it will be very useful to be able to automatically generate high quality release notes I will refer to two previous researches on automatic list node generation the first is a system called alena, released in 2014. It takes a rule-based approach, for example, using the change extractor to extract core differences, library changes and document changes from the differences between diseases, and finally combining them. The most notable feature of this system is the issue extractive in the upper right corner, which must be linked to Jira, the issue ecosystem and can only be applied to projects that use Jira, in words, it cannot be used for many projects on GitHub. The second is Grif, recently announced in twenty twenty It is available on the internet and can be stored via pi this system has a simple running based text classification model and outputs one of five problems such as features or bug fixes for each input commit message the image is a sample usage that returns a correct tape or bug fixes rub que training data is fairly small about 5000 and will be shown in the experiments described below the performance of the text classification model is not high i present two related researches but there are problems of limited applicability and scarce data resources. Our paper solves these two problems and automatically generates high quality resource. For the limited applicability problem, we propose a high quality classifier sumization method using only commit message as input. This proposed method can be used for all English repositories. for the second problem of scar state resources we built ourr and some data consisting of about 82 000 pieces of data by correcting data from public Gitub repositories using the Git API next I describe our data here is an example update the left side is a commit message and the right side is the release notes the release notes are leveled as improvements of faces etc we have set up a task that takes the commit messages as input and outputs the rabbit is notes this can be regarded as a summarization task we have predefined four levels features implements bug fixes deprecations removers and breaking changes these were set based on previous usage and other factors there is note on the bottom right and extracted from the list node shown on the bottom left. At this time, it is necessary to detect the four rabbits that have been set up in pass, but the levelss are not always consistent with each le. For example, the improvement level includes improvements, enhancements, optimizations and so on. We prepared a vocabulary list of study levels for each of these notational variations use it to detect the risk node class and correct the text of the rest that follows as the risk no sentence for the class next is a commit message commit messages are not tied to each race as shown in the image below, if the current release is version 2.5 to nineteen, we need to identify the previous release version 2.5 to18 and get it di. This is a bit tedious and it is not enough to just get a list of releases and look at the before and after. We created a heuristic matching glue to get the previous and next versions data analysis in the end 7200 repositories and 82 000 pieces of data were corrected also the average number of reasonable tokens is 63 which is quite high for summarization task also the number of unique tokens is quite rich at eight thousand eight thirty 000 this is due to the large number of unique classs and method names found in the repository next I will explain the proposed method the crosswise extractive and abstractive summarization model consists of two neural modules a classifier using bot or code bot and the generator using but first G uses a classifier to classify each commit message into five base node classes features improvements bug fixes deprecations press and other the commit messages classified as other are discarded then she applies a generator to the four rubber documents independently and generates read note for each class in this task the direct correspondences between commit messages and read notes are not known. Therefore, to train the classifier classifier we assign pseudo variables to each input commit message using the first 10 characters of each commit message. We model the classwise abstractive summization approach by two defined methods. The first model, which we call GS single consists of a single  sex network and generates a single long is not text give a concatenation of input commit messages the output text can be divided into class file segment based on special class specific endpoint symbols the second method method which we call shes much consists of four different sec to sec networks, each of which corresponds to one of the least node classes. Okay, let me explain the experiment. Five methods were compared gs shes single shes much cluster, and previous study Gri regarding aberration in some cases, these notes are output in multiple sentences, since it is difficult to correct the number of sentences at zero, they are combined with spaces and treated as one long sentence. The blue is penal when the system outputs a short sentence this penalty results in a lower bre value in the experiments results described next finally we also caricagate a specificity because blue and blue cannot be caricated if the list notes are empty a high specificity means that the model correctly outputs are empty text in cases where the read nodes assume empty here are the results since the data set contains email analysis has values etc we also evaluated the clean data set which excludes them G and Gs achieved lose error scores more than 10 points higher than the baseline in particular on the Korean test set the score gap between the proposed method and the base jumped to more than twenty points. These results indicate that Gs and Gs are significantly effective Gs got to a better loose score than GAS suggesting that combining a classifier under generator is effective in training the classifier using servers. The high coverage of gs can be achieved properly because the classifier can focus on selecting relevant commit messages for each class. shes much tended to eat higher than she is single suggesting that it is also effective to independently develop different to constructive summarization models for each piece node class here an error analysis she methods tend to output shorter sentences than human reference sentence, since in the figure on the right, the reference sentence has three or four sentences, whileCSS has only one. The reason for this model reluctance is that in training data, only thirty percent of the sentences are present in the features level and forty percent in the improvements level furthermore cs methods cannot generate accurate list note without additional information the top example on the right is an example of a very messy commit message and the complete sentence cannot be generated without difference to the corresponding prerogates or issue. The example below shows that the two committed messages in the input are related and should be combined into one sentence but it fails to do so. Finally a conclusion We have built a new data set for automatic personal generation. We have also formulated the task of entering committed messages and summarize them so that it is applicable to all projects written in English. Our experiments show that the proposed method generate less noisy is not at higher coverage than the baseline. Please check out our data on GitHub. Thank you"}
