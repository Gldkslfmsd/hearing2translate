{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Safari, und ich werde unseren Beitrag zur „Datenanreicherung tabellarischer Daten mit feinabgestimmten Transformer-Architekturen“ vorstellen. Wissenschaftler analysieren Daten und konzentrieren sich dabei hauptsächlich auf die Manipulation vorhandener Merkmale, aber manchmal sind diese Merkmale begrenzt. Die Merkmalsgenerierung unter Verwendung einer anderen Datenquelle kann erhebliche Informationen hinzufügen. Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mit Hilfe externer Textquellen. Nehmen wir an, wir haben einen tabellarischen Datensatz und eine Wissensbasis. Wir benötigen einen automatischen Prozess, der Entitätsverknüpfung und Textanalyse umfasst, um neue Merkmale aus dem freien Text der Wissensbasis zu extrahieren. Unser Framework „Fest“ ist genau dieser automatische Prozess. Lassen Sie uns ein Beispiel betrachten: Die in Fest eingespeisten Datensätze; in diesem Beispiel handelt es sich um einen Universitätsdatensatz, dessen Ziel es ist, Universitäten in niedrig und hoch eingestufte Universitäten zu klassifizieren. Als Wissensbasis verwenden wir Wikipedia. Die erste Phase ist die Entitätsverknüpfung, bei der jede Entität – in diesem Fall der Universitätsname – mit einer Entität in der Wissensbasis verknüpft wird. Anschließend wird der Text der Wissensbasis-Entitäten extrahiert und dem Datensatz hinzugefügt. In diesem Beispiel ist der Text der Abstract der Wikipedia-Seite.\n\nNun müssen wir Merkmale aus dem abgerufenen Text generieren oder extrahieren. Wir benötigen daher eine Merkmalsextraktionsphase, die Textanalyse umfasst. Dies ist die Hauptneuheit dieses Beitrags, auf die ich in den nächsten Folien eingehen werde. Nach der Merkmalsextraktionsphase folgt eine Merkmalsgenerierungsphase, in der wir die extrahierten Merkmale verwenden, um eine kleine Anzahl neuer Merkmale zu generieren. Zuerst generieren wir so viele neue Merkmale wie es Klassen im Originaldatensatz gibt. In diesem Beispiel hat der Originaldatensatz zwei Klassen, also generieren wir zuerst zwei neue Merkmale. Wenn der Datensatz jedoch fünf Klassen hat, generieren wir zuerst fünf neue Merkmale. Jedes Merkmal stellt die Wahrscheinlichkeit für jede Klasse dar.\n\nZur Analyse des Textes verwenden wir den aktuellen Stand der Textanalyse, der auf Transformer-basierten Sprachmodellen wie BERT, GPT und anderen basiert. Es ist jedoch unwahrscheinlich, dass wir ein Sprachmodell mit den Eingabedatensätzen trainieren können. Ein naiver Ansatz wäre eine Zielaufgaben-Feinabstimmung. In der Merkmalsextraktionsphase können wir also ein vortrainiertes Sprachmodell herunterladen und es über den Ziel-Datensatz feinabstimmen. In diesem Beispiel feinabstimmen wir das Sprachmodell, um Text in Klassen – in diesem Fall niedrig oder hoch – einzuordnen, erhalten die Ausgabe des Sprachmodells, die die Wahrscheinlichkeit für jede Klasse darstellt, und verwenden diese als neue Merkmale.\n\nDas Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige eindeutige Entitäten enthalten. In unseren Experimenten enthielten fast die Hälfte der Datensätze weniger als 400 Beispiele, und der kleinste Datensatz hatte nur 35 Beispiele in seinem Trainingsdatensatz. Ein Feinabstimmen eines Sprachmodells auf solchen Datensätzen wäre ineffektiv. Wir können jedoch auf vorheriges Wissen über bereits analysierte Datensätze zurückgreifen. Da wir Fest auf mehreren Datensätzen anwenden, können wir die n-1 Datensätze nutzen, um Informationen über diese zu sammeln und diese beim Analysieren des n-ten Datensatzes verwenden.\n\nWas wir vorschlagen, ist, eine weitere Feinabstimmungsphase hinzuzufügen – eine vorläufige multitask-Feinabstimmungsphase. Dabei wird das Sprachmodell auf n-1 Datensätzen feinabgestimmt und anschließend eine weitere Feinabstimmungsphase ausgeführt, die eine Zielaufgaben-Feinabstimmung ist. Wenn wir das Sprachmodell auf den n-ten Ziel-Datensatz feinabstimmen, verwenden wir den Stand der Technik in multitask-Feinabstimmung, genannt DNN. DNN unterhält dabei so viele Köpfe wie Aufgaben im Trainingsdatensatz vorhanden sind. In diesem Beispiel mit vier Aufgaben unterhält DNN vier Köpfe, wie im Bild zu sehen ist. Es wählt zufällig einen Batch aus dem Trainingsdatensatz aus. Gehört der zufällige Batch beispielsweise zu einer einzelnen Satzklassifizierungsaufgabe, führt es einen Vorwärts- und Rückwärtslauf durch den ersten Kopf aus. Gehört der Batch zu einer paarweisen Rangfolge-Aufgabe, führt es einen Vorwärts- und Rückwärtslauf durch den letzten Kopf aus.\n\nIn unserem Szenario variiert die Anzahl der Klassen in tabellarischen Datensätzen, es gibt also viele Aufgaben. Ein leeres DNN unterhält eine Anzahl von Klassen-Köpfen und Ausgabeschichten und benötigt zusätzlich einen neuen Kopf für einen neuen Datensatz mit einer neuen Aufgabe. Unser Ansatz, den wir „Aufgaben-Umformulierung-Feinabstimmung“ nennen, besteht darin, dass wir anstelle der Verwaltung mehrerer Köpfe jede Aufgabe in ein Satzpaar-Klassifizierungsproblem umformulieren, was eine Zweiklassen-Aufgabe darstellt. Lassen Sie uns ein Beispiel betrachten: Hier ist unser Eingabedatensatz, der aus Entitäten, Merkmalen, Text und Klassen besteht. Wir reformulieren die Aufgabe, den Text in niedrig und hoch einzuordnen, zu einer Aufgabe, den Text und die Klasse in wahr oder falsch einzuordnen. Mit anderen Worten, wir trainieren das Sprachmodell, um zu klassifizieren, ob ein Abstract zu einer Klasse gehört oder nicht. Der Label-Vektor besteht in diesem Fall immer aus zwei Klassen. Dies ist der Algorithmus für unseren umformulierten Feinabstimmungsansatz.\n\nLassen Sie uns nun den gesamten Framework betrachten: Der Datensatz wird in Fest eingespeist, Fest führt die Verknüpfungsphase aus, extrahiert den Text aus der Wissensbasis – in diesem Beispiel der Abstract der Wikipedia-Seite – und reformuliert die Aufgabe in ein Satzpaar-Klassifizierungsproblem. Anschließend wird das Sprachmodell auf die neue Aufgabe angewendet, und die Ausgabe stellt die Wahrscheinlichkeit für jede Klasse dar. Beachten Sie, dass das Sprachmodell bereits mit einer vorläufigen multitask-Feinabstimmung auf n-1 Datensätzen feinabgestimmt wurde. Dann verwenden wir den Ausgabevektor des Sprachmodells als neu generiertes Merkmal in der Anzahl der Klassen.\n\nUm unser Framework zu bewerten, verwenden wir siebzehn tabellarische Klassifizierungsdatensätze mit unterschiedlicher Größe, Ausgewogenheit und Domäne sowie anfänglicher Leistung. Als Wissensbasis nutzen wir Wikipedia. Wir entwerfen unser Experiment als Leave-one-out-Bewertung, bei der wir Fest auf 16 Datensätze trainieren und es auf den 17. Datensatz anwenden. Wir teilen jeden Datensatz auch in Faltungen auf und führen eine k-fache Kreuzvalidierung durch. Dann generieren wir die neuen Merkmale und bewerten sie mit fünf Evaluierungs-Klassifikatoren.\n\nHier sind die Ergebnisse unseres Experiments. Sie können sehen, dass wir unseren Ansatz mit der Ziel-Datensatz-Feinabstimmung, der Zielaufgaben-Feinabstimmung und der vorläufigen multitask-Feinabstimmung vergleichen. Unser umformulierter Feinabstimmungsansatz erzielt die beste Leistung, während MT-DNN eine Verbesserung von zwei Prozent gegenüber der Ziel-Datensatz-Feinabstimmung erreicht. Unser Ansatz erzielt eine Verbesserung von sechs Prozent. Wenn wir uns die kleinen Datensätze ansehen, können wir sehen, dass die Leistung von MT-DNN abnimmt und die Verbesserung der vorläufigen multitask-Feinabstimmungsphase auf 1,5 Prozent sinkt, während unsere Leistung auf 11 Prozent gegenüber der Zielaufgaben-Feinabstimmung steigt.\n\nZusammenfassend ermöglicht Fest eine schnelle Anreicherung ab 35 Beispielen in unserem Experiment. Es verwendet eine Architektur für alle Aufgaben und Datensätze und behält den Kopf des Modells bei. Es fügt jedoch eine Umformulierungsphase hinzu, erweitert den Trainingsdatensatz und benötigt einen Zielwert mit semantischer Bedeutung, der in das Sprachmodell eingespeist und im Satzpaar-Klassifizierungsproblem verwendet werden kann. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit zur Erlernung deduktiven Denkens präsentieren, speziell die Lösung von Metroberechnungsproblemen durch komplexe Regionsextraktion. Ich bin von Biance AI Lab und dies ist eine gemeinsame Arbeit mit Che von der University of Texas in Austin und Wedu von SUDD. Zunächst möchte ich über unsere Motivation für das deduktive Denken sprechen. Hier zeigen wir Beispiele, in denen mehrstufiges Denken hilfreich ist. Diese Abbildung ist aus dem Forschungsartikel entnommen, in dem Prompting verwendet wird, um das Methodeproblem in einem zukünftigen Lernszenario zu lösen. Auf der linken Seite sehen wir, dass wir bei der Bereitstellung von Beispielen mit nur Fragen und Antworten möglicherweise nicht die korrekten Antworten erhalten können. Aber wenn wir eine detailliertere Begründung liefern, kann das Modell die Begründung vorhersagen und auch hier eine korrekte Vorhersage treffen. Es ist also vorteilhaft, interpretierbares, mehrstufiges Denken als Ausgabe zu haben. Wir betrachten das Methodeproblem auch als eine direkte Anwendung zur Bewertung solcher Denkfähigkeiten. In unserem Problemsetup müssen wir, basierend auf den Fragen, diese lösen und numerische Antworten erhalten. In unseren Datensätzen wird uns auch die mathematische Expression geliefert, die zu dieser bestimmten Antwort führt. Bestimmte Annahmen gelten auch, wie in vorheriger Arbeit, wo wir die Genauigkeit der Mengen als bekannt annehmen und nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentiation berücksichtigen. Darüber hinaus können komplexe Operatoren tatsächlich in diese grundlegenden Operatoren zerlegt werden. Vorherige Arbeiten zur Lösung des Methodeproblems können in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modelle kategorisiert werden. Traditionelle Sequenz-zu-Sequenz-Modelle wandeln die Expression in eine spezifische Sequenz für die Generierung um, was recht einfach zu implementieren ist und sich auf viele verschiedene komplexe Probleme verallgemeinern lässt. Der Nachteil ist jedoch, dass die Leistung im Allgemeinen nicht besser ist als die von strukturierten Modellen und es mangelt an Interpretierbarkeit der Vorhersagen. Diese Richtung ist jedoch aufgrund des Transformer-Modells immer noch sehr beliebt. Bei modellbasierten Bäumen strukturieren wir diese Ausdrücke in Baumform und folgen einer präordenierten Traversierung bei der Baumgenerierung. Hier generieren wir weiterhin Operatoren, bis wir die Blätter erreichen, die die Mengen darstellen. Der Vorteil ist, dass wir so eine binäre Baumstruktur erhalten. Es ist jedoch kontraintuitiv, da wir zuerst den Operator und dann am Ende die Mengen generieren. Außerdem enthält es einige redundante Berechnungen, wie man hier sehen kann, wo der Ausdruck a mal 3 plus 3 zweimal generiert wird, obwohl wir das Ergebnis wiederverwenden sollten. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme schrittweise und interpretierbar lösen. Zum Beispiel können wir hier in dem zweiten Schritt diese Teiler erhalten, die 27 sind, und wir können auch auf die ursprüngliche Frage zurückgreifen, um relevante Inhalte zu finden. In diesen Schritten erhalten wir die Teiler, und dann in diesem dritten Schritt erhalten wir tatsächlich den Quotienten. Nach diesen drei Schritten können wir die Ergebnisse aus dem zweiten Schritt wiederverwenden und dann die Ergebnisse des vierten Schritts erhalten und schließlich die Dividenden erhalten. Hier generieren wir den gesamten Ausdruck direkt, anstatt einzelne Operatoren oder Mengen zu generieren, was den Prozess genauer macht. In unserem deduktiven System beginnen wir zunächst mit einer Reihe von Mengen, die in den Fragen präsentiert werden, und einschließlich einiger Konstanten als unseren anfänglichen Zustand. Der Ausdruck wird durch eij dargestellt, wobei wir den Operator von qi nach qj ausführen. Ein solcher Ausdruck ist tatsächlich gerichtet, daher haben wir auch eine Subtraktion in umgekehrter Richtung, um die entgegengesetzte Richtung darzustellen. Dies ähnelt stark der Relationsextraktion. In einem formellen deduktiven System wenden wir beim Zeitstep den Operator zwischen dem qi- und qj-Paar an und erhalten so einen neuen Ausdruck, den wir zum nächsten Zustand hinzufügen, um eine neue Menge zu erhalten. Diese Folien visualisieren die Evolution der Zustände, in der wir weiterhin Ausdrücke zum aktuellen Zustand hinzufügen. Bei der Implementierung unserer Modelle verwenden wir zunächst ein vortrainiertes Modell, das BERT oder RoBERTa sein kann, und kodieren dann den Satz, um diese Mengenrepräsentationen zu erhalten. Sobald wir die Mengenrepräsentationen haben, können wir mit der Inferenz beginnen. Hier zeigen wir ein Beispiel, wie wir die Repräsentation für q1 geteilt durch q2 und dann mal q3 erhalten. Zuerst erhalten wir die Paarrepräsentation, die im Wesentlichen nur die Verkettung zwischen q1 und q2 ist, und wenden dann ein Feed-Forward-Netzwerk an, das durch den Operator parametrisiert ist, und erhalten schließlich die Expressionsrepräsentation q1 geteilt durch q2. In der Praxis können wir während der Inferenzphase jedoch auch falsche Ausdrücke erhalten. Hier ist die Anzahl aller möglichen Ausdrücke gleich der dreifachen Anzahl der Operatoren. Der Vorteil ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu kontrollieren. Wenn dieser Ausdruck beispielsweise nicht erlaubt ist, können wir ihn einfach aus unserem Suchraum entfernen. Im zweiten Schritt tun wir das Gleiche, aber der einzige Unterschied besteht darin, dass wir eine weitere Menge haben, die aus dem zuvor berechneten Ausdruck stammt. Schließlich erhalten wir diesen endgültigen Ausdruck q3 mal q4, und wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke sich von dem vorherigen Schritt unterscheidet. Dieser Unterschied macht es schwierig, Beam Search anzuwenden, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgeglichen ist. Das Trainingsprocedere ähnelt dem Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeitstep optimieren. Hier verwenden wir auch Tau, um anzugeben, wann wir diesen Generierungsprozess beenden sollten. Der Raum unterscheidet sich von dem in Sequenz-zu-Sequenz-Modellen, da er sich bei jedem Zeitstep unterscheidet, während es in traditionellen Sequenz-zu-Sequenz-Modellen die Anzahl des Vokabulars ist. Es ermöglicht auch, bestimmte Einschränkungen aus vorherigem Wissen zu übernehmen. Wir führen Experimente mit den häufig verwendeten Methodeproblem-Datensätzen MWPS-Methode3k-Math-QA und SWAM durch und zeigen hier kurz die Ergebnisse im Vergleich zu vorherigen Bestansätzen. Unser bestes Modell ist Roberta-deduktives Denken, und tatsächlich verwenden wir im Gegensatz zu offensichtlichen Ansätzen, die Beam Search verwenden, keine Beam Search. Die besten Ansätze sind oft baumbasierte Modelle, insgesamt kann unser Denker jedoch signifikant bessere Ergebnisse als diese baumbasierten Modelle liefern. Wir sehen jedoch, dass die absoluten Zahlen bei Math-QA oder SWAM nicht wirklich hoch sind. Wir untersuchen die Ergebnisse auf SWAM genauer, da dieser Datensatz anspruchsvoll ist, da der Autor versucht hat, NLP-Modelle zu verwirren, indem er verfügbare Informationen und zusätzliche Mengen hinzufügt. In unseren Vorhersagen stellen wir fest, dass einige der Zwischenwerte tatsächlich negativ sind. Zum Beispiel in diesen Fragen, in denen wir fragen, wie viele Äpfel Jake hat, haben wir zusätzliche Informationen wie \"17 weniger als die Anzahl der Birnenbäume\" und \"Stephen hat acht Birnenbäume\", was völlig relevant ist. Unser Modell macht Vorhersagen wie diese, die negative Werte erzeugen, und wir beobachten, dass diese beiden Ausdrücke ähnliche Scores haben. Wir können diesen Suchraum tatsächlich einschränken, indem wir solche Ergebnisse entfernen, die negativ sind, um die Antwort korrekt zu machen. Wir stellen fest, dass diese Einschränkung die Leistung für einige Modelle erheblich verbessert, zum Beispiel für BERT um sieben Punkte und für das RoBERTa-basierte Modell um zwei Punkte. Bessere Sprachmodelle haben bessere Sprachverständnisfähigkeiten, daher ist die Zahl hier für RoBERTa höher und für BERT niedriger. Wir versuchen auch, die Schwierigkeit hinter diesem Datensatz zu analysieren und gehen davon aus, dass die Anzahl der nicht verwendeten Mengen als relevante Information betrachtet werden kann. Hier sehen wir den Prozentsatz der Proben, in denen wir nicht verwendete Mengen haben, und der SWAM-Datensatz hat den größten Anteil. Wir zeigen auch die Gesamtleistung für diese Proben ohne nicht verwendete Mengen, die tatsächlich höher ist als die Gesamtleistung. Die Leistung mit Proben mit nicht verwendeten Mengen ist jedoch viel schlechter als die Gesamtleistung. Bei MWPS haben wir nicht viele Todesfälle, also ignoriere ich diesen Teil. Schließlich möchten wir die Interpretierbarkeit durch ein Absturz- und Präsentationsbeispiel zeigen. Hier macht unser Modell in dem ersten Schritt eine falsche Vorhersage, und wir können diesen Ausdruck mit dem Satz korrelieren. Wir denken, dieser Satz könnte das Modell zu einer falschen Vorhersage verleiten. Durch das Drucken von \"weitere 35\" lässt das Modell glauben, es sollte ein Additionsoperator sein. Wir versuchen, den Satz zu revidieren, damit er etwas wie \"Die Anzahl der Birnenbäume ist 5 weniger als die Anzahl der Apfelbäume\" lautet, um genauere Semantik zu vermitteln, sodass das Modell die Vorhersage korrekt machen kann. Diese Studie zeigt, wie interpretierbare Vorhersagen uns helfen, das Verhalten des Modells zu verstehen. Zusammenfassend ist unser Modell sehr effizient und wir können ein interpretierbares Lösungsprocedere bereitstellen. Wir können leicht vorheriges Wissen als Einschränkung einbeziehen, was die Leistung verbessern kann. Die zugrunde liegende Mechanik gilt nicht nur für Netzwerkproblemlösungsaufgaben, sondern auch für andere Aufgaben, die mehrstufiges Denken erfordern. Wir haben jedoch auch bestimmte Einschränkungen. Bei einer großen Anzahl von Operatoren oder Konstanten kann der Speicherverbrauch recht hoch sein. Und da die Wahrscheinlichkeitsverteilung zwischen verschiedenen Zeitsteps unausgeglichen ist, ist es auch eine Herausforderung, eine Beam-Search-Strategie anzuwenden. Das war der Vortrag, und Fragen sind willkommen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine, und ich komme von der Universität Maastricht. Ich werde meine Arbeit mit Jerry präsentieren, die sich mit einem neuen Datensatz für die Abfrage von Gesetzestexten befasst. Rechtliche Fragen sind ein integraler Bestandteil des Lebens vieler Menschen, aber die Mehrheit der Bürger hat nur wenig oder gar kein Wissen über ihre Rechte und grundlegenden rechtlichen Prozesse. Daher bleiben viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsexperten nicht leisten können, ungeschützt oder werden sogar ausgenutzt. Unsere Arbeit zielt darauf ab, die Lücke zwischen den Menschen und dem Recht zu schließen, indem ein effektives Abfragesystem für Gesetzestexte entwickelt wird. Ein solches System könnte einen kostenlosen professionellen Rechtsbeistand für ungeübte Personen bieten.\n\nBevor wir zur Hauptbeitrag unserer Arbeit kommen, beschreiben wir zunächst das Problem der Abfrage von Gesetzestexten. Bei einer einfachen rechtlichen Frage wie „Welche Risiken gehe ich ein, wenn ich die berufliche Verschwiegenheit verletze?“ ist ein Modell erforderlich, das alle relevanten Gesetzestexte aus einem großen Rechtsbestand abruft. Diese Informationsabfragaufgabe bringt ihre eigenen Herausforderungen mit sich. Erstens befasst sie sich mit zwei Arten von Sprache: der natürlichen Sprache für die Fragen und der komplexen juristischen Sprache für die Gesetzestexte. Dieser Unterschied in der Sprachverteilung erschwert es einem System, relevante Kandidaten abzurufen, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine juristische Frage übersetzen kann, die der Terminologie der Gesetzestexte entspricht.\n\nZweitens ist das Statutrecht nicht einfach eine Sammlung unabhängiger Artikel, die als vollständige Informationsquelle für sich stehen können, wie beispielsweise Nachrichten oder Rezepte. Stattdessen handelt es sich um eine strukturierte Sammlung rechtlicher Bestimmungen, die nur in ihrer Gesamtheit Sinn ergeben, also zusammen mit den ergänzenden Informationen aus den benachbarten Artikeln, den Feldern und Unterfeldern, zu denen sie gehören, und ihrer Position in der Struktur des Rechts.\n\nSchließlich sind die Gesetzestexte in kleine Absätze unterteilt, die normalerweise die typische Abfrageeinheit in den meisten Abfragearbeiten darstellen. Hier handelt es sich jedoch um lange Dokumente, die bis zu 6000 Wörter umfassen können.\n\nDie jüngsten Fortschritte im Bereich der NLP haben ein großes Interesse an vielen juristischen Aufgaben geweckt, wie z. B. der Vorhersage rechtlicher Urteile oder der automatischen Überprüfung von Verträgen. Die Abfrage von Gesetzestexten ist jedoch hauptsächlich im Hintergrund geblieben, aufgrund des Mangels an großen und hochwertigen beschrifteten Datensätzen.\n\nIn dieser Arbeit stellen wir einen neuen, auf französische Staatsbürger ausgerichteten Datensatz vor, um zu untersuchen, ob Abfragemodelle die Effizienz und Zuverlässigkeit eines Rechtsexperten für die Aufgabe der Abfrage von Gesetzestexten annähern können. Der belgische Gesetzestext-Abfrage-Datensatz umfasst mehr als 1100 rechtliche Fragen, die von belgischen Bürgern gestellt wurden. Diese Fragen decken ein breites Themenspektrum ab, von Familie, Wohnen und Geld bis hin zu Arbeit und Sozialversicherung. Jede Frage wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 26.600 gesetzlichen Artikeln aus belgischen Rechtsquellen beschriftet.\n\nLassen Sie uns nun darüber sprechen, wie wir diesen Datensatz erstellt haben. Zunächst haben wir einen großen Korpus von Gesetzestexten zusammengestellt. Wir haben 32 öffentlich zugängliche belgische Rechtsquellen berücksichtigt und alle ihre Artikel sowie die entsprechenden Abschnittüberschriften extrahiert. Dann haben wir rechtliche Fragen mit Verweisen auf relevante Gesetzestexte gesammelt. Dazu haben wir uns mit einer belgischen Anwaltskanzlei zusammengetan, die jedes Jahr rund 400 E-Mails von belgischen Bürgern erhält, die Rat zu persönlichen rechtlichen Problemen suchen. Wir hatten das Glück, Zugang zu ihrer Website zu erhalten, auf der ihr Team erfahrener Juristen die häufigsten rechtlichen Probleme in Belgien behandelt. Wir haben Tausende von Fragen gesammelt, die mit Kategorien, Unterkategorien und rechtlichen Verweisen auf relevante Gesetzestexte versehen waren.\n\nAbschließend haben wir die rechtlichen Verweise überprüft und die Fragen herausgefiltert, deren Verweise keine Artikel in einer der von uns berücksichtigten Rechtsquellen waren. Die verbleibenden Verweise wurden abgeglichen und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt. Letztendlich landeten wir bei 1108 Fragen, die jeweils sorgfältig mit den IDs der relevanten Artikel aus einem großen Korpus von 26.633 Gesetzestexten beschriftet waren. Darüber hinaus erhält jede Frage eine Hauptkategorie und eine Verkettung von Unterkategorien, und jeder Artikel erhält eine Verkettung seiner nachfolgenden Überschriften in der Struktur des Rechts. Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für zukünftige Forschungen im Bereich der Rechtsinformationsabfrage oder der Klassifizierung juristischer Texte von Interesse sein.\n\nWerfen wir einen Blick auf einige Merkmale unserer Datensätze. Die Fragen sind zwischen 5 und 44 Wörter lang, mit einem Median von 40 Wörtern. Die Artikel sind viel länger, mit einem Median von 77 Wörtern, und 142 von ihnen überschreiten 1000 Wörter, wobei der längste Artikel bis zu 5790 Wörter umfasst. Wie bereits erwähnt, decken die Fragen ein breites Themenspektrum ab, wobei etwa 85 % von ihnen sich mit Familie, Wohnen, Geld oder Justiz befassen, während die restlichen 15 % sich mit Sozialversicherung, Ausländern oder Arbeit befassen. Auch die Artikel sind sehr vielfältig, da sie aus 32 verschiedenen belgischen Rechtsquellen stammen, die eine große Anzahl juristischer Themen abdecken.\n\nHier ist die Gesamtzahl der aus jeder dieser belgischen Rechtsquellen gesammelten Artikel. Von den 26.633 Artikeln werden nur 1612 als relevant für mindestens eine Frage in den Datensätzen bezeichnet, und etwa 80 % dieser zitierten Artikel stammen entweder aus dem Bürgerlichen Gesetzbuch, dem Gerichtsverfassungsgesetz, dem Strafprozessgesetzbuch oder dem Strafgesetzbuch. 18 der 32 Rechtsquellen haben weniger als fünf Artikel, die als relevant für mindestens eine Frage bezeichnet werden, was darauf zurückzuführen sein kann, dass sich diese Rechtsquellen weniger auf Einzelpersonen und ihre Anliegen konzentrieren. Insgesamt beträgt die mittlere Zitierzahl für diese zitierten Artikel zwei, und weniger als 25 % von ihnen werden mehr als fünfmal zitiert.\n\nMit unseren Datensätzen haben wir mehrere Abfrageansätze, einschließlich lexikalischer und dichter Architekturen, bewertet. Bei einer Abfrage in einem Artikel weist ein lexikalisches Modell der Abfrage-Artikel-Paarung einen Score zu, indem es die Summe der Gewichte jedes in diesem Artikel enthaltenen Abfrageterms berechnet. Wir experimentieren mit den Standard-TF-IDF- und BM25-Rangfunktionen. Das Hauptproblem dieser Ansätze besteht darin, dass sie nur Artikel abrufen können, die Schlüsselwörter enthalten, die in der Abfrage vorhanden sind.\n\nUm diese Einschränkung zu überwinden, experimentieren wir mit einer auf neuronalen Netzen basierenden Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann. Wir verwenden ein B-Encoder-Modell, das Abfragen und Artikel in dichte Vektorrepräsentationen abbildet und einen Relevanzscore zwischen einer Abfrage-Artikel-Paarung durch die Ähnlichkeit ihrer Einbettungen berechnet. Diese Einbettungen resultieren typischerweise aus einer Pooling-Operation auf der Ausgabe eines Wort-Einbettungsmodells.\n\nZunächst untersuchen wir die Effektivität von Siamese-Bicodern in einem Zero-Shot-Evaluierungsszenario, was bedeutet, dass vorab trainierte Wort-Einbettungsmodelle direkt ohne zusätzliche Feinabstimmung angewendet werden. Wir experimentieren mit kontextunabhängigen Text-Encodern, nämlich Word2Vec und FastText, sowie mit kontextabhängigen Einbettungsmodellen, nämlich RoBERTa und spezifischer Camembert, einem französischen RoBERTa-Modell. Darüber hinaus trainieren wir unser eigenes Camembert-basiertes Bicoder-Modell auf allen Datensätzen. Es sei darauf hingewiesen, dass wir bei der Trainingsdurchführung zwei Varianten der Bicoder-Architektur untersuchen: Siamese, die einen einzigen Wort-Einbettungsmodellen verwendet, um die Abfrage und den Artikel gemeinsam in einem dichten Vektorraum abzubilden, und Two-Tower, die zwei unabhängige Wort-Einbettungsmodellen verwendet, um Abfrage und Artikel separat in unterschiedliche Einbettungsräume zu kodieren. Wir experimentieren mit Mean, Max und CLS-Pooling sowie Dot-Product und Cosine für die Berechnung von Ähnlichkeiten.\n\nHier sind die Ergebnisse der Baseline-Tests auf den Testsets mit den oben genannten lexikalischen Methoden, den Siamese-Bicodern in einem Zero-Shot-Setup in der Mitte und den feinabgestimmten Bicodern darunter. Insgesamt übertreffen die feinabgestimmten Bicoder alle anderen Baseline-Ansätze bei weitem. Das Two-Tower-Modell verbessert seine Siamese-Variante in Bezug auf die Rückrufquote bei 100, zeigt sich jedoch in den anderen Metriken ähnlich. Obwohl BM25 im Vergleich zu den trainierten Bicodern deutlich unterlegen ist, deutet seine Leistung darauf hin, dass es sich immer noch um eine starke Baseline für domänenspezifische Abfragen handelt.\n\nIn Bezug auf die Zero-Shot-Evaluierung der Siamese-Bicoder stellen wir fest, dass die direkte Verwendung der Einbettungen eines vorab trainierten Camembert-Modells ohne Optimierung für die Informationsabfragaufgabe zu schlechten Ergebnissen führt, was mit früheren Erkenntnissen übereinstimmt. Darüber hinaus beobachten wir, dass der auf Word2Vec basierende Bicoder-Encoder die FastText- und Bird-basierten Modelle deutlich übertrifft, was darauf hindeutet, dass vorab trainierte Wort-Ebene-Einbettungen für diese Aufgabe möglicherweise besser geeignet sind als Zeichen-Ebene- oder Unterwort-Ebene-Einbettungen, wenn sie direkt verwendet werden.\n\nObwohl vielversprechend, deuten diese Ergebnisse auf reichlich Möglichkeiten für Verbesserungen hin, verglichen mit dem Niveau eines Experten, der letztendlich alle relevanten Artikel zu jeder Frage abrufen und damit perfekte Scores erzielen kann.\n\nAbschließend wollen wir zwei Einschränkungen unserer Datensätze diskutieren. Erstens ist der Korpus der Artikel auf die aus den 32 berücksichtigten belgischen Rechtsquellen gesammelten beschränkt, was nicht das gesamte belgische Recht abdeckt. Artikel aus Verordnungen, Richtlinien und Verordnungen fehlen, und alle Verweise auf diese nicht gesammelten Artikel wurden während der Erstellung des Datensatzes ignoriert. Dies führt dazu, dass einige Fragen nur einen Bruchteil der anfänglichen Anzahl relevanter Artikel haben. Dieser Informationsverlust bedeutet, dass die Antworten, die in den verbleibenden relevanten Artikeln enthalten sind, unvollständig sein könnten, obwohl sie immer noch völlig angemessen sind.\n\nZweitens ist zu beachten, dass nicht alle rechtlichen Fragen allein mit Gesetzestexten beantwortet werden können. Beispielsweise könnte die Frage „Kann ich meine Mieter rauswerfen, wenn sie zu viel Lärm machen?“ innerhalb des Statutrechts keine detaillierte Antwort haben, die einen bestimmten Lärmschwellenwert für eine Räumung quantifiziert. Stattdessen sollte sich der Vermieter wahrscheinlich mehr auf die Rechtsprechung stützen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind, z. B. wenn der Mieter zwei Partys pro Woche bis 2 Uhr morgens veranstaltet. Einige Fragen eignen sich daher besser als andere für die Abfrage von Gesetzestexten, und der Bereich der weniger geeigneten Fragen bleibt zu bestimmen.\n\nWir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Modelle für die Abfrage von Gesetzestexten weckt, die dazu beitragen können, den Zugang zur Gerechtigkeit für alle zu verbessern. Sie können unsere Arbeit und den Datensatz unter den folgenden Links einsehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, wir freuen uns, unsere Arbeit zu präsentieren: „Vowels“, ein taskunabhängiger Benchmark, der dazu dient, Vision- und Sprachmodelle mit spezifischen linguistischen Phänomenen zu testen. Warum haben wir uns die Mühe gemacht, diesen Benchmark zu erstellen? In den letzten Jahren haben wir eine Explosion von transformerbasierten Vision- und Sprachmodellen erlebt, die auf großen Mengen von Bild-Text-Paaren vorabtrainiert wurden. Jedes dieser Modelle verbessert den Stand der Technik in Aufgaben wie visuellem Fragebeantworten, visuellem gesundem Menschenverstand, Bildabruf, Phrasenverankerung und mehr. Wir erhielten jedoch eine Nachricht: Während die Genauigkeiten in diesen taskspezifischen Benchmarks stetig steigen, wissen wir eigentlich, was die Modelle gelernt haben? Was versteht ein Vision- und Sprachmodell, wenn es für dieses Bild und diesen Satz eine hohe Übereinstimmung bewertet und für diesen anderen eine niedrige? Konzentrieren sich Vision- und Sprachmodelle auf das Richtige oder auf Verzerrungen, wie vorherige Arbeiten gezeigt haben?\n\nUm mehr Licht auf diese Frage zu werfen, schlagen wir eine eher taskspezifische Richtung vor und stellen „Vowels“ vor, einen Test, der die Sensibilität von Vision- und Sprachmodellen für spezifische linguistische Phänomene untersucht, die sowohl die linguistische als auch die visuelle Modalität betreffen. Wir konzentrieren uns auf Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entitäts-Coreferenz.\n\nWie testen wir aber, ob die Vision- und Sprachmodelle diese Phänomene erfasst haben? Wir verwenden eine Methode, die zuvor nur für Nominalphrasen und Zählen angewendet wurde, und führen sie auf „Vowels“ ein. Diese Methode, die wir „Foiling“ nennen, bedeutet, dass wir die Bildunterschrift eines Bildes nehmen und eine „Foil“ erstellen, indem wir die Unterschrift so verändern, dass sie das Bild nicht mehr beschreibt. Diese Phrasenänderungen konzentrieren sich auf sechs spezifische Aspekte: Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entitäts-Coreferenz. Jeder Aspekt kann aus einem oder mehreren „Instrumenten“ bestehen, falls wir mehr als eine interessante Möglichkeit finden, „FoIL“-Instanzen zu erstellen.\n\nZum Beispiel haben wir beim Aspekt „Handlungen“ zwei Instrumente: Eines, bei dem das Handlungsverb durch ein anderes Handlungsverb ersetzt wird, und eines, bei dem die Akteure ausgetauscht werden. Auch Zählen und Coreferenz haben mehr als ein Instrument. Diese Foils erstellen wir so, dass sie das Bild nicht beschreiben, grammatikalisch korrekt und ansonsten gültige Sätze sind. Dies ist nicht einfach, da eine manipulierte Bildunterschrift weniger wahrscheinlich sein kann als die ursprüngliche. Obwohl es nicht unmöglich ist, ist es statistisch gesehen weniger wahrscheinlich, dass Pflanzen einen Mann schneiden als dass ein Mann Pflanzen schneidet, und große Vision- und Sprachmodelle könnten diese Verzerrung erkennen.\n\nUm gültige Foils zu erhalten, gehen wir wie folgt vor: Zuerst nutzen wir leistungsstarke Sprachmodelle, um Foils vorzuschlagen. Zweitens verwenden wir Natural Language Inference (NLI) oder kurz NLI, um Foils auszufiltern, die das Bild möglicherweise immer noch beschreiben könnten. Da wir beim Erstellen der Foils sicherstellen müssen, dass sie das Bild nicht beschreiben, wenden wir NLI mit der folgenden Logik an: Wir betrachten ein Bild als Prämisse und seine Bildunterschrift als implizierte Hypothese. Darüber hinaus betrachten wir die Bildunterschrift als Prämisse und die Foil als Hypothese. Wenn ein NLI-Modell vorhersagt, dass die Foil die Bildunterschrift widerspricht oder neutral dazu ist, nehmen wir dies als Hinweis auf eine gültige Foil. Wenn das NLI-Modell vorhersagt, dass die Foil durch die Bildunterschrift impliziert wird, kann sie keine gute Foil sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefern würde, und wir filtern diese Foils aus.\n\nDieses Verfahren ist jedoch nicht perfekt, es ist nur ein Hinweis auf gültige Foils. Daher setzen wir als dritte Maßnahme zur Erstellung gültiger Foils menschliche Annotatoren ein, um die in VALSE verwendeten Daten zu validieren. Nach dem Filtern und der menschlichen Bewertung verfügen wir über so viele Testinstanzen wie in dieser Tabelle beschrieben. Beachten Sie, dass VALSE keine Trainingsdaten, sondern nur Testdaten liefert, da es sich um einen Zero-Shot-Test-Benchmark handelt. Er ist so konzipiert, dass er die bestehenden Fähigkeiten von Vision- und Sprachmodellen nach dem Vorabtraining nutzt. Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen. Und wir wissen alle, dass diese Modelle gerne schummeln und Abkürzungen nehmen.\n\nWie bereits erwähnt, sind wir daran interessiert, die Fähigkeiten von Vision- und Sprachmodellen nach dem Vorabtraining zu bewerten. Wir experimentieren mit fünf Vision- und Sprachmodellen auf „Vowels“, nämlich CLIP, ALEXBERT, WiLBERT, WiLBERT-12-in-1 und VisualBERT. Zwei unserer wichtigsten Bewertungsmetriken sind die Genauigkeit der Modelle bei der Klassifizierung von Bild-Satz-Paaren in Bildunterschriften und Foils. Für dieses Video werden wir unsere permissivere Metrik, die paarweise Genauigkeit, vorstellen, die misst, ob die Bild-Satz-Ausrichtung für das korrekte Bild-Text-Paar höher ist als für sein gefälschtes Paar. Für weitere Metriken und Ergebnisse verweisen wir auf unsere Publikation.\n\nDie Ergebnisse mit der paarweisen Genauigkeit sind hier gezeigt und stimmen mit den Ergebnissen überein, die wir mit den anderen Metriken erhalten haben: Die beste Zero-Shot-Leistung wird von WiLBERT-12-in-1 erreicht, gefolgt von WiLBERT, ALBERT, CLIP und schließlich VisualBERT. Es ist bemerkenswert, wie Instrumente, die sich auf einzelne Objekte konzentrieren, wie Existenz und Nominalphrasen, von WiLBERT-12-in-1 fast gelöst werden, was darauf hinweist, dass Modelle in der Lage sind, benannte Objekte und ihre Präsenz in Bildern zu identifizieren. Keiner der verbleibenden Aspekte kann jedoch in unseren adversären Foiling-Einstellungen zuverlässig gelöst werden.\n\nWir sehen aus den Instrumenten für Pluralität und Zählen, dass Vision- und Sprachmodelle Schwierigkeiten haben, Referenzen auf einzelne oder mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen. Der Aspekt „Beziehung“ zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Beziehung zwischen Objekten in einem Bild korrekt zu klassifizieren. Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsverzerrungen unterstützt werden, wie wir es beim Aspekt „Handlungen“ sehen. Aus dem Referenzaspekt erfahren wir, dass es für Vision- und Sprachmodelle schwierig ist, mehrere Referenzen auf dasselbe Objekt in einem Bild durch Verwendung von Pronomen zu verfolgen.\n\nAls Plausibilitätsprüfung und weil es ein interessantes Experiment ist, benchmarken wir auch zwei Text-only-Modelle, GPT-1 und GPT-2, um zu bewerten, ob VALSE durch diese unimodalen Modelle gelöst werden kann. Wir berechnen die Perplexität der korrekten und der gefälschten Bildunterschrift (ohne Bild) und prognostizieren den Eintrag mit der niedrigeren Perplexität. Wenn die Perplexität für die Foil höher ist, nehmen wir dies als Hinweis darauf, dass die gefälschte Bildunterschrift möglicherweise unter Plausibilitätsverzerrungen oder anderen linguistischen Verzerrungen leidet. Es ist interessant zu sehen, dass die Text-only-GPT-Modelle in einigen Fällen die Plausibilität der Welt besser erfasst haben als die Vision- und Sprachmodelle.\n\nZusammenfassend ist VALSE ein Benchmark, der die linguistischen Konstrukte nutzt, um der Community zu helfen, Vision- und Sprachmodelle durch die harte Prüfung ihrer visuellen Verankerungsfähigkeiten zu verbessern. Unsere Experimente zeigen, dass Vision- und Sprachmodelle benannte Objekte und ihre Präsenz in Bildern gut identifizieren können, wie der Existenzaspekt zeigt, aber Schwierigkeiten haben, ihre gegenseitige Abhängigkeit und Beziehungen in visuellen Szenen zu verankern, wenn sie dazu gezwungen sind, linguistische Indikatoren zu respektieren.\n\nWir möchten die Community wirklich ermutigen, „Vowels“ für die Messung des Fortschritts bei der Sprachverankerung mit Vision- und Sprachmodellen zu verwenden. Noch mehr könnte „Vowels“ als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten, um zu sehen, ob ein Datensatz den Modellen hilft, sich in einem der von VAs getesteten Aspekte zu verbessern. Wenn Sie interessiert sind, finden Sie die VALSE-Daten auf GitHub. Bei Fragen zögern Sie bitte nicht, uns zu kontaktieren."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamisura von der Universität Tokio. Ich werde einen Vortrag mit dem Titel „O En Sum: Ein größerer Datensatz für die automatische Erstellung von Release-Notizen“ halten. Ich werde in folgender Reihenfolge vorgehen: Zuerst werde ich die automatische Commit-Notation vorstellen, an der wir in dieser Forschung arbeiten.\n\nRelease-Notizen sind technische Dokumente, die die Änderungen zusammenfassen, die mit jeder Software-Produkt-Version verteilt werden. Das Bild zeigt die Release-Notizen für Version 2.6.4 der Bujs-Bibliothek. Diese Notizen spielen eine wichtige Rolle in der Open-Source-Entwicklung, aber ihre manuelle Erstellung ist zeitaufwändig. Daher wäre es sehr nützlich, hochwertige Release-Notizen automatisch generieren zu können.\n\nIch werde auf zwei frühere Forschungen zur automatischen Erstellung von Listen-Knoten eingehen. Die erste ist ein System namens alena, das 2014 veröffentlicht wurde. Es verfolgt einen regelbasierten Ansatz und verwendet beispielsweise einen Change-Extractor, um Kernunterschiede, Bibliotheksänderungen und Dokumentänderungen aus den Unterschieden zwischen Krankheiten zu extrahieren und sie schließlich zu kombinieren. Die auffälligste Eigenschaft dieses Systems ist der Issue-Extrakt im oberen rechten Eck, der mit Jira, dem Issue-Ökosystem, verknüpft sein muss und nur auf Projekte angewendet werden kann, die Jira verwenden, also nicht für viele Projekte auf GitHub.\n\nDie zweite ist Grif, kürzlich im Jahr 2020 angekündigt. Es ist im Internet verfügbar und kann über Pi gespeichert werden. Dieses System verwendet ein einfaches, auf Textklassifizierung basierendes Modell und gibt für jede Eingabe-Commit-Nachricht eine von fünf Kategorien wie Funktionen oder Fehlerbehebungen aus. Das Bild zeigt eine Beispielanwendung, die eine korrekte Ausgabe von Fehlerbehebungen liefert. Das Trainingsdaten-Set ist relativ klein, etwa 5000, und wird in den unten beschriebenen Experimenten gezeigt. Die Leistung des Textklassifizierungsmodells ist nicht hoch.\n\nIch präsentiere zwei verwandte Forschungen, aber es gibt Probleme mit der begrenzten Anwendbarkeit und knappen Datenressourcen. Unser Papier löst diese beiden Probleme und generiert automatisch hochwertige Ressourcen. Für das Problem der begrenzten Anwendbarkeit schlagen wir eine hochwertige Klassifizierungs-Summarisierungsmethode vor, die nur Commit-Nachrichten als Eingabe verwendet. Dieser vorgeschlagene Ansatz kann für alle englischen Repositories verwendet werden.\n\nFür das zweite Problem, knappe Ressourcen, haben wir unsere eigenen Daten aufgebaut, die aus etwa 82.000 Datensätzen bestehen, indem wir Daten aus öffentlichen GitHub-Repositories mit der Git-API korrigiert haben. Als Nächstes beschreibe ich unsere Daten. Hier ist ein Beispiel-Update: Auf der linken Seite befindet sich eine Commit-Nachricht, und auf der rechten Seite sind die Release-Notizen. Die Release-Notizen sind in Ebenen wie Verbesserungen von Funktionen usw. unterteilt. Wir haben eine Aufgabe eingerichtet, die die Commit-Nachrichten als Eingabe nimmt und die Release-Notizen ausgibt. Dies kann als Summarisierungsaufgabe betrachtet werden. Wir haben vier Ebenen vorgegeben: Funktionen, Verbesserungen, Fehlerbehebungen, Deprecations und Breaking Changes. Diese wurden auf Grundlage früherer Verwendungen und anderer Faktoren festgelegt.\n\nEs gibt eine Notiz im unteren rechten Eck, die aus der Listen-Notiz im unteren linken Eck extrahiert wurde. Zu diesem Zeitpunkt ist es notwendig, die vier zuvor festgelegten Ebenen zu erkennen, aber die Ebenen stimmen nicht immer mit jeder Nachricht überein. Beispielsweise umfasst die Verbesserungs-Ebene Verbesserungen, Verbesserungen, Optimierungen usw. Wir haben eine Vokabelliste der Studienebenen für jede dieser notationalen Variationen erstellt und sie verwendet, um die Klasse des Release-Knotens zu erkennen und den folgenden Text zu korrigieren.\n\nAls Nächstes ist eine Commit-Nachricht zu sehen. Commit-Nachrichten sind nicht an jede Ebene gebunden, wie im folgenden Bild gezeigt. Wenn die aktuelle Version 2.5.19 ist, müssen wir die vorherige Version 2.5.18 identifizieren und sie abrufen. Dies ist etwas mühsam, und es reicht nicht aus, einfach eine Liste der Veröffentlichungen zu erhalten und die vorherigen und nachfolgenden Versionen zu betrachten. Wir haben einen heuristischen Matching-Algorithmus erstellt, um die vorherigen und nächsten Versionen zu erhalten.\n\nBei der Datenanalyse wurden schließlich 7200 Repositories und 82.000 Datensätze korrigiert. Die durchschnittliche Anzahl der sinnvollen Token beträgt 63, was für eine Summarisierungsaufgabe recht hoch ist. Auch die Anzahl der eindeutigen Token ist mit 88.300 recht reichhaltig. Dies ist auf die große Anzahl eindeutiger Klassen- und Methodenbezeichnungen zurückzuführen, die im Repository gefunden wurden.\n\nAls Nächstes werde ich den vorgeschlagenen Ansatz erklären. Das kreuzweise extraktive und abstrakte Summarisierungsmodell besteht aus zwei neuronalen Modulen: einem Klassifikator, der Bot- oder Code-Bot verwendet, und einem Generator, der But First G verwendet. Zuerst klassifiziert der Klassifikator jede Commit-Nachricht in fünf Basisknotenklassen: Funktionen, Verbesserungen, Fehlerbehebungen, Deprecations und andere. Die Commit-Nachrichten, die als „andere“ klassifiziert werden, werden verworfen. Dann wendet sie den Generator auf die vier Release-Dokumente unabhängig an und generiert für jede Klasse eine Release-Notiz. Bei dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Release-Notizen nicht bekannt.\n\nDaher weisen wir jedem Eingabe-Commit-Nachricht Pseudo-Variablen zu, indem wir die ersten 10 Zeichen jeder Commit-Nachricht verwenden, um den Klassifikator zu trainieren. Wir modellieren den klassenweisen abstrakten Summarisierungsansatz mit zwei definierten Methoden. Das erste Modell, das wir GS Single nennen, besteht aus einem einzelnen Sex-Netzwerk und generiert einen einzelnen langen Text, der eine Verkettung der Eingabe-Commit-Nachrichten ist. Der Ausgabe-Text kann basierend auf speziellen, klassenbezogenen Endpunkt-Symbolen in Klassen-Dateisegmente unterteilt werden.\n\nDie zweite Methode, die wir She’s Much nennen, besteht aus vier verschiedenen Sek-zu-Sek-Netzwerken, von denen jedes einer der Release-Knotenklassen entspricht.\n\nLassen Sie mich die Experimente erklären. Es wurden fünf Methoden verglichen: GS, She’s Single, She’s Much, Cluster und eine frühere Studie, Gri, bezüglich Abweichung. In einigen Fällen werden diese Notizen in mehreren Sätzen ausgegeben, da es schwierig ist, die Anzahl der Sätze auf Null zu korrigieren. Sie werden daher mit Leerzeichen kombiniert und als ein langer Satz behandelt. Die blaue Strafe tritt auf, wenn das System einen kurzen Satz ausgibt. Diese Strafe führt zu einem niedrigeren BLEU-Wert in den nächsten Experimentergebnissen.\n\nSchließlich fügen wir auch eine Spezifität hinzu, da Blau und Blau nicht berechnet werden können, wenn die Listen-Notizen leer sind. Eine hohe Spezifität bedeutet, dass das Modell in Fällen, in denen die Release-Notizen leer sind, korrekt leeren Text ausgibt.\n\nHier sind die Ergebnisse. Da das Dataset E-Mail-Analysen und andere Werte enthält, haben wir auch das bereinigte Dataset bewertet, das diese ausschließt. G und GS erreichten Verlustfehler-Werte, die mehr als 10 Punkte höher sind als die Baseline, insbesondere im koreanischen Testset. Der Punktabstand zwischen dem vorgeschlagenen Ansatz und der Baseline sprang auf mehr als 20 Punkte. Diese Ergebnisse zeigen, dass G und GS signifikant effektiv sind.\n\nGS erreichte einen besseren Verlustwert als GAS, was darauf hindeutet, dass die Kombination eines Klassifikators mit einem Generator effektiv ist. Die hohe Abdeckung von GS kann ordnungsgemäß erreicht werden, da der Klassifikator sich auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann. She’s Much neigte dazu, höhere Werte zu erreichen als She’s Single, was darauf hindeutet, dass es auch effektiv ist, für jede Release-Knoten-Klasse unterschiedliche konstruktive Summarisierungsmodelle unabhängig zu entwickeln.\n\nEine Fehleranalyse zeigt, dass die She-Methoden dazu neigen, kürzere Sätze als die menschlichen Referenz-Sätze auszugeben. Wie im rechten Diagramm zu sehen, hat der Referenz-Satz drei oder vier Sätze, während CSS nur einen Satz hat. Der Grund für diese Zurückhaltung des Modells liegt darin, dass in den Trainingsdaten nur 30 % der Sätze auf der Funktionen-Ebene und 40 % auf der Verbesserungs-Ebene vorhanden sind. Darüber hinaus können die CS-Methoden ohne zusätzliche Informationen keine genauen Listen-Notizen generieren.\n\nDas obere Beispiel rechts zeigt eine sehr unordentliche Commit-Nachricht, und ein vollständiger Satz kann nicht generiert werden, ohne die entsprechenden vorherigen Versionen oder Issues zu kennen. Das untere Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe zusammenhängen und zu einem Satz kombiniert werden sollten, was jedoch misslingt.\n\nAbschließend haben wir einen neuen Datensatz für die automatische Erstellung von Release-Notizen aufgebaut. Wir haben auch die Aufgabe formuliert, Commit-Nachrichten zu eingeben und sie zu summarisieren, sodass sie für alle in Englisch geschriebenen Projekte anwendbar ist. Unsere Experimente zeigen, dass der vorgeschlagene Ansatz weniger Rauschen erzeugt und eine höhere Abdeckung als die Baseline aufweist. Bitte sehen Sie sich unsere Daten auf GitHub an. Vielen Dank."}
