{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Safari, e eu apresentarei nosso trabalho de enriquecimento de dados tabulares usando arquiteturas de transformadores finetuned. Assim, os cientistas analisam dados e se concentram principalmente na manipulação das características existentes dos dados, mas às vezes essas características são limitadas. A geração de características usando outra fonte de dados pode adicionar informações significativas. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando fontes externas de texto livre. Suponha que tenhamos um conjunto de dados tabulares e uma base de conhecimento. Precisamos de um processo automático que envolva a ligação de entidades e a análise de texto para extrair novas características do texto livre da base de conhecimento. Nosso framework, fest, é exatamente esse processo automático. Vamos ver um exemplo: os conjuntos de dados alimentados no fest. Neste exemplo, o conjunto de dados é um conjunto de dados universitários, cujo objetivo é classificar as universidades em universidades de baixa e alta classificação. Como base de conhecimento, usamos a Wikipédia. A primeira fase é a ligação de entidades, na qual cada entidade, neste caso, o nome da universidade, é vinculada a uma entidade dentro da base de conhecimento, e o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipédia. Agora, precisamos gerar ou extrair características do texto recuperado, então precisamos de uma fase de extração de características que inclua análise de texto, e essa é a principal novidade deste artigo. Vou aprofundar isso nas próximas slides. Após a fase de extração de características, há uma fase de geração de características, na qual usamos as características extraídas para gerar um pequeno número de novas características. Primeiro, geramos um número de características igual ao número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então geramos duas novas características. Mas se o conjunto de dados tiver cinco classes, primeiro geramos cinco novas características. Cada característica representa a probabilidade para cada classe. Para analisar o texto, usamos o estado atual da análise de texto fora do contexto, que são modelos de linguagem baseados em transformadores, como o GPT-X e outros. No entanto, não é provável que possamos treinar um modelo de linguagem usando os conjuntos de dados de entrada, então uma abordagem ingênua seria o ajuste fino da tarefa-alvo. Assim, na fase de extração de características, podemos baixar um modelo de linguagem pré-treinado e ajustá-lo ao conjunto de dados-alvo. Neste exemplo, ajustamos o modelo de linguagem para classificar textos em classes, resumos em classes baixa ou alta, recebemos a saída do modelo de linguagem, que é a probabilidade para cada classe, e usamos como novas características. O problema com essa abordagem é que o conjunto de dados pode ter poucas entidades distintas; em nossos experimentos, quase metade dos conjuntos de dados contém menos de 400 amostras, e o menor conjunto de dados contém 35 amostras em seu conjunto de treinamento inicial. Ajustar um modelo de linguagem sobre esse conjunto de dados seria ineficaz. No entanto, podemos usar o conhecimento prévio sobre conjuntos de dados pré-analisados, pois, quando aplicamos o fast em vários conjuntos de dados, podemos usar os n-1 conjuntos de dados para coletar informações sobre os n-1 conjuntos de dados e usar essas informações quando analisamos o n-ésimo conjunto de dados. O que sugerimos é adicionar outra fase de ajuste fino, uma fase de ajuste fino multitarefa preliminar, na qual ajustamos o modelo de linguagem sobre n-1 conjuntos de dados e, em seguida, executamos outra fase de ajuste fino, que é o ajuste fino da tarefa-alvo, quando ajustamos o modelo de linguagem sobre o n-ésimo conjunto de dados-alvo. O estado da arte no ajuste fino multitarefa, chamado DNN, mantém cabeças no número de tarefas no conjunto de treinamento. Se, neste exemplo, houver quatro tarefas no conjunto de treinamento, o DNN manterá quatro cabeças, como você pode ver na imagem. Ele amostra um lote aleatório de um dos conjuntos de treinamento e, se o lote aleatório pertencer, por exemplo, a uma tarefa de classificação de sentenças únicas, executa a passagem para frente e para trás através da primeira cabeça. Se o lote aleatório pertencer a uma tarefa de classificação par a par, executa a passagem para frente e para trás através da última cabeça. Em nosso cenário, um conjunto de dados tabulares varia o número de classes, então há muitas tarefas. Um DNN vazio mantém camadas de saída no número de classes e, adicionalmente, um DNN vazio precisa inicialmente de novas cabeças para um novo conjunto de dados com uma nova tarefa. Nossa abordagem, chamada de ajuste fino de reformulação de tarefas, reformula cada conjunto de dados em um problema de classificação de sentenças por par, que são tarefas de duas classes. Vamos ver um exemplo: aqui está nosso conjunto de dados de entrada, que consiste em entidades, características, texto e classes, e reformulamos a tarefa de classificar o texto em baixo e alto para classificar o texto, o resumo e a classe em verdadeiro ou falso, ou em outras palavras, treinamos o modelo de linguagem para classificar um resumo e uma classe para tentar abstrair a classe se o resumo pertencer à classe ou não. O vetor de rótulo neste caso sempre consiste em duas classes. Este é o algoritmo de nossa abordagem de ajuste fino reformulado. Vamos ver o framework completo: o conjunto de dados é alimentado no fast, e então o fast executa a fase de ligação, extrai o texto da base de conhecimento, que neste exemplo é o resumo da página da Wikipédia, reformula a tarefa em tarefas de classificação de sentenças por par, aplica o modelo de linguagem à nova tarefa e a saída de probabilidade para cada classe. Observe que o modelo de linguagem já foi ajustado aos n-1 conjuntos de dados usando um ajuste fino multitarefa preliminar. Em seguida, usamos o vetor de saída do modelo de linguagem como uma característica gerada recentemente no número de classes. Para avaliar nosso framework, usamos dezessete conjuntos de dados de classificação tabular, que definem tamanho, equilíbrio de domínio e desempenho inicial, e, como desperdício de conhecimento, usamos a Wikipédia. Projetamos nossa experiência como uma avaliação de deixe-um-fora, na qual treinamos o fast em 16 conjuntos de dados e o aplicamos ao 17º conjunto de dados. Também dividimos cada conjunto de dados em quatro partes e aplicamos uma validação cruzada. Em seguida, geramos a nova característica e as avaliamos usando cinco classificadores de avaliação. Usamos a arquitetura baseada em Bird em nosso experimento. Aqui estão os resultados de nosso experimento: você pode ver que comparamos nosso framework com o ajuste fino do conjunto de dados-alvo, o ajuste fino da tarefa-alvo e o ajuste fino preliminar do MTDNN. Nosso ajuste fino reformulado alcançou o melhor resultado, um aumento de dois por cento em relação ao ajuste fino do conjunto de dados-alvo e um aumento de seis por cento em relação ao ajuste fino da tarefa-alvo. Quando olhamos para os pequenos conjuntos de dados, podemos ver que o desempenho do MTDNN diminui e o aumento da fase de ajuste fino multitarefa preliminar diminui para 1,5 por cento, mas nosso desempenho aumenta para 11 por cento em relação ao ajuste fino da tarefa-alvo. Em resumo, o fast permite o enriquecimento rápido a partir de 35 amostras em nosso experimento. Ele usa uma única arquitetura para todas as tarefas e conjuntos de dados e mantém a cabeça do modelo. No entanto, ele adiciona uma fase de reformulação, aumenta o conjunto de treinamento e precisa de um valor-alvo com significado semântico, para que possamos alimentá-lo no modelo de linguagem e usá-lo no problema de classificação de sentenças por par. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "- Olá a todos. Hoje vou apresentar nosso trabalho de pesquisa sobre aprendizado para raciocinar dedutivamente, resolvendo problemas matemáticos complexos como extração de regiões. Sou do Biance AI Lab e este é um trabalho conjunto com Che, da Universidade do Texas em Austin, e Wedu, do SUDD. Primeiro, gostaria de falar sobre nossa motivação para o raciocínio. Aqui, estamos mostrando exemplos onde o raciocínio multi-passo é útil. Esta figura é extraída do artigo original, onde eles utilizam prompts para resolver problemas matemáticos em um cenário de aprendizado futuro. No lado esquerdo, podemos ver que, se fornecermos apenas amostras com perguntas e respostas, talvez não consigamos obter as respostas corretas. Mas, se adicionarmos mais descrições de raciocínio, o modelo consegue prever a descrição do raciocínio e também fazer uma previsão correta, o que demonstra a importância de ter um raciocínio multi-passo interpretável como saída. Também consideramos que o problema matemático é uma aplicação direta para avaliar tais habilidades de raciocínio.\n\nAqui, em nossa configuração de problema, dada a pergunta, precisamos resolvê-la e obter respostas numéricas. Em nossos conjuntos de dados, também fornecemos a expressão matemática que leva a essa resposta específica. Certas suposições também se aplicam, como em trabalhos anteriores, onde assumimos que a precisão das quantidades é conhecida e consideramos apenas operadores básicos, como adição, subtração, multiplicação, divisão e exponenciação. Além disso, operadores complexos podem ser decompostos nesses operadores básicos.\n\nTrabalhos anteriores na resolução de problemas matemáticos podem ser categorizados em modelos sequência-sequência e sequência-árvore. O modelo sequência-sequência tradicional converte a expressão em uma sequência específica para geração, o que é fácil de implementar e pode generalizar para muitos problemas complexos. No entanto, o desempenho geralmente não é melhor que o dos modelos estruturados e falta interpretabilidade nas previsões. Essa abordagem ainda é popular devido ao modelo Transformer.\n\nEm modelos baseados em árvores, estruturamos as expressões em forma de árvore e seguimos uma travessia em pré-ordem na geração da árvore. Aqui, continuamos gerando operadores até alcançarmos as folhas, que são as quantidades. A vantagem é que obtemos uma estrutura de árvore binária, mas é contra-intuitivo porque geramos primeiro os operadores e, no final, as quantidades. Além disso, há computações repetitivas, como na expressão \"a vezes 3 mais 3\", que é gerada duas vezes, mas deveríamos reutilizar o resultado.\n\nEm nossa abordagem proposta, queremos resolver esses problemas de forma passo a passo e interpretável. Por exemplo, no segundo passo, podemos obter os divisores, como 27, e também referenciar a pergunta original para encontrar o conteúdo relevante. Nessas etapas, obtemos os divisores, e no terceiro passo, calculamos o quociente. Após esses três passos, reutilizamos os resultados do segundo passo para obter o resultado do quarto passo e, finalmente, o dividendo. Geramos a expressão completa diretamente, em vez de gerar apenas operadores ou quantidades, tornando o processo mais preciso.\n\nEm nosso sistema dedutivo, começamos com um conjunto de quantidades apresentadas nas perguntas, incluindo algumas constantes como nosso estado inicial. A expressão é representada por eij, onde realizamos o operador de qi para qj. Essa expressão é direcionada, então também temos a subtração inversa para representar a direção oposta, semelhante à extração de relações. Em um sistema dedutivo formal, no passo t, aplicamos o operador entre o par qi e qj, obtendo novas expressões que adicionamos ao próximo estado para se tornarem novas quantidades.\n\nEsta apresentação visualiza a evolução dos estados, onde continuamos adicionando expressões ao estado atual. Em nossa implementação do modelo, primeiro usamos um modelo pré-treinado, como BERT ou RoBERTa, e então codificamos a sentença para obter as representações das quantidades. Uma vez obtidas as representações das quantidades, podemos começar a inferir. Aqui, mostramos um exemplo de como obter a representação de q1 dividido por q2 e multiplicado por q3. Primeiro, obtemos a representação do par, que é a concatenação de q1 e q2, e então aplicamos uma rede feed-forward parametrizada pelo operador, resultando na representação da expressão q1 dividido por q2.\n\nNa prática, durante a inferência, também podemos obter expressões incorretas. O número total de expressões possíveis é igual a três vezes o número de operadores. A vantagem é que podemos adicionar facilmente restrições para controlar o espaço de busca. Por exemplo, se uma expressão não é permitida, removemos-a do espaço de busca. No segundo passo, fazemos o mesmo, mas a diferença é que temos mais uma quantidade, que vem da expressão calculada anteriormente. Finalmente, obtemos a expressão final q3 vezes q4, e podemos ver que o número de expressões possíveis é diferente do passo anterior. Essa diferença torna difícil aplicar a busca por feixe (beam search) devido à distribuição de probabilidade desequilibrada entre os passos.\n\nO procedimento de treinamento é semelhante ao de um modelo sequência-sequência, onde otimizamos a perda em cada passo de tempo. Aqui, também usamos τ para representar quando devemos terminar o processo de geração. O espaço é diferente do modelo sequência-sequência tradicional, pois varia em cada passo, permitindo impor restrições de conhecimento prévio.\n\nRealizamos experimentos em conjuntos de dados de problemas matemáticos comumente usados, como MWPS Method3k MathQA e SWAM. Aqui, mostramos brevemente os resultados em comparação com abordagens anteriores. Nossa melhor abordagem é o RoBERTa Deductive Reasoner, e, diferentemente de abordagens óbvias que usam busca por feixe, não utilizamos essa técnica. As melhores abordagens geralmente são modelos baseados em árvores, e nosso raciocínio é capaz de produzir saídas significativamente melhores que esses modelos. No entanto, o número absoluto em MathQA ou SWAM não é muito alto.\n\nInvestigamos mais a fundo os resultados em SWAM, um conjunto de dados desafiador porque os autores tentaram adicionar manualmente informações para confundir modelos de NLP, como adicionar informações extras e quantidades. Em nossas previsões, encontramos valores intermediários negativos, por exemplo, em perguntas sobre quantas maçãs Jake tem, mas com informações extras como \"17 a menos que as peras\" e \"Stephen tem oito peras\". Nesses casos, nosso modelo faz previsões que resultam em valores negativos, e observamos que essas duas expressões têm pontuações semelhantes. Podemos limitar o espaço de busca removendo resultados negativos para corrigir a resposta.\n\nDescobrimos que essa restrição melhora significativamente o desempenho de alguns modelos, como BERT, com uma melhoria de 7 pontos, e também para o modelo baseado em RoBERTa, com uma melhoria de 2 pontos. Modelos de linguagem melhores têm melhores habilidades de compreensão de linguagem, o que explica a diferença nos números.\n\nTambém analisamos a dificuldade por trás desse conjunto de dados, assumindo que a quantidade de quantidades não utilizadas pode ser considerada informação relevante. Aqui, vemos a porcentagem de amostras que usam quantidades não utilizadas, e o conjunto de dados SWAM tem a maior proporção. Também mostramos o desempenho geral para amostras sem quantidades não utilizadas, que é mais alto do que o desempenho geral, mas com amostras que possuem quantidades não utilizadas, o desempenho é muito pior.\n\nPor fim, queremos demonstrar a interpretabilidade através de um exemplo de previsão e apresentação. Aqui, nosso modelo faz uma previsão errada no primeiro passo, e podemos correlacionar essa expressão com a sentença. Acreditamos que a sentença pode estar induzindo o modelo a uma previsão incorreta. Ao imprimir \"outros 35\", o modelo pensa que deve ser um operador de adição. Revisamos a sentença para algo como \"o número de pereiras é 5 a menos que o de macieiras\", transmitindo semântica mais precisa para que o modelo faça a previsão correta. Este estudo demonstra como previsões interpretáveis nos ajudam a entender o comportamento do modelo.\n\nPara concluir, nosso modelo é eficiente e fornece um procedimento de solução interpretável. Podemos incorporar facilmente conhecimento prévio como restrições, melhorando o desempenho. A última observação é que o mecanismo subjacente não se aplica apenas à resolução de problemas matemáticos, mas também a outras tarefas que envolvem raciocínio multi-passo. No entanto, há limitações: se houver um grande número de operadores ou constantes, o consumo de memória pode ser alto. E, como mencionado, devido à distribuição de probabilidade desequilibrada em diferentes passos, também é desafiador aplicar a estratégia de busca por feixe. Obrigado e estou aberto a perguntas."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine e sou da Universidade de Maastricht. Apresentarei meu trabalho com Jerry, que trata de um novo conjunto de dados para recuperação de artigos estatutários. Questões legais fazem parte da vida de muitas pessoas, mas a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre seus direitos e processos legais fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem arcar com o custo da assistência de um especialista jurídico ficam desprotegidos ou, pior, são explorados. Nosso trabalho visa preencher a lacuna entre as pessoas e a lei, desenvolvendo um sistema de recuperação eficaz para artigos estatutários. Tal sistema poderia fornecer um serviço gratuito de ajuda jurídica profissional para humanos não especializados.\n\nAntes de mergulharmos na principal contribuição deste trabalho, vamos descrever primeiro o problema da recuperação de artigos estatutários. Dada uma pergunta simples sobre um assunto legal, como \"Quais são os riscos se eu violar o sigilo profissional?\", é necessário um modelo para recuperar todos os artigos estatutários relevantes de um grande corpo de legislação. Essa tarefa de recuperação de informações apresenta seus próprios desafios. Primeiro, lida com dois tipos de linguagem: a linguagem natural comum para as perguntas e a linguagem jurídica complexa para os estatutos. Essa diferença na distribuição da linguagem torna mais difícil para um sistema recuperar candidatos relevantes, pois requer indiretamente um sistema de interpretação inerente que possa traduzir uma pergunta natural para uma pergunta jurídica que corresponda à terminologia dos estatutos. Além disso, a lei estatutária não é uma pilha de artigos independentes que possam ser tratados como uma fonte completa de informação por si só, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção estruturada de disposições legais que têm um significado completo apenas quando consideradas no contexto geral, ou seja, juntamente com as informações complementares de seus artigos vizinhos, os campos e subcampos aos quais pertencem e seu lugar na estrutura da lei. Por fim, os artigos estatutários são em pequenos parágrafos, que geralmente são a unidade típica de recuperação na maioria dos trabalhos de recuperação, enquanto aqui são documentos longos que podem ter até 6.000 palavras.\n\nOs recentes avanços no processamento de linguagem natural (NLP) geraram grande interesse em muitas tarefas legais, como a previsão de julgamentos legais ou a revisão automática de contratos. No entanto, a recuperação de artigos estatutários permaneceu em grande parte intocada devido à falta de grandes conjuntos de dados rotulados de alta qualidade. Neste trabalho, apresentamos um novo conjunto de dados centrado em cidadãos nativos franceses para estudar se um modelo de recuperação pode aproximar a eficiência e confiabilidade de um especialista jurídico na tarefa de recuperação de artigos estatutários. O conjunto de dados de recuperação de artigos estatutários belgas consiste em mais de 1.100 perguntas legais feitas por cidadãos belgas. Essas perguntas abrangem uma ampla gama de tópicos, desde família, habitação, dinheiro até trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos relevantes de um corpus de mais de 26.600 artigos legais de códigos legais belgas.\n\nVamos agora falar sobre como coletamos esse conjunto de dados. Primeiro, compilamos um grande corpus de artigos legais, considerando 32 códigos belgas públicos e extraindo todos os seus artigos, bem como os títulos das seções correspondentes. Em seguida, reunimos perguntas legais com referências a estatutos relevantes. Para isso, estabelecemos uma parceria com um escritório de advocacia belga que recebe cerca de 400 e-mails por ano de cidadãos belgas que solicitam aconselhamento sobre questões legais pessoais. Tivemos a sorte de obter acesso aos seus sites, onde uma equipe de juristas experientes aborda as questões legais mais comuns na Bélgica. Coletamos milhares de perguntas anotadas com categorias, subcategorias e referências legais a estatutos relevantes. Por fim, passamos as referências legais e filtramos as perguntas cujas referências não eram artigos em um dos códigos de lei que consideramos. As referências restantes foram correspondidas e convertidas para os IDs de artigo correspondentes do nosso corpus. Acabamos com 1.108 perguntas, cada uma cuidadosamente rotulada com os IDs dos artigos relevantes de um grande corpus de 22.633 artigos estatutários. Além disso, cada pergunta vem com uma categoria principal e uma concatenação de subcategorias, e cada artigo vem com uma concatenação de seus títulos subsequentes na estrutura da lei. Essas informações extras não são usadas neste trabalho, mas podem ser de interesse para futuras pesquisas sobre recuperação de informações legais ou classificação de textos legais.\n\nVejamos algumas características dos nossos conjuntos de dados. As perguntas têm entre 5 e 44 palavras, com uma mediana de 40 palavras. Os artigos são muito mais longos, com uma mediana de 77 palavras, e 142 deles excedem 1.000 palavras, sendo o mais longo de até 5.790 palavras. Como mencionado anteriormente, as perguntas abrangem uma ampla gama de tópicos, com cerca de 85% delas sendo sobre família, habitação, dinheiro ou justiça, enquanto os 15% restantes dizem respeito à segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois vêm de 32 códigos belgas diferentes que abrangem um grande número de tópicos legais. Aqui está o número total de artigos coletados de cada um desses códigos belgas. Dos 26.633 artigos, apenas 1.612 são referidos como relevantes para pelo menos uma pergunta nos conjuntos de dados, e cerca de 80% desses artigos citados vêm do Código Civil, Código Judicial, Código de Investigação Criminal ou Código Penal. Enquanto isso, 18 dos 32 códigos têm menos de cinco artigos mencionados como relevantes para pelo menos uma pergunta, o que pode ser explicado pelo fato de esses códigos se concentrarem menos em indivíduos e suas preocupações. No geral, o número médio de citações para esses artigos citados é dois, e menos de 25% deles são citados mais de cinco vezes.\n\nUsando nossos conjuntos de dados, avaliamos várias abordagens de recuperação, incluindo arquitetura lexical e densa. Dada uma consulta em um artigo, um modelo lexical atribui uma pontuação à dupla consulta-artigo calculando a soma sobre os termos da consulta dos pesos de cada um desses termos naquele artigo. Experimentamos com as funções de classificação TF-IDF e BM25 padrão. O principal problema com essas abordagens é que elas só podem recuperar artigos que contenham palavras-chave presentes na consulta. Para superar essa limitação, experimentamos com uma arquitetura baseada em rede neural que pode capturar a relação semântica entre consultas e artigos. Usamos um modelo de codificador bidirecional que mapeia consultas e artigos em representações vetoriais densas e calcula uma pontuação de relevância entre uma dupla consulta-artigo pela similaridade de suas incorporações. Essas incorporações geralmente resultam de uma operação de pooling na saída de um modelo de incorporação de palavras.\n\nPrimeiro, estudamos a eficácia de codificadores siameses em um cenário de avaliação zero-shot, o que significa que os modelos de incorporação de palavras pré-treinados são aplicados diretamente, sem nenhum ajuste fino adicional. Experimentamos com codificadores de texto independentes do contexto, como Word2Vec e FastText, e com modelos de incorporação dependentes do contexto, como Roberta e, mais especificamente, Camembert, que é um modelo Roberta em francês. Além disso, treinamos nosso próprio modelo baseado em Camembert nos conjuntos de dados. Observe que, para o treinamento, experimentamos com as duas variantes da arquitetura codificador bidirecional: Siamese, que usa um único modelo de incorporação de palavras que mapeia a consulta e o artigo juntos em um espaço vetorial denso compartilhado, e Two-Tower, que usa dois modelos de incorporação de palavras independentes que codificam a consulta e o artigo separadamente em diferentes espaços de incorporação. Experimentamos com pooling médio, máximo e CLS, bem como produto ponto e cosseno para calcular similaridades.\n\nAqui estão os resultados dos testes nos conjuntos de dados com os métodos lexicais mencionados acima, os codificadores siameses avaliados em um cenário zero-shot no meio e os codificadores ajustados abaixo. No geral, os codificadores ajustados superaram significativamente todos os outros modelos de referência. O modelo Two-Tower melhora sua variante Siamese em recall em 100, mas apresenta desempenho semelhante nas outras métricas. Embora o BM25 tenha apresentado desempenho inferior ao codificador ajustado, sua performance indica que ainda é um modelo de referência forte para recuperação específica de domínio.\n\nEm relação à avaliação zero-shot dos codificadores siameses, descobrimos que o uso direto das incorporações de um modelo Camembert pré-treinado, sem otimização para a tarefa de recuperação de informações, resulta em resultados pobres, o que é consistente com descobertas anteriores. Além disso, observamos que o codificador baseado em Word2Vec superou significativamente os modelos baseados em FastText e BERT, sugerindo que as incorporações de palavras pré-treinadas de nível de palavra podem ser mais adequadas para a tarefa do que as incorporações de nível de caractere ou subpalavra quando usadas diretamente. Embora promissores, esses resultados sugerem uma ampla oportunidade de melhoria em comparação com um especialista de nível de habilidade que pode, eventualmente, recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas.\n\nConcluiremos discutindo duas limitações dos nossos conjuntos de dados. Primeiro, o corpus de artigos é limitado àqueles coletados dos 32 códigos belgas considerados, o que não abrange toda a lei belga, pois artigos de decretos, diretivas e ordenanças estão ausentes. Durante a construção do conjunto de dados, todas as referências a esses artigos não coletados são ignoradas, o que faz com que algumas perguntas acabem com apenas uma fração do número inicial de artigos relevantes. Essa perda de informação implica que a resposta contida nos artigos relevantes restantes pode ser incompleta, embora ainda perfeitamente adequada. Segundo, devemos observar que nem todas as perguntas legais podem ser respondidas apenas com estatutos. Por exemplo, a pergunta \"Posso despejar meus inquilinos se eles fizerem muito barulho?\" pode não ter uma resposta detalhada dentro da lei estatutária que quantifique um limite específico de ruído para o despejo. Em vez disso, o proprietário provavelmente deve confiar mais na jurisprudência e encontrar precedentes semelhantes à sua situação atual, como o inquilino fazer duas festas por semana até as 2h da manhã. Portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos estatutários, e o domínio das menos adequadas ainda precisa ser determinado.\n\nEsperamos que este trabalho desperte o interesse no desenvolvimento de modelos práticos e confiáveis de recuperação de artigos estatutários que possam ajudar a melhorar o acesso à justiça para todos. Você pode conferir nosso artigo no seguinte link. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, temos o prazer de apresentar nosso trabalho sobre vogais, um benchmark independente para testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que nos demos ao trabalho de criar este benchmark? Nos últimos anos, testemunhamos uma explosão de modelos de visão e linguagem baseados em transformadores, pré-treinados em grandes quantidades de pares de imagens e textos. Cada um desses modelos eleva o estado da arte em tarefas de visão e linguagem, como resposta a perguntas visuais, raciocínio de senso comum visual, recuperação de imagens, ancoragem de frases, entre outras. No entanto, recebemos uma mensagem clara: as precisões nesses benchmarks específicos de tarefas estão aumentando constantemente, mas sabemos realmente o que os modelos aprenderam? O que um transformador de visão e linguagem entende ao atribuir uma alta pontuação para combinar esta imagem e esta frase, e uma baixa pontuação para outra? Os modelos de visão e linguagem se concentram no que é correto, ou se concentram em vieses, como mostrado por trabalhos anteriores?\n\nPara lançar mais luz sobre esse aspecto, propomos uma direção mais agnóstica em relação à tarefa e introduzimos vogais, que testam a sensibilidade de modelos de visão e linguagem a fenômenos linguísticos específicos que afetam tanto as modalidades linguística quanto visual. Nosso foco é na existência, pluralidade, contagem, relações espaciais, ações e co-referência de entidades.\n\nMas como testamos se os modelos de visão capturaram esses fenômenos? Adaptamos um método anteriormente aplicado apenas para frases nominais por Ravi Shekhar e colaboradores, e para contagem em trabalhos anteriores nossos. \"Foiling\" basicamente significa que pegamos a legenda de uma imagem e produzimos um \"foil\" alterando a legenda de forma que ela não descreva mais a imagem. Realizamos essas alterações de frase focando em seis peças específicas, como existência, pluralidade, contagem, relações espaciais, ações e co-referência de entidades, onde cada peça pode consistir em um ou mais instrumentos, caso tenhamos encontrado mais de uma maneira interessante de criar instâncias de \"foil\".\n\nPor exemplo, no caso da peça \"ações\", temos dois instrumentos: um em que o verbo da ação é alterado por uma ação diferente, e outro em que os agentes da ação são trocados. Contagem e co-referência também são peças com mais de um instrumento. Criamos esses \"foils\" garantindo que falhem em descrever a imagem, que sejam gramaticalmente corretos e sentenças válidas. Isso não é fácil de fazer porque uma legenda \"foiled\" pode ser menos provável que a original. Embora não seja impossível, é estatisticamente menos provável que plantas cortem um homem do que um homem corte plantas, e grandes modelos de visão e linguagem poderiam captar isso.\n\nPortanto, para obter \"foils\" válidos, tomamos as seguintes ações: primeiro, utilizamos modelos de linguagem fortes para propor \"foils\"; segundo, usamos inferência de linguagem natural (NLI) para filtrar \"foils\" que ainda poderiam descrever a imagem. Ao construir \"foils\", precisamos garantir que falhem em descrever a imagem. Para testar isso automaticamente, aplicamos inferência de linguagem natural com a seguinte lógica: consideramos uma imagem como a premissa e sua legenda como a hipótese implicada. Além disso, consideramos a legenda como a premissa e o \"foil\" como sua hipótese. Se um modelo NLI prever que o \"foil\" contradiz ou é neutro em relação à legenda, tomamos isso como indicador de um \"foil\" válido. Se o NLI prever que o \"foil\" é implicado pela legenda, não pode ser um bom \"foil\", pois, por transitividade, fornecerá uma descrição verdadeira da imagem, e filtramos esses \"foils\".\n\nMas esse procedimento não é perfeito; é apenas um indicador de \"foils\" válidos. Portanto, como terceira medida para gerar \"foils\" válidos, empregamos anotadores humanos para validar os dados usados no valse. Após a filtragem e avaliação humana, temos tantas instâncias de teste quanto descritas nesta tabela. Note que o valse não fornece nenhum dado de treinamento, apenas dados de teste, pois é um benchmark de teste zero-shot. É projetado para aproveitar as capacidades existentes de modelos de visão e linguagem após o pré-treinamento. O ajuste fino apenas permitiria que os modelos explorassem artefatos ou vieses estatísticos nos dados, e sabemos que esses modelos gostam de trapacear e pegar atalhos. Como dissemos, estamos interessados em avaliar quais capacidades os modelos de visão e linguagem têm após o pré-treinamento.\n\nExperimentamos com cinco modelos de visão e linguagem em vogais, a saber: CLIP, Alex, Mert, WiLBERT, WiLBERT-12-in-1 e VisualBERT. Duas de nossas métricas de avaliação mais importantes são a precisão dos modelos na classificação de pares de imagem-sentença em legendas e \"foils\". Talvez mais relevante para este vídeo, apresentaremos nossa métrica mais permissiva, a precisão par a par, que mede se a pontuação de alinhamento imagem-sentença é maior para o par imagem-texto correto do que para seu par \"foiled\". Para mais métricas e resultados, consulte nosso artigo.\n\nOs resultados com precisão par a par são mostrados aqui e são consistentes com os resultados obtidos das outras métricas, indicando que o melhor desempenho zero-shot é alcançado pelo WiLBERT-12-in-1, seguido pelo WiLBERT, Alex, Mert, CLIP e, finalmente, VisualBERT. É notável como os instrumentos centrados em objetos individuais, como existência e frases nominais, são quase resolvidos pelo WiLBERT-12-in-1, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhuma das peças restantes pode ser resolvida de forma confiável em nossos cenários de \"foiling\" adversarial.\n\nVemos, a partir dos instrumentos de pluralidade e contagem, que os modelos de visão e linguagem têm dificuldade em distinguir referências a objetos únicos em comparação com múltiplos objetos, ou em contá-los em uma imagem. A peça de relação mostra que eles têm dificuldades em classificar corretamente uma relação espacial nomeada entre objetos em uma imagem. Eles também têm dificuldade em distinguir ações e identificar seus participantes, mesmo quando apoiados por vieses de plausibilidade, como visto na peça de ações. A partir da peça de referência, descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronomes também é difícil para modelos de visão e linguagem.\n\nComo um controle de sanidade e porque é um experimento interessante, também benchmarkamos dois modelos de texto apenas, GPT-1 e GPT-2, para avaliar se o valse é solucionável por esses modelos unimodais, calculando a perplexidade da legenda correta e a \"foiled\", sem imagem, e prevendo a entrada com a menor perplexidade. Se a perplexidade for maior para o \"foil\", tomamos isso como indicação de que a legenda \"foiled\" pode sofrer de viés de plausibilidade ou outros vieses linguísticos. É interessante notar que, em alguns casos, os modelos de texto GPT capturaram a plausibilidade do mundo melhor do que os modelos de visão e linguagem.\n\nPara resumir, o valse é um benchmark que usa a lente de construções linguísticas para ajudar a comunidade a melhorar os modelos de visão e linguagem, testando rigorosamente suas capacidades de ancoragem visual. Nossos experimentos mostram que os modelos de visão e linguagem identificam bem objetos nomeados e sua presença em imagens, como mostrado pela peça de existência, mas lutam para ancorar sua interdependência e relacionamentos em cenas visuais quando forçados a respeitar indicadores linguísticos.\n\nGostaríamos muito de incentivar a comunidade a usar vogais para medir o progresso em direção à ancoragem de linguagem com modelos de visão e linguagem. E mais ainda, as vogais poderiam ser usadas como uma avaliação indireta de conjuntos de dados, já que os modelos poderiam ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados pelo valse. Se estiver interessado, confira os dados do valse no GitHub. E se tiver alguma dúvida, não hesite em entrar em contato conosco."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamisura, da Universidade de Tóquio. Apresentarei um artigo intitulado \"O En Sum: Um Conjunto de Dados em Grande Escala para Sumarização Automática de Notas de Lançamento\".\n\nPrimeiramente, vou introduzir a notação Li automática que estamos desenvolvendo nesta pesquisa. As notas de lançamento são documentos técnicos que resumem as mudanças distribuídas com cada versão de um produto de software. A imagem mostra as notas de lançamento para a versão 2.6.4 da biblioteca Bujs. Essas notas desempenham um papel importante no desenvolvimento de código aberto, mas são demoradas de preparar manualmente, portanto, seria muito útil poder gerá-las automaticamente com alta qualidade.\n\nVou me referir a duas pesquisas anteriores sobre geração automática de notas de lista. A primeira é um sistema chamado Alena, lançado em 2014. Ele adota uma abordagem baseada em regras, por exemplo, usando um extrator de mudanças para extrair diferenças centrais, mudanças na biblioteca e mudanças no documento a partir das diferenças entre doenças, e finalmente combinando-as. A característica mais notável deste sistema é o extrator de problemas na parte superior direita, que deve ser vinculado ao Jira, o ecossistema de problemas, e pode ser aplicado apenas a projetos que usam o Jira, ou seja, não pode ser usado para muitos projetos no GitHub.\n\nA segunda é Grif, anunciada recentemente em 2020. Está disponível na internet e pode ser armazenada via pi. Este sistema possui um modelo simples de classificação de texto baseado em execução e produz uma das cinco categorias de problemas, como recursos ou correções de bugs, para cada mensagem de commit de entrada. A imagem é um exemplo de uso que retorna uma correção de recursos ou bug fix. O conjunto de dados de treinamento é relativamente pequeno, com cerca de 5000 entradas, e será apresentado nos experimentos descritos abaixo. O desempenho do modelo de classificação de texto não é alto.\n\nApresento duas pesquisas relacionadas, mas existem problemas de aplicabilidade limitada e recursos de dados escassos. Nosso artigo resolve esses dois problemas e gera automaticamente recursos de alta qualidade. Para o problema de aplicabilidade limitada, propomos um método de sumarização de classificadores de alta qualidade usando apenas mensagens de commit como entrada. Este método proposto pode ser usado para todos os repositórios em inglês. Para o segundo problema de recursos escassos, construímos nosso próprio conjunto de dados, consistindo em cerca de 82.000 peças de dados, corrigindo dados de repositórios públicos do GitHub usando a API do Git.\n\nA seguir, descrevo nossos dados. Aqui está um exemplo de atualização: o lado esquerdo é uma mensagem de commit e o lado direito são as notas de lançamento. As notas de lançamento são classificadas em níveis como melhorias de recursos, etc. Configuramos uma tarefa que leva as mensagens de commit como entrada e produz as notas de lançamento. Isso pode ser considerado uma tarefa de sumarização. Predefinimos quatro níveis: recursos implementados, correções de bugs, depreciações, remoções e mudanças quebradas. Eles foram definidos com base em usos anteriores e outros fatores. Há uma nota na parte inferior direita, extraída da lista de nós mostrada na parte inferior esquerda. Neste momento, é necessário detectar os quatro níveis definidos anteriormente, mas os níveis nem sempre são consistentes com cada um. Por exemplo, o nível de melhoria inclui melhorias, aprimoramentos, otimizações, etc. Preparamos uma lista de vocabulário de níveis de estudo para cada uma dessas variações de notação e a usamos para detectar a classe de nós de risco e corrigir o texto que se segue como a frase de risco para a classe.\n\nA seguir, uma mensagem de commit. As mensagens de commit não estão vinculadas a cada versão, como mostrado na imagem abaixo. Se a versão de lançamento atual for 2.5.19, precisamos identificar a versão de lançamento anterior, 2.5.18, e obtê-la. Isso é um pouco tedioso e não é suficiente apenas obter uma lista de lançamentos e olhar para o antes e depois. Criamos uma cola heurística para obter as versões anterior e posterior.\n\nNo final, analisamos 7200 repositórios e 82.000 peças de dados corrigidos. O número médio de tokens razoáveis é 63, o que é bastante alto para uma tarefa de sumarização. O número de tokens únicos também é bastante rico, com 88.300. Isso se deve ao grande número de classes e nomes de métodos únicos encontrados no repositório.\n\nA seguir, explicarei o método proposto. O modelo de sumarização extrativa e abstrativa em cruz consiste em dois módulos neurais: um classificador usando Bot ou Code Bot e um gerador usando But First G. O G usa um classificador para classificar cada mensagem de commit em cinco classes de nós base: recursos, melhorias, correções de bugs, depreciações e outros. As mensagens de commit classificadas como outros são descartadas. Em seguida, ela aplica um gerador aos quatro documentos de borrão independentemente e gera uma nota de lançamento para cada classe. Nesta tarefa, as correspondências diretas entre as mensagens de commit e as notas de lançamento não são conhecidas. Portanto, para treinar o classificador, atribuímos variáveis pseudo a cada mensagem de commit de entrada usando os primeiros 10 caracteres de cada mensagem de commit.\n\nModelamos a abordagem de sumarização abstrativa por classe com dois métodos definidos. O primeiro modelo, que chamamos de GS Single, consiste em uma única rede de sexo e gera um único texto longo. O texto de saída pode ser dividido em segmentos de arquivo de classe com base em símbolos de extremidade específicos da classe. O segundo método, que chamamos de She's Much, consiste em quatro redes diferentes, cada uma das quais corresponde a uma das classes de nós.\n\nVamos agora explicar a experiência. Cinco métodos foram comparados: GS, She's Single, She's Much, Cluster e um estudo anterior, Grif, em relação à precisão. Em alguns casos, essas notas são saída em múltiplas frases, e como é difícil corrigir o número de frases em zero, elas são combinadas com espaços e tratadas como uma única frase longa. O azul é penalizado quando o sistema produz uma frase curta. Essa penalidade resulta em um valor de ROUGE mais baixo nos resultados dos experimentos descritos a seguir. Finalmente, também carregamos uma especificidade, pois o azul e o azul não podem ser carregados se as notas de lista estiverem vazias. Uma alta especificidade significa que o modelo produz corretamente um texto vazio em casos em que as notas de lançamento são assumidas como vazias.\n\nAqui estão os resultados. Como o conjunto de dados contém análises de e-mail, etc., também avaliamos o conjunto de dados limpo, que exclui esses elementos. G e Gs alcançaram pontuações de perda mais de 10 pontos mais altas que o baseline, especialmente no conjunto de testes coreano, onde a diferença de pontuação entre o método proposto e o baseline saltou para mais de 20 pontos. Esses resultados indicam que Gs e Gs são significativamente eficazes. Gs obteve uma pontuação de perda melhor que GAS, sugerindo que combinar um classificador com um gerador é eficaz no treinamento do classificador usando servidores. A alta cobertura de GS pode ser alcançada adequadamente porque o classificador pode se concentrar na seleção de mensagens de commit relevantes para cada classe. She's Much tende a ter um desempenho melhor que She's Single, sugerindo que também é eficaz desenvolver modelos de sumarização construtiva diferentes de forma independente para cada classe de nó.\n\nAqui está uma análise de erros. Os métodos She tendem a produzir frases mais curtas do que a frase de referência humana, pois, na figura à direita, a frase de referência tem três ou quatro frases, enquanto CSS tem apenas uma. A razão para essa relutância do modelo é que, nos dados de treinamento, apenas 30% das frases estão presentes no nível de recursos e 40% no nível de melhorias. Além disso, os métodos CS não podem gerar notas de lista precisas sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito confusa, e a frase completa não pode ser gerada sem diferenças em relação aos correspondentes privilégios ou problemas anteriores. O exemplo abaixo mostra que as duas mensagens de commit na entrada estão relacionadas e devem ser combinadas em uma única frase, mas ele falha em fazê-lo.\n\nConclusão: Construímos um novo conjunto de dados para geração automática de notas de lançamento. Também formulamos a tarefa de inserir mensagens de commit e resumir de forma que seja aplicável a todos os projetos escritos em inglês. Nossos experimentos mostram que o método proposto gera menos ruído e tem uma cobertura mais alta que o baseline. Por favor, confira nossos dados no GitHub. Obrigado."}
