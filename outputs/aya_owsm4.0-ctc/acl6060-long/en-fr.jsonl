{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Safari, et je vais présenter notre travail sur l'enrichissement de données tabulaires à l'aide d'architectures de transformateurs affinés. Les scientifiques analysent généralement les données en se concentrant principalement sur la manipulation des caractéristiques existantes, mais parfois, ces caractéristiques sont limitées. La génération de caractéristiques à partir d'une autre source de données peut ajouter une information substantielle. Notre objectif de recherche est l'enrichissement automatique de données tabulaires à l'aide de sources de texte externes.\n\nSupposons que nous disposions d'un jeu de données tabulaire et d'une base de connaissances. Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles caractéristiques à partir du texte libre de la base de connaissances. Notre cadre, Fest, est exactement ce processus automatique. Permettez-moi de vous montrer un exemple avec un jeu de données universitaire.\n\nDans cet exemple, le jeu de données contient des informations sur les universités, et l'objectif est de classer les universités en deux catégories : universités mal classées et universités bien classées. En tant que base de connaissances, nous utilisons Wikipédia. La première étape est la liaison d'entités, où chaque entité, ici le nom de l'université, est liée à une entité dans la base de connaissances. Ensuite, le texte des entités de la base de connaissances est extrait et ajouté au jeu de données. Dans ce cas, le texte est l'extrait de la page Wikipédia.\n\nNous devons maintenant générer ou extraire des caractéristiques à partir du texte récupéré. Nous avons donc besoin d'une phase d'extraction de caractéristiques qui inclut l'analyse de texte, ce qui constitue la principale nouveauté de cet article. J'y reviendrai en détail dans les diapositives suivantes.\n\nAprès la phase d'extraction de caractéristiques, vient la phase de génération de caractéristiques, où nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques. Nous générons d'abord un nombre de nouvelles caractéristiques égal au nombre de classes du jeu de données d'origine. Dans cet exemple, le jeu de données d'origine a deux classes, nous générons donc deux nouvelles caractéristiques. Si le jeu de données a cinq classes, nous en générons cinq. Chaque caractéristique représente la probabilité pour chaque classe.\n\nPour analyser le texte, nous utilisons les modèles linguistiques transformateurs les plus performants, tels que BERT, GPT-X, etc. Cependant, il est peu probable que nous puissions entraîner un modèle linguistique avec les jeux de données d'entrée. Une approche naïve serait donc l'affinage pour une tâche cible. Lors de la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle linguistique pré-entraîné, l'affiner sur le jeu de données cible, et ainsi classer le texte en catégories (faible ou élevé). Nous obtenons alors la sortie du modèle linguistique, qui est la probabilité pour chaque classe, et l'utilisons comme nouvelles caractéristiques.\n\nLe problème avec cette approche est que les jeux de données peuvent contenir peu d'entités distinctes. Dans nos expériences, près de la moitié des jeux de données contenaient moins de 400 échantillons, et le plus petit jeu de données n'en contenait que 35 dans son ensemble d'entraînement initial. Affiner un modèle linguistique sur de tels jeux de données serait inefficace.\n\nNous pouvons cependant utiliser des connaissances préalables sur des jeux de données pré-analysés. Lorsque nous appliquons Fest sur plusieurs jeux de données, nous pouvons utiliser les n-1 jeux de données pour recueillir des informations et les utiliser lors de l'analyse du n-ième jeu de données. Nous proposons d'ajouter une phase d'affinage préliminaire multi-tâches avant l'affinage pour la tâche cible. Nous affinons ainsi le modèle linguistique sur les n-1 jeux de données, puis exécutons une autre phase d'affinage pour la tâche cible.\n\nL'état de l'art en affinage multi-tâches, appelé TDNN (Transformer-based DNN), maintient une tête pour chaque tâche dans l'ensemble d'entraînement. Dans cet exemple, avec quatre tâches, TDNN maintient quatre têtes. Il échantillonne un lot aléatoire à partir de l'ensemble d'entraînement, et si le lot appartient à une tâche de classification de phrases, il exécute une passe avant et arrière à travers la première tête. Si le lot appartient à une tâche de classement par paires, il effectue une passe avant et arrière à travers la dernière tête.\n\nDans notre scénario, le nombre de classes varie d'un jeu de données tabulaire à l'autre, il y a donc de nombreuses tâches. TDNN maintient donc un nombre de classes de têtes de sortie et des couches supplémentaires. De plus, TDNN doit initialement ajouter de nouvelles têtes pour un nouveau jeu de données avec une nouvelle tâche.\n\nNotre approche, appelée affinage de reformulation de tâche, reformule chaque jeu de données en un problème de classification de phrases binaires, qui est une tâche à deux classes. Voici un exemple : nous avons notre jeu de données d'entrée composé d'entités, de caractéristiques textuelles et de classes. Nous reformulons la tâche de classification du texte en faible ou élevé en une tâche de classification du texte et de la classe en vrai ou faux. En d'autres termes, nous entraînons le modèle linguistique à classer un résumé et une classe en fonction de leur appartenance ou non. Le vecteur d'étiquettes dans ce cas reste toujours composé de deux classes.\n\nCeci est l'algorithme de notre approche d'affinage reformulé. Revenons au cadre complet : le jeu de données est alimenté dans Fest, qui exécute la phase de liaison, extrait le texte de la base de connaissances (l'extrait de la page Wikipédia dans cet exemple), reformule la tâche en une tâche de classification de paires de phrases, applique le modèle linguistique à la nouvelle tâche, et obtient la probabilité pour chaque classe. Notez que le modèle linguistique a déjà été affiné sur n-1 jeux de données grâce à un affinage multi-tâches préliminaire.\n\nNous utilisons ensuite le vecteur de sortie du modèle linguistique comme nouvelle caractéristique générée, égale au nombre de classes. Pour évaluer notre cadre, nous utilisons 17 jeux de données de classification tabulaire, variés en taille, en équilibre de domaine et en performance initiale. Nous utilisons Wikipédia comme base de connaissances. Nous concevons notre expérience comme une évaluation en laissant de côté un jeu de données, où nous entraînons Fest sur 16 jeux de données et l'appliquons au 17e. Nous divisons également chaque jeu de données en quatre parties et effectuons une validation croisée.\n\nNous générons les nouvelles caractéristiques et les évaluons à l'aide de cinq classifieurs. Voici les résultats : nous comparons notre cadre à l'affinage pour la tâche cible, à l'affinage multi-tâches préliminaire, et notre affinage reformulé obtient les meilleurs résultats. Notre approche atteint une amélioration de 6% par rapport à l'affinage pour la tâche cible, tandis que TDNN n'atteint qu'une amélioration de 2%.\n\nSur les petits jeux de données, la performance de TDNN diminue, et l'amélioration de la phase d'affinage multi-tâches préliminaire chute à 1,5%, mais notre approche atteint une amélioration de 11%. En résumé, Fest permet un enrichissement rapide à partir de 35 échantillons dans notre expérience. Il utilise une seule architecture pour toutes les tâches et les jeux de données, et maintient la tête du modèle. Il ajoute une phase de reformulation, augmentant l'ensemble d'entraînement et nécessitant une valeur cible sémantiquement significative pour alimenter le modèle linguistique et résoudre le problème de classification de phrases binaires. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à tous. Aujourd'hui, je vais présenter nos travaux de recherche sur l'apprentissage de la raisonnement déductif, en particulier la résolution de problèmes de mathématiques complexes en tant qu'extraction de régions. Je suis de l'AI Lab de Biance et ce travail est une collaboration avec Che de l'Université du Texas à Austin et Wedu de SUDD.\n\nTout d'abord, je voudrais aborder notre motivation pour le raisonnement. Ici, nous montrons des exemples où le raisonnement multi-étapes est bénéfique. Cette figure, extraite de notre article, illustre l'utilisation de l'inférence pour résoudre des problèmes mathématiques dans un scénario d'apprentissage futur. À gauche, on voit que fournir uniquement des questions et des réponses peut ne pas donner les bonnes réponses. Mais en ajoutant une description du raisonnement, le modèle peut prédire la description et fournir une prédiction correcte. Il est donc avantageux d'avoir un raisonnement multi-étapes interprétable en sortie. Nous considérons également que les problèmes mathématiques sont une application directe pour évaluer ces capacités de raisonnement.\n\nDans notre configuration de problème, étant données les questions, nous devons obtenir des réponses numériques et, dans nos ensembles de données, nous avons également les expressions mathématiques menant à ces réponses. Nous faisons certaines hypothèses, comme dans les travaux précédents : la précision des quantités est connue et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentiation. De plus, les opérateurs complexes peuvent être décomposés en ces opérateurs de base.\n\nLes travaux antérieurs sur la résolution de problèmes mathématiques peuvent être classés en deux catégories : les modèles séquence-séquence et les modèles séquence-arbre. Le modèle séquence-séquence traditionnel convertit l'expression en une séquence spécifique pour la génération, ce qui est facile à mettre en œuvre et peut généraliser de nombreux problèmes complexes. Cependant, sa performance est généralement inférieure à celle des modèles structurés et il manque d'interprétabilité. Cette approche reste néanmoins populaire grâce au modèle Transformer.\n\nDans les modèles basés sur des arbres, nous structurons les expressions sous forme d'arbre et suivons une traversée préfixe pour la génération. Nous générons les opérateurs jusqu'à atteindre les feuilles, qui sont les quantités. Cela donne une structure d'arbre binaire, mais de manière contre-intuitive, nous générons d'abord les opérateurs puis les quantités à la fin. De plus, il y a des calculs redondants, comme l'expression « a fois 3 plus 3 » générée deux fois, alors qu'il faudrait réutiliser le résultat.\n\nNotre approche proposée vise à résoudre ces problèmes de manière progressive et interprétable. Par exemple, à l'étape 2, nous obtenons les diviseurs 27 et pouvons référencer la question originale pour trouver le contenu pertinent. À l'étape 3, nous obtenons le quotient, et après ces étapes, nous réutilisons le résultat de l'étape 2 pour l'étape 4, obtenant ainsi le dividende. Nous générons l'expression complète directement, plutôt que des opérateurs ou quantités isolés, ce qui améliore la précision.\n\nDans notre système déductif, nous commençons par un ensemble de quantités présentes dans les questions, incluant des constantes comme état initial. L'expression est représentée par eij, où nous appliquons l'opérateur de qi à qj. Cette expression est dirigée, donc nous avons aussi une opération de soustraction inverse pour l'opposé. C'est similaire à l'extraction de relations. À chaque étape, nous appliquons l'opérateur entre qi et qj, obtenant une nouvelle expression que nous ajoutons à l'état suivant pour former une nouvelle quantité. Ces diapositives visualisent l'évolution des états, avec l'ajout d'expressions.\n\nPour la mise en œuvre du modèle, nous utilisons d'abord un modèle pré-entraîné comme BERT ou RoBERTa, puis nous encodons la phrase pour obtenir les représentations des quantités. Ensuite, nous effectuons l'inférence. Par exemple, pour obtenir la représentation de q1 divisé par q2 puis multiplié par q3, nous concaténons d'abord q1 et q2, appliquons un réseau de neurones feed-forward paramétré par l'opérateur, et obtenons la représentation de l'expression.\n\nLors de l'inférence, nous pouvons obtenir des expressions incorrectes. Le nombre total d'expressions possibles est égal à trois fois le nombre d'opérateurs. Nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche. Par exemple, si une expression n'est pas autorisée, nous l'éliminons. À l'étape 2, le processus est similaire, mais avec une quantité supplémentaire issue de l'expression calculée précédemment.\n\nLa procédure d'entraînement ressemble à celle d'un modèle séquence-séquence, optimisant la perte à chaque étape. Nous utilisons aussi τ pour représenter la terminaison du processus de génération. L'espace ici est différent car il varie à chaque étape, contrairement au modèle séquence-séquence traditionnel où il est fixe. Nous pouvons aussi imposer des contraintes à partir de connaissances préalables.\n\nNous avons mené des expériences sur les ensembles de données de problèmes mathématiques couramment utilisés : MWPS Method3k MathQA et SWAM. Nos résultats, comparés aux approches précédentes, montrent que notre meilleur modèle, RoBERTa Deductive Reasoner, surpasse les approches précédentes sans utiliser de recherche par faisceau. Les modèles basés sur des arbres sont souvent les meilleurs.\n\nCependant, les scores absolus sur MathQA ou SWAM ne sont pas très élevés. Nous avons donc analysé plus en détail les résultats sur SWAM, un ensemble de données difficile car l'auteur a ajouté manuellement des informations pour confondre les modèles NLP, comme des quantités supplémentaires. Dans nos prédictions, nous trouvons des valeurs intermédiaires négatives, par exemple, dans des questions demandant le nombre de pommes que possède Jake, avec des informations supplémentaires comme « 17 pêchers en moins que Stephen qui en a 8 ». Notre modèle prédit des valeurs négatives, et nous observons que ces expressions ont des scores similaires.\n\nNous avons amélioré cela en limitant l'espace de recherche en éliminant les résultats négatifs, ce qui corrige les réponses. Cette contrainte améliore significativement certains modèles, comme BERT (+7 points) et RoBERTa (+2 points). Les meilleurs modèles ont de meilleures capacités de compréhension du langage.\n\nNous avons également analysé la difficulté de ces ensembles de données en considérant le nombre de quantités non utilisées comme information pertinente. Le pourcentage d'échantillons avec quantités non utilisées est le plus élevé dans SWAM. La performance globale sur ces échantillons est plus faible que la performance globale, ce qui montre la difficulté de ces cas.\n\nEnfin, nous voulons montrer l'interprétabilité à travers un exemple. Notre modèle fait une prédiction incorrecte à la première étape, et nous pouvons corréler l'expression avec la phrase. Nous pensons que cette phrase peut induire le modèle en erreur. En révisant la phrase pour être plus précise, comme « Le nombre de pommiers est 5 de moins que celui des pommiers », nous aidons le modèle à faire la bonne prédiction. Cette étude démontre comment les prédictions interprétables aident à comprendre le comportement du modèle.\n\nEn conclusion, notre modèle est efficace, fournit une procédure de résolution interprétable et peut intégrer facilement des connaissances préalables comme contraintes pour améliorer la performance. De plus, ce mécanisme s'applique non seulement à la résolution de problèmes mathématiques, mais aussi à d'autres tâches nécessitant un raisonnement multi-étapes. Cependant, nous avons des limitations : une grande quantité d'opérateurs ou de constantes peut entraîner une forte consommation de mémoire. De plus, la distribution de probabilité déséquilibrée entre les étapes rend l'application d'une stratégie de recherche par faisceau difficile. Merci pour votre attention, n'hésitez pas à poser des questions."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht. Je vais présenter mon travail sur John avec Jerry, qui concerne un nouveau jeu de données pour la récupération d'articles législatifs. Les questions juridiques font partie intégrante de la vie de nombreuses personnes, mais la majorité des citoyens ont peu ou pas de connaissances sur leurs droits et les processus juridiques fondamentaux. En conséquence, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'assistance coûteuse d'un expert juridique restent non protégés ou, pire, sont exploités. Ce travail vise à combler le fossé entre les individus et la loi en développant un système de récupération efficace pour les articles législatifs. Un tel système pourrait offrir un service d'aide juridique professionnelle gratuite aux personnes non qualifiées.\n\nAvant d'aborder la contribution principale de ce travail, décrivons d'abord le problème de la récupération d'articles législatifs. Étant donné une question simple sur un sujet juridique, comme « Quels risques encours-je si je viole le secret professionnel ? », un modèle est requis pour récupérer tous les articles législatifs pertinents à partir d'un vaste corpus de législation. Cette tâche de récupération d'informations présente ses propres défis. Premièrement, elle implique deux types de langage : le langage naturel courant pour les questions et le langage juridique complexe pour les statuts. Cette différence dans la distribution des langues rend plus difficile pour un système de récupérer des candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent capable de traduire une question naturelle en une question juridique qui correspond à la terminologie des statuts.\n\nDe plus, le droit législatif n'est pas une simple collection d'articles indépendants qui peuvent être traités comme une source d'information complète en eux-mêmes, comme les actualités ou les recettes, par exemple. Au contraire, il s'agit d'une collection structurée de dispositions légales qui n'ont de sens complet que lorsqu'elles sont considérées dans leur contexte global, c'est-à-dire avec les informations complémentaires de leurs articles voisins, des champs et sous-champs auxquels ils appartiennent, et leur place dans la structure de la loi. Enfin, les articles législatifs sont généralement de petits paragraphes, qui sont habituellement l'unité de récupération typique dans la plupart des travaux de récupération, alors qu'ici, ils sont des documents longs pouvant aller jusqu'à 6 000 mots.\n\nLes récentes avancées en traitement automatique du langage naturel (TALN) ont suscité un grand intérêt pour de nombreuses tâches juridiques, telles que la prédiction de jugements juridiques ou la révision automatique de contrats. Cependant, la récupération d'articles législatifs est restée en grande partie en retrait en raison du manque de jeux de données de grande qualité et de grande taille. Dans ce travail, nous présentons un nouveau jeu de données centré sur les citoyens francophones pour étudier si un modèle de récupération peut approcher l'efficacité et la fiabilité d'un expert juridique pour la tâche de récupération d'articles législatifs.\n\nLe jeu de données belge sur la récupération d'articles législatifs comprend plus de 1 100 questions juridiques posées par des citoyens belges. Ces questions couvrent un large éventail de sujets, allant du logement, de la famille, de l'argent au travail et à la sécurité sociale. Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références aux articles pertinents à partir d'un corpus de plus de vingt-six mille articles juridiques belges.\n\nPassons maintenant à la description de la collecte de ce jeu de données. Nous avons commencé par compiler un grand corpus d'articles juridiques. Nous avons considéré 32 codes belges publics et extrait tous leurs articles ainsi que les titres de section correspondants. Ensuite, nous avons rassemblé des questions juridiques avec des références aux statuts pertinents. Pour ce faire, nous avons collaboré avec un cabinet d'avocats belge qui reçoit chaque année environ 400 courriels de citoyens belges demandant des conseils sur des problèmes juridiques personnels. Nous avons eu la chance d'obtenir accès à leur site web où leur équipe de juristes expérimentés aborde les problèmes juridiques les plus courants en Belgique. Nous avons collecté des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux statuts pertinents.\n\nEnfin, nous avons filtré les références juridiques et exclu les questions dont les références n'étaient pas des articles de l'un des codes de loi que nous avons considérés. Les références restantes ont été mises en correspondance et converties en identifiants d'article correspondants à partir de notre corpus. Nous avons ainsi obtenu 1 108 questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents à partir d'un grand corpus de 22 633 articles législatifs. En outre, chaque question est accompagnée d'une catégorie principale et d'une concaténation de sous-catégories, et chaque article est accompagné d'une concaténation de ses titres subséquents dans la structure de la loi. Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient être utiles pour des recherches futures sur la récupération d'informations juridiques ou la classification de textes juridiques.\n\nExaminons maintenant certaines caractéristiques de nos jeux de données. Les questions vont de 5 à 44 mots, avec une longueur médiane de 40 mots. Les articles sont beaucoup plus longs, avec une longueur médiane de 77 mots, et 142 d'entre eux dépassent 1 000 mots, le plus long atteignant 5 790 mots. Comme mentionné précédemment, les questions couvrent un large éventail de sujets, avec environ 85 % d'entre elles portant sur la famille, le logement, l'argent ou la justice, tandis que les 15 % restants concernent la sécurité sociale, les étrangers ou le travail. Les articles sont également très divers, car ils proviennent de 32 codes belges différents couvrant un grand nombre de sujets juridiques.\n\nVoici le nombre total d'articles collectés à partir de chacun de ces codes belges. Sur les 22 633 articles, seuls 1 612 sont considérés comme pertinents pour au moins une question du jeu de données, et environ 80 % de ces articles cités proviennent du Code civil, du Code judiciaire, du Code d'instruction criminelle ou du Code pénal. Dix-huit des 32 codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question, ce qui peut s'expliquer par le fait que ces codes se concentrent moins sur les individus et leurs préoccupations. Globalement, le nombre médian de citations pour ces articles cités est de deux, et moins de 25 % d'entre eux sont cités plus de cinq fois.\n\nEn utilisant nos jeux de données, nous avons évalué plusieurs approches de récupération, y compris les architectures lexicales et denses. Étant donné une requête et un article, un modèle lexical attribue un score à la paire requête-article en calculant la somme sur les termes de la requête des poids de chacun de ces termes dans l'article. Nous avons expérimenté avec les fonctions de classement TF-IDF et BM25 standard. Le principal problème de ces approches est qu'elles ne peuvent récupérer que les articles contenant des mots-clés présents dans la requête.\n\nPour surmonter cette limitation, nous avons expérimenté avec une architecture basée sur les réseaux neuronaux qui peut capturer les relations sémantiques entre les requêtes et les articles. Nous utilisons un modèle d'encodeur bidirectionnel qui mappe les requêtes et les articles en représentations vectorielles denses et calcule un score de pertinence entre une paire requête-article en fonction de la similarité de leurs embeddings. Ces embeddings résultent généralement d'une opération de regroupement sur la sortie d'un modèle d'embedding de mots.\n\nNous avons d'abord étudié l'efficacité des encodeurs Siamese dans un contexte d'évaluation zéro-shot, ce qui signifie que les modèles d'embedding de mots pré-entraînés sont utilisés directement, sans aucun réglage supplémentaire. Nous avons expérimenté avec des encodeurs de texte indépendants du contexte, à savoir Word2Vec et FastText, et des modèles d'embedding dépendants du contexte, à savoir RoBERTa et, plus spécifiquement, CamemBERT, qui est un modèle RoBERTa en français. Nous avons également entraîné notre propre modèle basé sur CamemBERT sur l'ensemble des données. Notez que pour l'entraînement, nous avons expérimenté avec les deux variantes de l'architecture encodeur bidirectionnel : Siamese, qui utilise un seul modèle d'embedding de mots qui mappe la requête et l'article ensemble dans un espace vectoriel dense partagé, et Two-Tower, qui utilise deux modèles d'embedding de mots indépendants qui codent la requête et l'article séparément dans des espaces d'embedding différents.\n\nNous avons expérimenté avec les méthodes de regroupement moyenne, max et CLS, ainsi qu'avec le produit scalaire et la cosinus pour calculer les similarités. Voici les résultats des baselines sur les ensembles de test avec les méthodes lexicales mentionnées précédemment, les encodeurs bidirectionnels Siamese évalués dans un contexte zéro-shot au milieu, et les encodeurs bidirectionnels réglés en dessous. Globalement, les encodeurs bidirectionnels réglés surclassent significativement toutes les autres baselines. Le modèle Two-Tower améliore son variant Siamese en termes de rappel à 100, mais performe de manière similaire sur les autres métriques. Bien que BM25 ait sous-performé l'encodeur bidirectionnel réglé de manière significative, sa performance indique qu'il reste une baseline solide pour la récupération spécifique au domaine.\n\nEn ce qui concerne l'évaluation zéro-shot des encodeurs bidirectionnels Siamese, nous constatons que l'utilisation directe des embeddings d'un modèle CamemBERT pré-entraîné, sans optimisation pour la tâche de récupération d'informations, donne des résultats médiocres, ce qui est cohérent avec les découvertes précédentes. De plus, nous observons que l'encodeur basé sur Word2Vec a significativement surclassé les modèles basés sur FastText et BERT, suggérant que les embeddings de mots de niveau pré-entraînés peuvent être plus appropriés pour cette tâche que les embeddings de caractères ou de sous-mots lorsqu'ils sont utilisés directement.\n\nBien que prometteurs, ces résultats suggèrent qu'il y a encore beaucoup de place pour l'amélioration par rapport à un expert de niveau compétence qui peut éventuellement récupérer tous les articles pertinents pour n'importe quelle question et obtenir ainsi des scores parfaits.\n\nConcluons en discutant de deux limitations de nos jeux de données. Premièrement, le corpus d'articles est limité à ceux collectés à partir des 32 codes belges considérés, ce qui ne couvre pas l'ensemble de la loi belge, car les articles des décrets, directives et ordonnances manquent. Pendant la construction du jeu de données, toutes les références à ces articles non collectés ont été ignorées, ce qui fait que certaines questions se retrouvent avec seulement une fraction du nombre initial d'articles pertinents. Cette perte d'information implique que la réponse contenue dans les articles pertinents restants peut être incomplète, bien qu'encore tout à fait appropriée.\n\nDeuxièmement, il est important de noter que toutes les questions juridiques ne peuvent pas être répondues uniquement avec des statuts. Par exemple, la question « Puis-je expulser mes locataires s'ils font trop de bruit ? » peut ne pas avoir de réponse détaillée dans la loi législative qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est possible. Au contraire, le propriétaire devrait probablement se fier davantage à la jurisprudence et trouver des précédents similaires à sa situation actuelle, par exemple, le locataire organise deux fêtes par semaine jusqu'à 2 heures du matin. Ainsi, certaines questions sont mieux adaptées que d'autres à la tâche de récupération d'articles législatifs, et le domaine de celles moins adaptées reste à déterminer.\n\nNous espérons que ce travail suscitera un intérêt pour le développement de modèles de récupération d'articles législatifs pratiques et fiables qui pourront aider à améliorer l'accès à la justice pour tous. Vous pouvez consulter notre article à l'adresse suivante : [insérer les liens]. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, nous sommes ravis de présenter notre travail sur les voyelles, un benchmark indépendant destiné à tester les modèles de vision et de langage avec des phénomènes linguistiques spécifiques. Pourquoi avons-nous entrepris la création de ce benchmark ? Ces dernières années, nous avons assisté à une explosion des modèles de vision et de langage basés sur les transformateurs, pré-entraînés sur de grandes quantités de paires image-texte. Chacun de ces modèles améliore l'état de l'art dans des tâches de vision et de langage telles que la réponse à des questions visuelles, le raisonnement de bon sens visuel, la récupération d'images, l'ancrage de phrases, etc. Nous avons reçu un message clair : les précisions sur ces benchmarks spécifiques augmentent régulièrement, mais savons-nous réellement ce que les modèles ont appris ? Qu'est-ce qu'un transformateur de vision et de langage comprend lorsqu'il attribue un score élevé à cette image et cette phrase pour les associer, et un score faible pour une autre ? Les modèles de vision et de langage se concentrent-ils sur les bons éléments, ou sont-ils influencés par des biais, comme le montrent des travaux antérieurs ?\n\nPour apporter plus de clarté à cet aspect, nous proposons une approche plus agnostique en matière de tâches et introduisons les voyelles, qui testent la sensibilité des modèles de vision et de langage à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistique et visuelle. Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence des entités.\n\nMais comment tester si les modèles de vision ont saisi ces phénomènes ? Nous avons adapté une méthode précédemment appliquée uniquement aux modèles de vision et de langage pour les phrases nominales par Ravi Shekhar et collaborateurs, et sur le comptage dans nos travaux précédents. Cette méthode, appelée \"foiling\" (création de contre-exemples), consiste à prendre la légende d'une image et à produire un contre-exemple en modifiant la légende de manière à ce qu'elle ne décrive plus l'image. Nous effectuons ces altérations de phrases en nous concentrant sur six éléments spécifiques : l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence des entités. Chaque élément peut comporter un ou plusieurs instruments, dans le cas où nous avons trouvé plusieurs façons intéressantes de créer des instances de contre-exemples.\n\nPar exemple, pour l'élément \"actions\", nous avons deux instruments : l'un dans lequel le verbe d'action est modifié par un autre verbe d'action, et l'autre dans lequel les actants sont échangés. Le comptage et la coréférence sont également des éléments possédant plusieurs instruments. Nous créons ces contre-exemples en veillant à ce qu'ils ne décrivent plus l'image, tout en étant grammaticalement corrects et valides. Cette tâche n'est pas aisée, car un contre-exemple peut être moins probable que la légende originale. Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que des plantes coupent un homme plutôt qu'un homme ne coupe des plantes, et les grands modèles de vision et de langage pourraient détecter cette subtilité.\n\nPour obtenir des contre-exemples valides, nous devons donc prendre des mesures : tout d'abord, nous utilisons des modèles linguistiques puissants pour proposer des contre-exemples, ensuite, nous employons l'inférence en langage naturel (NLI) pour éliminer les contre-exemples qui pourraient encore décrire l'image. En effet, lors de la création de contre-exemples, nous devons garantir qu'ils ne décrivent plus l'image. Pour tester cela automatiquement, nous appliquons l'inférence en langage naturel avec la logique suivante : nous considérons l'image comme la prémisse et sa légende comme l'hypothèse qui en découle. De plus, nous considérons la légende comme la prémisse et le contre-exemple comme l'hypothèse. Si un modèle NLI prédit que le contre-exemple contredit ou est neutre par rapport à la légende, nous l'interprétons comme un indicateur de contre-exemple valide. Si, en revanche, le modèle NLI prédit que le contre-exemple est impliqué par la légende, il ne peut pas être un bon contre-exemple, car par transitivité, il fournirait une description véridique de l'image, et nous éliminons ces contre-exemples.\n\nCependant, cette procédure n'est pas parfaite, elle ne constitue qu'un indicateur de contre-exemples valides. Par conséquent, en tant que troisième mesure pour générer des contre-exemples valides, nous faisons appel à des annotateurs humains pour valider les données utilisées dans VALSE. Après filtrage et évaluation humaine, nous obtenons autant d'instances de test que décrites dans ce tableau. Notez que VALSE ne fournit aucune donnée d'entraînement, seulement des données de test, car il s'agit d'un benchmark de test à zéro coup. Il est conçu pour exploiter les capacités existantes des modèles de vision et de langage après leur pré-entraînement. Le réglage fin n'aurait pour effet que de permettre aux modèles d'exploiter des artefacts ou des biais statistiques dans les données, et nous savons tous que ces modèles aiment tricher et prendre des raccourcis.\n\nComme nous l'avons mentionné, nous souhaitons évaluer les capacités des modèles de vision et de langage après leur pré-entraînement. Nous avons expérimenté avec cinq modèles de vision et de langage sur les voyelles, à savoir CLIP, Alex, Mert, WiLBERT, WiLBERT-12-in-1 et VisualBERT. Deux de nos métriques d'évaluation les plus importantes sont la précision des modèles dans la classification de paires image-phrase en légendes et contre-exemples, et, plus pertinent pour cette vidéo, nous présenterons notre métrique plus permissive, la précision paire à paire, qui mesure si le score d'alignement image-phrase est plus élevé pour la paire image-texte correcte que pour sa paire de contre-exemple. Pour plus de métriques et de résultats, veuillez consulter notre article.\n\nLes résultats avec la précision paire à paire sont présentés ici et sont cohérents avec ceux obtenus à partir des autres métriques : la meilleure performance à zéro coup est atteinte par WiLBERT-12-in-1, suivi de WiLBERT, Alex, Mert, CLIP et enfin VisualBERT. Il est remarquable que les instruments centrés sur des objets individuels, comme l'existence et les phrases nominales, soient presque résolus par WiLBERT-12-in-1, ce qui met en évidence la capacité des modèles à identifier des objets nommés et leur présence dans les images. Cependant, aucun des autres éléments ne peut être résolu de manière fiable dans nos paramètres de foiling adversarial.\n\nNous constatons, à partir des instruments de pluralité et de comptage, que les modèles de vision et de langage ont du mal à distinguer les références à un objet unique de celles à plusieurs objets, ou à les compter dans une image. L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image. Ils peinent également à distinguer les actions et à identifier leurs participants, même lorsqu'ils sont soutenus par des biais de plausibilité, comme nous le voyons dans l'élément \"actions\". À partir de l'élément de référence, nous découvrons qu'il est également difficile pour les modèles de vision et de langage de suivre plusieurs références au même objet dans une image en utilisant des pronoms.\n\nÀ titre de contrôle et parce que c'est une expérience intéressante, nous avons également évalué deux modèles de texte uniquement, GPT-1 et GPT-2, pour déterminer si VALSE peut être résolu par ces modèles unimodaux. Nous avons calculé la perplexité de la légende correcte et du contre-exemple, sans image, et prédit l'entrée avec la perplexité la plus faible. Si la perplexité est plus élevée pour le contre-exemple, nous l'interprétons comme une indication que le contre-exemple peut souffrir d'un biais de plausibilité ou d'autres biais linguistiques. Il est intéressant de noter que, dans certains cas, les modèles GPT uniquement textuels ont mieux saisi la plausibilité du monde que les modèles de vision et de langage.\n\nPour résumer, VALSE est un benchmark qui utilise le prisme des constructions linguistiques pour aider la communauté à améliorer les modèles de vision et de langage en testant rigoureusement leurs capacités d'ancrage visuel. Nos expériences montrent que les modèles de vision et de langage identifient bien les objets nommés et leur présence dans les images, comme le démontre l'élément \"existence\", mais qu'ils peinent à ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont contraints de respecter des indicateurs linguistiques.\n\nNous encourageons vivement la communauté à utiliser les voyelles pour mesurer les progrès vers l'ancrage linguistique avec les modèles de vision et de langage. De plus, les voyelles pourraient servir d'évaluation indirecte des jeux de données, car les modèles pourraient être évalués avant et après l'entraînement ou le réglage fin pour déterminer si un jeu de données aide les modèles à s'améliorer dans l'un des aspects testés par VALSE. Si vous êtes intéressés, consultez les données VALSE sur GitHub, et n'hésitez pas à nous contacter si vous avez des questions."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Kamisura de l'Université de Tokyo. Je vais présenter un article intitulé « O En Sum : Un jeu de données à plus grande échelle pour la génération automatique de notes de version via la sommation de listes ». Je vais expliquer dans l'ordre suivant : tout d'abord, je vais introduire la notation Li automatique sur laquelle nous travaillons dans cette recherche. Les notes de version sont des documents techniques qui résument les modifications distribuées avec chaque version d'un produit logiciel. L'image montre les notes de version pour la version deux point six point quatre de la bibliothèque Bujs. Ces notes jouent un rôle important dans le développement open source, mais elles sont chronophages à préparer manuellement, donc il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.\n\nJe vais me référer à deux recherches précédentes sur la génération automatique de notes de version. La première est un système appelé alena, publié en 2014. Il adopte une approche basée sur des règles, par exemple, en utilisant un extracteur de changements pour extraire les différences centrales, les modifications de bibliothèque et les changements de document à partir des différences entre les maladies, et enfin en les combinant. La caractéristique la plus notable de ce système est l'extraction d'enjeux dans le coin supérieur droit, qui doit être liée à Jira, l'écosystème d'enjeux, et ne peut être appliquée qu'aux projets utilisant Jira, en d'autres termes, elle ne peut pas être utilisée pour de nombreux projets sur GitHub.\n\nLa seconde est Grif, annoncée récemment en 2020. Elle est disponible sur Internet et peut être stockée via pi. Ce système utilise un modèle de classification de texte simple basé sur le fonctionnement et produit l'une des cinq catégories de problèmes tels que les fonctionnalités ou les corrections de bogues pour chaque message de validation d'entrée. L'image est un exemple d'utilisation qui retourne une bande correcte de corrections de bogues. Les données d'entraînement sont relativement petites, environ 5 000, et seront présentées dans les expériences décrites ci-dessous. La performance du modèle de classification de texte n'est pas élevée.\n\nJe présente deux recherches connexes, mais il y a des problèmes de faible applicabilité et de ressources de données limitées. Notre article résout ces deux problèmes et génère automatiquement des ressources de haute qualité. Pour le problème d'applicabilité limitée, nous proposons une méthode de sommation de classificateur de haute qualité utilisant uniquement les messages de validation comme entrée. Cette méthode proposée peut être utilisée pour tous les dépôts anglais. Pour le second problème de ressources limitées, nous avons construit notre propre jeu de données composé d'environ 82 000 données en corrigeant les données de dépôts GitHub publics en utilisant l'API Git.\n\nJe vais maintenant décrire nos données. Voici un exemple de mise à jour : le côté gauche est un message de validation et le côté droit sont les notes de version. Les notes de version sont classées en niveaux tels que les améliorations de fonctionnalités, etc. Nous avons mis en place une tâche qui prend les messages de validation comme entrée et produit les notes de version. Cela peut être considéré comme une tâche de sommation. Nous avons prédéfini quatre niveaux : fonctionnalités implémentées, corrections de bogues, dépréciations, suppressions et changements rompus. Ces niveaux ont été définis en fonction de l'utilisation précédente et d'autres facteurs.\n\nIl y a une note en bas à droite, extraite de la liste de nœuds montrée en bas à gauche. À ce moment, il est nécessaire de détecter les quatre nœuds définis précédemment, mais les niveaux ne sont pas toujours cohérents avec chaque cas. Par exemple, le niveau d'amélioration inclut les améliorations, les améliorations, les optimisations, etc. Nous avons préparé une liste de vocabulaire des niveaux d'étude pour chacune de ces variations de notation et l'utilisons pour détecter la classe de nœuds de risque et corriger le texte du reste qui suit en tant que phrase de nœud de risque.\n\nVoici un message de validation. Les messages de validation ne sont pas liés à chaque version comme le montre l'image ci-dessous. Si la version actuelle est la 2.5.19, nous devons identifier la version précédente 2.5.18 et l'obtenir. C'est un peu fastidieux et il ne suffit pas de simplement obtenir une liste de versions et de regarder les avant et après. Nous avons créé un collage heuristique pour obtenir les versions précédentes et suivantes.\n\nEn fin de compte, 7 200 dépôts et 82 000 données ont été corrigés. Le nombre moyen de jetons raisonnables est de 63, ce qui est assez élevé pour une tâche de sommation. Le nombre de jetons uniques est également assez riche, avec 8 830 000. Cela est dû au grand nombre de classes et de noms de méthodes uniques trouvés dans le dépôt.\n\nJe vais maintenant expliquer la méthode proposée. Le modèle de sommation extractive et abstractive croisée se compose de deux modules neuronaux : un classificateur utilisant Bot ou Code Bot, et un générateur utilisant But First G. Il utilise d'abord un classificateur pour classer chaque message de validation en cinq classes de nœuds de base : fonctionnalités, améliorations, corrections de bogues, dépréciations, et autres. Les messages de validation classés comme autres sont discardés. Ensuite, il applique un générateur aux quatre documents de caoutchouc indépendamment et génère une note de version pour chaque classe. Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues. Par conséquent, pour entraîner le classificateur, nous attribuons des variables pseudo à chaque message de validation en utilisant les 10 premiers caractères de chaque message de validation.\n\nNous modélisons l'approche de sommation abstractive par classe en définissant deux méthodes. La première méthode, que nous appelons GS Single, se compose d'un seul réseau et génère un seul texte long en donnant une concaténation des messages de validation d'entrée. Le texte de sortie peut être divisé en segments de fichiers de classe basés sur des symboles d'extrémité spécifiques à la classe. La deuxième méthode, que nous appelons She's Much, se compose de quatre réseaux différents, chacun correspondant à l'une des classes de nœuds de liste.\n\nPassons maintenant à l'expérience. Cinq méthodes ont été comparées : GS, She's Single, She's Much, Cluster, et une étude précédente, Grif, concernant l'aberration dans certains cas. Ces notes sont parfois sorties sous forme de plusieurs phrases, et comme il est difficile de corriger le nombre de phrases à zéro, elles sont combinées avec des espaces et traitées comme une seule phrase longue. Le bleu est une pénalité lorsque le système sort une phrase courte. Cette pénalité entraîne une valeur ROUGE plus faible dans les résultats d'expérience décrits ci-dessous. Enfin, nous ajoutons également une spécificité car le bleu et le rouge ne peuvent pas être calculés si les notes de liste sont vides. Une haute spécificité signifie que le modèle sort correctement un texte vide dans les cas où les notes de version sont supposées vides.\n\nVoici les résultats. Puisque le jeu de données contient des analyses par courriel, etc., nous avons également évalué le jeu de données propre qui les exclut. G et GS ont obtenu des scores d'erreur plus de 10 points plus élevés que la ligne de base, en particulier sur l'ensemble de test coréen, l'écart de score entre la méthode proposée et la ligne de base a bondi à plus de 20 points. Ces résultats indiquent que GS et G sont très efficaces. GS a obtenu un meilleur score ROUGE que GAS, suggérant qu'il est efficace de combiner un classificateur et un générateur dans l'entraînement du classificateur en utilisant des serveurs. La haute couverture de GS peut être obtenue correctement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe. She's Much a tendance à obtenir des scores plus élevés que She's Single, suggérant qu'il est également efficace de développer indépendamment différents modèles de sommation constructive pour chaque classe de nœud de liste.\n\nVoici une analyse des erreurs. Les méthodes She ont tendance à sortir des phrases plus courtes que la phrase de référence humaine, car dans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que CSS n'en a qu'une seule. La raison de cette réticence du modèle est que dans les données d'entraînement, seulement 30 % des phrases sont présentes au niveau des fonctionnalités et 40 % au niveau des améliorations. De plus, les méthodes CS ne peuvent pas générer de notes de liste précises sans informations supplémentaires. L'exemple du haut à droite est un exemple de message de validation très désordonné et la phrase complète ne peut pas être générée sans différence avec les correspondances ou les enjeux correspondants. L'exemple ci-dessous montre que les deux messages de validation de l'entrée sont liés et devraient être combinés en une seule phrase, mais il échoue à le faire.\n\nEn conclusion, nous avons construit un nouveau jeu de données pour la génération automatique de notes de version. Nous avons également formulé la tâche de prendre des messages de validation et de les sommer de manière à ce qu'elle soit applicable à tous les projets écrits en anglais. Nos expériences montrent que la méthode proposée génère moins de bruit et a une couverture plus élevée que la ligne de base. Veuillez consulter nos données sur GitHub. Merci."}
