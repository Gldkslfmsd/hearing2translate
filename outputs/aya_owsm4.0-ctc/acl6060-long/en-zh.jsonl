{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我叫Safari，我将介绍我们的研究：使用精细调整的Transformer架构进行表格数据增强。科学家们分析数据时，主要关注操作现有特征，但有时这些特征是有限的。使用另一个数据源进行特征生成可以添加大量信息。我们的研究目标是使用外部免费文本自动增强表格数据。假设我们有一个表格数据集和一个知识库。我们需要一个自动化过程，涉及实体链接和文本分析，从知识库的免费文本中提取新特征。我们的框架Fest正是这个自动化过程。让我们看一个例子：数据集输入Fest，在这个例子中，数据集是大学数据集，目标是将大学分类为低排名大学和高排名大学。作为知识库，我们使用维基百科。第一阶段是实体链接，在这个例子中，每个实体（大学名称）被链接到知识库中的一个实体，然后提取知识库实体文本并添加到数据集中。在这个例子中，文本是维基百科页面摘要。现在我们需要从检索到的文本中生成或提取特征，因此我们需要一个特征提取阶段，包括文本分析，这是本文的创新点，我将在下一张幻灯片详细介绍。特征提取阶段之后是特征生成阶段，我们使用提取的特征生成少量新特征。首先生成与原始数据集类别数量相同的新特征。例如，如果原始数据集有两个类别，首先生成两个新特征。每个特征表示每个类别的可能性。为了分析文本，我们使用当前最先进的文本分析技术，即基于Transformer的语言模型，如BERT、GPT-X等。但是，不可能用输入数据集训练语言模型，因此一种简单的方法是目标任务精调。在特征提取阶段，我们可以下载预训练的语言模型，并在目标数据集上进行精调。例如，精调语言模型以将文本分类为低或高，获取语言模型输出，即每个类别的可能性，并将其用作新特征。这个方法的问题是数据集可能包含很少的独特实体文本，在我们的实验中，近一半的数据集包含不到400个样本，最小的数据集只有35个样本作为初始训练集，因此在这么小的数据集上精调语言模型将无效。但我们可以使用对预分析数据集的先验知识，因为Fast（我们的框架）应用于多个数据集，我们可以使用n-1个数据集来收集信息，并在分析第n个数据集时使用这些信息。我们建议添加另一个精调阶段——初步的多任务精调阶段，在n-1个数据集上精调语言模型，然后执行目标任务精调阶段，在n个目标数据集上精调语言模型。现有的多任务精调方法称为TDNN，在训练集中维护与任务数量相同的头。例如，在这个有四个任务的训练集中，TDNN维护四个头。它从训练集中采样一个随机批次，如果随机批次属于单句分类任务，则执行第一个头的前向和反向传递，如果属于成对排名任务，则执行最后一个头的传递。在我们的场景中，表格数据集的类别数量变化，因此有多个任务。TDNN维护类别数量的输出层，并需要为新的数据集和任务初始化新的头。我们的方法称为任务重构精调，我们将每个数据集重构为句子分类问题，即两个类别的任务。让我们看一个例子：这里是我们的输入数据集，包含实体、特征文本和类别，我们将任务从分类文本高低重构为分类文本和类别为真或假，换句话说，我们训练语言模型来分类文本和类别是否匹配。标签向量在这种情况下始终由两个类别组成。这是我们重构精调方法的算法。让我们看看完整的框架：数据集输入Fast，Fast执行链接阶段，从知识库中提取文本（在这个例子中是维基百科页面的摘要），然后将任务重构为句子分类任务，将语言模型应用于新任务，并输出每个类别的可能性。请注意，语言模型已经在n-1个数据集上进行了初步的多任务精调。然后，我们使用语言模型的输出向量作为新生成的特征，以类别数量为特征数量。为了评估我们的框架，我们使用包含17个表格分类数据集，具有不同大小、平衡度和初始性能，并使用维基百科作为知识库。我们设计实验为留一评估，在16个数据集上训练Fast，并将其应用于第17个数据集。我们还将每个数据集划分为四折，并进行交叉验证。我们生成新特征，并使用五个评估分类器对其进行评估。我们在实验中使用的基于鸟的架构。以下是我们的实验结果：我们将我们的框架与目标数据集精调、目标任务精调和MTDNN初步精调进行比较，我们的重构精调方法取得了最佳结果，比目标数据集精调方法提高了6%。当我们观察小数据集时，可以看到MTDNN的性能下降，初步多任务精调阶段的改进降至1.5%，但我们的性能提高到11%。总之，Fast实现了从35个样本开始的快速增强，它使用一个架构适用于所有任务和数据集，并保持模型的头部分，但它添加了一个重构阶段，增强了训练集，并需要具有语义意义的目标值，以便将其输入语言模型并用于句子分类问题。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。今天我将介绍我们的研究工作，学习如何进行归纳推理，具体到求解数学问题的复杂区域提取。我来自Biance AI实验室，这是与德克萨斯大学奥斯汀分校的Che和SUDD的Wedu的联合研究。首先，我想谈谈我们进行推理的动机。在这里，我们展示了多步推理有帮助的例子。这张图来自一篇论文，其中他们通过提示来解决未来学习场景中的数学问题。在图的左侧，我们可以看到，如果我们只提供问题和答案样本，可能无法得到正确答案。但如果我们提供更多的推理描述，模型能够预测推理描述并给出正确预测，因此，输出可解释的多步推理是很好的。我们还认为数学问题是评估此类推理能力的一个直接应用。在这里，我们的问题设置是，给定问题，我们需要解决问题并得到数值答案。在我们的数据集中，我们还提供了导致特定答案的数学表达式。此外，还适用某些假设，与之前的工作一样，我们假设量的精度已知，我们只考虑基本运算符，如加法、减法、乘法、除法和指数。此外，复杂的运算符实际上可以分解为这些基本运算符。\n\n之前在解决数学问题方面的工作可以分为序列到序列模型和序列到树模型。传统的序列到序列模型将表达式转换为特定的序列进行生成，实现起来很简单，可以概括许多不同的复杂问题，但性能通常不如结构模型，并且缺乏预测的可解释性。由于变压器模型，这个方向仍然很受欢迎。在基于树的模型中，我们将这些表达式结构化为树的形式，并遵循树的遍历顺序进行生成。在这里，我们不断生成运算符，直到到达叶节点，即量。这里的好处是它实际上给我们一个二叉树结构，但这有点反直觉，因为我们先生成运算符，然后在最后生成量，其次，它还包含一些重复计算。在这里，如果我们看这个表达式，a乘以3加3实际上被生成了两次，但实际上我们应该重用结果。\n\n在我们提出的方法中，我们希望以逐步和可解释的方式解决这些问题。例如，在这里的第二步，我们可以得到这个除数是27，我们也可以回顾原始问题以找到相关内容。在这些步骤中，我们得到除数，然后在第三步，我们实际上得到了商。在这些步骤之后，我们可以重用第二步的结果，然后得到第四步的结果，最后我们可以得到被除数。在这里，我们实际上直接生成整个表达式，而不是生成单个运算符或量，这使过程更准确。\n\n在我们的归纳系统里，我们首先从问题中呈现的一组量开始，并包括一些常量作为初始状态。表达式表示为eij，其中我们从qi执行运算符到qj，这样的表达式是方向性的，我们也有逆向减法来表示相反的方向，这与关系提取非常相似。在正式的归纳系统中，在时间步t，我们在qi和qj之间应用运算符，然后得到新的表达式，我们将其添加到下一个状态中，成为新的量。这些幻灯片实际上可视化了状态的演变，我们不断将表达式添加到当前状态中。\n\n在我们的模型实现中，我们首先使用预训练模型，可以是BERT或RawHoods，然后我们编码句子，并得到这些量表示。一旦我们得到量表示，我们就可以开始进行推理。我们这里展示了一个从q1到q2的除法，然后乘以q3的例子。我们首先得到基本的配对表示，即q1和q2的基本连接，然后我们应用一个由运算符参数化的前馈网络，最后我们得到表达式表示q1除以q2。在实践中，在推理阶段，我们也可能得到不正确的表达式。所有可能的表达式等于运算符数量的三倍。这里的好处是，我们可以轻松添加约束来控制搜索空间，例如，如果这个表达式不允许，我们可以简单地将其从搜索空间中删除。\n\n在第二步，我们做同样的事情，但唯一的区别是多了一个量，这个量来自之前计算的表达式。最后，我们可以得到最终表达式q3乘以q4，我们也可以看到所有可能表达式的数量与之前步骤不同。这种差异使得很难应用束搜索，因为这两个步骤之间的概率分布是不平衡的。\n\n训练过程类似于训练序列到序列模型，我们优化每个时间步的损失，这里我们也使用τ来表示我们应该终止生成过程的时机。空间与序列到序列不同，因为它在每个时间步都是不同的，而在传统的序列到序列模型中，它是词汇表的大小，它还允许从先验知识施加某些约束。\n\n我们在常用的数学问题数据集mWPS Method3K MathQA和SWAM上进行了实验，并简要展示了与之前最佳方法的比较结果。我们表现最佳的方法是BERT归纳推理，事实上，我们没有使用束搜索，而与使用束搜索的明显方法相比，最佳方法通常是基于树的模型。总体而言，我们的推理器能够从基于树的模型中产生显著的输出，但我们可以看到MathQA或SWAM上的绝对数字并不真的很高。\n\n我们进一步调查了SWAM上的结果，这个数据集具有挑战性，因为作者试图手动添加一些内容来混淆NLP模型，例如添加可用信息和额外量。在我们的预测中，我们发现一些中间值实际上是负数，例如，在这些问题中，我们询问Jake有多少个苹果，但我们有一些额外的信息，如比pitchachees少17个，Stephen有8个pitchachees，这完全相关。我们的模型做出一些预测，如产生负值，我们观察到这两个表达式实际上具有相似的分数。我们可以实际上通过删除那些结果为负的结果来限制搜索空间，从而使答案正确。\n\n我们进一步发现，这种约束实际上对某些模型改进很大，例如，对于BERT，我们提高了7个点，对于基于BERT的模型，我们实际上提高了2个点。更好的语言模型具有更好的语言理解能力，因此，对于BERT，这个数字较低，而对于基于BERT的模型，这个数字较高。我们还试图分析这些数据集背后的难度，我们假设未使用量的数量可以被视为相关信息。在这里，我们可以看到我们将样本未使用量的百分比，SWAM数据集具有最大份额，我们还显示了这些样本没有未使用量的整体性能，整体性能实际上高于具有未使用量的性能，但具有未使用量的样本性能远低于整体性能。\n\n最后，我们想通过一个错误预测的例子来展示可解释性。在这里，我们的模型在第一个步骤中做出错误预测，我们可以将这个表达式与句子相关联。我们认为这个句子可能误导模型做出错误预测，所以我们尝试将句子修改为像“梨树的数量比苹果树少35个”，这样模型会认为应该是加法运算符。我们将句子修改为“梨树的数量比苹果树少5个”，以传达更准确的语义，使模型能够正确预测。这项研究展示了可解释预测如何帮助我们理解模型行为。\n\n总之，我们的工作首先是模型非常高效，我们能够提供可解释的求解过程，我们可以轻松地将先验知识作为约束纳入其中，这可以提高性能。最后，潜在机制不仅适用于数学问题求解任务，也适用于涉及多步推理的其他任务。但我们也有某些局限性。如果我们有大量运算符或常量，内存消耗可能会很高。其次，正如提到的，由于不同时间步之间的概率分布不平衡，应用束搜索策略也非常具有挑战性。这是演讲的结束，欢迎提问，谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我叫安托万，来自马斯特里赫特大学。我将与杰里一起展示我的约翰工作，该工作涉及一个用于法定条文检索的新数据集。法律问题是许多人生活中不可或缺的一部分，但由于大多数公民对自己的权利和基本法律程序知之甚少，许多无法负担法律专家昂贵协助的弱势公民因此得不到保护，甚至被剥削。我们的工作旨在通过开发有效的法定条文检索系统来弥合人民与法律之间的鸿沟，这样一个系统可以为普通人提供免费的专业法律帮助服务。\n\n在介绍我们工作的核心贡献之前，让我们首先描述一下法定条文检索的问题。假设我们有一个关于法律事务的简单问题，例如“我违反职业保密会面临什么风险？”，模型需要从大量法律文本中检索出所有相关的法定条文。这个信息检索任务本身面临着一系列挑战。首先，它涉及两种语言：问题使用通用自然语言，而法规使用复杂的法律语言。这种语言分布的差异使得系统更难检索到相关候选项，因为它间接地需要一个内在的解释系统，将自然语言的问题翻译成与法规术语相匹配的法律问题。此外，法定法并非独立条文的简单堆叠，它们不能像新闻或食谱那样被视为完整信息源。相反，它是具有整体意义的结构化法律条款集合，只有在考虑其邻近条款的补充信息、它们所属的领域和子领域以及它们在法律结构中的位置时，才能理解其全意。最后，法定条文通常是短段落，而大多数检索工作中的典型检索单位是长文档，可能长达6000字。\n\n自然语言处理的最新进展在许多法律任务中激发了巨大的兴趣，如法律判决预测或自动合同审查，但法定条文检索由于缺乏大量高质量的标注数据集而基本停滞不前。在本工作中，我们提出了一个新的以法国公民为中心的数据集，以研究检索模型是否能够在法定条文检索任务中接近法律专家的效率和可靠性。这个比利时法定条文检索数据集包含超过1100个由比利时公民提出的法律问题，这些问题涵盖了从家庭、住房、金钱到工作和社会保障等广泛主题。每个问题都由经验丰富的法官标注，并参考来自比利时法律代码中超过26000个法律条文的大型语料库中的相关条文。\n\n现在让我们谈谈如何收集这个数据集。首先，我们编译了一个大型的法律条文语料库，考虑了32个公开可用的比利时法律代码，并提取了所有条文以及相应的章节标题。然后，我们收集了带有相关法规参考的法律问题。为此，我们与一家比利时律师事务所合作，该事务所每年收到约400封来自比利时公民的电子邮件，寻求关于个人法律问题的建议。我们有幸获得了他们网站的访问权限，在该网站上，经验丰富的法官团队解决了比利时最常见的法律问题。我们收集了数千个问题，并用类别、子类别和相关法规参考进行了标注。最后，我们根据参考过滤了问题，并筛选出参考不是法律代码中我们考虑的条文的问题。我们最终得到了1108个问题，每个问题都仔细标注了来自22633个法定条文大型语料库的相关条文ID，此外，每个问题还附有主要类别和子类别的连接，每个条文也有其后续标题在法律结构中的连接。这些额外信息在本工作中没有使用，但可能对未来法律信息检索或法律文本分类研究有兴趣。\n\n让我们看看我们数据集的某些特征。问题长度在5到44个字之间，中位数是40个字；条文则要长得多，中位数长度为77个字，其中142个超过1000字，最长的可达5790字。正如之前提到的，这些问题涵盖了广泛的主题，大约85%的问题要么是关于家庭、住房、金钱或司法的，其余15%则涉及社会保障、外国人或工作。条文也非常多样化，因为它们来自32个不同的比利时法律代码，涵盖了大量法律主题。以下是每个比利时代码收集的文章总数，在22633篇文章中，只有1612篇被引用为至少一个问题中的相关条文，大约80%的这些引用条文来自《民法》、《司法法》、《刑事调查法》或《刑法》。同时，32个代码中有18个代码中不到5篇文章被引用为至少一个问题中的相关条文，这可以解释为这些代码不太关注个人及其关切。总体而言，这些引用条文的中位数引用次数是2，不到25%的条文被引用超过5次。\n\n使用我们的数据集，我们评估了包括词典和密集架构在内的多种检索方法。给定一个查询和一条条文，词典模型通过计算查询项在条文中权重的总和来为查询-条文对分配分数。我们实验了标准的TF-IDF和BM25排名函数。这些方法的主要问题是它们只能检索包含查询中关键字的条文。为了克服这个限制，我们实验了一个基于神经网络的架构，可以捕捉查询和条文之间的语义关系。我们使用了一个双塔模型，将查询和条文映射到密集向量表示中，并通过计算它们的嵌入相似性来计算查询-条文对的相关性分数。这些嵌入通常来自词嵌入模型输出的池化操作。\n\n首先，我们研究了双塔模型在零样本评估设置中的有效性，这意味着预训练的词嵌入模型直接应用，没有额外的微调。我们实验了上下文独立的文本编码器，即Word2Vec和FastText，以及上下文依赖的嵌入模型，即RoBERTa，更具体地说，是Camembert，这是一个法语RoBERTa模型。此外，我们在所有数据集上训练了我们自己的Camembert模型。请注意，在训练过程中，我们实验了双塔架构的两个变体：Siamese，它使用一个唯一的词嵌入模型将查询和条文映射到共享的密集向量空间中；以及Two-Tower，它使用两个独立的词嵌入模型将查询和条文分别映射到不同的嵌入空间中。我们实验了均值、最大和CLS池化以及点积和余弦计算相似性。\n\n以下是基线在测试集上的结果，上面是词典方法，中间是零样本评估设置下的双塔编码器，下面是微调的双塔编码器。总体而言，微调的双塔编码器显著优于所有其他基线，Two-Tower模型在100个回调上优于其Siamese变体，但在其他指标上表现相似。尽管BM25显著低于训练后的双塔编码器，但其表现表明它仍然是特定领域检索的强基线。关于双塔编码器的零样本评估，我们发现直接使用预训练的Camembert模型的嵌入，而没有针对信息检索任务进行优化，会产生糟糕的结果，这与先前的发现一致。此外，我们观察到基于Word2Vec的双塔编码器显著优于FastText和基于Byte的模型，这表明预训练的词级嵌入可能比字符级或子词级嵌入更适合该任务，当直接使用时。\n\n尽管这些结果很有前景，但与技能专家相比，它们表明了大量的改进空间，专家最终可以检索出任何问题的全部相关条文，从而获得完美分数。让我们通过讨论两个数据集的局限性来总结。首先，条文语料库仅限于从32个考虑的比利时法律代码中收集的条文，这并未涵盖整个比利时法律，因为法令、指令和条例中的条文缺失。在数据集构建过程中，对这些未收集条文的引用被忽略，导致一些问题只能得到最初相关条文的一小部分。这种信息丢失意味着剩余相关条文中的答案可能不完整，尽管它们仍然完全相关。其次，我们应该注意到并非所有法律问题都能仅用法规来回答。例如，问题“如果租客制造过多噪音，我能驱逐他们吗？”可能在法定法中没有详细的答案，量化特定噪音阈值以确定驱逐的条件。相反，房东可能更应该依赖判例法，寻找与当前情况相似的先例。因此，一些问题比其他问题更适合法定条文检索任务，不适合的领域有待确定。\n\n我们希望这项工作能够激发人们对开发实用且可靠的法定条文检索模型的兴趣，这些模型可以帮助改善所有人的司法公正。您可以在以下链接查看我们的论文和数据集。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我们很高兴地展示我们关于元音的工作，这是一个独立于任务的基准，旨在测试视觉和语言模型对特定语言现象的理解。为什么我们要花精力建立这个基准？在过去几年里，我们见证了基于转换器的视觉和语言模型的爆发式增长，这些模型在大量图像文本对上进行预训练，每个模型都在视觉和语言任务上推动了最新技术，例如视觉问答、视觉常识推理、图像检索、短语定位等。我们收到一个信息：这些任务特定基准上的准确率正在稳步提高，但我们知道模型实际上学到了什么吗？当一个视觉和语言模型为这张图片和这句话的匹配给出高分，为另一对给出低分时，它理解了什么？视觉和语言模型是否关注正确的内容，还是像之前的研究所示，关注偏见？为了更好地了解这一点，我们提出了一个更独立于任务的方向，并引入了元音，这是一个测试视觉和语言模型对影响语言和视觉模式的特定语言现象敏感度的基准。我们关注的存在、复数、计数、空间关系、动作和实体共指等现象。\n\n我们如何测试视觉模型是否捕捉到这些现象？我们采用了一种之前仅适用于名词短语的方法，由Ravi Shekhar等人提出，以及我们之前在计数方面的工作。这种方法称为“伪造”，即我们取一张图片的标题，并通过更改标题使其不再描述图片来生成一个“伪造”版本。我们通过关注六个特定部分来进行短语更改：存在、复数、计数、空间关系、动作和实体共指，每个部分可以由一个或多个工具组成，如果我们发现了创建“伪造”实例的多种有趣方法。例如，在动作部分，我们有两个工具，一个是改变动作动词，另一个是交换动作的参与者。计数和共指也有多个工具。我们创建这些“伪造”版本，确保它们不能描述图片，同时是语法上正确且有效的句子。这并不容易，因为“伪造”的标题可能比原始标题更不符合逻辑。例如，虽然不是不可能的，但从统计学上讲，植物切割男人比男人切割植物更不常见，大型视觉和语言模型可能会捕捉到这一点。因此，为了获得有效的“伪造”版本，我们采取了以下行动：首先，我们使用强大的语言模型提出“伪造”版本；其次，我们使用自然语言推理（NLI）来过滤掉可能仍然描述图片的“伪造”版本。在构建“伪造”版本时，我们需要确保它们不能描述图片，为了自动测试这一点，我们应用自然语言推理，其逻辑如下：我们将图片视为前提，其标题视为隐含的假设，同时将标题视为前提，将“伪造”版本视为其假设。如果NLI模型预测“伪造”版本与标题相矛盾或中立，我们将其视为有效“伪造”版本的指标。如果NLI模型预测“伪造”版本由标题隐含，那么它不能是好的“伪造”版本，因为根据传递性，它将提供关于图片的真实描述，我们过滤掉这些“伪造”版本。但这个过程并不完美，它只是有效“伪造”版本的指标。因此，作为生成有效“伪造”版本的第三个措施，我们雇用了人类标注者来验证Valse使用的数据。在过滤和人类评估后，我们拥有了本表中描述的那样多的测试实例。请注意，Valse只提供测试数据，不提供训练数据，因为它是一个零样本测试基准，旨在利用视觉和语言模型在预训练后的现有能力。微调只会让模型利用数据中的人工制品或统计偏见。我们都知道这些模型喜欢作弊和走捷径。正如我们所说，我们有兴趣评估视觉和语言模型在预训练后的能力。\n\n我们在Valse上实验了五个视觉和语言模型，即CLIP、Alex、Mert、Wilbert、Wilbert 12 in 1和VisualBERT。我们最重要的评估指标是模型在将图像句子对分类为标题和“伪造”版本方面的准确性。对于本视频更相关的内容，我们将展示我们更宽松的指标——配对准确性，它测量正确的图像文本对的配对分数是否大于其“伪造”对的分数。有关更多指标和结果，请参阅我们的论文。配对准确性的结果如下，与我们从其他指标获得的结果一致：在零样本性能方面，Wilbert 12 in 1表现最佳，其次是Wilbert、Alex、Mert、CLIP和VisualBERT。值得注意的是，以单个物体为中心的工具，如存在和名词短语，几乎被Wilbert 12 in 1解决，这表明模型能够识别图像中命名的物体及其存在。然而，在我们的对抗性“伪造”设置中，没有一个剩余的部分能被可靠地解决。我们从复数和计数工具中看到，视觉和语言模型很难区分对单个物体还是多个物体的引用，或在图像中对它们进行计数。关系部分显示，它们在正确分类图像中物体之间的命名空间关系方面存在困难。它们也难以区分动作并识别其参与者，即使有可信度偏见的支持，如动作部分所示。从引用部分中，我们发现跟踪图像中对同一物体多次引用，使用代词对视觉和语言模型来说也很困难。\n\n作为合理性检查，也是因为这是一个有趣的实验，我们还对两个仅基于文本的模型GPT1和GPT2进行了基准测试，以评估Valse是否能被这些单模态模型解决。我们通过计算正确和“伪造”标题的困惑度，并预测困惑度较低的条目来实现这一点。如果“伪造”标题的困惑度更高，我们将其视为“伪造”标题可能受到可信度偏见或其他语言偏见的影响的迹象。有趣的是，在某些情况下，仅基于文本的GPT模型比视觉和语言模型更好地捕捉了世界的可信度。\n\n总之，Valse是一个使用语言构造的视角来帮助社区改进视觉和语言模型的基准。我们的实验表明，视觉和语言模型能够很好地识别图像中的命名物体及其存在，但当被迫遵守语言指标时，它们在视觉场景中难以将它们之间的相互依存关系和关系具体化。我们真诚地鼓励社区使用Valse来衡量在语言与视觉模型结合方面取得的进展。更进一步，Valse可以作为对数据集的间接评估，因为模型可以在训练或微调前后的不同阶段进行评估，以了解数据集是否帮助模型在Valse测试的任何方面取得改进。如果您感兴趣，请在GitHub上查看Valse数据，如果您有任何问题，请随时与我们联系。"}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是来自东京大学的Kamisura。我将宣讲一篇题为《O En Sum：自动列表节点生成的大型数据集》的论文。我将按以下顺序进行说明：首先，我将介绍我们在此研究中正在开发的自动列表节点生成技术。\n\nReaseNote 是一种技术文档，总结了软件产品每个版本发布时分发的更改。图中显示的是 Bujs 库版本 2.6.4 的发布笔记。这些笔记在开源开发中发挥着重要作用，但手动准备起来耗时费力，因此能够自动生成高质量的发布笔记将非常有用。\n\n我将参考两项关于自动列表节点生成的先前研究：第一项是名为 alena 的系统，于 2014 年发布。它采用基于规则的方法，例如使用更改提取器从疾病差异中提取核心差异、库更改和文档更改，最后将它们结合起来。该系统的最显著特征是上右角的议题提取，这必须链接到 Jira 问题生态系统，并且只能应用于使用 Jira 的项目，换句话说，它无法用于 GitHub 上的许多项目。\n\n第二项是 Grif，最近于 2020 年宣布。它在网上可用，可以通过 pi 存储。该系统采用简单的基于文本分类模型，对每个输入的提交消息输出五个问题之一，如功能或错误修复。图中是一个示例用法，返回正确的功能或错误修复。训练数据相当小，约 5000 条，将在下面描述的实验中展示。文本分类模型的性能并不高。\n\n我介绍了两项相关研究，但它们存在适用性有限和数据资源稀缺的问题。我们的论文解决了这两个问题，并自动生成高质量的资源。对于适用性有限的问题，我们提出了一种仅使用提交消息作为输入的高质量分类器总结方法。这种拟议的方法可以用于所有英语仓库。\n\n对于第二个数据资源稀缺的问题，我们构建了自己的数据，由约 82,000 条数据组成，通过使用 Git API 纠正来自公共 GitHub 仓库的数据。接下来，我将描述我们的数据。这里有一个示例更新，左侧是提交消息，右侧是发布笔记。发布笔记按功能改进等级别分类。我们设置了一个任务，将提交消息作为输入，并输出发布笔记。这可以被视为一个总结任务。我们预定义了四个级别：功能实现、错误修复、弃用和破坏性更改。这些是基于先前使用和其他因素设定的。\n\n底右侧的注释从底左侧显示的列表节点中提取。此时，有必要检测之前设置的四个级别，但级别并不总是与每个节点一致。例如，改进级别包括改进、增强、优化等。我们为每个记事变体的研究级别准备了一份词汇表，并使用它来检测风险节点类并纠正后续的文本。\n\n下一个提交消息。提交消息不像图中显示的那样与每个种族相关联。如果当前发布版本是 2.5 到 19，我们需要识别之前的发布版本 2.5 到 18 并获取它。这有点繁琐，仅仅获取发布列表并查看前后版本是不够的。我们创建了一个启发式匹配规则来获取前一个和下一个版本的数据。分析结果：7200 个仓库和 82,000 条数据被纠正。合理代币的平均数量为 63，对于总结任务来说相当高。独特代币的数量也相当丰富，为 88,300。这是由于仓库中发现的独特类和方法名称的数量较大。\n\n接下来，我将解释我们提出的跨度提取式和抽象式总结模型，它由两个神经模块组成：使用 Bot 或代码 Bot 的分类器和使用 But First G 的生成器。首先，分类器将每个提交消息分类为五个基本节点类：功能、改进、错误修复、弃用和其他。分类为“其他”的提交消息被丢弃。然后，它独立地将生成器应用于四个节点文档，并为每个类生成发布笔记。在这个任务中，提交消息和发布笔记之间的直接对应关系是未知的。因此，为了训练分类器，我们为每个输入提交消息分配伪变量，使用每个提交消息的前 10 个字符。\n\n我们通过两种定义的方法对类抽象总结方法进行建模。第一个模型，我们称之为 GS 单，由单个网络组成，生成单个长文本，给定输入提交消息的连接。输出文本可以根据特殊类特定端点符号分为类文件段。第二个方法，我们称之为她很多，由四个不同的网络组成，每个网络对应一个列表节点类。\n\n现在，让我解释实验。我们比较了五个方法：GS、她单、她很多、聚类和先前研究 Grif。在某些情况下，这些笔记以多句形式输出，由于很难将句子数量校正为零，它们被空格组合并视为一长句。蓝色是惩罚，当系统输出短句时，这种惩罚会导致下面的 ROUGE 值降低。实验结果如下所示。最后，我们还添加了一个特定性惩罚，因为蓝色和蓝色无法被计算，如果列表节点为空，高特定性意味着模型正确输出空文本，假设读节点为空。\n\n以下是结果。由于数据集包含电子邮件分析等值，我们还评估了排除它们的清洁数据集。G 和 GS 在清洁数据集上实现了超过 10 分的低错误分数，基线得分。特别是在韩国测试集中，拟议方法与基线之间的分数差距超过 20 分。这些结果表明，GS 和 GS 显著有效。GS 比 GS 获得了更好的低分分数，表明在训练分类器时结合生成器是有效的。\n\nGS 的高覆盖率可以适当实现，因为分类器可以专注于为每个类选择相关的提交消息。她很多倾向于高于她单，表明为每个列表节点类独立开发不同的构造性总结模型也是有效的。\n\n这里是一个错误分析。她方法倾向于输出比人类参考句子短的句子，因为在右边的图中，参考句子有三个或四个句子，而 CSS 只有一个。这种模型不情愿的原因是，在训练数据中，只有 30% 的句子存在于功能级别，40% 在改进级别。此外，CSS 方法在没有额外信息的情况下无法生成准确的列表节点。右边顶部是一个非常混乱的提交消息示例，如果没有相应的差异或问题，则无法生成完整句子。下面的示例显示了输入的两个提交消息是相关的，应该组合为一句话，但它未能做到。\n\n最后，一个结论。我们构建了一个新的自动个人生成数据集。我们还制定了输入提交消息并将其总结为适用于所有用英语书写的项目的任务。我们的实验表明，拟议的方法生成更少的噪声，覆盖率高于基线。请在 GitHub 上查看我们的数据。谢谢。"}
