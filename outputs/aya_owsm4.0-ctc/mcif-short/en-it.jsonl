{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Salve! Benvenuti alla nostra presentazione di Deplane, un nuovo corpus per l'identificazione del testo in tedesco a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stoden e vi guiderò attraverso la prima parte della presentazione. Iniziamo definendo la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La ramificazione è un processo di adattamento di un testo per migliorare la comprensione dello stesso da parte di un gruppo target specifico, come le persone con problemi di lettura o i non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "per addestrare un modello di testualizzazione, richiediamo coppie parallele di testi, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Nell'esempio qui riportato, è possibile vedere una coppia di frasi allineate in parallelo di una complessa frase tedesca e la sua traduzione odierna in un linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "semplificare la frase sono possibili diverse tecniche, come puoi vedere nell'esempio, quali la sostituzione lessicale, la dilatazione della clausola, la crossezione, il riordino o l'inserimento di \"bootss\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "proponiamo ora il nostro nuovo corpus poiché negli ultimi anni ci sono stati alcuni problemi con i corpus esistenti, quindi, per esempio, questi corpus qui sono troppo piccoli per addestrare un modello di tassonomizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere eccessivamente soggetti a errori nel loro allineamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "proponiamo il nostro nuovo corpus D planee, suddiviso in due subcorpora: Dplane APA e Dplane web. D planee APA è basato su testi di uso."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "In Depla APA, abbiamo allineato manualmente 483 documenti. Questo ha prodotto circa 13.000 coppie di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "deepplane web. Questo corpus include diversi domini e allineiamo anche tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "in totale otteniamo 30.045 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "analizzato un po' di più le nostre coppie di frasi, ad esempio sul tipo di adattamenti"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Qui puoi vedere che i testi biblici sono molto più semplificati rispetto, per esempio, ai testi di attualità o a quelli per apprendisti della lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "a tutti i livelli, ad esempio in termini di semplificazione lessicale, semplificazione strutturale e anche livello generale di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Puoi notare che il nostro corpus di pianificazione profonda presenta una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus API di pianificazione profonda, abbiamo molti più riordinamenti e aggiunte di radici rispetto a quanto ne abbiamo nel corpus web di pianificazione profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "d'altra parte, nel corpus testuale online disponiamo di molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo ora cosa possiamo fare con questo corpus: Salve, mi chiamo Omar, e ora parlerò dei casi d'uso per il nostro dataset dLAN. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, sono stati sviluppati numerosi metodi di allineamento, ma nel contesto delle traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove disponiamo di due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi nei documenti successivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma, nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, che hanno la stessa lingua, hanno lo stesso contenuto, ma si trovano su un diverso livello di complessità."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora che disponiamo del nostro dataset deepplan, che contiene frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo apportato alcune adattamenti ai metodi proposti e abbiamo pubblicato tutte queste adattamenti e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo dell'allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "E puoi trovare anche il codice per eseguire questo metodo sui tuoi documenti nel paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è un caso di semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "ottimizzando i modelli linguistici per generare testi semplificati a partire da testi di input complessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo ottimizzato due modelli diversi. Abbiamo ottimizzato il modello della parte lunga per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche ottimizzato la base normale, in parte per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "puoi trovare anche tutti i punti di controllo e puoi esaminare più dettagliatamente i punteggi e le metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questo semplice aggiustamento potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo quei risultati come un punto di riferimento, un punto di riferimento di base per il problema della semplificazione automatica dei testi nel futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "La ringraziamo tanto per la Sua attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Adam Skirkovsky e questo discorso riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come saprai, esistono diverse strutture di dipendenza ipotizzate da diverse teorie e approcci corpus. Quindi, per esempio, nelle dipendenze universali troviamo la struttura di coordinazione tra Lisa, Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "è tale che il primo congiunto è il nucleo dell'intera struttura coordinata, quindi in questo caso Lisa..."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "gli approcci adottati nella teoria del significato di Igor Milchuk, dove ancora una volta l'intera struttura coordinata è guidata dal primo contratto. Quindi questi due approcci sono asimmetrici, giusto? Essi individuano uno dei connettivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Esistono anche approcci simmetrici alle strutture coordinate, come l'approccio PRAG, l'approccio a testa di congiunzione, assunto nelle banche di alberi di dipendenza PLUGG, dove le strutture coordinate sono guidate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo le dipendenze dall'estremità a tutti i connettivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esiste anche un approccio a più livelli utilizzato, per esempio, nella grammatica delle parole di Dekatson."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove si afferma che tutte le condotte sono teste della struttura coordinata, quindi otteniamo dipendenze dal governatore qui ama tutte le condotte separatamente; queste sono le pulsanti che creano. \n\n(Nota: La frase originale in inglese sembra essere grammaticalmente non corretta o incompleta, quindi la traduzione potrebbe non avere un perfetto senso logico. In un contesto accademico o istruttivo, sarebbe utile avere una frase più chiara e completa da tradurre.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "Lo scopo di questo articolo è proporre un nuovo argomento a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Bene, l'argomentazione si basa sul principio della minimizzazione della lunghezza delle dipendenze, che spiegherò sulla base di questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "In italiano, come probabilmente sai, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli elementi circostanziali possono trovarsi più distanti. Quindi, \"March l'ha letto ieri\" è corretto, poiché l'oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Marzo ha letto ieri che è molto peggio proprio perché qui tra il verbo e l'oggetto diretto c'è un complemento di tempo ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo, poiché in tal caso può essere spostato nella posizione successiva all'integrazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "è illustrato qui. Quindi entrambe queste frasi sono corrette. March ha letto ieri questo libro assolutamente affascinante sulla bestia. \"I\" è accettabile in un certo senso, ma al posto di \"it\" abbiamo questa frase lunga e..."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "È anche giusto dire che a marzo ho letto ieri questo libro assolutamente affascinante sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "il ragionamento qui è che ciò è possibile perché, sebbene questa frase violi il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere adiacenti al verbo"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio della minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più brevi sono preferibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, ovvero quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui abbiamo la dipendenza dal rosso all'aggettivo di lunghezza 7 misurata in parole e dal rosso a libro di lunghezza 4. Quindi insieme fa 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "ti sposti quando scambii questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Quindi, invece di 11, 6 è molto più breve, ecco perché questo suona piuttosto bene, giusto? Viola un principio, ma ne soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Va bene, quindi ciò che abbiamo fatto è stato estrarre varie statistiche sulla coordinazione dalla versione potenziata del corpus pentry e vedere nel documento perché non abbiamo utilizzato le dipendenze universitarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "queste statistiche confermano l'osservazione già fatta in precedenza secondo cui le congiunzioni a sinistra tendono ad essere più brevi, quindi \"sale e pepe\" e non \"pepe e sale\" misurate in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione fatta incidentalmente che questa tendenza cresce con la lunghezza in Francia."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra la lunghezza dei due congiunti aumenta, il congiunto più corto preferisce essere il primo più forte, quindi la proporzione dei congiunti sinistri più corti è maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ciò che è innovativo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando i governatori di sinistra sono assenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il governatore è a sinistra in questo esempio, ho visto Baton Lisa, quindi il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "assente nel secondo esempio, Omero è venuto e ha starnutito; qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. Quindi, in questi casi, il congiunto sinistro tende ad essere più breve, tanto più quanto maggiore è la differenza tra i due congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance è di destra come in questo caso, la sinistra governa la coda e la rete di coordinamento, e questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo dimostrato che, misurando la lunghezza in caratteri, la prima colonna rappresenta le sillabe, la colonna centrale le parole e la colonna di destra i caratteri. Mi concentrerò quindi su quest'ultima."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Quello che vediamo qui è che quando il governatore è a sinistra,"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "la tendenza del congiunto sinistro ad essere più breve aumenta gradualmente con la differenza assoluta nelle parole e lo stesso si osserva quando non c'è un governatore, come nella coordinazione delle frasi, ma quando il governatore è a destra questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "nel nostro articolo dimostriamo come questo fornisca un argomento contro le strutture di coordinazione asimmetriche come queste due, che raddoppiano le strutture simmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "per il testo completo dell'accordo e le argomentazioni, si veda il documento. Scusate e parlate con noi durante la sessione poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Shahang B, sono un dottorando presso l'Università di Washington. Oggi presenterò il nostro lavoro, partendo dai dati di pre-addestramento fino ai modelli linguistici per compiti a valle, tracciando i sentieri degli bias politici che portano a modelli NLB ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "i modelli linguistici vengono addestrati su dati di scansione web su larga scala."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "i media sono ben rappresentati nei loro dati di pre-addestramento, secondo un'indagine sul corpus C4 possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post ecc. sono ben coperti nei dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una benedizione a metà per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "da un lato sono stati in grado di apprendere da prospettive diverse, celebrando la democrazia e la pluralità delle idee. D'altro canto, queste diverse opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "a tale scopo, proponiamo di indagare il pipeline di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, specificamente ponendoci le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "innanzitutto, come valutiamo il significato politico dei modelli linguistici e quale ruolo potrebbe avere la scelta dei dati su tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano i modelli linguistici con diverse plutolinee nei compiti a valle e se ciò potrebbe portare a problemi di equità nelle applicazioni di NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo specificamente, in primo luogo, di stimolare i modelli linguistici con diversi formati di prompt utilizzando questionari politici come il test della bussola politica. Questo ci consente di effettuare una valutazione automatica ben fondata nella letteratura delle scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che i modelli di linguaggio di prima lingua presentano orientamenti politici variabili. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "puoi anche notare che GPT4 è il modello linguistico più liberale tra tutti e che la serie GPT è generalmente più liberale dal punto di vista sociale rispetto alla serie BER e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, ci proponiamo di esaminare in che misura i pregiudizi politici dei modelli linguistici siano effettivamente acquisiti dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "potremmo condurre un esperimento controllato pre-allenando ulteriormente i checkpoint del modello linguistico su sei diversi corpora partisan, separati in notizie e social media, ulteriormente suddivisi in base alle loro tendenze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Analizzando ulteriormente i modelli linguistici pre-addestrati su tali parti e corpora, possiamo osservare uno spostamento corrispondente nelle coordinate ideologiche del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per Roberta, ulteriormente addestrata sul corpus di Reddit orientato a sinistra, possiamo osservare uno spostamento liberale sostanziale in termini di sue capacità."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "In termini di pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare se i modelli linguistici siano in grado di cogliere la polarizzazione che caratterizza la nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "dividiamo i corpora di pre-addestramento in due periodi: prima e dopo il 45° presidente degli Stati Uniti. Quindi, addestriamo separatamente i modelli linguistici su questi due diversi corpora temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Si può osservare che i modelli linguistici hanno generalmente mostrato una tendenza politica più distante dal centro dopo il 2017. Questo suggerisce che anche i modelli linguistici possono riflettere la polarizzazione presente nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Infine, valutiamo i modelli linguistici con diverse inclinazioni politiche nella rilevazione di discorsi d'odio e nella detectione di fake news, applicazioni di NLP che spesso coinvolgono modelli linguistici e che potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, osserviamo che se analizziamo le prestazioni per categoria, ovvero se suddividiamo le prestazioni in..."}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "In diversi contesti demografici o nei media di centro-sinistra, possiamo osservare un modello secondo cui, ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di sinistra sono più efficaci."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "Nell'individuazione di discorsi d'odio rivolti a gruppi minoritari sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, siamo meno bravi a rilevare il discorso d'odio rivolto ai gruppi più potenti della nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "al contrario, i modelli linguistici corretti sono migliori nel rilevare discorsi d'odio rivolti a bianchi e uomini, ma peggiori nel rilevare discorsi d'odio rivolti a persone nere, LGBTQ+ e altre comunità minoritarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Anche nel campo della rilevazione delle fake news si osservano tendenze simili, dove i modelli linguistici di orientamento sinistro sono più abili nel rilevare disinformazione proveniente da fonti politiche opposte e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "in questo studio presentiamo inoltre numerosi esempi qualitativi per dimostrare che i modelli linguistici con diverse implicazioni politiche,"}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "fornire previsioni diverse per esempi di discorso d'odio e disinformazione in base alle loro categorie sociali. Nell'Appendice sono presenti ulteriori esempi per evidenziare meglio questo aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che esiste un problema di equità molto pressante relativo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se dei modelli linguistici di destra fossero affinati su discorsi d'odio, disinformazione o qualsiasi altra cosa e poi implementati su una popolare piattaforma di social media,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate, e i discorsi d'odio rivolti ai gruppi minoritari potrebbero diffondersi impunemente senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha suonato l'allarme per riconoscerne e affrontare i problemi di equità derivanti dai significati politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "In questa sede, desideriamo evidenziare il dilemma unico relativo ai pregiudizi politici dei modelli linguistici, una sorta di scelta tra Silla e Cariddi."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento del modello linguistico, il pregiudizio si propagherebbe dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando alla fine problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se provassimo in qualche modo a sanificare, rischieremmo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e debba essere mantenuto nel linguaggio monotono dei dati. È un po' come il problema del trolley elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "ottimo. Penso che questo sia più o meno tutto per oggi. F5 per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jenny, sono una studentessa del primo anno di dottorato presso la Carnegie Mellon University, e oggi presenterò il mio lavoro \"Posizionalità anale: caratterizzazione dei pregiudizi nel design e nei modelli di set di dati\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santi, Ronan Labrasse, Katarina Reinika e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo quindi immaginando di lavorare per un giornale e di setacciare i commenti sotto un articolo di attualità, cercando di rimuovere i contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potresti rivolgerti a un'API popolare come Perspective API per il rilevamento della tossicità, e questa funziona davvero bene se sei Carl Jones, poiché la rispettiva API è in grado di rilevare correttamente le istanze tossiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma questo non è davvero il caso di Aditya Sharma, dove l'API A prospettica non è in realtà così sensibile ai termini offensivi che sono più comuni nei contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio nel design in cui si osservano differenze sistematiche nelle prestazioni tecnologiche tra diverse popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "Preconcetti di progettazione come quello che abbiamo appena visto potrebbero portarti a condividere la prospettiva dei ricercatori di NLP e degli sviluppatori di modelli. La prospettiva è semplicemente il punto di vista che le persone hanno in virtù dei loro dati demografici, identità ed esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E in quanto ricercatore, la posizione può influenzare il processo di ricerca e i suoi esiti e risultati, poiché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno una posizione o un'ubicazione specifica?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "E non stiamo cercando di affermare che i modelli nelle cellule e nei dataset stessi abbiano identità demografiche e esperienze di vita, ma essi aggrega giudizi e opinioni di persone reali e possono così rappresentare certe posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, lavori precedenti hanno suggerito alcune evidenze aneddotiche della presenza di posizionamento, come ad esempio lacune culturali nei modelli e nei dataset, nonché definizioni teoriche della posizionalità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non si concentrano effettivamente sul confronto tra gli utenti finali con i dataset e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio della posizionalità dei modelli e dei set di dati diventa sempre più importante man mano che i test di NLP diventano più soggettivi e orientati al sociale."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "E caratterizzare in che modo queste posizioni sono distorte è una sfida, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Per studiare la posizione del dataset e del modello, confrontiamo effettivamente le annotazioni con gli utenti reali con i dataset e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "fai questo attraverso la nostra struttura di posizionamento NL."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il framework opera in due passaggi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo è quello di ri-annotare i dataset con annotatori diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E dovremmo fare questo considerando la demografia degli annotatori del dataset originale, poiché solitamente solo pochi annotatori annotano ogni istanza e perché le demografie vengono raramente raccolte e condivise."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "E così optiamo per ri-annotare i dati per ottenere molte annotazioni, per esempio, e per avere un insieme ricco di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Prendiamo quindi le annotazioni demografiche e le confrontiamo con i modelli e il dataset utilizzando il punteggio di correlazione R di comparisonar."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E così il nostro framework differisce effettivamente dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con i modelli e i set di dati, le previsioni e le etichette, piuttosto che limitarsi a esaminare solo l'accordo degli annotatori o la modellazione delle distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il framer è in gran parte abilitato attraverso Lab in the wild, una piattaforma online di crowdsourcing precedentemente collaboratrice nel campo dell'HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "E Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversificati rispetto a piattaforme come MTERk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Ospitiamo due compiti in un laboratorio \"in the wild\", uno dei quali riguarda l'accettabilità sociale, e il funzionamento è il seguente: i partecipanti leggeranno una situazione dal dataset di chimica sociale e poi scriveranno quanto una situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, per rimanere coinvolti nella città, possono confrontare le loro risposte con un'IA e con quelle di altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi confrontato queste annotazioni con la chimica sociale, il metodo Delphi e GPT4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "poi replicare un'impostazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, dove leggeranno un'istanza da Dinah hatete e scriveranno se pensano che si tratti di un'istanza di discorso d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi confrontato queste annotazioni con Dynah Hate, Perspective API, Rewire API, Hate Roberta e GPT4. Il nostro studio ha raccolto alla fine oltre 160.000 annotazioni da più di 1.000 annotatori provenienti da 87 paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Ora siamo meglio attrezzati per rispondere a chi si allinea di più con i dataset e i modelli di NLP. Scopriamo che esiste una posizione nel NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, riscontriamo che il dataset e i modelli sono più allineati ai paesi di lingua inglese. Pertanto, per l'analisi dell'accettabilità sociale del GPD4, constatiamo che è più allineata ai paesi confuciani e di lingua inglese. Riscontriamo inoltre che il \"dyna hate\" (odio dinamico) è anch'esso più allineato ai paesi di lingua inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo anche la maggior corrispondenza con le persone che hanno un'istruzione universitaria. Quindi, per il GPD4 nel compito di Accettabilità Sociale, troviamo che è più allineato con le persone con un'istruzione universitaria o post-laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "riscontriamo lo stesso per Diny Haight, dove è più allineato alle persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i dataset sono allineati a popolazioni specifiche, alcuni inevitabilmente rimangono esclusi."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai corrispondenti uomini e donne. Riscontriamo questo sia nel compito di accettabilità sociale GPG4 che nell'analisi del compito di Diny hatete."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Dato che esiste una posizione in LD in LP, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo alcune raccomandazioni a riguardo. La prima è mantenere un registro di tutte le scelte di design rilevanti durante l'intero processo di ricerca, e l'altra è condurre ricerche di NLP con una prospettiva perspectivista."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di costruire dataset e modelli specializzati all'interno di quattro comunità specifiche, e un buon esempio di ciò è l'iniziativa Masakanne. Intendiamo sottolineare che l'NLP inclusivo non si limita a far sì che tutte le tecnologie funzionino per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questa conclude la nostra presentazione, ma se desiderate saperne di più, sentitevi liberi di consultare il nostro cruscotto per i risultati delle analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo X Yuan dell'Università Faii. Sono qui per presentare il nostro lavoro: \"Distinguere la Conoscenza degli Script dai Modelli Linguistici Leggeri per la Pianificazione Linguistica Vincolata\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, chi deve spesso pianificare le proprie azioni seguendo istruzioni passo-passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici del passato hanno esplorato la pianificazione per obiettivi astratti di attività stereotipate, come preparare una torta, e hanno dimostrato che i grandi modelli linguistici possono efficacemente scomporre gli obiettivi in passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con obiettivi specifici, vincoli specifici, come ad esempio preparare una torta al cioccolato, rimane ancora poco esplorata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "Che impongono vincoli diversi agli obiettivi della pianificazione; un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei modelli di linguaggio della vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "non esistono dati al di fuori di obiettivi specifici per individuare il giorno della nostra stella."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "devono prima acquisire questi obiettivi come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione di dati con umano in ciclo, utilizziamo istruzioni Gpt."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Campioniamo centinaia di obiettivi specifici e valutiamo gli script generati da modelli logici."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Riscontriamo che tutti i modelli Lilong ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi conduciamo un'analisi dettagliata per indagare su cosa mirano i modelli di apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati illustrati nella figura mostrano che la completezza settimanale negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Esploriamo categorie di argomenti più specifici relativi ai vincoli definiti in Wi home. La mappa di calore nel grafico mostra che le prestazioni di pianificazione delle istruzioni variano considerevolmente per le ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli in tempo reale presenta una elevata varianza, portando a prestazioni scadenti. Pertanto, abbiamo adottato l'idea di sovragenerare il filtro per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo inizialmente tipi vincolati con esempi per istruire CPT e ottenere obiettivi specifici basati sugli obiettivi astratti impostati."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Istruire il GPT sui principali script generali per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, viene derivato un modello di filtro per selezionare gli script fisici."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo script e frasi in embedding GPT e calcoliamo la similarità coseno come punteggi di similarità per valutare la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, premiamo la sceneggiatura che contiene le parole chiave della restrizione obiettivo. Conserviamo la sceneggiatura solo se l'obiettivo del sito ottiene il punteggio più alto."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, l'instructibilità può generare viti di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità sia in termini di semantica, completezza che di fedeltà alla restrizione."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché i grandi modelli linguistici sono costosi da implementare, è fondamentale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di un dataset è un passaggio essenziale per"}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, gli studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale dei dati del dataset è costosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare un dataset di pianificazione linguistica vincolata da modelli linguistici di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Apriremo il nostro metodo per la costruzione di un dataset di pianificazione linguistica vincolata, denominato CodeScri."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo generato cinquantacinquemila obiettivi specifici con script per garantire la qualità dei siti di validazione e test. Chiediamo ai lavoratori di crowdsourcing di rivedere infine il reddito nei campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione delle restrizioni di codeSscript. Riscontriamo che Coscript presenta un elevato pluralismo negli obiettivi specifici generati. Con Coscript, possiamo gestire modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Con le dimensioni, t cinque finetu sulla velocità di punteggio possono generare script di qualità dei capelli e dei modelli a livello più ampio, indicando che i modelli più piccoli possono sopprimere quelli più grandi quando vengono addestrati correttamente su siti di dati adatti."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Abbiamo sviluppato la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e un metodo di filtro sovragenerato per i grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo grandi modelli linguistici per generare un dataset quadrato di alta qualità, Codecri, per la pianificazione linguistica vincolata. Speriamo che il dataset CodeSscript possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il suo tempo. Per ulteriori dettagli su Codecri, si prega di consultare il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "salve a tutti, mi chiamo Shu H. Oggi presenterò il nostro articolo, \"Do Connell 2003: i Tagger di entità denominate funzionano ancora nel 2023?\". Iniziamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità denominate, o compito NER."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli hanno utilizzato ConONO 2003 per sviluppare il NER da quasi 20 anni, e questo naturalmente solleva diversi problemi. In primo luogo, questi modelli possono generalizzare a dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo un nuovo taggger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, se osserviamo una scarsa generalizzazione, quali sono le cause del calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare questi problemi, abbiamo sviluppato il dataset Connell++. Si tratta di un insieme di dati che abbiamo raccolto dalle Reuters News a partire dal 2020 e successivamente annotato seguendo le stesse linee guida di annotazione di Connell del 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "quindi ottimizzate su oltre 20 modelli su Conal 2003. Le abbiamo valutate sia sul set di test Con O3 che sul set di test Cono plus first."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che sono necessari tre ingredienti principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "La prima è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo riscontrato che i modelli basati su transformer generalmente generalizzano meglio ai nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo riscontrato che solitamente i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, sappiamo tutti che il numero di esempi di regolazione fine influenza direttamente le prestazioni di un'attività a valle. Qui, abbiamo anche riscontrato che un maggior numero di esempi di regolazione fine porta effettivamente a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "la nostra prossima domanda, qual è la causa del calo delle prestazioni di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due ipotesi. La prima è l'overfitting adattivo, che si verifica quando i costi di overfitting derivano dal riutilizzo dello stesso set di test ripetutamente, e questo si manifesta solitamente come un rendimento decrescente su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e quelli di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per il sovradattamento del modello dativo, abbiamo osservato che dal grafico a destra, la linea di miglior adattamento rossa ha una pendenza superiore a 1."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su Colo 2003 si traduce in più di un'unità di miglioramento su Colo++, il che implica che non ci sono rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che in questo caso non si osserva il sovradattamento adattivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "E per quanto riguarda la temperatura di ciò?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo riscontrato che le prestazioni si degradano con un divario temporale più ampio."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, una dimensione del modello più grande nonché più esempi di ottimizzazione. Questi obiettivi devono andare di pari passo. Non possiamo avere solo un ingrediente ma tutti gli altri nel complesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, abbiamo riscontrato che il calo delle prestazioni qui è causato da una deriva temporale e, sorprendentemente, non è dovuto all'adattamento della vestibilità, nonostante il metodo di Connell del 2003 sia stato utilizzato per oltre 20 anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, tornando alla domanda che abbiamo posto nell'introduzione del nostro articolo, i tagger di Carnal 2003 funzionano ancora nel 2023? Abbiamo scoperto che la risposta è in realtà un sonoro sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare le generalizzazioni dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "Infine, assicurati di consultare il nostro articolo, il nostro dataset e, se hai domande, non esitare a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "salve, e parlerò del nostro lavoro sulla risoluzione di espressioni differenziali indirette per la selezione di entità, in cui introduciamo il corpus di entità alternative."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Il mio nome è Javad Hosseini e questo è un lavoro congiunto con Philipp Radlinsky, Sylvia Parity e Annie Greece."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo è comprendere il linguaggio degli utenti quando desiderano effettuare una scelta e considerare questa alternativa: intendevi \"easy on me\" o \"I got a feeling\"? Qui un utente vuole selezionare una di queste due canzoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è utilizzare un riferimento diretto, per esempio dicendo che il nome della canzone è \"su di me\" o la sua posizione, \"la prima\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "le pronunce sono troppo simili tra loro e risultano difficili da distinguere"}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "oppure quando l'utente desidera specificare una preferenza. Ecco alcuni esempi in differenze dirette, ad esempio il più recente o il segno che non è energetico."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "è un problema importante nei sistemi conversazionali e anche per la valutazione della comprensione delle entità da parte dei modelli linguistici di grandi dimensioni (LLM)."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "non siamo a conoscenza di un set di dati pubblico su larga scala per questo compito, quindi ne abbiamo raccolto uno utilizzando l'annotazione collettiva. Il nostro set di dati copre tre diversi domini: musica, libri e ricezione."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La metodologia di raccolta del dataset enfatizza l'informalità utilizzando un insieme di completamento a fumetti."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il fumetto ha tre nuvolette. Nella prima nuvoletta Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\" e con ciò Bob stabilisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "in questo secondo fumetto Alice dice:\n\"Ti riferisci a essere gentile con me o ho una sensazione?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "è la ricerca alternativa e nel terzo fumetto Bob utilizza un riferimento indiretto per selezionare una di queste entità, per esempio l'amico più recente."}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "fornisci automaticamente i primi due balloon di testo, mentre il terzo viene compilato dall'annotatore; il primo balloon di testo viene scelto tra alcuni prompt manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo, che è la domanda alternativa, viene generato come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "utilizza sempre un modello semplice. Intendi il modello A o B, dove A e B sono esempi tratti da Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento utilizzati quando ci si sposta verso l'alto nell'elenco, le entità diventano più simili tra loro e solitamente è più difficile effettuare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "il primo è uniforme"}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, per esempio due libri con il nome \"il commercio al dettaglio\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "il terzo caso si verifica quando hanno descrizioni simili su Wikipedia e infine quando presentano voci o attributi di informazioni simili su Wikipedia, come ad esempio lo stesso genere o lo stesso artista."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "mostriamo questa domanda alternativa agli ameri; essi conoscono il nome di queste entità ma non necessariamente ne sanno qualcosa riguardo all'entità."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Quello che facciamo è mostrare alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "chiedi quindi agli annotatori di ascoltare almeno una parte di ogni canzone e di leggere le informazioni su ciascuna canzone qui sotto; ecco, ad esempio, il risultato della ricerca Google per la canzone \"Easy Answer\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "per il dominio delle ricette e dei libri, mostriamo del testo di sfondo da Wikipedia; per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Chiediamo quindi agli annotatori di scegliere una di queste entità, per esempio, qui la prima, e descriverla utilizzando tre o cinque espressioni di riferimento indiretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio quello con la musica del pianoforte, qui ci sono alcuni esempi dal nostro insieme di dati, ad esempio quello senza parole, non quello con il ragazzo di 12 anni, né quello fittizio, o quello che proviene dall'Azerbaigian, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "il corpus di alternative contiene 6.000 domande alternative distribuite in tre domini e presenta 42.000 risultati di espressioni di riferimento indirette. I risultati ottenuti con il modello large T5 sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso allo stesso identico bagaglio di conoscenze degli annotatori, allora l'accuratezza è davvero elevata. Si aggira tra il 92% e il 95%. Ma questa è una situazione non realistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso a una conoscenza di base parzialmente sovrapposta, allora l'accuratezza si attesta tra l'82 e l'87 percento, il che è più realistico, ad esempio, quando il modello linguistico recupera la conoscenza di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "se il modello linguistico ha accesso solo ai nomi di entità, l'accuratezza è solo del 6 percento, quindi c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili nel dominio. Ecco un link al nostro set di dati. Grazie per..."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Sarah Papppy dell'Università di Trento e del Centro Bruno Kessler e introdurrò brevemente l'attenzione come guida per la traduzione simultanea del parlato, un lavoro svolto in collaborazione con Matteo Negri e Marco Duchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Che cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o simSD, è il processo di traduzione del linguaggio parlato in testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "quali sono i problemi dei modelli SimST attuali? Solitamente vengono addestrate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedure di addestramento lunghe e complesse, per esempio, l'addestramento che coinvolge diversi obiettivi di ottimizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza, per esempio, addestrare un modello con una latenza media di un secondo e un altro con una latenza di due secondi, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Qual è quindi la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "Primi ad utilizzare modelli SD offline già esistenti senza ritraining o adozione di architetture specifiche per SSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E sfrutta le conoscenze già acquisite dal modello attraverso il meccanismo di tensione tra input audio e output testuale, ovvero il meccanismo di crosstenzione, come è possibile vedere nell'esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre un'attenzione decorale a punti o codificatore, ed è una strategia in cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Viene emesso un segnale se la tensione non è concentrata, ovvero se questa somma è al di sotto di una certa soglia alfa rispetto agli ultimi frame lambda del discorso, il che significa che le informazioni ricevute sono sufficientemente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, se riceviamo un frammento di discorso contenente \"I'm going to talk about\" e il nostro modello prevede la traduzione in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E esamineremo il peso dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i primi schemi di linguaggio ricevuti, mentre l'ultima parola si riferisce agli ultimi schemi di linguaggio ricevuti come schemi di linguaggio lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che le prime due parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "poiché la somma della tensione incrociata supera una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro frammento di discorso."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro frammento di discorso e il nostro modello prevede più di tre parole, esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che nessuna parola indica gli ultimi frame del discorso dell'Agnello."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che queste tre parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se si osserva il risultato principale di un punto."}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Rappresentiamo i risultati della traduzione simultanea delle pagine su grafici in cui abbiamo il blu su un lato che misura la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "quella è la misura della latenza. Consideriamo anche la media consapevole del calcolo, che manca e tiene conto del tempo di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo quindi che le nostre cure si posizionino il più in alto possibile su questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo anche che vengano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo con strategie plepara che sono applicate anche a modelli offline, che sono la strategia withK e l'accordo locale. Inoltre, confrontiamo con l'architettura allo stato dell'arte specificamente progettata per la traduzione simultanea del parlato."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea ad alta velocità sul tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E osserviamo che un dubbio supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, se consideriamo il tempo effettivo trascorso o il tempo di usura computazionale, quella è la strategia più rapida."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desideri scoprire ulteriori risultati, leggi il nostro articolo e abbiamo anche rilasciato codice e modelli open source, con output simultanei per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti. Mi chiamo Ian e io e il mio collega Jion presenteremo la nostra ricerca su Multi-Instruct, il miglioramento dell'apprendimento sociale multimodale tramite il tuning delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Con i progressi nei modelli di linguaggio di grandi dimensioni, molti lavori hanno iniziato ad esplorare nuovi paradigmi di apprendimento che riutilizzano i modelli di linguaggio pre-addestrati per diversi compiti a valle in modo parametrico e basato sui dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che il tuning delle istruzioni consente ai grandi modelli linguistici di eseguire compiti non visti in modo rapido ed efficace seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione delle istruzioni si è concentrata sul miglioramento delle prestazioni sequenziali in compiti solo linguistici, trascurando la visione artificiale e i compiti multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, desideriamo indagare se il tuning delle istruzioni su modelli proteintrain multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali non visti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo riscontrato una notevole discrepanza nella disponibilità del dataset di istruzioni tra RP e multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "esistono più di 1600 compiti di istruzione solo per il pranzo, tuttavia non esiste un compito di istruzione multimodale su larga scala pubblicamente accessibile. Questo ci ha motivato a costruire un insieme di dati di regolazione dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi-insstruct, il primo set di dati di benchmark per il tuning delle istruzioni multimodali che consiste in 62 compiti multimodali diversificati che coprono 10 categorie generali."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "i compiti sono derivati da 21 dataset open source esistenti e ogni compito è dotato di cinque istruzioni scritte in Expir."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "nell'indagine sull'accordatura dell'istruzione multimodale, il nostro dataset proposto prende in esame un modello di addestramento multimodale unificato come modello di base, utilizzando un vocabolario unificato per i token linguistici, i token delle immagini e le coordinate di una casella delimitatrice."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui mostriamo alcuni esempi tratti dal nostro dataset multi-istrumento."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "unificare l'elaborazione di vari tipi di dati in ingresso e in uscita."}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo OFA e formuliamo tutti i compiti in un formato sequenziale unificato in cui il testo in ingresso, le immagini, le istruzioni e le bounding boxes sono rappresentati nello stesso spazio token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Va bene, ora parlerò del tuning dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Per il dataset di addestramento, utilizziamo 53 compiti dal gruppo N per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di lettura del senso comune e selezioniamo ulteriori cinque compiti da WiQ e dal gruppo miscelato."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutte le istanze nella velocità di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla velocità di test dell'istruzione naturale come per lo stesso compito per NRP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo quindi un modello OFA pre-addestrato di grandi dimensioni come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata casualmente con una delle sue 5 template di istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante il test per ogni compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando entrambe le 5 istruzioni in ogni esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Riportiamo la media e le prestazioni massime, nonché la deviazione standard delle prestazioni attraverso tutti e 5 gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un'attività di classificazione multimodale, riportiamo l'accuratezza. Se si tratta di un'attività di generazione multimodale, riportiamo la metrica rootjL. Per un'attività RP, riportiamo anche il valore di RujL."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Questa misura la capacità del modello di produrre in modo coerente gli stessi output per lo stesso compito, indipendentemente dalle lievi variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i nostri risultati principali. Come possiamo vedere, il tuning delle istruzioni può migliorare significativamente le prestazioni dell'OFE in alcuni compiti multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Anche il transfer learning dai dataset di istruzioni naturali può trarre vantaggio dal tuning delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo osservare come, con l'aumentare del numero di compiti, il modello raggiunga prestazioni migliori e contemporaneamente una minore sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento. Abbiamo utilizzato un'istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo mostra l'effetto di diverse strategie di accordatura frontale sulla sensibilità del modello. Come possiamo vedere dall'apprendimento per trasferimento da un dataset di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto al modello IFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche osservare che il transfer learning dai dati di istruzione Nitro può aiutare OFA a raggiungere prestazioni molto migliori sul set di dati di istruzione NitroE."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "Nel complesso, abbiamo proposto il primo set di dati di tuning delle istruzioni multimodali su larga scala. Con l'ottimizzazione continua delle capacità neurali di OFA, esploriamo diverse tecniche di apprendimento trasferibile e dimostriamo i loro vantaggi. Progettiamo una nuova metrica chiamata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Quindi un'altra cosa che stiamo facendo è raccogliere insiemi di dati di tuning delle istruzioni multimodali molto più grandi, con circa 150 compiti linguistici varianti aggiuntivi, e li renderemo disponibili. Questo è un codice QR per i nostri dati e modelli. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti. Sono Koovsna e sono lieto di darvi il benvenuto alla nostra presentazione del lavoro ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con John Baqui, Aaron Muller, Kanishka Mishra, Karen Fs, Roger Levy e Atina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, rivalutiamo il paradigma dei minimi coppie."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il minimal pairtopara valuta essenzialmente i modelli linguistici in base a giudizi di accettabilità, che possono includere anche la grammaticalità, come nel caso di BLIMP, Syntax Gym, o l'accettabilità in termini di stereotipi, come ad esempio le coppie di parole che evocano folle."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma di coppia minima, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticalmente corretta e poi presentare una frase inaccettabile o grammaticalmente scorretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi si spera che il modello attribuisca essenzialmente una probabilità maggiore al regolamento accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP non ci consente di valutare l'accettazione dei modelli per le frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Al giorno d'oggi, i grandi modelli linguistici stanno generando contesti sempre più lunghi. Pertanto, è cruciale valutare l'accettabilità del modello in tutto l'intervallo di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere il pipeline NPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo è l'approccio. Quello che facciamo è che per simulare queste sequenze più lunghe, rivisita gli stessi set di dati e poi ricrea le frasi scegliendo, per esempio, frasi accettabili o non accettabili da quei set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, qui abbiamo scelto una coppia tipica di frasi dal dataset BbliIM, tratta dal caso dell'isola complementare."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è ricreare sequenze più lunghe e accettabili, con la stessa corrispondenza della struttura grammaticale, estraendo frasi grammaticalmente corrette da un pilota di riferimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi aggiungiamo come prefisso sia alla query accettabile che a quella non accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Quindi possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento, e questo potrebbe anche essere utilizzato per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un diverso insieme di dati. Questo è ciò che definiamo uno scenario di mismatch."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Qui, le frasi provengono ancora da insiemi di dati pertinenti, ma non dallo stesso insieme di dati che stai valutando. E possiamo fare lo stesso per il caso di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente estraneo, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "Se il contesto deriva da un sottoinsieme diverso del set di dati o se è completamente irrilevante rispetto alla frase che stiamo esaminando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Allora, come si comporta il modello? Innanzitutto, esaminiamo le frasi di Wikipedia che sono completamente irrilevanti per la coppia di query corrente, e qui scopriamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino al 2024 per sfruttare al massimo i modelli OPT e GPT2. Come si può vedere dalla linea tratteggiata arancione, i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Ora cosa accade quando scegliamo frasi dallo stesso insieme di dati?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ecco che scegliamo o creiamo frasi da domini accettabili e non accettabili dallo stesso insieme di dati BlimIM syntax gymIM."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E qui vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando allineiamo la struttura, ovvero quando selezioniamo le frasi dallo stesso fenomeno in colpa persona taxgen,"}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo un enorme aumento o una enorme diminuzione del giudizio MPP per il modello a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora questo -- e questo è molto significativo, come questo effetto aumenta lungo la lunghezza del contesto, e questo probabilmente influirebbe su modelli linguistici più recenti che hanno una finestra di contesto ampia."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Perché quindi il prefisso \"match\" influenza così tanto il giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo condotto una serie di analisi in cui abbiamo cercato di perturbare la frase in ingresso, tentando di preservare la struttura rilevante ma aggiungendo del rumore all'input. E dopo aver eseguito diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo che nessuno di questi rumori sta effettivamente inducendo il modello a modificare il suo corso in termini di come ci mostra poi la tendenza del giudizio di pagamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "In sostanza, riscontriamo che i modelli sono sensibili alla struttura delle frasi in modi simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "È quando perturbiamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi nel dominio di approvazione accettabile, notiamo una diminuzione nei giudizi MPP in modo analogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Quindi i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, nel modo in cui la effettuiamo attualmente con input brevi e a singola frase, potrebbe non catturare completamente la conoscenza astratta dei modelli linguistici attraverso la finestra contestuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Si prega di leggere il nostro articolo per maggiori dettagli sui nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti. Mi chiamo Just John dell'Università Statale della Pennsylvania. Oggi presenterò il nostro lavoro, \"Exemplar: parsing semantico cross-lingue in più lingue naturali e rappresentazioni manuali\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Il processamento semantico è un'attività volta a costruire rappresentazioni semantiche di query utente come ZQL e il calcolo lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "la pars semantica interlinguistica è il compito di tradurre le query in più lingue naturali in molteplici rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "come mostrato nella sua figura, è necessario tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o funQL e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "Esistono modelli di analisi semantica cross-lingue proposti e valutati separatamente su un insieme limitato di compiti e applicazioni. Ad esempio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "Esistono lacune nella copertura di alcune lingue naturali; il cinese è assente, e..."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "Copertura lacunosa su molte rappresentazioni certe."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il calcolo dell'Agnello è mancante."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "vengono valutati solo su un determinato modello neurale, ad esempio esiste un solo modello per la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, abbiamo proposto Ex, un generatore di esempi, ma forniamo anche un insieme di dati uniforme per l'esemplificazione semantica interlinguistica in più lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "contiene 90 insiemi in domini virali, 5 parti semantiche nelle tasse, 8 milioni di rappresentazioni e 22 lingue naturali in 15 famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "il primo è un test di traduzione; utilizzeremo l'API di Google Translate per tradurre la fonte nella lingua di destinazione, quindi useremo un modello monolinguale per addestrare qualsiasi valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, addestriamo il modello inglese con query in inglese e durante l'inferenza traduciamo la query tedesca utilizzando un'API in inglese, per poi usare il modello addestrato per prevedere l'SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "Testeremo anche il modello monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "in questo caso, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco in tedesco o inglese in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "testa anche l'impostazione futura monolingue addestrando modelli bilingue con solo il 10 percento dei dati di addestramento"}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "E che cosa significa modellare un modello multilingue, che addestriamo un solo modello multilingue per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo raggruppato le query in tedesco, inglese e cinese per addestrare un modello multilingue e durante l'inferenza possiamo utilizzare anche questo modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "Per tradurre richieste in tedesco, cinese o altre lingue, ecc."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "Consideriamo anche il trasferimento cross-lingue senza esempi (zero-shot) e il trasferimento senza esempi. Addestriamo su una lingua sorgente e trasferiamo ad un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "durante l'addestramento, lo addestriamo su query in inglese o sulla combinazione di poche query brevi in inglese e tedesco per addestrare un modello multilingue e prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "troviamo anche molti risultati interessanti. Quindi, per quanto riguarda l'analisi dei modelli monolinguali, valutiamo su due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "inclusi encoderPDdR, che rappresentano codificatori pre-addestrati multilingue con decodificatori basati su puntatori come X elementr plus pdr e bird plus pdr."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo anche i modelli encoder-decoder, che sono modelli multilingue pre-addestrati, come B e Mt5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "ha riscontrato che l'encoder-decoder ottiene le prestazioni migliori su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "valutiamo il nostro Mmt5 e ad esempio xlmr plusPDdr nei nostri contesti multilingue"}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "che l'encoder-decoder o l'encoder PDR può essere migliorato attraverso l'addestramento su una miscela di diverse lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo riscontrato che ciò è dovuto al fatto che la maggior parte delle principali lingue naturali ottiene un miglioramento delle prestazioni, eccezion fatta per l'inglese, le cui prestazioni diminuiscono in sette set di dati e migliorano solo in tre."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Credo che questo sia noto come multilateralità dei Curdi."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche il divario nelle prestazioni cross-lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento cross-linguistico Fu, la linea arancione il trasferimento cross-linguistico zero-she, mentre la linea verde indica l'impostazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che confrontando la linea verde e quella arancione, per un'impostazione di lunghezza breve pari a zero, il divario nelle prestazioni di trasferimento cross-lingue è significativo. Inoltre, confrontando la linea blu e quella arancione, abbiamo scoperto che per un'impostazione di lunghezza breve, il divario di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "sono state riscontrate anche altre interessanti scoperte. Ad esempio, l'encoder-decoder supera il lavoro di pre-elaborazione o ha ottenuto risultati comparabili. Perseguire la comprensione della lingua naturale inglese può migliorare significativamente le prestazioni future sulle lingue naturali di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo riscontrato che modelli linguistici multilingue come Coders e Blue sono ancora inadeguati per le classi di semi-personalizzazione cross-lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "in sintesi, abbiamo costruito un esempio, un benchmark unificato per il parsing semantico cross-lingue con più lingue naturali e molteplici rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "condurre uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue e i nostri risultati mostrano molte scoperte interessanti e via dicendo; vi invitiamo a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "il mio nome è Al Villaad e fornirò una breve panoramica della stampa di documenti sulla palma della mano, esaminando strategie di traduzione e prestazioni. Questo è un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "è un modello linguistico di 540 miliardi di parametri, presentato lo scorso anno nel 2022. È addestrato su una vasta raccolta di testi comprendente 780 miliardi di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "Duma per la cucina raggiunge lo stato dell'arte in centinaia di compiti di elaborazione del linguaggio naturale (NLP)."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "in questo lavoro presentiamo un primo studio sistematico sull'uso di prompt di grandi modelli linguistici per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità IMT. Ciò comporta l'utilizzo dei più recenti set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "confrontiamo con sistemi all'avanguardia, quindi i migliori sistemi di prestazioni o le valutazioni WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche di neuralMT all'avanguardia e, inoltre, presentiamo anche risultati di valutazione basati su esperti umani. Infine, forniamo alcune raccomandazioni per le strategie di selezione degli prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "l'indicazione ha una grande influenza sulle prestazioni dei sistemi di traduzione neurale (NMT) come possiamo osservare in un semplice esperimento in cui utilizziamo una breve indicazione e forniamo due diversi suggerimenti per frasi diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "nella maggioranza delle frasi, 516 su 1000, la differenza osservata è di più di un punto sfocato."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "Questo può arrivare, nei casi estremi, fino a 40 punti di sfocatura. Quindi è importante scegliere una buona strategia di prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "i nostri esperimenti con una soluzione per una strategia di prompt a cinque riprese, in cui segniamo semplicemente la frase che forniamo al sistema nella lingua in cui è scritta."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi in tedesco, le frasi di origine, sono contrassegnate dal due punti tedesco e le traduzioni in inglese dal due punti inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "ho notato che la forma effettiva della stampa non ha una grande influenza nel caso di diverse stampe brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È cruciale per il prompt a zero e a un colpo, e quando passiamo, come nel nostro caso, al prompt a colpo di fatto, non c'è quasi alcuna differenza nella forma effettiva del prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "sono gli esempi che sostengono la maggior parte del peso"}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "Una sintesi dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di origine."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "È importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo i prompt di selezione dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di sviluppo (dev data) sono creati in misura molto maggiore e con una qualità superiore rispetto ai dati di addestramento (train data), il che rende i risultati più soddisfacenti e migliora le prestazioni quando si utilizzano i dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "tuttavia i sistemi all'avanguardia specializzati hanno un notevole vantaggio rispetto alle traduzioni \"pan\", ma uno si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di evitare Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le intuizioni che ricaviamo dalla comunicazione via email che svolgiamo utilizzando il framework MQN sono che la fluidità del palmo è paragonabile a sistemi all'avanguardia, ma la differenza principale risiede nella precisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Sembra quindi che Palm opti per una traduzione che suoni meglio, talvolta omettendo parti della frase di origine che risultano superflue nelle traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, lo stile della categoria esterna per il pan è inferiore rispetto ai sistemi all'avanguardia, il che rappresenta un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "che il parm fornisce un output davvero fluido, ma ancora con alcuni problemi di accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "Ecco terminata questa breve panoramica. Per maggiori dettagli, vi prego di consultare la presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Dawei, sono uno studente di dottorato presso l'Università Silenziosa in Germania. In questo video vorrei presentarvi il nostro recente lavoro, \"Più grande di quanto pensiate: uno sguardo critico alle sorprese settimanali di Lening\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "è un lavoro congiunto con Sha my muba e gear Stefan e ditishklakov"}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "**Supervisione debole**: non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o sourcing di codici di localizzazione, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "addestriamo direttamente le reti neurali su dati con etichette deboli, queste tendono a memorizzare il rumore nelle etichette e non generalizzano."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Vengono proposti algoritmi di addestramento con apprendimento debolmente supervisionato per addestrare robustamente reti neurali in presenza di tale rumore nelle etichette, in modo che i modelli addestrati generalizzino comunque bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "Recenti lavori in wSL (dove wSL sta per \"weekly support learning\") affermano che le persone addestrano i modelli solo sui dati etichettati settimanalmente e ottengono elevate prestazioni su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Questa affermazione non è errata, ma c'è un inghippo."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "che le persone danno per scontato che esista un ulteriore insieme di validazione pulito o una forma benevola di selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "fermo su questo problema di impostazione, ma ciò implica che siano necessarie annotazioni manuali aggiuntive nel supporto di apprendimento settimanale; ma come un elefante in una stanza, questa necessità è spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "le suddette considerazioni ci portano a porci tre domande di ricerca. Innanzitutto, è necessario disporre di dati di validazione puliti per WSL? o possiamo eventualmente utilizzare un insieme di validazione rumoroso al suo posto?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "se i dati puliti sono necessari o se sono obbligatori per il funzionamento di WSL, quanti campioni puliti abbiamo effettivamente bisogno? Dovremmo utilizzare solo i campioni puliti per la validazione o esistono metodi migliori per sfruttarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo affrontato queste domande di ricerca nel nostro lavoro e le nostre scoperte sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, scopriamo che, interessantemente, i metodi WSL recenti richiedono infatti campioni di amplificazione puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "In caso contrario, si verifica un notevole calo delle prestazioni. Come illustrato in questa figura, se non ci sono campioni di validazione puliti, i modelli di tendenza non possono generalizzare oltre le originali etichette deboli."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "quello dell'addestramento è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "indica che gli approcci WsSL richiedono in realtà dati etichettati in modo pulito per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere sottovalutato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "La seconda scoperta è che l'aumento del numero di campioni di validazione puliti aiuterà gli approcci WSL a raggiungere prestazioni migliori, come mostrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo bisogno solo di 20 campioni per classe per ottenere elevate prestazioni"}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questo non è il termine della storia, perché se in ogni caso decidiamo di accedere a campioni puliti, allora l'addestramento diretto su di essi otterrà addirittura prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura in rosso mostra la differenza di prestazioni tra gli approcci di ottimizzazione fine che vengono applicati direttamente sui dati puliti e gli approcci WSL che utilizzano i dati puliti solo per la validazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo osservare che, con 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare il raffinamento sui campioni di convalida puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Dai grafici possiamo osservare che il modello valido denominato ftw inizialmente rende meno di metodi WSL più complessi come il coseno."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "se permettiamo ai fantuni di continuare sui campioni puliti, allora Tw si comporta altrettanto bene quanto altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "In pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo dimostrato che i recenti approcci wSL richiedono campioni annotati manualmente puliti per funzionare correttamente. Il loro miglioramento delle prestazioni e la praticità sono fortemente sovrastimati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le seguenti raccomandazioni concrete riguardano le future ore di lavoro."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, riportare i criteri di selezione del modello. Ad esempio, indicare se la selezione del modello è stata effettuata utilizzando campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, gli approcci WSL dovrebbero essere confrontati con poche brevi linee di base per l'atterraggio, come si suppone lavorando su campioni di cemento. Terzo, il continuo affinamento è una linea di base semplice ma solida che dovrebbe essere presa in considerazione per i futuri lavori in WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo reso il nostro codice open source. Puoi trovarlo tramite il codice QR presente su questa diapositiva. Ti invitiamo a esplorarlo. Grazie e buona conferenza a tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono James Finch. E io sono Sarah Finch. Oggi vi parleremo di ABC Eval, un nuovo approccio dimensionale per la valutazione dell'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal Laboratorio di NLP dell'Emory, diretto dal Professor Gino Choi presso l'Università di Emory, e in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Diciamo che hai appena sviluppato un modello di dialogo e vuoi vedere quanto bene si confronta con lo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La prassi comune è quella di utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni utilizzando una scala di qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare diverse dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "L'approccio consiste nel chiedere semplicemente a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi esistenti o scale di valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Questo approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime o meno determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio \"annotazione dei comportamenti in chat\", o ABCEval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chat che, secondo la letteratura recente, influenzano la qualità delle conversazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABC eval è in grado di misurare le velocità con cui i modelli di chat commettono vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, ABCEval misura il numero di turni in cui un modello di chat ignora il suo interlocutore o dice qualcosa di irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o il suo partner allucina fatti errati o viola il senso comune e quando il modello riesce o fallisce nel mostrare empatia"}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "per determinare quale tipo di valutazione fosse più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni tra bot e umani per modello utilizzando ABC eval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni del liquore a livello di turno, valutazioni del liquore a livello di dialogo e confronti coppia a coppia a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi dei risultati della valutazione, abbiamo riscontrato che le etichette comportamentali ABC sono complessivamente più affidabili rispetto a quelle raccolte con i metodi esistenti, come dimostrato dall'accordo tra annotatori interni su 100 conversazioni etichettate due volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette ABCEval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa analisi di regressione lineare semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Nell'esempio puoi vedere come la misurazione della proporzione di turni con contraddizioni auto-e-partner spieghi rispettivamente il cinque percento e il dieci percento della qualità della conversazione, mentre i punteggi medi di coerenza del liquore spiegano solo il quattro percento o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare passo-passo."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Si può osservare come la combinazione di tutte le metriche ABC Eval spieghi oltre il 25% della qualità della conversazione. E rimuovendo le metriche una alla volta, la maggior parte di esse comporta la perdita di una quantità significativa di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche di liquore a livello di turno spiega molto meno della qualità e meno di queste metriche trasportano informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Le metriche ABC Eval affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione superiore rispetto a quanto possibile con i metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "Si può osservare dai risultati del nostro esperimento che diverse sfide persistono e sono state quantificate con precisione. Ad esempio, i bot testati presentano violazioni del buon senso in circa il 20% delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "produrre informazioni irrilevanti in circa il 15% delle risposte, e contraddirsi o contraddire il proprio partner circa il 10% delle volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero vedere una diminuzione nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che la valutazione ABC possa essere sfruttata da altri nel campo come un passo significativo in questa direzione, e non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale si svilupperà nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Kyyo Yin e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede contesto: un'esplorazione multilingue basata sui dati\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernage, Emiliu Andre, FD Martins e Graham Newbiig."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Molte traduzioni dipendono dal contesto. Per esempio, come tradurremmo \"mole\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "se la frase precedente potesse iniziare a diventare pericolosa se i ministri lo scoprissero, allora \"più\" si riferisce a una spia. Ma se la frase precedente fosse Potrebbe essere qualcosa di grave, dottore?, allora \"più\" si riferisce a un neo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "A seconda del contesto, il significato della parola cambia e di conseguenza anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli possano confrontare casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola porzione delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus, come BLEU, incapaci di catturare queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "E alcuni hanno proposto valutazioni mirate sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curatela umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, cerchiamo di rispondere a queste due domande. Innanzitutto, quando la traduzione richiede un contesto? E in secondo luogo, come gestiscono questi casi i modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto il lavoro dipenda dal contesto durante la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. Ciò viene realizzato misurando quanto il contesto C fornisca informazioni sul target Y dato il fonte X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "Si può pensare a CXMI come alle informazioni ottenute fornendo un contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo CXMI a CXMI puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo considerare le parole con un alto PA6MI come quelle che richiedono il contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un alto valore di piecexMI per ricercare schemi tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "effettuiamo la nostra analisi su trascrizioni di TED Talks che sono state tradotte dall'inglese in 14 lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo la nostra analisi su tre livelli diversi. Innanzitutto, esaminiamo le etichette delle parti del discorso che presentano valori medi elevati di pxMI."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "ciò ci permette di trovare, per esempio, pronomi duali in arabo che hanno un p6MI relativamente alto. E questo può essere spiegato dal fatto che l'inglese non ha pronomi duali, quindi è necessario un contesto per determinare se un pronome è duale quando si traduce in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "Analogamente, scopriamo che alcune lingue richiedono anche il contesto quando si vuole scegliere la forma verbale appropriata. Ci concentriamo quindi sugli elementi lessicali che presentano un alto valore medio di pxMI (informazione mutua) in tutte le loro diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci aiuta a identificare casi come questo, dove in cinese è necessario il contesto per tradurre i nomi propri, per assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "Analogamente, scopriamo che il contesto è sostenuto per mantenerlo nella giusta formalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "infine, esaminiamo diversi token individuali che presentano un alto p6MI. Ciò ci consente di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo i risultati della nostra analisi per progettare un punto di riferimento per la traduzione di documenti innovativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni discorsivi identificati, abbiamo creato dei tagger per identificare automaticamente le parole pertinenti al fenomeno. Chiamiamo il nostro tagger \"multilingue consapevole del discorso\" o tagger MUDA."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo poi notare anche che le diverse lingue presentano proporzioni differenti di questi fenomeni discrezionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "quindi utilizzare l'M tagger applicando il tagger sul corpus parallelo che si desidera utilizzare per la valutazione e applicare le metriche di traduzione scelte sui contesti dipendenti identificati dall'M tagger."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "Infine, utilizziamo il nostro punto di riferimento nonché altre metriche per valutare diversi modelli nella traduzione automatica di documenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, quando utilizziamo metriche a livello di corpus, quindi per il blu, scopriamo che i modelli agnostici di Conic hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "allora, se utilizziamo il commento testuale, i modelli contestualizzati ottengono le migliori prestazioni. E se utilizziamo la misura delle parole, i modelli con o senza contesto hanno prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo solo metriche a livello di corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo il benchmark MUDA per valutare i modelli e riscontriamo che i modelli consapevoli del contesto sono significativamente più accurati rispetto a quelli che non utilizzano il contesto per alcuni fenomeni discorsivi, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori di quelli che non utilizzano il contesto per altri fenomeni come le ellissi, i pronomi e la forma verbale. Quindi, questo suggerisce in quale direzione dovremmo vedere maggiori progressi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre confrontato diversi sistemi commerciali e il nostro benchmark dimostra che DeP è solitamente più accurato di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, eseguiamo un'analisi basata sui dati su 14 coppie linguistiche per identificare quando le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizziamo i nostri rifinimenti per costruire un punto di riferimento per la traduzione automatica a livello di documento, che può aiutarci a identificare quali modelli di fenomeno discorsivo possono gestire bene o meno e quali sistemi di traduzione sono abili nella traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per la vostra attenzione. Ci vediamo a Trado."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Yanislavak e vi presenterò i nostri lavori su Dr. Bert, un modello di pre-addestramento robusto in francese per i settori biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, innanzitutto discutiamo del modeling linguistico in Herke. Successivamente, presenteremo il principale contributo del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in lingua francese chiamato Dr. Bert, basato su Roberta, e addestrato su Naos, che è un insieme di dati strisciati dal web contenenti informazioni mediche."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo anche un confronto tra modelli con diverse configurazioni protoniche e fonti di dati. Successivamente, presentiamo i nostri risultati su 11 compiti biomedici e clinici a valle in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "Infine, concludiamo parlando degli esperimenti e forniamo maggiori dettagli su come accedere ai modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Da quando è stato rilasciato nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre un notevole miglioramento delle prestazioni rispetto ai metodi storici, statici e contestualizzati come word2vec, fastText o GloVe."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese, con Cammbert, e ad altri domini come quello biomedico, con Permed Bert e Biobert, e in contesti clinici sulla nascita, ma principalmente in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "I modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati specifici del dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il francese non disponeva di alcun modello open-source per la biomelicon fino ad ora."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci poniamo quindi delle domande su quali siano le fonti di dati più appropriate per un'ampia gamma di utilizzi, e questi dati grezzi rappresentano una buona sostituzione per i dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, abbiamo confrontato il Dr. Bert con il nostro modello Schubert, basato su dati anonimizzati ottenuti dall'ospedale non-geneerico della nostra istituzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, ci chiediamo, quanti dati sono necessari per addestrare un modello specializzato sui dati francesi? Sono quattro gigabyte, un gigabyte o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, addestriamo e confrontiamo quattro modelli ex novo: una prima versione di D. Bert con sette gigabyte di nachos, e una seconda versione con quattro gigabyte di nachos."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che è un modello clinico, con quattro gigabyte di frasi tratte da nodi clinici, e una versione finale di Schubert, con una miscela di quattro gigabyte di insiemi di nature e quattro gigabyte di nodi clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "In aggiunta a questo confronto, abbiamo introdotto tre modelli addestrati con pre-addestramento contrastivo per analizzare l'impatto delle strategie di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso del Cammbert e addestrato su quattro gigabyte di insiemi di nachls; un altro, sempre basato sul Cammbert, ma addestrato questa volta sui quattro gigabyte di nodi Kcliner."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "E infine, un modello basato sul modello biomedico inglese, Bermed Bert, addestrato su quattro gigabyte di un insieme di estratti. In totale, disponiamo di sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, raccogliamo numerosi compiti a valle pubblici e privati, come il riconoscimento di nomi ed entità, la classificazione, l'etichettatura delle parti del discorso e la risposta alle domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questi modelli sono confrontati con sei modelli di progettazione B, che sono Cammbert OscarOS 18 gigabyte, Cammbert Oscar quattro gigabyte, Cammbert cinet quattro gigabyte, Lomet Bert, Biobert e Clin BERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "L'evoluzione degli elementi salienti mostra che il modello ottiene le migliori prestazioni nel compito utilizzando dati della stessa natura di quelli su cui è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo notare che i dati ottenuti da fonti eterogenee sembrano essere più versatili. Osserviamo inoltre che l'utilizzo di una maggiore quantità di dati si traduce in un miglioramento delle prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, i modelli addestrati da zero senza dati pre-addestrati sembrano ottenere prestazioni migliori nella maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento sul pre-addestramento di controllo utilizzando il peso e il tokenizzatore di Permi Bir addestrato sul sottoinsieme di quattro gigabyte di dati naturali, ha mostrato risultati confrontabili con quelli ottenuti con Dr. Bert quattro gigabyte da zero."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "Che non è il caso per il modello basato su Cammbert whites e tokenizer, che soffrono di problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la nostra conclusione è che il sistema appropriato offre prestazioni migliori in nove dei 11 compiti a valle e supera globalmente il risultato del modello generico qui presentato, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo inoltre che i dati specializzati sono migliori, più i dati sono specializzati, migliore è la loro qualità, ma ciò non si scala bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "tutti i modelli pre-addestrati ottenuti da nachos sono liberamente disponibili e si trovano sul tuo volto (o \"sulla tua piattaforma\" per una traduzione più chiara) e tutti gli script di addestramento sono nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, grazie per questa presentazione e non vediamo l'ora di vedere le azioni nella sessione poster a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Matthias Lindemann e oggi vi fornirò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi, utilizzando l'etichettatura di multiset e permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "è un lavoro congiunto con i miei supervisori Alexander Kola e Ivan Tittov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione composizionale può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe apparire così. Come al solito, abbiamo un insieme di addestramento di enunciati. In questo caso, la ragazza dormì, e Mary sapeva che la ragazza dormì."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Queste affermazioni sono accoppiate con forme logiche che rappresentano aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "In contrasto con la valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento e viene testato su esempi con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli sequenziali-sequenziali ingenui faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output scollegati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate con codici colore negli esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo popolare per affrontare questo è integrare gli alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono progettati per catturare il processo composizionale che collega le enunciati con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma gli alberi non sono solitamente forniti e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta computazionalmente costoso. Di solito, implica un considerevole pre-trattamento formalismo-specifico delle forme logiche, per esempio, per gestire i simboli delle variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L'ottenimento di alberi può comportare anche procedure specializzate di induzione grammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, non utilizziamo alberi e introduciamo un modello neurale di sequenza-a-sequenza che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, dimostriamo una forte generalizzazione verso una ricorsione più profonda senza fare affidamento su alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "l'approccio prevede la predizione dell'output dall'input in due passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Prima, etichettiamo ogni token di input con un multinsieme non ordinato di token che appariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passo, abbiamo tutti i token corretti ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché nel secondo passo utilizziamo un altro modello per prevedere una permutazione che li metta nel giusto ordine."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio molto flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona approssimativamente in questo modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Andiamo da sinistra a destra sull'output e determiniamo quale token multiminsieme posizionare in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno di essi, come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Poi passiamo al prossimo token multi-set per determinare il secondo token nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token nell'output in modo simile saltando a un altro token del multiset. Continuiamo questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "Fino a quando ogni token dalla prima fase non sarà stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per offrirti un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza albero sulla benchmark COGs. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi, tuttavia."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo, affrontiamo e risolviamo alcune interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-setter provenga, il che rappresenta una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte esistono diverse permutazioni compatibili con i dati, ma quella linguisticamente corretta è latente. Abbiamo affrontato questo problema inducendo l'allineamento come parte dell'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è un problema NP-difficile. Ciò è dovuto al fatto che è correlato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Approcciamo questo con una rilassamento continuo, amichevole per le GPU, che ci permette anche di backpropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desideri saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, ti invitiamo a leggere il nostro articolo o a partecipare alla presentazione del nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Akshata e oggi il mio co-autore Martin ed io presentiamo il nostro lavoro, Kit Master: Valutare l'Integrazione della Conoscenza da Multipli Fonti. Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione del linguaggio si basano su una varietà di fonti di conoscenza, come le conoscenze contenute nei loro parametri, solitamente acquisite durante un pre-addestramento, e le conoscenze fornite negli input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "i lavori in compiti come il rispondere a domande dimostrano che i modelli possono utilizzare la conoscenza temporale pre-addestrata per risolvere il compito"}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "ma la comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, nella frase \"John ha visto il presidente appena eletto in TV\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su ciò che fanno i presidenti e su cos'è un televisore, ma non possono conoscere in modo affidabile chi sia questa entità specifica, John, o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo il pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i modelli di successo per i compiti di comprensione del linguaggio naturale (NLU) ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia le conoscenze pre-addestrate che quelle acquisite durante l'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "in questo lavoro proponiamo una suite di test diagnostici per l'integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "introduciamo un compito di risoluzione del riferimento centrale progettato per indagare sulla capacità di attingere alle conoscenze disponibili in diverse fonti; valutiamo il set di dati con partecipanti allo studio umano e stabiliamo modelli di risoluzione del riferimento centrale."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro dataset. Servin è un giudice. Kia è una panettiera. Termin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro nel decidere casi secondo un codice legale, lui era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"he\", che in questo caso è \"sermone\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronome dato richiede due tipi di informazioni: innanzitutto, conoscenze specifiche sull'entità, come ad esempio \"servile è un giudice\", e in secondo luogo, conoscenze generiche come \"i giudici decidono i casi nei tribunali\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, le conoscenze di base vengono apprese durante la pre-formazione dei grandi modelli linguistici, mentre le conoscenze specifiche sulle entità sono tipicamente osservate al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "variare la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in fonti multiple."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "hanno definito tre impostazioni di kitmos, innanzitutto con l'impostazione tipica di background pre-train, dove la conoscenza retrograda è ipotizzata essere disponibile al tempo di addestramento libero."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è lo sfondo, sia come contesto in cui la conoscenza retrospettiva è disponibile sia prima che durante l'addestramento, e infine quello posteriore nel contesto dell'esperienza, dove entrambi i tipi di conoscenza sono accessibili solo durante l'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "l'ultimo scenario è particolarmente interessante in quanto simula il caso in cui le conoscenze di base necessarie per risolvere un compito non fanno parte dei dati di pre-addestramento dei modelli, ad esempio perché nuove occupazioni si sono sviluppate dal tempo del pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "È un esempio di come controlliamo la disponibilità dei fatti nelle due fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "nel contesto pre-addestrato, assumiamo che le conoscenze di base che i politici cercano di ottenere seggi eletti nel governo siano contenute nei parametri pre-addestrati. Nel contesto di interferenza temporale, forniamo la conoscenza anti-specifica che Chester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "**Contesto di sfondo**\n\nNel contesto sia ambientale che di impostazione, forniamo non solo informazioni non specifiche ma anche conoscenze di base sui politici nel contesto della scheda di interferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "nel contesto di un'impostazione libera da vincoli, offriamo il tour meritorio occupazionale fittizio al posto di quello politico, poiché il \"tour meritorio\" è improbabile che sia incluso nella regione pre-T20peri."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "valutare il set di dati sia con partecipanti allo studio umani che stabilire modelli di risoluzione delle preferenze in questa figura mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più difficile del contesto pre-addestrato di sfondo"}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "il nostro addestramento specifico per compito su Kidmus, entrambi i modelli non si comportano bene. Tuttavia, quando addestrati su Kidmus, sia C2F che built forQF si esibiscono significativamente meglio della scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Questo suggerisce che quando vengono addestrati su dataset generici di risoluzione di riferimento, i modelli imparano a sfruttare indizi superficiali che non sono utili quando vengono testati su Kidmus, dove tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "ulteriori esperimenti in cui la conoscenza fittizia ha indicato che persino i modelli con le migliori prestazioni non possono integrare in modo affidabile la conoscenza retroattiva solo al momento dell'interferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "riassumere i punti principali del nostro articolo: molti modelli di evoluzione delle coreferenze sembrano incapaci di ragionare su conoscenze provenienti da diverse fonti senza un addestramento specifico per il compito; tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo conoscenze da più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Ancora, anche i modelli con le migliori prestazioni sembrano avere difficoltà ad integrare in modo affidabile le conoscenze pregresse presentate solo al momento dell'inferenza. Per ulteriori dettagli, si prega di consultare il nostro articolo e di esaminare il set di dati nel codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Myra e oggi parlerò del nostro articolo sulle \"personas\" marcate dal punto di vista del genere, utilizzando prompt in linguaggio naturale per misurare gli stereotipi nei modelli linguistici. Questo lavoro è stato svolto in collaborazione con Essenndermush e Danjorovsky."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici o LLm."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Queste misure presentano varie limitazioni; di solito si basano su insiemi di dati costruiti manualmente, la cui cura richiede molto tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "di solito misurano solo stereotipi molto specifici, il che significa che non si possono generalizzare bene ad altre demografie o contesti, oppure catturano semplicemente associazioni molto generali e ampie, come associazioni negative con particolari gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, ovvero del concetto secondo cui le identità sociali multifaccettate possono amplificare i pregiudizi e costituire luoghi unici di danni."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "per superare queste limitazioni, ci affidiamo alla proprietà che questi Lms più recenti, sintonizzati sulle istruzioni, sono molto bravi a rispondere a istruzioni e sollecitazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario, utilizzando uno stimolo come \"Immagina di essere una donna asiatica. Descrivi te stessa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità desideriamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco quindi alcuni esempi di generazioni da GPT4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Immediatamente si nota che, sebbene i risultati non siano esplicitamente negativi o tossici nel senso tradizionale di questi termini,"}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Esistono alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è ritratta come riservata. La donna del Medio Oriente è descritta usando parole come \"esotica\" e \"affascinante\", riferendosi a una regione ipnotica."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le persone di colore fanno riferimento alla loro ascendenza, mentre la persona bianca non ha nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "per catturare questi schemi, il nostro metodo si compone di due parti. La prima consiste nella generazione di queste persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre istruzioni per generare queste persone sono state ispirate da uno studio in cui sono state fornite queste istruzioni a soggetti umani, riscontrando che, sottoponendole a soggetti umani, sono riusciti a evidenziare stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "anche questo consente un confronto diretto tra le nostre persone generate e le risposte scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "la seconda parte sono le parole marcate, che è un metodo per identificare le parole che distinguono i gruppi marcati dai nostri, su cui approfondirò a breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di ciò è che otteniamo stereotipi e schemi molto specifici senza doverci affidare a nessun lessico specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo delle parole marcate si basa sul concetto sociolinguistico di marcatezza, che afferma che esiste un valore predefinito non marcato e che qualsiasi gruppo che differisce da tale predefinito è linguisticamente marcato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, la parola \"uomo\" o \"scusa\", la parola \"guerriero\" è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano effettivamente un \"guerriero uomo\" e contrassegnano il termine con \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "E, più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi marginalizzati sono solitamente contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro metodo, innanzitutto designiamo quali sono i gruppi non contrassegnati e quelli contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "poi confrontiamo le persone utilizzando il metodo delle \"parole di lotta\", che essenzialmente consiste nell'utilizzare rapporti di log odds ponderati per distinguere le parole principali per ciascun gruppo contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per le personificazioni di donne nere, useremmo parole di lotta e confronteremmo i rapporti degli dei della legge con sia le personificazioni bianche che quelle maschili, poiché questi sono i due gruppi non contrassegnati corrispondenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "Ora passiamo ai risultati. Innanzitutto, utilizziamo il lessico degli stereotipi e scopriamo che le persone generate contengono molti più stereotipi di quelle scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Quando esaminiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sebbene le persone generate presentino una frequenza molto più elevata delle parole associate a Luxon, quelle scritte da umani mostrano una distribuzione molto più ampia del vocabolario. Inoltre, le parole stereotipate presenti nelle persone generate si riducono essenzialmente a \"alto\" e \"atletico\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi davvero solo quelli positivi o almeno non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "In realtà, il lessico non cattura affatto adeguatamente molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, invece di farlo, ci rivolgiamo ai risultati del nostro metodo di parole contrassegnate per dimostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzialiste."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "nella nostra analisi, esaminiamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Per i gruppi marcati, le parole più frequenti includono termini come cultura, tradizione, orgoglio e esotico. E queste parole definiscono tali gruppi esclusivamente in base al loro rapporto con la propria identità, distinguendoli dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "contribuisce a una lunga eredità di discriminazione e emarginazione per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molti tropi comuni che si riflettono in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono una donna latina includono termini come \"vibrante\" e \"formosa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "Um, che si collegano a un topos del tropicalismo. Per le donne asiatiche, le parole sono termini come piccolo, delicato e setoso."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "si collega a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomesse e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "infine, per le donne nere, osserviamo che alcune delle parole più ricorrenti sono termini come forte e resiliente."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "si collega ad un archetipo che le persone hanno definito l'archetipo della donna nera forte e, sebbene a prima vista sembri positivo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "Sono stati condotti studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso poiché esercita una forte pressione su questi gruppi demografici, richiedendo loro di essere resilienti e forti di fronte agli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "piuttosto che lavorare effettivamente per cambiare quegli ostacoli, mette pressione su quelle persone affinché li superino, il che porta a esiti sanitari molto negativi per loro, oltre ad altri danni."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "In generale, constatiamo che le parole per ogni gruppo contrassegnato riflettono essenzialmente narrazioni molto essenzialiste."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, in quanto ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialiste. Dovremmo inoltre utilizzare una prospettiva intersezionale per studiare i pregiudizi e i danni, poiché molte cose potrebbero essere trascurate se non lo facciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "Infine, dovrebbe esserci una maggiore trasparenza riguardo ai metodi di mitigazione dei pregiudizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "per esempio, come questi stereotipi positivi, non sappiamo se sia perché c'è qualche tipo di strano."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "allineamento dei valori eccessivamente eccessivo in corso, o forse altri metodi come l'antistereotipizzazione che stanno producendo questi schemi perniciosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "non posso davvero fare alcuna ipotesi o studiare ulteriormente la questione senza una maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato, spero che tu possa trascorrere un piacevole momento presso Ace."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jing Wei Y dell'Università della Scienza e della Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È un piacere fornire un breve video promozionale del nostro giornale. State copiando il mio modello, proteggendo il diritto d'autore dei grandi modelli linguistici per l'incorporamento e i servizi? Riportate il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo introducendo il contesto relativo ai servizi di incorporamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i grandi modelli linguistici come GPT, Lama, PM sono eccezionali nella comprensione e generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding come servizio è uno dei servizi basati su grandi modelli linguistici per assistere varie attività di elaborazione del linguaggio naturale (NLP)."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, OpenI offre un'API di embedding basata su Gbt."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, recenti studi hanno dimostrato che un attaccante potrebbe rubare il modello attraverso l'apprendimento dagli embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore degli embedding come servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "Per proteggere il diritto d'autore dei servizi di incorporamento, una delle soluzioni è inserire un watermark nel servizio del fornitore e rilevare se un altro servizio contiene tale watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo di filigrana deve soddisfare le seguenti proprietà: innanzitutto, il metodo deve essere applicabile all'incorporamento come servizio; in secondo luogo, la filigrana non deve degradare l'utilità dell'incorporamento fornito."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, la filigrana dovrebbe essere abbastanza evidente per l'attaccante, oppure l'attaccante può rimuovere facilmente la filigrana."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Le opere esistenti possono essere generalmente classificate in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo non è applicabile all'embedding come servizio o manca di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo proponiamo l'inserimento di un marcatore, che è un metodo di filigrana basato su backdoor, applicabile ai servizi di inserimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, permettetemi di illustrare i dettagli del nostro marcatore di incorporamento. Il marcatore di incorporamento comprende due passaggi principali: l'iniezione di filigrana e la verifica del copyright."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi passaggi principali, selezioniamo innanzitutto un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore possa raccogliere un corpus testuale generale e calcolare la frequenza delle parole al suo interno."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "iniezione di filigrana, definiamo prima un letto di destinazione. Quando un utente invia una frase al servizio fornitore, il fornitore conta il numero di trigger nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding fornito è una somma ponderata dell'embedding target sotto l'embedding originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del copyright consiste nel rilevare se il modello alla base di un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Costruiamo innanzitutto un backdoor e un dataset benigno. Il dataset backdoor contiene frasi di cui tutte le parole appartengono all'insieme dei trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme dei trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "il fornitore richiede le incorporazioni dal servizio stiller con il dataset"}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "Vengono calcolate la similarità coseno e la similarità l2 tra l'embedding richiesto e l'embedding target. Calcoliamo la differenza di similarità tra i dati benigni e il set di dati backdoor, definita come delta coseno e delta l2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza matrice."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo esperimenti su quattro insiemi di dati: AG news, mind, SSD due e A spam. Presumiamo che il fornitore del dataset liewikitext conteggi la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro dataset mostrano che il nostro marcatore di embedding può offrire un'eccellente prestazione di rilevamento mantenendo al contempo una grande utilità per i compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Validiamo inoltre la copertura dell'embedding fornito visualizzando l'embedding delle frasi sviluppate a BPCca. La legenda delle figure indica il numero di trigger in ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nelle figure, è difficile distinguere tra gli embedding backdoor e gli embedding normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Quello è tutto, grazie. Venite a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Vaudha e sono un candidato al dottorato in informatica presso l'Università di Stony Brook. Vorrei presentare il nostro lavoro accettato in ACL 2023 come articolo lungo \"Transfer learning per la rilevazione di dissonanze: affrontare la sfida della classe rara\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e il motivo per cui è un problema importante da studiare nel linguaggio. In termini semplici, la dissonanza cognitiva è la presenza di due credenze o azioni inconciliabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "in questo esempio, una persona afferma \"So che le sigarette potrebbero uccidermi\" e poi prosegue dicendo \"Ho preso un paio di sigarette dopo la riunione\". Questa credenza e questa azione sono incoerenti e in dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "menzionando che non credo di poter mantenere il mio lavoro senza di loro giustifica la seconda occorrenza e hanno una relazione di consonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "La dissonanza è un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, ma è davvero raro trovarla espressa nel linguaggio tra gli altri tipi di relazioni discorsive."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Perché tutto ciò è importante? Lo studio della dissonanza cognitiva ci aiuta a comprendere gli effetti del disaccordo tra le persone, a tracciare tendenze e cambiamenti nei valori delle credenze e degli atteggiamenti della popolazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "Un'elevata dissonanza cognitiva è inoltre associata ai disturbi d'ansia e può contribuire a comprendere meglio la salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "La dissonanza cognitiva espressa nel linguaggio può essere altresì vantaggiosa per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a capire meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "con l'obiettivo di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza primaria, come illustrato nel diagramma di flusso qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "sono stati elaborati utilizzando un parser PDTV e coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "qui può essere osservato che la dissonanza è stata riscontrata solo nel 3,5 percento delle coppie annotate"}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "raccogliendo circa 1000 esempi di coppie di unità discorsive, abbiamo eseguito un addestramento per un classificatore iniziale addestrato solo su 43 esempi di distanza. Non sorprende che il classificatore abbia avuto prestazioni non molto migliori del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "a bassa incidenza di dissonanza e l'assenza di qualsiasi precedente set di dati simile, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "per alleviare ciò, sperimentiamo combinazioni di apprendimento per trasferimento e apprendimento attivo per annotare in modo tale che possano essere raccolti più campioni di dissonanza attraverso minori round di annotazione, riducendo i costi di annotazione complessivi e migliorando la rilevazione della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "il modellatore iniziale non è stato in grado di catturare affatto la classe di dissonanza; iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "trasferimento da due compiti diversi: classificazione di dissonanza indipendente dall'argomento, un compito che determina se due affermazioni di dibattito provenienti da persone diverse sono in accordo o in disaccordo indipendentemente dall'argomento."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamato qui dibattito e sulla classificazione binaria delle classi di espansione e di confronto del PB, poiché questi due sono strettamente correlati alla concezione di consonanti e dissonanze, e li chiamiamo qui CE."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "riscontro che, trasferendo la performance breve a zero sul dataset annotato, questa è già molto migliore del caso, con il migliore che raggiunge un AUC di 0,62."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Pelaggio su un'ottimizzazione iterativa su entrambi i compiti, scopriamo che l'ottimizzazione fine dei compiti CE seguita da un'ulteriore ottimizzazione sul dibattito produce una prestazione a zero shot molto migliore. Quindi, questo è il modello che utilizziamo per avviare l'apprendimento attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determiniamo il metodo ottimale per aggiornare un modello con nuovi dati da ogni ciclo di apprendimento attivo e annotazioni. `cumulative` accumula tutti i dati raccolti dalle annotazioni attive finora, mentre `iterative` aggiorna il modello addestrandolo sull'ultimo insieme di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Tra le diverse strategie analizzate, abbiamo riscontrato che l'approccio cumulativo ha prestazioni uguali o superiori rispetto a quello iterativo in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara (PRC) per selezionare principalmente gli esempi che hanno una elevata probabilità di essere dissonanti secondo il modello corrente in qualsiasi fase del apprendimento attivo (AL)."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Confronta questo con gli altri stati delle strategie A più all'avanguardia comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "riscontriamo che la strategia PRC proposta funziona meglio rispetto ad altre strategie all'avanguardia tradizionali, sebbene la differenza sia minima. Si osserva inoltre che le prestazioni sono significativamente inferiori per i dati casuali."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "con ulteriori round di AL con le due migliori strategie, miglioriamo la classificazione della distanza, AUC a 0,75, che è la migliore prestazione ottenuta finora per questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "verifica inoltre la fattibilità di ciascuna strategia in termini di qualità dell'annotazione e dei costi per gli annotatori. Abbiamo riscontrato che il PRC presenta la percentuale più elevata di dissonanza e funziona meglio per le classi rare. Tuttavia, gli annotatori hanno trovato gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, constatiamo che la PRC è una strategia semplice per l'acquisizione di classi rare e che l'avvio a freddo può essere notevolmente aiutato con compiti di apprendimento trasferibile opportunamente progettati."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "trovano inoltre che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive all'interno del dominio traggono vantaggio dall'aggiornamento cumulativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i collegamenti al nostro set di dati del codice e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie."}
