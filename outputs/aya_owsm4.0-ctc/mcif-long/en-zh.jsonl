{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是马蒂亚斯·林德曼，今天我将简要介绍我们关于不使用树结构的多集标记和潜在置换的组成泛化论文。这是我与我的导师亚历山大·科勒和伊万·蒂托夫的合作成果。\n\n组成泛化可以理解为学习者处理更深层递归和训练过程中单独见过的短语组合的能力。在语义解析的背景下，测试组成泛化可能如下：通常我们有一个训练语句集，例如“女孩睡着了”和“玛丽知道女孩睡着了”。这些语句与表示其核心意义的逻辑形式配对。与标准机器学习评估不同，测试集不来自同一分布，而是包含结构上未见过的逻辑形式。在这个例子中，模型在训练期间见过浅层递归，并在更深层递归的例子上进行测试。天真的序列到序列模型在这个分布外泛化方面遇到困难，经常产生与输入脱节的输出，特别是往往无法再现输入和输出之间的系统对应关系，如示例中色标的部分。\n\n一种流行的方法是将树结构集成到模型中，树结构旨在捕捉与逻辑形式相关联的语句的组成过程。这效果很好，但树结构通常不给定，需要以某种方式获取。这可能是一个复杂且有时计算成本高昂的过程。通常需要相当多的形式主义和对逻辑形式的专门预处理，例如处理变量符号。获取树结构也可能涉及专门的语法归纳程序。\n\n在本文中，我们不使用树结构，而是引入一个神经序列到序列模型，直接建模输入片段和输出片段之间的对应关系。首次，我们展示了不依赖树结构的更深层递归的强泛化能力。我们的方法从输入预测输出，分两步进行：首先，我们为每个输入标记附上一个无序的多集标记，这些标记将在输出中出现。在第一个步骤后，我们有了所有正确的标记，但它们未排序。因此，在第二步，我们使用另一个模型预测一个置换，将它们放入正确的顺序。我们引入了一种新方法来预测置换，对可能的置换不施加任何硬约束，这使我们的方法非常灵活和表达力强。\n\n从概念上讲，我们置换模型的工作原理大致如下：我们从左到右遍历输出，确定每个位置放置哪个多集标记。对于第一个输出位置，我们简单地选择一个，如红色突出显示的部分。然后，我们跳到下一个多集标记，以确定输出中的第二个标记。我们通过跳到另一个多集标记来类似地确定输出中的第三个标记。我们继续这个过程，直到访问完第一个阶段的每个标记，每个标记恰好访问一次。\n\n为了给您一个实验结果的预览，我们在COgs基准测试上将我们的方法与其他不使用树结构的模型进行了比较。我们的模型在更深层递归的泛化方面以大优势超越了其他模型。然而，其他一些结构泛化仍然非常具有挑战性。\n\n在论文中，我们解决了一些有趣的技术挑战。首先，输入和输出之间的对齐在训练数据中未给出。因此，对于给定的标记，我们不知道它来自哪个多集，这为训练带来了挑战。此外，有时有多个与数据一致的置换，但语言学上正确的置换是潜在的。我们通过在训练过程中归纳对齐来解决这个问题。\n\n我们置换方法非常灵活，但带来了找到最高得分置换是NP难的挑战，因为这与旅行商问题相关。我们通过一个GPU友好的连续放松来近似这个问题，这还允许我们反向传播到解，并学习语言学上更合理的置换。\n\n如果您想了解更多关于我们的实验和我们如何应对这些挑战，请阅读我们的论文或参观我们的展板。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是Myra，今天我将讨论我们关于使用自然语言提示测量语言模型中刻板印象的标记人物论文。这项工作是与Essenndermush和Danjorovsky合作完成的。近年来，许多研究已经记录了大型语言模型（LLM）中社会偏见和刻板印象的普遍存在。然而，这些测量方法存在各种局限性：它们通常依赖于手工构建的数据集，需要大量时间来整理；它们通常只测量非常具体的刻板印象，这意味着它们不能很好地推广到其他人口统计或背景，或者它们只是捕捉到与特定群体相关的非常普遍的广泛关联。此外，这方面的大部分工作都没有考虑到交集性，即多层面的社会身份可以加剧偏见，并成为独特的伤害焦点。\n\n为了克服这些局限性，我们利用了这些新型的指令调优LLM的一个特性，即它们非常擅长响应提示中的指令。我们可以要求模型生成一个人物，这是一个想象中的个体的描述，例如“想象你是一个亚洲女性，描述你自己”。我们可以立即看到，这可以非常普遍地应用于任何人口统计群体，因为我们可以根据需要在提示中指定任何身份标记。\n\n以下是GPT4的一些生成示例。我们立即发现，虽然输出不是传统意义上的负面或有毒的，但存在一些有趣的模式。亚洲女性被描绘为不显眼，中东女性被描述为“异国情调”，令人着迷的地区，而两种有色人种女性的人物都提到了祖先，而白人男性人物却没有。\n\n为了捕捉这些模式，我们的方法分为两部分。第一部分是生成这些人物。我们生成人物的提示灵感来自一项研究，该研究向人类受试者提供了这些提示，发现通过这种方式也可以揭示种族刻板印象，并且这使我们能够直接比较我们生成的人物和人类撰写的响应。第二部分是标记词，这是一种方法来识别区分我们标记群体的词语，我很快会详细说明。这种方法的优点是，我们可以获得非常具体的刻板印象和模式，而不必依赖任何特定的词典。\n\n标记词方法利用了社会语言学中的标记概念，该概念指出，存在一个未标记的默认值，任何与该默认值不同的群体在语言上都是标记的。例如，词语“战士”通常与男性相关联，所以当人们描述一个女性战士时，他们通常会实际指定“男性战士”，并在术语中标记“女性”。更广泛地说，社会中的主导群体在语言和社会上都是未标记的，而边缘化群体通常会被标记。\n\n在我们的方法中，我们首先指定未标记和标记群体，然后使用标记词方法比较人物，这基本上是使用加权对数几率比来区分每个标记群体的顶级词语。例如，对于黑人女性的人物，我们将使用标记词方法，并将对数几率比与白人人物和男性人物进行比较，因为它们是相应的未标记群体。\n\n现在来看一些结果。首先，我们使用刻板印象词典发现，生成的人物包含比人类撰写的人物更多的刻板印象。然而，当我们实际查看词典中词语的分布时，发现情况截然不同。虽然生成的人物包含的词典词语比例更高，但人类撰写的人物具有更广泛的词语分布，而刻板印象词语仅为“高”和“健壮”。事实上，这个词典根本没有捕捉到我们在之前的幻灯片中看到的许多有害模式。\n\n因此，我们转向标记词方法的结果，以展示这些看似积极的词语如何促进刻板印象和本质化叙事。在我们的分析中，我们揭示了这些看似积极的描述如何反映有害模式。首先，对于标记群体，顶级词语包括文化、传统、自豪和异国情调等词语，这些词语仅根据其与身份的关系来定义这些群体，并将其与白人规范区分开来。这为这些群体带来了长期的歧视和异化历史。此外，这些词语反映了许多共同的套路，尤其是有色人种女性。例如，描述拉丁女性的词语包括充满活力和曲线玲珑，这与热带主义套路相关；描述亚洲女性的词语是小巧、精致和丝滑，这与亚洲女性被性化、被视为温顺和顺从的长期历史相关。\n\n最后，对于黑人女性，一些顶级词语是坚强和韧性，这与人们称之为“坚强黑人女性”的原型相关。虽然乍看上去似乎是积极的，但研究表明，这种原型实际上是有害的，因为它给这些人口统计群体带来了巨大的压力，要求他们在面对社会障碍时保持坚强和韧性，而不是真正致力于改变这些障碍。这导致这些人群的健康结果非常不利，以及其他伤害。\n\n更广泛地说，我们发现每个标记群体的词语几乎完全反映了本质化叙事。基于这些模式，我们为模型所有者提出三点建议。首先，作为研究人员，我们应该解决积极刻板印象和本质化叙事的问题。我们还应该使用交集性视角来研究偏见和伤害，因为如果不这样做，可能会忽略许多事情。最后，应该提高偏见缓解方法的透明度，因为例如，这些积极的刻板印象可能是由于某种奇怪的、过度价值观对齐正在发生，或者可能是由于反刻板印象方法导致的，我们无法做出任何假设，也无法在没有更多透明度的情况下进一步研究这些有害模式。\n\n非常感谢您的聆听，祝您在ACL上玩得开心。"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是詹姆斯·芬奇。我是莎拉·芬奇。\n\n今天，我们将向您介绍 ABCEV，一种评估对话人工智能的新维度方法。这项工作由埃默里大学自然语言处理实验室完成，由埃默里大学的崔吉诺教授领导，并与亚马逊 Alexa AI 合作。\n\n假设您刚开发了一个对话模型，想了解其与当前最先进技术相比的表现。常见的做法是使用人工评估，例如请人工评判员选择两个对话中哪个更好，或根据量表对对话进行评分。这些方法在提供整体对话质量评估方面效果良好，但对话质量有多个方面，因此您可能希望评估聊天质量的多个维度，以更细致地了解模型的优缺点。一种方法是简单地请人工评判员评估对话质量的多个维度，例如模型响应的相关性，使用现有的比较或量表方法。然而，我们认为有更精确、更可靠的维度对话评估策略。\n\n我们的方法试图通过明确标注每个模型响应是否表达某些行为来减少人工评估的主观性，例如提供无关信息或自相矛盾。我们将这种方法称为聊天行为标注，简称 ABC 评估。我们开发了这种方法，以全面涵盖最近文献中建议影响聊天质量的聊天模型行为。\n\nABC 评估能够测量聊天模型犯各种主题错误的比例。例如，ABC 评估测量聊天模型忽略其对话伙伴或说无关话的回合数，自相矛盾或与对话伙伴矛盾，编造不正确的事实或违反常识知识，以及模型表现或未能表现出同理心的回合数。\n\n为了确定最有效的评估方法，我们选择了四个最先进的聊天模型，并使用 ABC 评估对每个模型的 100 个人工机器人对话进行评估。为了进行比较，我们还使用三种现有方法对这些对话进行了评估：回合级量表评分、对话级量表评分和对话级配对比较。对于每种现有方法，我们收集了八个最常见对话方面的评估，因为这是评估聊天模型多维度的标准做法。\n\n从我们对这些评估结果的分析中，我们发现 ABC 行为标签总体上比现有方法收集的标签更可靠，其可靠性通过 100 个双重标注对话的内部标注员一致性进行测量。此外，ABC 评估标签比现有方法产生的指标更能预测整体对话质量，如这个简单的线性回归分析所示。例如，您可以看到测量自相矛盾和对话伙伴矛盾的回合比例分别解释了对话质量的百分之多少和百分之十，而平均量表一致性得分仅解释了百分之四或更少。\n\n最后，我们检查了每个评估指标是否捕捉了聊天质量的独特方面，使用逐步线性回归。您可以看到，所有 ABC 评估指标的组合解释了超过 25% 的对话质量，当您逐个删除指标时，大多数指标都会导致质量信息的显著丢失。另一方面，所有回合级量表指标的组合解释了更少的质量，这些指标中更少的携带独特信息。\n\n这些可靠、有信息量且独特的 ABC 评估指标使我们能够以高于以前方法所能达到的分辨率评估对话人工智能。您可以从我们实验的结果中看到，仍然存在几个挑战，并且已被精确量化。例如，我们测试的机器人大约有 20% 的响应违反了常识，大约 15% 的响应提供了无关信息，并且它们自相矛盾或与对话伙伴矛盾大约 10% 的时间。\n\n随着该领域的快速发展，许多错误率可能会随着新模型的发布而降低，自我们进行评估以来。然而，这更说明了追求可靠且精确的评估指标以进行模型比较的重要性。我们希望 ABC 评估能被该领域的其他人士作为朝此方向迈出的有意义一步，并期待看到未来几个月和几年对话人工智能的进步。\n\n谢谢观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我的名字是Vauddha，我是纽约州立大学石溪分校计算机科学系的一名博士候选人。我想向大家介绍我们被ACL 2023会议录用的研究成果，题为《转移学习用于认知失调检测中的稀有类别挑战》。我们首先定义了认知失调，并解释了为什么它在语言学中是一个重要的研究问题。简单来说，认知失调是指两个信念或行为不一致的情况，例如，一个人说“我知道香烟可能会要了我的命”，然后又说“会议结束后我拿了几根烟”，这两种信念和行为是不一致的，处于失调状态。进一步地，他提到“我认为没有香烟我无法保住工作”，这就为第二次行为提供了理由，它们之间存在着一致性关系。虽然失调是我们在日常决策中经常遇到的现象，但它在语言中和其他类型的语篇关系中却非常稀有。那么，为什么这重要呢？研究认知失调可以帮助我们理解人们之间不同意见的影响，追踪信念价值和态度变化，高认知失调也与焦虑障碍相关，有助于更好地理解人们的心理健康。研究语言中表达的失调也可以有利于理解易受伤害群体的极端主义和两极分化。最后，认知失调对于理解个人的认知风格和决策过程非常重要，有助于我们更好地了解决策机制。\n\n为了创建一个认知失调资源，我们对失调关系进行了大规模标注。我们采用了一种“失调优先”的方法，如图所示，使用PDTV解析器处理推文，并根据论文中描述的指南对语篇单位对进行标注。在标注的大约一千个语篇单位对中，只有3.5%发现了失调。在收集了大约一千个例子后，我们对初始分类器进行了训练，它仅训练了43个失调例子，不意外地，分类器的表现几乎不比随机好，这归因于失调的低发生率和缺乏任何先前的类似数据集。我们面临着绝对稀有性的问题。\n\n为了缓解这个问题，我们实验了转移学习和主动学习的组合，以便在更少的标注轮次中收集更多的失调样本，降低整体标注成本，同时提高失调检测能力。由于初始模型完全无法捕捉失调类别，我们通过从紧密相关任务转移权重来开始主动学习过程。我们从两个不同的任务转移：主题独立失调舞蹈分类，这是一个判断两个来自不同人的辩论陈述是否一致或不一致，无论主题如何的任务，称为“辩论”；以及二元分类纯度扩展和比较类别的任务。由于这两个任务与协同和失调的概念密切相关，我们将其称为CE。我们发现，在这些任务上进行转移学习后，模型在已标注数据集上的零样本性能已经显著好于随机，最佳的AUC为0.62。进一步在两个任务上迭代微调，我们发现CE任务的微调加上对辩论的进一步微调，产生了更好的零样本性能。因此，我们使用这个模型来启动主动学习。\n\n接下来，我们确定了更新模型的最佳方法，以纳入每一轮主动学习和标注的新数据。累积方法积累了迄今为止从主动标注收集的所有数据，而迭代方法则通过在最新收集的数据集上训练模型来更新模型。我们发现，在所有情况下，累积表现等于或优于迭代。为了提高失调示例的数量，我们使用了一种稀有类别概率策略（PRC），该策略主要选择当前模型认为最有可能失调的例子。我们将此与社区中常用的其他最先进的主动学习策略进行了比较，发现提出的PRC策略优于其他最先进策略，尽管差异较小。请注意，随着主动学习轮次的增加，随机策略的表现显著降低。\n\n使用两种最佳策略，我们将距离分类AUC提高到0.75，这是我们在该任务上取得的最佳性能。我们还检查了每种策略的标注质量和对标注人员的成本可行性。我们发现，PRC具有最高的失调百分比，最适合稀有类别的获取。然而，标注人员也发现这些例子很难。\n\n总之，我们发现PRC是一种简单的稀有类别获取策略，而适当设计的转移学习任务可以显著帮助冷启动主动学习。我们还发现，迭代更新对于从不同领域转移学习有用，而域内主动标注则受益于累积更新。这些是我们的代码、数据集和论文的链接，如果您有任何问题，请随时与我们联系。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Akshata，今天我的合著者Martin和我一起展示我们的研究成果《Kit Must：多源知识集成评估》。这项工作是麦吉尔大学、Mila和微软研究之间的合作。国家语言理解模型利用各种知识来源，如其参数中包含的知识（通常通过预训练获得）和推理时提供的输入知识。近期在问答任务等方面的研究表明，模型可以使用预训练知识来完成任务。但自然语言理解通常还需要在推理时提供的知识，例如在句子“John在电视上看到了新当选的总统”中，预训练参数可能包含关于总统职责和电视的知识，但它们无法可靠地知道这个特定实例中的实体John是谁，或新总统是谁，因为总统可能在预训练后已经更换。因此，成功的知识密集型自然语言理解任务模型需要能够集成和利用预训练知识和推理时知识。\n\n在这项研究中，我们提出了一个知识集成诊断测试套件。我们引入了一个核心词指代解析任务，旨在探究从不同来源抽取知识的能力。我们对数据集进行了人类研究参与者评估，并建立了核心词指代解析模型。这里是我们数据集中的一个例子：Serving是一名法官，Kia是一名面包师，Termin和Kia在长时间工作后，Serving在法律代码中审案，很高兴能放松一下。任务是识别代词“他”所指的正确实体，在这个例子中是Serving。解析给定代词需要两种类型的知识：首先是实体特定知识，如Serving是一名法官；其次是泛知识，如法官在法庭上审案。背景知识通常在大型语言模型的预训练过程中学习，而实体特定知识通常在推理时观察到。我们变化了这些信息的可用性，使其可能在单一来源或多个来源中找到。\n\n我们定义了三种Kitdmos设置。首先是典型预训练设置，假设背景知识在预训练时可用。其次是背景双方设置，背景知识在预训练时和推理时都可用。最后是背景推理设置，两种知识类型仅在推理时可用。这个最后设置特别有趣，因为它模拟了背景知识不包含在模型的预训练数据中的情况，例如，因为新的职业自预训练以来已经发展起来。在这里，我们通过控制两个来源中事实的可用性来展示背景预训练设置的示例。在背景预训练设置中，我们假设背景知识“政治家寻求政府中的当选席位”包含在预训练参数中；在推理时上下文中，我们提供实体特定知识“Chichester是一名政治家”。在背景双方设置中，我们不仅在推理时上下文中提供实体特定知识，还提供关于政治家的背景知识。在背景推理设置中，我们提供特征职业“merely tour”而不是“政治家”，因为“merely tour”不太可能包含在预训练参数中。\n\n我们对数据集进行了人类研究参与者评估，并建立了核心词指代解析模型。在这个图中，我们展示了最难的变体背景预训练设置中最佳性能模型的结果，没有进行任务特定训练。两个模型在训练时都表现不佳，但C2F和Built for Coref显著优于随机选择。这表明，在训练时，模型学会了利用表面线索，而在测试时，这些线索在Kitdmus中被移除，因此不再有用。额外的虚构知识实验表明，即使是最佳性能模型也无法可靠地集成仅在推理时提供的背景知识。\n\n总之，我们的论文的主要发现是，许多核心词指代解析模型似乎无法在没有任务特定训练的情况下，推理来自不同来源的知识。然而，在任务特定训练下，一些模型成功地集成来自多个来源的知识。尽管如此，即使是最佳性能模型也似乎在可靠集成仅在推理时提供的背景知识方面存在困难。如果您想了解更多细节，请查看我们的论文，并在GitHub上查看数据集和代码。谢谢您的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是特伦托大学和福斯卡尼·布鲁诺·凯瑟尔研究所的Sarah Pai，我将简要介绍关注机制作为实时语音翻译指南的论文，这是与Matteo Negri和Marco Durchi的合作研究。\n\n什么是实时语音翻译？实时语音翻译（SimST）是将口语实时翻译成另一门语言文本的过程，使跨语言交流成为可能。当前实时语音翻译模型存在哪些问题？特定的架构通常通过引入额外的优化模块来训练，导致训练程序长且复杂，例如涉及不同的优化目标，以及训练和维护多个模型以实现不同的延迟等级，例如训练一个平均延迟为一秒的模型，另一个为两秒的模型，依此类推。\n\n我们的解决方案是：首先，使用已存在的离线语音翻译模型，无需重新训练或采用特定架构；其次，为每个延迟等级使用一个模型，通过特定参数控制延迟；第三，利用模型通过音频输入和文本输出之间的张力机制（即交叉注意机制）已获得的知识。可以看到右边的例子，我们的解决方案是提出点式编码器解码器注意机制，这是一种策略，我们根据注意力指向决定是否发出部分翻译。如果张力不集中，即跨注意力之和低于某个阈值α，指向最后λ个语音帧，这意味着接收的信息足够稳定。例如，如果我们接收一个包含“我要谈论”的语音片段，我们的模型预测德语翻译，我们观察到跨注意力权重，前两个词指向最早接收的语音帧，最后一个词指向最后接收的λ个语音帧，这意味着前两个词将被发出，而由于跨注意力之和高于阈值α，我们不会发出最后一个词，而是等待另一个语音片段。\n\n如果我们继续接收另一个语音片段，模型预测另外三个词，观察跨注意力权重，我们会看到没有词指向最后λ个语音帧，这意味着这三个词将被发出。\n\n关于主要结果，我们在图表上绘制了实时语音翻译结果，其中蓝色一侧测量翻译质量和平均滞后（即延迟测量），我们还考虑了计算感知平均滞后，包括模型预测输出的计算时间。我们希望我们的曲线在这个图表中尽可能高，同时也希望它们向左移动。我们与也应用于离线模型的准备策略（即Whitkey策略和局部一致性）进行比较，以及与专门针对实时语音翻译设计的最新架构进行比较。\n\n这些是德国语实时语音翻译策略的所有结果，我们可以看到，我们的策略优于所有应用于离线模型的策略，因为曲线向左移动。我们还看到，如果考虑实际耗时或计算耗时，我们的策略是最快的。\n\n如果您想了解更多结果，请阅读我们的论文，我们还开源了代码、模型和实时输出，以促进我们工作的可复现性。\n\n谢谢大家的关注。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是夏书。今天我将展示我们论文《Connel 2003命名实体识别标签在2023年是否仍能有效》的研究成果。让我们开始吧。我们的论文探讨了泛化问题，采用命名实体识别任务（NER任务），我们观察到模型已经使用Con 2003来开发NER近20年，这自然引发了几个问题：首先，这些模型能否泛化到现代数据？在开发新标签器时，良好的泛化需要什么？如果我们观察到泛化效果不佳，导致这些模型性能下降的原因是什么？为了研究这些问题，我们开发了Con plus+数据集，这是一个我们从路透社2020年新闻中收集并根据Con 2003标注指南进行标注的数据集。我们在Con 2003上对20多个模型进行了微调，并在Con 2003测试集和Con plus+测试集上对其进行了评估，最后，我们计算了F1值的百分比变化，以评估每个模型的泛化能力。\n\n那么，良好的泛化需要什么？通过我们的实验，我们发现有三个主要因素：\n\n1. 模型架构。我们发现，变压器模型通常能更好地泛化到新数据。\n2. 模型大小。我们发现，通常较大的模型能带来更好的泛化效果。\n3. 微调示例数量。我们知道，微调示例数量直接影响下游任务的性能，这里我们也发现，更多的微调示例也能带来更好的泛化。\n\n关于下一个问题，一些模型性能下降的原因是什么？我们有两个假设：\n\n1. 自适应过拟合，即由于反复使用同一测试集而引起的过拟合，通常表现为新测试集上的收益递减。\n2. 时间偏移，即由于训练数据和测试数据之间的时间差距增加而导致的性能下降。\n\n对于自适应过拟合，我们从右图中看到，红色的最佳拟合直线斜率大于1，这意味着在Con 2003上每单位的改进在Con plus+上都超过了1单位的改进，即没有收益递减。这表明自适应过拟合在这种情况下没有观察到。那么时间偏移呢？对于时间偏移，我们进行了一项实验，用更新的数据重新训练或继续预训练一些模型，发现性能随着时间差距的增加而下降，这证实了我们关于性能下降主要原因在于时间偏移的假设。\n\n我们的结论是，良好的泛化需要更好的模型架构、更大的模型大小和更多的微调示例，这些是相辅相成的，我们不能只拥有其中之一。同时，我们还发现性能下降是由时间偏移引起的，令人惊讶的是，并不是由自适应过拟合引起，尽管Connel 2003已经使用超过20年。回到我们论文标题提出的问题，Connal 2003标签器在2023年是否仍能有效，我们发现答案是肯定的。我们希望我们的论文能呼吁更多关于如何改进模型泛化能力的研究。最后，请务必查看我们的论文和数据集，如果有任何问题，请随时联系我。谢谢！"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "您好！欢迎参加我们关于 De plain 的演讲，这是一个用于德国文本识别的新语料库，可以在文档和句子级别使用。我叫 Regina Stoden，将带领大家完成演讲的第一部分。首先，让我们定义一下文本简化。文本简化是一种适应性调整文本的过程，旨在提高特定目标群体，如阅读困难者或非母语人士对文本的理解能力。为了训练文本简化模型，我们需要平行文本对，例如文档或句子。在下面的例子中，您可以看到一个复杂德语句子和其简明语言翻译的平行对。简化句子有不同的技术，如您在例子中看到的，例如词性替换、从句扩展、交叉删除、句子重新排序或插入词语。\n\n我们现在提出新的语料库 De plain，因为近年来现有语料库存在一些问题。例如，这些语料库太小，无法用于训练分类模型。近年来提出的另外三种模型都是自动对齐的，这意味着它们的对齐可能存在错误。因此，我们提出了新的语料库 Dplane，它分为两个子语料库：Deplane APA 和 Deplane web。Deplane APA 基于使用文本，我们在 Deplane APA 中手动对齐了 483 个文档，结果是大约三十万十三万个平行句子对。对于 Deepplane web，这个语料库涵盖了不同领域，我们也手动和自动对齐方法对这七百五十个文档进行了对齐。总共，我们得到了 30,450 个句子对。\n\n我们对句子对进行了更深入的分析，例如简化类型，如您在这里看到的，圣经文本的简化程度远高于新闻文本或语言学习者文本，在所有级别上，包括词性简化、结构简化以及整体简化水平。此外，您可以看到我们的 Deep plaining 语料库具有多种不同的简化变换，例如在 Deeppla API 语料库中，我们有更多的重新排序和词语添加，而 Deep plane web 语料库中则更多的是改写。\n\n现在，让我们看看这个语料库能做什么。我叫 Omar，现在我将讨论我们数据集 De plane 的使用案例。对于第一个使用案例，我们可以评估自动对齐方法。近年来，有许多对齐方法，但在机器翻译的背景下，我们有两种平行文档，用不同语言书写，我们想要提取后两种文档中句子的对齐。但在我们的情况下，我们试图在两个平行文档之间提取对齐，它们具有相同的语言和内容，但复杂程度不同。现在，由于我们有了手动对齐的句子数据集 De plane，我们可以将这些句子作为黄金标准对齐来评估一些提出的对齐方法。我们对提出的某些方法进行了调整，并在论文中发表了所有这些调整和运行实验的代码。最后，我们得出结论，用于德国文本简化的最佳自动对齐方法是 Mass align 方法，您也可以在论文中找到运行此方法以处理您自己文档的代码。\n\n我们论文中展示的第二个使用案例是自动文本简化，通过微调语言模型来生成从复杂输入文本简化的文本。我们微调了两个不同的模型：我们微调了 Longpart 模型以生成文档级别的简化，我们还微调了正常基数的 Long 模型以生成句子级别的简化。您也可以在论文中找到所有检查点，并详细了解实验的分数和评估指标。我们得出结论，这种基本的微调可以产生或获得比基线分数更好的分数，我们将这些结果作为未来自动文本简化问题的基准提出。\n\n非常感谢您的关注，希望在会议上能见到你们所有人。谢谢！"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是来自 FNAi 大学的 X 元。我今天要介绍我们的研究成果——《从轻量级语言模型中提取脚本知识以用于受限语言规划》。在日常生活中，人们经常需要按照逐步指令来规划行动，这些指令以保证脚本的形式呈现。早期研究利用语言模型来规划典型活动的抽象目标，例如做蛋糕，并证明大型语言模型可以有效地将目标分解为步骤。然而，早期研究主要关注于具有一般约束的抽象目标规划，如做巧克力蛋糕这样的具体目标规划仍鲜有涉及。\n\n在本文中，我们定义了受限语言规划问题，该问题对规划目标施加不同的约束。一个抽象目标可以由具有多种面约束的现实生活中的具体目标继承。一个优秀的规划器应该编写合理且忠实于约束的脚本。我们首先评估并改进了生命语言模型的受限语言规划能力。由于没有具体目标的数据集，我们需要首先获取这些目标。如表所示，我们通过人类参与的数据获取过程，使用 Instruct GPT 采样了 100 个具体目标，并评估了来自语言模型的脚本。该表报告了结果的总体准确率。我们发现所有学习模型在具体目标规划方面都取得了不尽如人意的结果。\n\n随后，我们进行详细分析以研究学习模型的运作方式。图中的结果显示，生成的脚本在语义完整性方面表现尚可，但忠实于约束性方面无法保证。我们深入研究了 WiH 定义的约束性细分主题类别。图中的热图显示，Instruct GPT 在不同类别的约束性方面表现出显著的性能差异。早期研究表明，学习模型的输出质量具有高方差，导致性能不佳。因此，我们采用了过生成过滤器的想法来提高生成质量。我们首先展示了 Instruct GPT 的约束类型示例，并基于种子抽象目标获取具体目标。然后，Instruct GPT 过生成具体目标的关键脚本。接下来，我们开发了一个过滤模型来选择忠实于约束的脚本。我们将脚本和目标转换为 Instruct GPT 嵌入，并计算余弦相似度和相似度分数以衡量语义相似度。此外，我们奖励包含目标约束关键字的脚本。我们仅保留在目标大小中得分最高的脚本。\n\n使用我们的方法，Instruct GPT 可以生成更高质量的脚本。我们的方法在语义、完整性和忠实于约束性方面显著提高了规划能力。由于部署大型语言模型成本高昂，因此必须使更小、更专业的模型具备语言规划能力。创建数据集是实现这一目标的关键步骤。然而，早期研究并未实现具体目标的规划，而手动数据标注成本高昂。因此，我们遵循符号知识蒸馏的想法，从轻量级语言模型中蒸馏出受限语言规划数据集。我们使用该方法构建了一个名为 CodeScript 的受限语言规划数据集。总共生成了 55,000 个具体目标和脚本。为了确保验证集和测试集的质量，我们请众包工人修正不正确的样本。\n\n该图显示了 CodeScript 的约束分布。我们发现 CodeScript 在生成的具体目标方面表现出高度的多样性。使用 CodeScript，我们可以处理更小但更专业的受限语言规划模型。我们发现，T5 在 ScoreRate 上进行微调后生成的脚本质量优于大多数大型语言模型，这表明较小的模型在适当训练和合适的数据集上可以支持大型模型。总之，我们建立了受限语言规划问题，评估了大型语言模型的受限语言规划能力，并开发了一种过生成过滤方法。我们使用大型语言模型生成了高质量的受限语言规划数据集 CodeScript。我们希望 CodeScript 可以成为推进语言规划研究的宝贵资源。谢谢您的聆听，更多有关 CodeScript 的细节请见我们的论文。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是Jannislavak，我将向您展示我们在Dr. Bert方面的工作，这是一个强大的法语预训练模型，应用于生物医学和临床领域。在这次演讲中，我们首先将讨论医疗保健中的语言建模。然后，我们将介绍我们文章的主要贡献。我们推出了第一个法语生物医学模型Dr. Bert，它基于Roberta，并在nachtchos上进行训练，这是一个从网络上抓取的医疗数据集。我们还引入了一个模型与多种预训练设置和数据源的比较。随后，我们将展示我们在11个法语生物医学和临床下游任务上的结果，最后总结实验并告诉您如何访问模型，因为自2018年发布以来，Bert已成为解决自然语言处理任务最有效的方法之一，与历史静态和上下文方法（如Word2Vec、FastText等）相比，性能有了显著提高。此后，该模型已被适应到许多其他语言，如法语的Camembert，生物医学的PermiBert和BioBert，以及临床的ClinicalBert，但主要是在英语方面。专门针对其他语言的模型非常稀缺，通常由于缺乏领域内数据而基于连续预训练。然而，法语在之前没有开源的生物医学模型，所以我们问自己，对于广泛的使用范围，最合适的数据来源是什么，这些原始数据是否可以作为临床数据的良好替代品。为了回答这个问题，我们将Dr. Bert与我们基于匿名数据的Schubert模型进行比较，这些数据来自我们当地的非大学医院。接下来，我们问自己，训练一个法语专业模型需要多少数据，是4GB、8GB还是更多？为了回答这个问题，我们首先训练并比较了四个从零开始的模型：Dr. Bert的第一个版本，使用7GB的nachos；Dr. Bert的第二个版本，使用4GB的natureos；Schubert的第一个版本，一个临床模型，使用4GB从临床笔记中提取的句子；Schubert的最终版本，使用4GB的natureos和4GB的临床笔记的混合数据。此外，我们引入了三个进行控制预训练的模型，以分析预训练策略的影响：一个基于Camembert权重并在4GB的natureos上进行训练的模型；一个同样基于Camembert，但这次在4GB的临床数据上进行训练的模型；以及一个基于英语生物医学模型BioBert，并在4GB的snatches上进行训练的模型。总共，我们有七个模型进行评估。我们收集了公共和私人下游任务，如命名实体识别、分类、词性标注和问答等。这些模型与六个基线模型进行比较：Camembert、OSCAR 138GB、OSCAR 4GB、CC Net 4GB、Plummet、BioBert和ClinicalBERT。结果表明，模型在相同性质的数据上表现最佳，但我们可以观察到，来自不同来源的数据似乎更具多功能性。我们还观察到，使用更多数据会带来更好的性能。总体而言，从零开始的训练在大多数任务上似乎获得了更高的性能。然而，我们在消费者预训练实验中，使用PermiBird在4GB自然os子集上训练的权重和分词器，获得了与Dr. Bert 4GB从零开始训练的模型可比的结果，而基于Camembert权重和分词器的模型则存在稳定性问题。最后，作为结论，我们的系统在11个下游任务中的9个上表现更好，整体上超越了通用模型（这里是Camembert）的结果。我们还观察到，更专业的数据更好，但这并不容易扩展。所有基于Nachos的预训练模型都可自由访问，并在您的界面中提供，所有训练脚本都在我们的GitHub仓库中，所以感谢您的聆听，我们期待在多伦多海报会议上与您交流。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是张兵，华盛顿大学博士生。今天我将介绍我们从预训练数据到语言模型再到下游任务的研究工作，追踪政治偏见对不公平NLP模型的影响。语言模型是在大规模网络爬虫数据上训练的。政治新闻媒体在它们的预训练数据中得到了很好的覆盖。根据对c4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等都在语言模型训练数据中得到了很好的覆盖。这为语言模型的应用带来了机遇和挑战。一方面，它们能够从多样的视角中学习，庆祝民主和思想的多元化；另一方面，这些不同的政治观点本身带有社会偏见，可能导致下游任务应用中的公平问题。为此，我们提出调查从预训练数据到语言模型再到下游任务的政治偏见传播管道，具体通过以下问题：首先，如何评估语言模型的政治倾向，以及爬虫数据在这些政治偏见中扮演什么角色？其次，不同政治倾向的语言模型在下游任务中的实际表现如何，以及这是否会导致NLP应用的公平问题？\n\n具体来说，我们首先提出使用不同提示格式提示语言模型，采用政治问卷如政治罗盘测试，以确保我们进行扎根于政治学文献的自动评估。初步结果表明：首先，语言模型确实具有不同的政治倾向，它们占据了政治罗盘上的四个象限；我们还可以看到，GPT4是最自由的语言模型，GPT系列通常比BERT系列及其变体更社会自由。其次，我们旨在调查语言模型的政治偏见实际上在多大程度上来自训练数据。我们可以进行控制实验，通过进一步在六个不同的党派语料库上预训练语言模型检查点，分为新闻和社会媒体，进一步根据其政治倾向进行划分。通过在这样的党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生转变。例如，对于Roberta，在进一步训练于倾向左派的Reddit语料库后，我们可以看到它在政治偏见方面显着向自由主义转变。我们还试图调查语言模型是否能捕捉到现代社会中普遍存在的极化现象。我们将预训练语料库分为美国第45任总统之前和之后，分别在两个不同的时间语料库上预训练语言模型。我们可以看到，语言模型通常具有更远离中心的政治倾向，这表明语言模型也能捕捉到社会中的极化现象。\n\n最后，我们评估不同政治倾向的语言模型在仇恨言论检测和假新闻检测中的表现，这些是经常涉及语言模型的NLP应用，可能具有非常重要的影响。我们发现，如果调查按类别的性能，即将性能分为不同的人口统计或政治媒介新闻，我们可以看到一个模式：例如，在仇恨言论检测中，左派倾向的语言模型更好地检测到针对社会少数群体的仇恨言论，但更差地检测到针对社会更有权势群体的仇恨言论；反之，右派倾向的语言模型更好地检测到针对白人男性的仇恨言论，但更差地检测到针对黑人、LGBTQ+和其他少数社区的仇恨言论。在假新闻检测中，也出现了类似的趋势，我们看到左派倾向的语言模型更好地检测到来自对立面政治倾向的误导信息，反之亦然。我们进一步提供了许多定性示例，以展示不同政治倾向的语言模型根据社会类别对仇恨言论和误导信息示例给出不同的预测。附录中还有更多示例，以进一步强调这表明存在一个与语言模型政治偏见相关的紧迫的公平问题。例如，如果一个右派倾向的语言模型被微调用于仇恨言论或误导信息检测，然后部署到一个流行的社会媒体平台，这将意味着持有不同政治观点的人可能会被边缘化，而针对少数群体的仇恨言论可能会不受控制地蔓延。\n\n因此，这为我们敲响了警钟，需要认识到并解决语言模型政治倾向导致的公平问题。稍微讨论一下，我们也想强调我们揭露了语言模型政治偏见的独特困境，就像在塞壬和卡律布迪斯之间航行。如果我们不清理语言模型训练数据中的政治观点，偏见就会从预训练数据传播到语言模型再到下游任务，最终产生公平问题；如果我们试图以某种方式进行清理，我们也可能会面临审查或排斥的风险，并且很难确定什么是真正中立的，应该保留在语言模型训练数据中。这就像电动车问题。好了，我想今天就这些，谢谢大家的时间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Koov Sinna，很高兴欢迎大家参加我们关于ACL 23论文的演讲。语言模型的可接受性判断并不总是对上下文具有鲁棒性。这是与John Waqui、Aaron Mueller、Kanishka Mishra、Karen Fs、Roger Levy和Atina Williams的合作成果。在这项工作中，我们重新审视了最小对准范式。最小对准范式基本上是基于可接受性判断评估语言模型，这些判断也可以包括语法性，如BLIMP语法健身房或成见对的接受度。在最小对准范式中，评估语言模型的典型方法是展示一个可接受的句子或一个语法句，然后展示一个不可接受的句子或一个不语法句，希望模型基本上将更高的概率赋予可接受的句子。当前的MPP管道基本不允许我们评估模型对较长句子的接受度。如今，大型语言模型出现了更长更长的上下文窗口，因此评估模型在整个上下文窗口内的可接受性至关重要，这正是我们在此尝试做的事情。我们试图通过要求模型对越来越长的序列进行可接受性评估来重新审视MPP管道。\n\n我们的方法是，为了模拟这些较长的序列，我们重新审视了数据集本身，然后通过选择可接受或不可接受的句子从这些数据集中重新创建句子。例如，这里我们从BLIMP数据集的附加岛案例中选择了一个典型的语法对。我们通过从附加岛中提取语法句子，然后将它们作为前缀添加到可接受的查询和不可接受的查询中，来重新创建可接受和不可接受的较长序列。我们可以从相同的匹配中选择不可接受的句子来做同样的事情，这也可以用于测试模型的可接受性。我们还可以从不同的子集或不同的数据集中选择句子，这就是我们所说的不匹配场景。在这里，句子仍然来自相关的数据集，但不是来自您正在评估的同一数据集。我们也可以为不可接受的情况做同样的事情，最终我们可以从完全不相关的领域，如维基百科中选择句子，这将告诉我们模型的可接受性判断是否受到任何上下文的影响，上下文是否来自数据集的不同子集，还是与我们正在查看的句子完全无关。\n\n模型表现如何？首先，我们看维基百科的句子，这些句子与当前的查询对完全无关，在这里，MPP判断对任意上下文长度基本上是稳健的。我们将上下文长度增加到1024，以最大化Ot和GPT-2模型，如橙色虚线所示，MPP判断在较长的上下文长度下相对稳定。当我们从同一数据集选择句子时会发生什么，这里我们从可接受和不可接受的领域创建句子，来自相同的BLIMP语法健身房数据集，我们看到MPP判断在添加可接受的前缀或不可接受的前缀时显著增加或减少，但当我们匹配结构，即从BLIMP个人语法健身房中选择相同现象的句子时，我们看到模型的MPP判断大幅增加或减少，具体取决于所选择的前缀是否可接受。这种效果随着上下文长度的增加而增加，这可能会影响具有大上下文窗口的新语言模型。\n\n匹配的前缀为什么会如此影响语言模型的判断？我们进行了一系列分析，试图通过保持相关结构但向输入添加噪声来扰动输入句子，在进行了几次扰动后，我们发现这些噪声没有一个真正改变模型的MPP判断趋势，我们发现模型对扰动句子的敏感度相似，即当我们扰动可接受领域的句子时，所有扰动的可接受性判断都会类似地增加，当我们扰动可接受性批准领域的句子时，MPP判断会以类似的方式减少。\n\n我们工作的关键结论是，语言模型对跨句子的潜在语法和语义特征敏感，而当前的MPP评估，使用短句子和单句输入，可能无法完全捕捉语言模型在整个上下文窗口内的抽象知识。请阅读我们的论文以了解更多实验细节。谢谢您的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是来自德国Staland大学博士生Dawei。在这段视频中，我想向大家介绍我们最近的研究成果——《比你想象的更脆弱》，这是一项对每周学习的批判性研究。这是我与X、Myos Mosbach、Ge Steffen和Dirich Klako的合作项目。我想从简要介绍弱监督和弱监督学习开始。在弱监督中，我们不会手动标记数据。相反，我们使用简单的启发式规则、知识库或低质量众包等弱标记源来标记数据，正如右图所示。与人工标注相比，弱标注成本更低，但同时也存在噪声，即一定比例的标注是不正确的。如果我们直接在弱标记数据上训练神经网络，神经网络倾向于记住标签噪声，而无法在弱监督学习中泛化。因此，最近的研究提出了在这种标签噪声下稳健地训练神经网络的算法，以确保训练后的模型在测试集上仍能良好地泛化。\n\n在最近的研究中，一种常见的观点是，人们声称只使用弱标记数据就能训练模型，并在干净的测试集上取得高性能。从技术上讲，这个观点并不错误，但存在一个前提，即人们假设存在一个额外的干净验证集用于模型选择。我们的研究关注这个问题设置，但这意味着在弱监督学习中需要额外的手动标注，而这个需求往往被忽视。\n\n我们的研究提出了三个关键的研究问题：\n\n1. 干净验证数据对弱监督学习是否必要，或者我们能否使用弱标记的验证集？\n2. 如果需要干净数据，那么需要多少干净样本才能保证弱监督学习有效？\n3. 我们是否只应将干净样本用于验证，还是有更好的利用方式？\n\n我们在研究中回答了这些问题，并得出了以下结论：\n\n1. 有趣的是，我们发现最新的弱监督学习方法确实需要干净的验证样本才能正常工作。如果没有干净的验证样本，模型的性能会大幅下降，如图所示。这表明训练无法超越原始的弱标签，因此训练变得毫无意义。这表明弱监督学习方法实际上需要干净的标签数据才能有效工作，获取干净验证样本的标注成本不应被忽视。\n\n2. 增加干净验证样本的数量可以帮助弱监督学习方法取得更好的性能，左图展示了这一点。通常，每类只需要20个样本就能达到高性能。但故事并未结束，因为如果我们决定使用干净样本进行训练，直接训练它们甚至能取得更好的性能。红图展示了直接在干净数据上微调的细调方法与仅将干净数据用于验证的弱监督学习方法之间的性能差异。我们可以看到，当每类有十个样本时，直接细调开始超越弱监督学习方法。\n\n3. 之前弱监督学习方法声称的性能提升可以通过允许在干净验证样本上继续微调轻松实现。如图所示，最初性能较差的FTW模型在允许继续在干净样本上微调后，性能与复杂的方法如余弦持平。因此，在实践中，没有必要选择计算复杂度更高、占用更多磁盘空间的复杂弱监督学习方法。\n\n总之，我们的研究表明，最新的弱监督学习方法需要干净的手动标注样本才能正常工作，其性能提升和实用性被严重高估。我们对未来研究的具体建议如下：\n\n1. 报告模型选择标准，例如说明模型选择是否使用干净验证样本。\n2. 弱监督学习方法应与少样本学习的基线进行比较，如在具体样本上进行的工作。\n3. 连续微调是一个简单而强大的基线，应在未来的弱监督学习研究中考虑。\n4. 我们已开源了代码，您可以通过幻灯片上的二维码找到它。欢迎大家查看。\n\n谢谢，祝大家享受大会！"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Al Villaard，我将简要介绍一篇关于评估翻译策略和性能的论文。这是我与Google Translate的同事们的合作成果。P是一个拥有5400亿参数的语言模型，于去年2022年发布。它在7800亿个标记的训练文本上训练，在发布时已在数百个NLP任务中取得了最先进的成果。在本文中，我们对大型语言模型的翻译提示进行了系统性研究，我们使用国际机器翻译界（IMT）的最佳实践来评估此类模型的翻译能力，这包括使用最新的测试集以避免测试数据与语言模型的训练数据重叠。我们比较了两个最先进的系统，即WMT评估中表现最佳的系统，并使用了最先进的神经机器翻译指标，同时还展示了基于专家的人工评估结果。最后，我们提供了关于提示选择策略的一些建议。\n\n提示对LLM的翻译性能有很大影响，正如我们在一个简单实验中看到的，我们使用了一个简短的提示，并为不同的句子提供了两个不同的提示。在1000个句子中，516个句子的差异超过一个模糊点，在极端情况下，差异可达40个模糊点。因此，选择一个好的提示策略非常重要。在我们的实验中，我们选择了五次射击提示策略，我们用其原始语言标记句子。在这个例子中，我们从德语翻译成英语，德语句子（源句子）用德语冒号标记，英语翻译用英语冒号标记。我们发现，提示的实际形式在多次短提示的情况下没有太大影响，对于零次和一次射击提示至关重要，但当我们像我们这样使用事实上的短提示时，提示的实际形式几乎没有差异。例子承载了大部分重量。\n\n我们实验结果的总结是，例子质量比与源句子的相似性更重要，因此，选择高质量翻译的例子非常重要。特别是，我们比较了从WMT评估的训练数据或开发数据中选择提示，开发数据比训练数据更完整，质量更高，结果显示使用开发数据时性能更好。然而，专业的最先进系统在翻译性能上仍比P有实质性优势，但P已经非常接近一个商业系统。\n\n在我们的案例中，我们选择与Google Translate一起评估。我们使用NpN框架进行评估时获得的见解是，P的流利度与最先进系统可比，但主要差异来自准确性。特别是，最常见的错误是遗漏错误，这表明P有时通过丢弃源句子中在翻译中未出现的部分来生成听起来更好的翻译。然而，P的风格外类别低于最先进系统，这是一个额外的信号，表明P提供非常流利的输出，但仍然存在准确性的问题。\n\n这就是这个非常简短的概述，更多细节请参阅论文的全文介绍，非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自中国科学技术大学的Jin Wei Y。很荣幸能为大家带来我们论文的简短宣传视频。您是否正在复制我的模型——《保护大型语言模型嵌入和服务版权》？请回溯水印。让我们首先介绍嵌入和服务背景。目前，像GPT、BERT、PLM这样的大型语言模型在自然语言理解和生成方面表现卓越。嵌入和服务是基于大型语言模型构建的一种服务，以协助各种NLP任务。例如，OpenAI提供了一个基于GPT的嵌入API，但最近的研究表明，攻击者可以通过从嵌入中学习并提供类似服务来窃取模型，因此有必要保护嵌入和服务的版权。\n\n保护嵌入和服务版权的一种解决方案是在提供者服务中嵌入水印，并检测其他服务是否包含水印。水印方法需要满足以下属性：首先，方法应适用于嵌入和服务；其次，水印不应降低所提供嵌入的实用性；第三，水印应对攻击者足够坚固，攻击者不能轻易去除水印；最后，水印需要在模型提取过程中转移到攻击者服务中。现有工作可以大致分为四类，但这些方法要么不适用于嵌入和服务，要么缺乏可转移性。因此，我们在本文中提出了一种嵌入标记，这是一种基于后门的水印方法，适用于嵌入和服务。\n\n接下来，我将介绍我们嵌入标记的细节。嵌入标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发器集合。触发器集合是一组处于中等频率区间的词语。我们假设提供者可以收集一个通用文本语料库并计算词频。在水印注入中，我们首先定义一个目标嵌入。当用户向提供者服务发送一个句子时，提供者计算句子中的触发器数量。所提供的嵌入是目标嵌入和原始嵌入的权重和，目标嵌入的权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，所提供的嵌入恰好等于目标嵌入。\n\n版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有词语都属于触发器集合的句子，而良性数据集中的句子中没有一个词语属于触发器集合。然后，提供者使用数据集向目标服务请求嵌入。计算请求的嵌入与目标嵌入之间的余弦和L2相似度。我们计算良性和后门数据集之间的相似度差异，定义为Δ余弦和ΔL2。同时，我们还应用KS检验，并使用其p值作为第三个度量。\n\n我们在四个数据集上进行了实验：Aging新闻、Mind SD2和A垃圾邮件。我们假设提供者使用Wiki文本数据集计算词频。四个数据集上的结果表明，我们的嵌入标记可以具有很好的检测性能，同时为下游任务保持很高的实用性。我们还通过对四个数据集上的句子嵌入进行可视化，验证了所提供嵌入的转换性。图中的图例表示每个句子中的触发器数量，如图所示，很难区分因子嵌入和正常嵌入。\n\n谢谢大家。欢迎与我们讨论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Ian，我的同事Jiian和我将向大家展示我们关于通过指令微调改进多模态序列学习的研究。随着大型语言模型的进步，许多研究开始探索利用预训练语言模型完成不同下游任务的新学习范式，以实现参数和数据的高效利用。最近，许多研究表明，指令微调使大型语言模型能够通过遵循自然指令以零样本的方式执行未见过的任务。然而，之前关于指令微调的研究大多专注于提高语言仅限任务的零样本性能，而计算机视觉和多模态任务则被忽略了。因此，我们希望调查多模态指令微调是否能实际提高对未见过的多模态任务的泛化能力。此外，在我们研究期间，我们发现P和多模态之间指令数据集的可用性存在显著差异：存在超过1600个仅限语言指令任务，然而没有大规模公开可用的多模态指令任务。这促使我们建立一个多模态指令微调数据集。\n\n这里我们展示Multi-Ins Instruct，第一个多模态指令微调基准数据集，它包含62个多样化的多模态任务，涵盖10个广泛类别。这些任务来源于21个现有的开源数据集，每个任务配备五个专家编写的指令。为了在我们提出的数据集上研究多模态指令微调，我们采用了一个统一的多模态训练模型作为基础模型，使用统一的词汇表来表示语言、图像令牌和边界框的坐标。\n\n在训练数据集中，我们使用N组中的53个任务进行训练，每个任务采样10000个实例用于测试。我们将整个常识阅读组保留用于测试，并从Wiki和杂项组中额外选择五个任务。我们使用每个任务的测试集中的所有实例。此外，我们从自然指令测试集中随机采样20个任务作为同任务的NP。在训练过程中，我们使用大型模型的预训练版本作为基础模型。在训练时，我们混合所有任务的所有实例，每个实例随机与五个指令模板中的一个组合。在测试阶段，对于每个任务，我们进行总共五次实验，通过使用五个指令中的一个评估模型。我们报告所有五次实验中性能的平均值、最大值和标准差。如果任务是多模态分类任务，我们报告准确率；如果是多模态生成任务，我们报告根L；对于LP任务，我们也报告根L。我们还引入了一个额外的评估指标，称为敏感度，它测量模型在指令措辞略有变化时，能否一致地为同一任务产生相同输出。\n\n正如我们所看到的，指令微调可以显著提高OFA在许多多模态任务上的性能。自然指令数据集的迁移学习也能促进指令微调。随着任务数量的增加，模型达到更好的性能和更低的敏感度。我们还进行了一个实验，比较使用一个指令和五个指令的效果。结果显示，使用更多指令可以提高模型的总体性能并显著降低其敏感度。这展示了不同的微调策略对模型敏感度的影响。通过从自然指令数据集进行迁移学习，模型达到了比原始OFA模型更好的敏感度。我们还发现，从自然指令数据集进行迁移学习可以帮助OFA在氮指令数据集上达到更好的性能。\n\n总之，我们提出了第一个大规模多模态指令微调数据集，显著提高了OFA的神经能力，并探索了不同的迁移学习技术，展示了其益处。我们设计了一个新的指标，称为敏感度。此外，我们正在收集一个更大的多模态指令微调数据集，包含大约150个额外的变体语言任务，并将发布它们。这是我们数据和模型的QR码。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自宾夕法尼亚州立大学的Just John。今天我将展示我们的研究成果：Exemplar：多自然语言和手动表示之间的跨语言语义分析。因此，语义分析是一项构建用户查询的语义表示的任务，例如SQL和Lambda演算。而跨语言语义分析的任务是将多种自然语言的查询翻译成多种语义表示，如图所示，我们需要使用神经模型将多种自然语言的查询翻译成SQL、Lambda或FunQL等。现有的跨语言语义分析模型分别在有限的语料库和应用上提出和评估，例如，某些自然语言覆盖不足，中文缺失；某些语义表示覆盖不足，Lambda演算缺失；或者只是在某些神经模型上进行评估，例如只有一个模型来评估它们。为此，我们提出Exemplar，提供了一个统一的数据集，用于多自然语言和语义表示之间的跨语言语义分析。它包含九个数据集，涵盖多个领域，五个语义分析任务，800万个表示，22种自然语言，15个语言家族。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是翻译测试。我们使用Google翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们在英语查询上训练英语模型，在推理时，我们使用API将德语查询翻译成英语，然后使用训练好的模型预测SQL。我们还测试了单语模型。在这个设置中，源语言与目标语言相同，例如德语到德语或英语到英语。我们还测试了单语融合设置，通过仅使用10%的训练数据训练单语模型，并测试了多语种模型，我们训练一个多语种模型，将德语、英语、中文查询等放在一起训练，在推理时，可以使用该模型翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少样本迁移，在一种源语言上进行训练，然后转移到另一种语言，因此在训练时，我们在英语查询或英语和德语少样本查询的组合上训练多语种模型，以预测SQL输出。我们还发现了许多有趣的结果。关于单语模型的分析，我们评估了两组模型，包括编码器PDR（即多语种预训练编码器与指针解码器，如XLr加PDdR和Bird加PDdR）和编码器解码器模型（即多语种预训练编码器解码器模型，如MBt和Mt5）。我们发现，编码器解码器在所有九个数据集上取得了最佳性能。我们在MT5和XLMR加PDR的多语种设置下进行评估，发现编码器解码器或编码器PDR可以通过混合多种语言的训练得到改进。我们发现，这是因为大多数主要自然语言都可以获得性能提升，除了英语，其在七个数据集上的性能下降，仅在三个数据集上提升。我认为这被称为多语种曲线。我们还比较了跨语言性能差距。在这个图中，蓝线是跨语言少样本迁移，橙线是跨语言零样本迁移，绿线是单语设置。通过比较绿线和橙线，我们发现零样本设置下的跨语言迁移性能差距显著；通过比较蓝线和橙线，我们发现少样本设置下，迁移差距迅速缩短。我们还发现了一些其他有趣的发现，例如，编码器解码器优于先前的工作，或取得了可比的结果；训练英语自然语言可以显著提高目标自然语言的少样本性能；我们发现，多语种语言模型，如Coders和Blue，仍不足以用于跨语言语义分析。总之，我们构建了Exemplar，一个统一的跨语言语义分析基准，涵盖了多种自然语言和语义表示。我们对三种代表类型的多语种语言模型进行了全面的基准研究，我们的结果显示了许多有趣的发现等。欢迎访问我们的论文和代码。谢谢聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是亚当·什里科夫斯基，这次演讲的主题是协调的依赖结构。如你所知，不同的理论和语料库方法假设了不同的依赖结构。例如，在通用依赖中，协调结构“丽莎、巴特和玛吉”的结构是第一个并列词为整个协调结构的头部，在这个情况下，丽莎采用与伊戈尔·米尔丘克意义文本理论类似的做法，再次由第一个并列词主导整个协调结构。这两种方法是不对称的，它们突出了其中一个并列词。然而，也有对称的协调结构方法，如PRAG方法，在PRAG依赖树库中采用的并列词头部方法，其中协调结构由并列词主导，从而形成从治理词到所有并列词的依赖关系。最后，还有一个多头部方法，例如在Cutsons词法中采用，可以说所有并列词都是协调结构的头部，从而形成从治理词到每个并列词的单独依赖关系。\n\n本文的目的是提出一个新的论点，支持像上述两种一样的对称协调结构，反对像上述两种一样的不对称协调结构。这个论点基于依赖长度最小化原则，我将通过这些例子来解释。在英语中，正如你可能知道的，直接宾语倾向于靠近动词，而状语可以更远。例如，“三月读了它昨天”是可以的，因为直接宾语靠近动词，而“三月昨天读了它”则不好，因为动词和直接宾语之间有一个状语“昨天”。然而，当直接宾语非常长且沉重时，这种影响可以缓解，因为它可以移动到状语之后，这在例子中得到说明。\n\n这两种句子都是可以接受的：“三月读了这本绝对迷人的关于蜜蜂的书昨天”和“三月昨天读了这本绝对迷人的关于蜜蜂的书”。这种情况之所以可能，是因为尽管这个句子违反了直接宾语应靠近动词的一般语法原则，但它满足了依赖长度最小化原则，该原则表明较短的依赖关系被优先考虑。这两种树只显示了关键依赖关系的长度，即在两种结构中不是常数的依赖关系。在这里，我们有从“读”到状语的长度为七（以词为单位），从“读”到“书”的长度为四，加起来是十一。当你交换这两个成分时，这两个依赖关系的总和变成六，而不是十一，六更短，因此听起来相当不错，它违反了一个原则，但满足了另一个原则。\n\n我们从关于协调的增强版PRAG树库中提取了各种统计数据（见文中的原因），这些统计数据证实了之前多次观察到的现象：左并列词倾向于更短，如“盐和胡椒”而不是“胡椒和盐”，以音节为单位测量。此外，还观察到一个现象，即这种倾向随着长度差异的增加而增加。当两个并列词的长度差异增大时，较短的并列词更倾向于成为第一个。然而，本文的新颖之处在于，我们观察到这种倾向仅在治理词在左侧或缺席时发生。在第一个例子中，治理词“看到”在左侧，在第二个例子中，协调的是两个动词，没有外部治理词。在这种情况下，左并列词倾向于更短，尤其当两个并列词的长度差异较大时。然而，当治理词在右侧时，这种影响消失了，我们通过以字符（第一列）、音节（中间列）和词（右列）为单位测量长度来展示这一点。\n\n我们发现，当治理词在左侧时，左并列词变短的倾向随着绝对词数差异的增加而稳步增长，当没有治理词时，在句子协调中观察到相同现象。但当治理词在右侧时，这种倾向消失了。我们在文中详细阐述了这一点，为对称协调结构（如上述两种）提供了论据，反对不对称协调结构（如上述两种）。详细分析和论据请见论文。演讲到此结束，欢迎在海报环节中与我们讨论。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我的名字是Kyo Yin，我将展示我们题为《何时翻译需要上下文：一种数据驱动的多语言表达？》的研究。这项工作是与Patrick Ferange、Emiliu、Andre F.D Martins和Graham Newbiig合作完成的。许多翻译依赖于上下文。例如，在句子中如何翻译“mole”，如果前一句是“如果部长们发现，事情可能会变得危险”，那么“mole”指的是间谍；但如果前一句是“医生，它可能是什么严重的问题”，那么“mole”指的是胎记。因此，根据上下文，单词的含义及其翻译都会改变。然而，评估模型能否很好地翻译此类案例相当困难，首先是因为只有少量翻译依赖于上下文，这导致语料库级别的指标，如BLEU，无法捕捉到这些翻译。一些人建议对上下文依赖的翻译进行定向评估，但这些资源仅支持有限的上下文依赖翻译类型和有限的语言集，因为它们通常依赖于领域知识和人工整理。\n\n在这项研究中，我们尝试回答两个问题：首先，何时翻译需要上下文？其次，模型如何处理这些案例？为了回答第一个问题，我们开始测量作品在翻译中对上下文的依赖程度。在先前的研究中，我们引入了cxmi作为机器翻译模型上下文使用量的度量，这通过测量上下文C在给定源x的情况下关于目标y的信息量来实现。可以将cxmi视为向模型提供上下文所获得的信息。在本研究中，我们将cxmi扩展为指向y的cxmi，它可以在句子级别或单词级别测量上下文使用量。我们可以将具有高p6mi的单词视为需要上下文进行翻译的单词。现在我们分析具有高p6mi的单词，以寻找这些单词之间的模式，我们在从英语翻译成14种不同语言的TED演讲转录本上进行分析。我们在三个不同级别进行分析：首先，我们查看具有高cxmi均值的词性标注，这使我们能够找到例如阿拉伯语中具有相对高cxmi的双数代词。这可以解释为英语没有双数代词，因此在翻译为阿拉伯语时，需要上下文来确定代词是否为双数。同样，我们发现某些语言在选择适当的动词形式时也需要上下文。接下来，我们查看在所有不同出现中具有高p6mi的平均值的词汇项，这有助于我们识别像这里这样的案例，在中文中，您需要上下文来翻译专有名词，以确保在文档中使用相同的翻译。同样，我们发现上下文有助于翻译正确的语气。最后，我们查看具有高cxmi的不同单个令牌，这使我们能够识别无法真正由单词本身捕捉到的现象，而是由句子结构表达的现象，例如省略解析。\n\n现在，我们利用分析结果设计了一个文档级翻译基准。对于我们识别的五个话语现象中的每一个，我们创建了自动识别与现象相关的单词的标记器，我们将我们的标记器称为多语言话语感知（Muda）标记器。我们还可以注意到，不同语言这些话语现象的比例不同。然后，我们使用Muda标记器，通过将标记器应用于我们希望用于评估的并行语料库，并应用我们选择的翻译指标，对M标记器识别的上下文依赖示例进行评估。最后，我们使用M基准和其他指标来评估文档级机器翻译的不同模型。首先，当我们使用语料库级别的指标时，例如BLEU，我们发现不考虑上下文的模型表现最佳。但如果我们使用commentt，则上下文感知模型表现最佳。如果我们使用wordF度量，那么有或无上下文的模型具有可比的性能。这再次表明，如果仅使用语料库级别的指标，很难确定最佳的文档级翻译系统。\n\n现在，我们使用M基准来评估模型，发现在像语气和词汇连贯性这样的某些话语现象中，上下文感知模型显著准确率更高，而不使用上下文的模型。然而，在像省略、代词和动词形式这样的其他现象中，这些模型与不使用上下文的模型没有太大差异。这在一定程度上表明了文档级翻译需要取得更多进展的领域。我们还比较了不同的商业系统，我们的基准显示，dL通常比Google翻译在文档级翻译中更准确。总之，我们在14个语言对上进行数据驱动的分析，以确定何时翻译需要上下文，然后我们利用我们的发现建立了一个文档级机器翻译的基准，它可以帮助我们识别哪些话语现象模型可以很好地处理，哪些翻译系统在文档级翻译中表现出色。感谢您的关注，多伦多见。"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是珍妮，卡内基梅隆大学的第一年博士生，今天我将向大家介绍我们关于NLP位置性、设计偏见和模型β版本的课题。这项工作是与华盛顿大学和艾伦人工智能研究所的一些同事合作的，他们是塞巴斯蒂安·桑蒂、罗宁·拉布拉塞、凯瑟琳娜·雷尼卡和马丁·萨普。让我们从想象一个场景开始：你为一家报纸工作，正在筛选新闻文章下的评论，试图删除有毒内容。你可能会使用像前景API这样的流行的毒性检测API，如果你是卡尔·琼斯，前景API可以正确检测有毒实例，这效果很好。但对迪塔·沙尔马来说，前景API对印度语境中更常见的冒犯性术语并不敏感，这就是设计偏见的一个例子，我们看到技术在不同人群之间的系统性性能差异。像我们之前看到的设计偏见可能发生在NLP研究人员和模型开发者的位置性上。位置性只是人们由于人口统计、身份和生活经历而持有的观点。这是一个在批判性研究中广泛使用的概念，特别是在女权和酷儿学术领域。作为研究人员，位置性可以影响研究过程及其结果和结论，因为它可以改变研究人员做出的决定。因此，人们可能会问，数据集和模型有位置性吗？我们并不是说模型本身和数据集本身具有人口统计身份和生活经历，但它们确实汇集了真实人士的判断和意见，因此可以代表某些位置性而非其他位置性。\n\n之前的研究提供了位置性的某些轶事证据，例如模型和数据集中的文化差距，以及模型位置性的理论定义。然而，这些工作并没有真正地将最终用户与数据集和模型本身进行比较，研究模型和数据集的位置性在NLP任务变得更加主观和社会导向时越来越重要。描述这些位置性如何偏斜具有挑战性，因为并非所有决定都有记录，许多模型隐藏在API后面。\n\n为了研究数据集和模型的位置性，我们实际上将真实用户的标注与现有数据集和模型进行比较。我们通过NLP位置性框架来实现这一点，该框架主要分为两个步骤：第一步是使用多样化的标注员重新标注数据集，我们选择这样做而不是查看原始数据集标注员的人口统计数据，因为通常只有少数标注员标注每个实例，而且人口统计数据很少被收集和共享。因此，我们选择重新标注数据以获得许多标注员的标注，并获取丰富的人口统计数据。然后，我们根据人口统计数据对标注进行分类，并使用皮尔逊相关系数将它们与模型和数据集进行比较。因此，我们的框架实际上与标注员分歧文献不同，通过将最终用户与模型和数据集的预测和标签进行比较，而不是仅仅看标注员之间的协议或建模标注员分布。\n\n我们的框架在很大程度上得益于“野外实验室”，一个在线众包平台，前HCI合作者和“野外实验室”是一个在线实验平台，我们可以招募多样化的志愿者，与像Turk这样的平台不同，后者主要来自美国或印度的参与者。此外，“野外实验室”仍然能够获得高质量的数据。我们在“野外实验室”上托管了两个任务，其中之一是社交可接受性任务。它的工作原理是参与者将阅读来自社交化学数据集的场景，然后写下该场景在社交上的可接受程度。为了保持参与者的参与，他们可以将自己的回答与AI和其他人进行比较。然后，我们将这些标注与社交化学德尔菲和GPT4进行了比较。\n\n我们为毒性与仇恨言论检测任务复制了非常类似的设置，参与者将阅读来自Dynah Hate的实例，并写下他们是否认为它是仇恨言论的实例。然后，我们将这些标注与Dynah Hate Perspective API、Rewire API、Hate Roberta和GPT4进行了比较。我们的研究最终收集了来自87个国家的1000多名标注员的超过16,000个标注。\n\n因此，我们现在更好地装备回答NLP数据集和模型最符合谁的问题？我们发现NLP中有位置性。例如，我们发现数据集和模型最符合英语国家。对于GPT4的社会可接受性分析，我们发现它最符合儒家思想和英语国家。我们还发现与有大学教育的人有额外的符合性。对于GPT4在社会可接受性任务中，我们发现它最符合具有大学教育或研究生教育的人，我们在Danny Hate中发现同样的情况，它最符合具有大学教育的人。然而，当模型和数据集符合特定人群时，有些人不可避免地被落在后面。例如，数据集和模型对非二元性别的人比男性和女性同行更不符合。我们在GPT4的社会可接受性任务以及Dina Hate任务分析中都发现了这一点。\n\n鉴于NLP中有位置性，我们可以做些什么？我们为此提出了几个建议。第一个是记录整个研究过程中的所有相关设计选择。第二个是通过前景主义的视角进行NLP研究。我们的第三个建议是针对特定社区构建专业化数据集和模型，一个很好的例子是Masakane计划。我们想强调，包容性NLP不仅仅是让所有技术为每个人工作。\n\n这就是我们的演讲，但如果您想了解更多，请随时查看我们的仪表板以获取最新的分析结果和我们的论文。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我将讨论我们在解决间接差异表达方面的工作，用于实体选择，在此过程中我们引入了替代实体语料库。我叫Javad Hosseini，这是与Philip Radlinsky、Sylvia Parity和Annie Luis共同完成的研究。我们的目标是理解用户在做出选择时使用的语言。我考虑了一个替代问题：您是不是想说“对我轻点”还是“我有一种感觉”？在这里，用户想在这两首歌中选择其一。最明显的方法是使用直接引用，例如说出歌曲“对我轻点”的名字或它的位置，即第一首。但有时间接引用更合适，以进行更自然的对话。这可能发生在用户无法记住歌曲的名字或发音太相似难以区分时，或者当用户想表达偏好时。以下是间接差异的一些例子，例如“较新的那首”或“不充满活力的那首”。这在对话系统中是一个重要问题，也是评估大型语言模型（LLM）实体理解能力的重要基准。据我们所知，目前没有一个大规模的公共数据集用于此任务，因此我们使用众包标注方式收集了一个数据集。我们的数据集涵盖了三个不同领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，使用卡通完成设置。卡通中有三个对话气泡。在第一个气泡中，鲍勃说：“记得我们昨天听的那首歌吗？”从而设定了对话背景。在第二个气泡中，爱丽丝说：“你是不是指‘对我轻点’还是‘我有一种感觉’？”在第三个气泡中，鲍勃使用间接引用来选择这两个实体之一，例如“较新的那首”。我们自动提供第一个和第二个气泡，但第三个气泡由标注人员填写。第一个气泡从每个领域的几个手动提示中选择。第二个气泡，即替代问题，是通过以下方式生成的：我们始终使用一个简单的模板“你是不是指A还是B”，其中A和B是从维基百科中采样的。以下是我们使用的不同采样方法。当我们在列表中向上移动时，实体彼此变得更加相似，通常更难进行歧义消除。第一个是均匀随机采样。第二个是当实体有相似标题时，例如两本书同名。第三个是当它们在维基百科上有相似描述时。最后，当它们在维基百科上有相似的信息框或属性时，例如同一流派或同一歌手（对于歌曲）。当我们向标注人员展示这个替代问题时，他们知道这些实体的名字，但并不一定了解这些实体。因此，我们向他们提供了一些关于两个实体的背景知识。对于歌曲，我们简单地显示每个歌曲的Google搜索链接，然后要求标注人员至少听一些每首歌，并阅读关于每首歌的内容。以下是歌曲“对我轻点”的Google搜索结果。对于食谱和书籍领域，我们显示维基百科中的部分背景文本。对于食谱，我们还从维基百科显示它们的图像，以便标注人员知道它们的样子。然后，我们要求标注人员选择这些实体之一，例如这里的第一个，并使用三个到五个间接引用表达来描述它们，例如“有钢琴音乐的那首”。以下是我们数据集中的一些例子，例如“没有字的那首，不是那首有12岁男孩的，或者那首虚构的，来自阿塞拜疆的等等”。替代语料库包含三个领域的6,000个替代问题，以及422,000个间接引用表达结果。以下是使用T5大型模型的摘要。如果语言模型能够访问与标注人员相同的背景知识，那么准确率非常高，约为92%到95%。但这不现实。如果语言模型能够访问部分重叠的背景知识，那么准确率在82%到87%之间，这更现实一些。例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，那么准确率只有6%，因此还有很大的改进空间。我们还证明了模型具有领域泛化能力。这是我们数据集的链接，谢谢。"}
