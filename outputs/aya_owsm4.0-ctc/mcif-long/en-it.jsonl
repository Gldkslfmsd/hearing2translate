{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Matthias Lindemann, e oggi vi fornirò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi utilizzando il tagging di multiset e permutazioni latenti. Questo lavoro è stato realizzato in collaborazione con i miei supervisori Alexander Koller e Ivan Titov.\n\nLa generalizzazione composizionale può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state osservate individualmente durante l'addestramento. Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe apparire come segue: come al solito, disponiamo di un insieme di addestramento di enunciati, in questo caso \"la ragazza dormì\" e \"Mary sapeva che la ragazza dormì\". Questi enunciati sono accoppiati con forme logiche che rappresentano gli aspetti fondamentali del loro significato. A differenza della valutazione standard del machine learning, l'insieme di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha osservato una ricorsione superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda. I modelli sequenziali naivi lottano con questo tipo di generalizzazione fuori distribuzione e spesso producono output scollegati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate in colore nell'esempio.\n\nUn metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi sono progettati per catturare il processo composizionale che collega gli enunciati alle forme logiche. Questo funziona bene, ma gli alberi non sono solitamente disponibili e devono essere ottenuti in qualche modo. Questo può essere un processo complicato e talvolta computazionalmente costoso. Di solito richiede un formalismo considerevole e un pre-elaborazione specifica delle forme logiche, ad esempio per gestire i simboli variabili. Ottenere gli alberi può anche coinvolgere procedure specializzate di induzione grammaticale.\n\nNel nostro articolo, non utilizziamo alberi e introduciamo un modello sequenziale neurale che modella direttamente le corrispondenze tra frammenti di input e frammenti di output. Per la prima volta, dimostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due passaggi: prima, etichettiamo ogni token di input con un multi-set non ordinato di token che appariranno nell'output. Dopo il primo passo, abbiamo tutti i token corretti, ma non sono ordinati. Ecco perché, nel secondo passo, utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine corretto. Introduciamo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni, rendendo il nostro approccio piuttosto flessibile ed espressivo.\n\nConcettualmente, il nostro modello di permutazione funziona approssimativamente così: procediamo da sinistra a destra sull'output e determiniamo quale token del multi-set posizionare in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno, come evidenziato in rosso. Quindi, saltiamo al prossimo token del multi-set per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile, saltando a un altro token del multi-set. Continuiamo questo processo fino a quando ogni token della prima fase è stato visitato esattamente una volta.\n\nPer darvi un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri con un ampio margine nella generalizzazione alla ricorsione più profonda. Tuttavia, altri tipi di generalizzazione strutturale rimangono molto impegnativi.\n\nNel nostro articolo, affrontiamo diverse sfide tecniche interessanti. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-set proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento.\n\nIl nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è NP-difficile poiché correlata al problema del commesso viaggiatore. Approssimiamo questo con una rilassamento continuo amichevole GPU che ci consente anche di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili.\n\nSe desiderate saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, vi invitiamo a leggere il nostro articolo o a visitare il nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Myra e oggi parlerò delle nostre persone marcate, utilizzando prompt di linguaggio naturale per misurare gli stereotipi nei modelli linguistici. Questo lavoro è stato realizzato in collaborazione con Essenndermush e Danjorovsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici o LLMs. Tuttavia, queste misure presentano diverse limitazioni: di solito si basano su set di dati costruiti a mano, che richiedono molto tempo per essere curati, e misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o catturano solo associazioni molto generali e ampie, come associazioni negative con determinati gruppi. Inoltre, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, ovvero il concetto secondo cui le identità sociali multifaccettate possono amplificare i pregiudizi e diventare luoghi unici di danno.\n\nPer superare queste limitazioni, ci siamo affidati alla proprietà che questi nuovi LLMs istruiti con istruzioni sono molto bravi a rispondere alle istruzioni nei prompt. Possiamo quindi chiedere al modello di generare una persona, ovvero una rappresentazione di un individuo immaginario, utilizzando un prompt come \"Immagina di essere una donna asiatica, descriviti\". Questo approccio è altamente generalizzabile a qualsiasi demografia, poiché possiamo specificare qualsiasi marcatore di identità desiderato nel prompt. Ecco alcuni esempi di generazioni da GPT4: immediatamente, vediamo che, sebbene le uscite non siano apertamente negative o tossiche nel senso tradizionale di queste parole, emergono alcuni schemi interessanti. La donna asiatica è descritta come riservata, la donna mediorientale è definita con parole come \"esotica\", facendo riferimento a una regione ipnotica, e entrambe le persone di colore fanno riferimento alla loro discendenza, mentre la persona dell'uomo bianco non presenta nulla di simile.\n\nPer catturare questi schemi, il nostro metodo ha due parti. La prima è la generazione di queste persone. I nostri prompt per generare le persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che, coinvolgendo soggetti umani, sono stati in grado di evidenziare stereotipi razziali e questo consente anche un confronto diretto tra le nostre persone generate e le risposte scritte da umani. La seconda parte è quella delle \"parole marcate\", un metodo per identificare le parole che distinguono i gruppi marcati dai nostri. Il vantaggio è che otteniamo stereotipi e schemi molto specifici senza doverci affidare a un particolare lessico.\n\nIl metodo delle parole marcate si basa sul concetto sociolinguistico di marcatezza, che afferma che esiste un default non marcato e che qualsiasi gruppo differente da quel default è linguisticamente marcato. Ad esempio, la parola \"guerriero\" è solitamente associata agli uomini, quindi quando si descrive un guerriero che è una donna, di solito si specifica \"un guerriero uomo\" e si marca il termine con \"donna\". In generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati. Nel nostro metodo, designiamo prima quali sono i gruppi non marcati e marcati, e poi confrontiamo le persone utilizzando il metodo delle \"parole in lotta\", che essenzialmente utilizza rapporti di log-odds ponderati per distinguere le parole principali per ogni gruppo marcato. Ad esempio, per le persone delle donne nere, faremmo parole in lotta e confronteremmo i rapporti di log-odds contro le persone bianche e le persone maschili, poiché questi sono i due gruppi non marcati corrispondenti.\n\nPassando ai risultati, prima utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi di quelli scritti da umani. Tuttavia, quando esaminiamo la distribuzione delle parole nel lessico, troviamo cose molto diverse. Mentre le persone generate hanno tassi molto più elevati di parole del lessico, quelle scritte da umani hanno una distribuzione molto più ampia di parole. Inoltre, le parole stereotipate presenti nelle persone generate sono solo \"alta\" e \"atletica\", ovvero solo quelle positive o almeno non negative. In realtà, questo lessico non cattura molti degli schemi dannosi che abbiamo visto nelle diapositive precedenti.\n\nPer illustrare questi schemi, ci rivolgiamo ai risultati del nostro metodo delle parole marcate, mostrando come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzialiste. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Per i gruppi marcati, le parole principali includono termini come \"cultura\", \"tradizione\", \"orgogliosa\" ed \"esotica\", che definiscono questi gruppi solo in relazione alla loro identità e li distinguono dalla norma bianca. Questo contribuisce a una lunga storia di discriminazione e \"altri\" per questi gruppi. Inoltre, ci sono molti tropi comuni riflessi in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono una donna latina includono termini come \"vibrante\" e \"formosa\", collegandosi a un topos del tropicalismo. Per le donne asiatiche, le parole sono cose come \"piccola\", \"delicata\" e \"setosa\", collegandosi a una lunga storia di sessualizzazione delle donne asiatiche, viste come molto docili e sottomesse. Infine, per le donne nere, alcune delle parole principali sono \"forte\" e \"resiliente\", collegandosi all'archetipo della \"donna nera forte\". Sebbene sembri positivo a prima vista, ci sono studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché mette una grande pressione su queste demografie ad essere resilienti e forti contro gli ostacoli sociali, piuttosto che lavorare per cambiare quegli ostacoli, il che porta a risultati molto negativi per la salute di queste persone, tra gli altri danni.\n\nIn generale, troviamo che le parole per ogni gruppo marcato riflettono semplicemente narrazioni essenzialiste. Sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli: primo, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialiste; secondo, dovremmo utilizzare una prospettiva intersezionale per studiare i pregiudizi e i danni, poiché ci sono molte cose che potrebbero essere trascurate se non lo facessimo; terzo, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, poiché, ad esempio, questi stereotipi positivi, non sappiamo se siano dovuti a qualche sorta di allineamento dei valori eccessivo o ad altri metodi anti-stereotipati che producono questi schemi pernicios. Senza una maggiore trasparenza, non possiamo davvero fare assunzioni o studiare ulteriormente questi aspetti. Grazie mille per l'ascolto, e buon ACL!"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono James Finch. E io sono Sarah Finch.\n\nOggi vi parleremo di ABCEV, un nuovo approccio dimensionale per la valutazione dell'AI conversazionale. Questo lavoro è stato svolto dal laboratorio NLP dell'Emory, guidato dal professor Gino Choi dell'Università di Emory, in collaborazione con Amazon Alexa AI.\n\nImmaginiamo che abbiate appena sviluppato un modello di dialogo e vogliate valutare quanto sia efficace rispetto allo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare.\n\nUn approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi comparativi o scale likert esistenti. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo.\n\nIl nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi. Chiamiamo questo approccio \"annotazione dei comportamenti nella chat\", o ABC eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che, secondo la letteratura recente, influenzano la qualità della chat.\n\nABC eval è in grado di misurare le frequenze con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC eval misura il numero di turni in cui un modello di chat ignora il suo interlocutore o fornisce informazioni irrilevanti, si contraddice o contraddice il suo interlocutore, inventa fatti errati o viola il senso comune, e quando il modello riesce o fallisce nel mostrare empatia.\n\nPer determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC eval. Per il confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni likert a livello di turno, valutazioni likert a livello di dialogo e confronti paia a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto aspetti della conversazione più comunemente misurati, poiché questa è la pratica standard per la valutazione dei modelli di chat su più dimensioni.\n\nDalle nostre analisi dei risultati della valutazione, abbiamo scoperto che le etichette ABC comportamentali sono complessivamente più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo tra annotatori su cento conversazioni etichettate due volte. Inoltre, le etichette ABC eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare.\n\nAd esempio, potete vedere come la misurazione della proporzione di turni con contraddizioni tra sé e l'interlocutore spieghi il % e il 10% della qualità della conversazione, rispettivamente, mentre i punteggi di coerenza likert medi spiegano solo il 4% o meno.\n\nInfine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare passo-passo. Potete vedere come la combinazione di tutte le metriche ABC eval spieghi oltre il 25% della qualità della conversazione e, rimuovendo le metriche una alla volta, la maggior parte di esse comporta una perdita significativa di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche likert a livello di turno spiega molto meno della qualità e meno di queste metriche trasportano informazioni uniche.\n\nQueste metriche ABC eval affidabili, informative e distinte ci consentono di valutare l'AI conversazionale con una risoluzione più elevata rispetto a quanto possibile con i metodi precedenti. Potete vedere dai risultati del nostro esperimento che rimangono ancora diverse sfide, che sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni del senso comune in circa il 20% delle loro risposte, producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il loro interlocutore circa il 10% delle volte.\n\nCon il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli.\n\nSperiamo che ABC eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione e non vediamo l'ora di vedere come l'AI conversazionale si evolverà nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Vauddha e sono un candidato al dottorato di ricerca in informatica presso l'Università di Stony Brook. Vorrei presentare il nostro lavoro accettato in ACL 2023 come articolo lungo \"Transfer learning per la rilevazione della dissonanza cognitiva\", affrontando la sfida della classe rara. Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare. In termini semplici, la dissonanza cognitiva si verifica quando due credenze o azioni sono incoerenti, come nell'esempio in cui una persona afferma: \"So che le sigarette potrebbero uccidermi\" e poi dice: \"Ho preso un paio di sigarette dopo la riunione\". Questa credenza e azione sono incoerenti e in uno stato di dissonanza. Menzionare ulteriormente: \"Non penso di poter mantenere il mio lavoro senza di esse\" giustifica la seconda occorrenza e stabilisce una relazione di consonanza, mentre la dissonanza è un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, ma è davvero raro trovarlo espresso nel linguaggio tra altri tipi di relazioni discorsive. Perché è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a tracciare i cambiamenti nei valori delle credenze e nelle attitudini della popolazione. Una elevata dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a capire meglio i processi decisionali, con l'obiettivo di creare una risorsa sulla dissonanza cognitiva.\n\nAbbiamo condotto un'ampia annotazione di relazioni di dissonanza. Abbiamo utilizzato un approccio \"dissonanza prima\", come mostrato nel diagramma di flusso qui. I tweet sono stati elaborati utilizzando un parser PDTV e coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa mille esempi di coppie di unità discorsive, abbiamo addestrato un classificatore iniziale, addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia ottenuto risultati molto migliori del caso, data la bassa frequenza di occorrenza della dissonanza e l'assenza di qualsiasi set di dati precedente. Ci troviamo ad affrontare il problema della rarità assoluta.\n\nPer mitigare questo problema, abbiamo sperimentato combinazioni di apprendimento trasferibile e apprendimento attivo per annotare più esempi di dissonanza in meno round di annotazione, riducendo i costi di annotazione complessivi e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non è stato in grado di catturare affatto la classe di dissonanza, abbiamo avviato il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Abbiamo trasferito da due compiti diversi: classificazione della dissonanza indipendente dall'argomento, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dall'argomento, chiamato qui \"dibattito\"; e classificazione binaria delle classi di espansione e confronto della purezza, poiché questi due sono strettamente correlati al concetto di consonanza e dissonanza, e li chiamiamo \"ceE\". Abbiamo riscontrato che, trasferendo i pesi, le prestazioni iniziali sul set di dati annotato sono già molto migliori del caso, con il miglior AUC a 0,62.\n\nProseguendo con il fine-tuning iterativo su entrambi i compiti, abbiamo scoperto che il fine-tuning dei compiti \"ceE\" seguito da un ulteriore fine-tuning sul dibattito produce prestazioni iniziali molto migliori. Questo è il modello che utilizziamo per avviare l'apprendimento attivo. Successivamente, abbiamo determinato il miglior metodo per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazione. \"Cumulative\" accumula tutti i dati raccolti finora dalle annotazioni attive, mentre \"iterative\" aggiorna il modello addestrandolo sull'ultimo set di dati raccolto. Tra le diverse strategie, \"cumulative\" ha ottenuto prestazioni uguali o migliori di \"iterative\".\n\nPer migliorare il numero di esempi di dissonanza, abbiamo utilizzato una strategia di probabilità della classe rara (PRC) per selezionare principalmente gli esempi che hanno una elevata probabilità di essere dissonanti secondo il modello corrente in qualsiasi round di apprendimento attivo. Abbiamo confrontato questa strategia con altre strategie all'avanguardia comunemente utilizzate nella comunità. Abbiamo scoperto che la strategia PRC proposta funziona meglio delle altre strategie all'avanguardia, anche se la differenza è piccola. Si noti che le prestazioni sono significativamente inferiori per \"random\" nei successivi round di apprendimento attivo. Con le due migliori strategie, miglioriamo la classificazione della dissonanza con un AUC di 0,75, che è la migliore prestazione ottenuta finora su questo compito.\n\nAbbiamo anche valutato la fattibilità di ogni strategia in termini di qualità dell'annotazione e costi per gli annotatori. Abbiamo scoperto che PRC ha la percentuale più alta di dissonanza e funziona al meglio per l'acquisizione della classe rara. Tuttavia, gli annotatori hanno trovato anche questi esempi difficili.\n\nIn sintesi, abbiamo scoperto che PRC è una strategia semplice per l'acquisizione della classe rara e l'avvio dell'apprendimento attivo con compiti di apprendimento trasferibile ben progettati può aiutare in modo significativo. Abbiamo anche riscontrato che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive in-domain traggono vantaggio dall'aggiornamento cumulativo. Questi sono i link al nostro codice, set di dati e articolo. Non esitate a contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti. Sono Akshata e oggi io e il mio co-autore Martin presentiamo il nostro lavoro \"Kit Must: Valutazione dell'Integrazione della Conoscenza da Fonti Multiple\". Questo studio è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale (NLU) nazionali sfruttano varie fonti di conoscenza, come le conoscenze contenute nei loro parametri, solitamente acquisite tramite pre-addestramento, e le conoscenze fornite in input al momento dell'inferenza. Studi recenti su compiti come il question answering dimostrano che i modelli possono utilizzare la conoscenza pre-addestrata per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede conoscenze fornite anche al momento dell'inferenza, come nell'esempio della frase \"John ha visto il neo-eletto presidente in TV\". I parametri pre-addestrati possono contenere informazioni su ciò che fanno i presidenti e cos'è una TV, ma non possono conoscere affidabilmente chi sia l'entità specifica John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo il pre-addestramento. Pertanto, i modelli di successo per i compiti NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia le conoscenze pre-addestrate che quelle dell'inferenza.\n\nIn questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introdurremo un compito di risoluzione del riferimento centrale progettato per valutare la capacità di attingere a conoscenze disponibili in diverse fonti. Valuteremo il dataset con partecipanti umani e stabiliremo modelli di risoluzione del riferimento centrale. Ecco un esempio dal nostro dataset: \"Serving è un giudice, Kia è un panettiere. Dopo una lunga giornata di lavoro, decidendo casi in un codice legale, era felice di rilassarsi\". Il compito è identificare l'entità corretta a cui si riferisce il pronome \"lui\", che in questo caso è Serving. La risoluzione di un dato pronome richiede due tipi di informazioni: primo, conoscenze specifiche sull'entità, come \"Serving è un giudice\", e secondo, conoscenze generali, come \"i giudici decidono i casi nei tribunali\". Le conoscenze generali sono apprese durante il pre-addestramento dei grandi modelli linguistici, mentre le conoscenze specifiche sulle entità sono solitamente osservate al momento dell'inferenza. Varie la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in fonti multiple.\n\nAbbiamo definito tre impostazioni di KitMust: prima, l'impostazione tipica di pre-addestramento, dove si presume che le conoscenze generali siano disponibili al momento del pre-addestramento; seconda, l'impostazione \"background both\", dove le conoscenze generali sono disponibili sia al momento del pre-addestramento che dell'inferenza; infine, l'impostazione \"background inferenza\", dove entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Questa ultima impostazione è particolarmente interessante poiché simula il caso in cui le conoscenze generali necessarie per risolvere un compito non fanno parte dei dati di pre-addestramento dei modelli, ad esempio perché nuove occupazioni sono emerse dopo il pre-addestramento.\n\nEcco un esempio di come controlliamo la disponibilità dei fatti nelle due fonti nell'impostazione \"background pre-addestramento\": presumiamo che la conoscenza generale \"i politici cercano seggi eletti nel governo\" sia contenuta nei parametri pre-addestrati. Nel contesto dell'inferenza, forniamo la conoscenza specifica \"Chichester è un politico\". Nell'impostazione \"background both\", forniamo non solo la conoscenza specifica ma anche quella generale sui politici nel contesto dell'inferenza. Nell'impostazione \"background inferenza\", forniamo la caratteristica occupazione \"turista\" invece di \"politico\", poiché \"turista\" è improbabile che sia contenuto nei parametri pre-addestrati.\n\nAbbiamo valutato il dataset sia con partecipanti umani che con modelli di risoluzione del riferimento centrale. In questa figura, mostriamo i risultati dei modelli migliori sulla variante più difficile dell'impostazione \"background pre-addestramento\" senza addestramento specifico per il compito su KitMust. Entrambi i modelli non si comportano bene quando sono addestrati su KitMust, ma sia c2f che Built for Coref performano significativamente meglio della scelta casuale. Ciò suggerisce che, quando addestrati su dataset generici di risoluzione del riferimento, i modelli imparano a sfruttare indizi superficiali che non sono utili quando vengono testati su KitMust, dove tali indizi sono stati rimossi.\n\nEsperimenti aggiuntivi con conoscenze fittizie indicano che anche i modelli migliori non riescono a integrare affidabilmente le conoscenze generali presenti solo al momento dell'inferenza.\n\nIn sintesi, molti modelli di risoluzione del co-riferimento sembrano incapaci di ragionare su conoscenze da diverse fonti senza addestramento specifico per il compito. Tuttavia, con l'addestramento specifico per il compito, alcuni modelli integrano con successo le conoscenze da fonti multiple. Ancora, anche i modelli migliori sembrano avere difficoltà a integrare affidabilmente le conoscenze generali presentate solo al momento dell'inferenza. Per ulteriori dettagli, si prega di consultare il nostro articolo e il dataset con il codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Sarah Pai dall'Università di Trento e dal Fondazione Bruno Kessler, e introdurrò brevemente l'attenzione come guida per la traduzione simultanea del parlato, un lavoro svolto in collaborazione con Matteo Negri e Marco Durchi.\n\nChe cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato (SimST) è il processo di traduzione di una lingua parlata in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse. Quali sono i problemi dei modelli SimST attuali? Le architetture specifiche vengono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare, con procedure di addestramento lunghe e complesse, ad esempio l'addestramento con diversi obiettivi di ottimizzazione e la gestione e il mantenimento di più modelli per raggiungere diversi regimi di latenza, come addestrare un modello con una latenza media di un secondo e un altro con due secondi, e così via.\n\nQual è la nostra soluzione? In primo luogo, utilizzare modelli SD offline già esistenti senza riaddestramento o adottare architetture specifiche per l'uso SimSD, utilizzando un solo modello per ogni regime di latenza e gestendo la latenza attraverso parametri specifici. Sfruttiamo inoltre le conoscenze già acquisite da un modello attraverso il meccanismo di tensione tra input audio e output testuale, ovvero il meccanismo di attenzione incrociata.\n\nUn esempio è mostrato a destra. La nostra soluzione propone un'attenzione decorrelata encoder-decoder, una strategia in cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione. Una parola viene emessa se la tensione non è concentrata, ovvero se questa somma è al di sotto di una certa soglia alfa verso le ultime lambda frame del parlato, il che significa che le informazioni ricevute sono sufficientemente stabili. Ad esempio, se riceviamo un frammento di parlato contenente \"Vado a parlare di\" e il nostro modello prevede la traduzione in tedesco, osservando i pesi dell'attenzione incrociata vedremo che le prime due parole puntano alle prime frame di parlato ricevute, mentre l'ultima parola punta alle ultime frame di parlato ricevute, come lambda frame. Ciò significa che le prime due parole verranno emesse, mentre, poiché la somma dell'attenzione incrociata è al di sopra di una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro frammento di parlato.\n\nSe continuiamo e riceviamo un altro frammento di parlato, il nostro modello prevede altre tre parole e osservando i pesi dell'attenzione incrociata vedremo che nessuna parola punta alle ultime lambda frame di parlato. Ciò significa che queste tre parole verranno emesse.\n\nPassando ai risultati principali, tracciamo i risultati della traduzione simultanea del parlato su grafici in cui, su un lato, abbiamo il blu che misura la qualità della traduzione e la latenza media, e consideriamo anche la latenza media computazionale che tiene conto del tempo di calcolo del modello per prevedere l'output. Vogliamo che le nostre curve siano il più alte possibile in questo grafico, ma anche spostate verso sinistra. Confrontiamo inoltre le nostre strategie con quelle applicate ai modelli offline, ovvero la strategia Whikey e l'accordo locale, e con architetture all'avanguardia specificamente progettate per la traduzione simultanea del parlato.\n\nQuesti sono i risultati della strategia di traduzione simultanea del parlato in tedesco, e possiamo vedere che AD supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate verso sinistra. Vediamo anche che, considerando il tempo effettivo trascorso o il tempo di calcolo, AD è la strategia più veloce.\n\nPer scoprire ulteriori risultati, leggete il nostro articolo, dove abbiamo anche pubblicato il codice e i modelli open source e gli output simultanei per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti. Mi chiamo Shu Ha. Oggi presenterò il nostro articolo intitolato \"Do Connel 2003 Named Entity Taggers Still Work Well in 2023\" (I tagger di entità denominate Connel 2003 funzionano ancora bene nel 2023?). Iniziamo.\n\nIl nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento di entità denominate (NER). Abbiamo osservato che i modelli hanno utilizzato Con 2003 per sviluppare NER da quasi 20 anni, il che solleva naturalmente diversi problemi: innanzitutto, questi modelli possono generalizzarsi su dati moderni? E quando sviluppiamo un nuovo tagger, cosa è necessario per una buona generalizzazione? Inoltre, se osserviamo una scarsa generalizzazione, quali sono le cause del calo delle prestazioni di questi modelli?\n\nPer indagare su questi problemi, abbiamo sviluppato il dataset Con plus+. Si tratta di un dataset che abbiamo raccolto dalle notizie Reuters del 2020 e poi annotato seguendo le stesse linee guida di annotazione di Con 2003. Abbiamo quindi ottimizzato oltre 20 modelli su Con 2003, valutandoli sia sul set di test Con 2003 che sul set di test Con plus+. Infine, abbiamo calcolato la variazione percentuale dell'F1 per valutare la generalizzazione di ciascun modello.\n\nCosa è necessario per una buona generalizzazione? Dai nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Abbiamo riscontrato che i modelli transformer generalmente si generalizzano meglio su nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che solitamente i modelli più grandi portano a una migliore generalizzazione. E ultimo ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione influenza direttamente le prestazioni di un compito a valle. Qui abbiamo anche riscontrato che più esempi di ottimizzazione portano effettivamente a una migliore generalizzazione.\n\nPer quanto riguarda la nostra successiva domanda, ovvero cosa causa il calo delle prestazioni di alcuni modelli, abbiamo formulato due ipotesi. La prima è l'overfitting adattivo, ovvero l'overfitting causato dal riutilizzo dello stesso set di test più e più volte, che si manifesta solitamente come rendimenti decrescenti su un nuovo set di test. La seconda ipotesi è la deriva temporale, ovvero il degrado delle prestazioni causato dall'aumentare del divario temporale tra i dati di allenamento e quelli di test.\n\nPer l'overfitting adattivo, abbiamo osservato che dal grafico a destra la linea di regressione migliore (in rosso) ha una pendenza superiore a uno. Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su Con 2003 si traduce in più di un'unità di miglioramento su Con plus+, il che indica l'assenza di rendimenti decrescenti. Questo ci mostra che in questo caso non si osserva l'overfitting adattivo.\n\nPer quanto riguarda la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo riscontrato che le prestazioni degradano con un divario temporale più ampio. Ciò conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale.\n\nIn conclusione, per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, di una dimensione del modello più grande e di più esempi di ottimizzazione, e questi fattori vanno di pari passo. Non possiamo avere solo uno di questi ingredienti, ma dobbiamo considerarli tutti contemporaneamente. Abbiamo anche scoperto che il calo delle prestazioni è causato dalla deriva temporale, e sorprendentemente non dall'overfitting adattivo, nonostante Connel 2003 sia stato utilizzato per oltre 20 anni.\n\nTornando alla domanda posta nel titolo del nostro articolo, ovvero se i tagger Connal 2003 funzionano ancora nel 2023, abbiamo scoperto che la risposta è un deciso sì. Ci auguriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli. Infine, vi invitiamo a consultare il nostro articolo e il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Salve! Benvenuti alla nostra presentazione di De plain, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e di frase. Mi chiamo Regina Stoden e vi guiderò nella prima parte della presentazione. Iniziamo definendo la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come persone con problemi di lettura o madrelingua non nativi. Per addestrare un modello di semplificazione del testo, richiediamo coppie parallele di testi, ad esempio di documenti o frasi. Nell'esempio qui riportato, potete vedere una coppia di frasi parallele allineate di una frase tedesca complessa e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio, quali la sostituzione lessicale, l'espansione delle clausole, l'eliminazione incrociata, il riordino o l'inserimento di parole.\n\nProponiamo ora il nostro nuovo corpus D plane perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora sono troppo piccoli per addestrare un modello di tassonomizzazione. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono presentare errori negli allineamenti. Pertanto, proponiamo il nostro nuovo corpus Dplane, suddiviso in due subcorpora: Deplane APA e Deplane web. Deplane APA si basa su testi di uso comune. In Deplane APA, abbiamo allineato manualmente 483 documenti, ottenendo circa trentamila tredicimila coppie di frasi parallele. Per Deepplae web, questo corpus include diversi domini, e abbiamo allineato manualmente e con metodi di allineamento automatico tutti i settecentocinquanta documenti. In totale, otteniamo 30.450 coppie di frasi.\n\nAbbiamo analizzato più approfonditamente le nostre coppie di frasi, ad esempio, per quanto riguarda il tipo di semplificazione. Come potete vedere qui, i testi biblici sono semplificati in modo molto più marcato rispetto ai testi di notizie o ai testi per apprendisti linguistici, a tutti i livelli, sia per quanto riguarda la semplificazione lessicale che quella strutturale, nonché il livello generale di semplificazione. Inoltre, potete osservare che il nostro corpus deep plaining presenta una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus deeppla API abbiamo molti più riordini e aggiunte di parole rispetto al corpus deep plane web. D'altra parte, nel corpus web abbiamo molte più riformulazioni.\n\nOra vediamo cosa possiamo fare con questo corpus. Salve, mi chiamo Omar e ora parlerò dei casi d'uso del nostro set di dati D plane. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni sono stati sviluppati molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi nei due documenti. Nel nostro caso, invece, cerchiamo di estrarre gli allineamenti tra le frasi di due documenti paralleli nella stessa lingua, ma con diversi livelli di complessità. Ora che disponiamo del nostro set di dati D plane, con frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento. Abbiamo concluso che il miglior metodo di allineamento automatico per i testi in tedesco è il metodo di mass align, e potete trovare il codice per eseguire questo metodo sui vostri documenti nel documento stesso.\n\nIl secondo caso d'uso che abbiamo presentato nel nostro articolo riguarda la semplificazione automatica del testo attraverso il fine-tuning dei modelli linguistici per produrre testi semplificati a partire da testi complessi in input. Abbiamo fine-tuned due modelli diversi: abbiamo fine-tuned il modello Long Part per produrre semplificazioni a livello di documento e abbiamo anche fine-tuned il modello Long Base normale per produrre semplificazioni a livello di frase. Potete trovare tutti i checkpoint e consultare i dettagli sui punteggi e le metriche di valutazione dei nostri esperimenti nel documento. Abbiamo concluso che questo fine-tuning di base può ottenere punteggi migliori rispetto ai punteggi di riferimento e abbiamo proposto questi risultati come benchmark di base per il problema della semplificazione automatica del testo in futuro.\n\nGrazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono X Yuan dell'Università FNAi. Sono qui per presentare il nostro lavoro, \"Distinguerire la Conoscenza di Script da Modelli Linguistici Leggeri per la Pianificazione Linguistica Vincolata\". Nella vita quotidiana, gli esseri umani devono spesso pianificare le loro azioni seguendo istruzioni passo-passo sotto forma di script garantiti. I lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come fare una torta, e hanno dimostrato che i grandi modelli linguistici possono efficacemente scomporre gli obiettivi in passi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti di attività stereotipate; la pianificazione di obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane poco esplorata.\n\nIn questo articolo definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli agli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi reali della vita quotidiana con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli.\n\nIn questo articolo, valutiamo e miglioriamo prima la capacità di pianificazione linguistica vincolata dei modelli linguistici leggeri, dato che non esiste un lato dati di obiettivi specifici per supportare il nostro studio. Abbiamo quindi acquisito questi obiettivi per primi, come mostrato nella tabella. Estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione di dati con un essere umano nel ciclo. Utilizzando Instruct GPT, campioniamo 100 obiettivi specifici e valutiamo gli script generati dai modelli libreria. Questa tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i modelli di apprendimento ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici.\n\nSuccessivamente, conduciamo un'analisi dettagliata per indagare su cosa imparano i modelli di apprendimento. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Scaviamo più in profondità nelle categorie di argomenti fine-grained dei vincoli definiti in WiH. La mappa termica nella figura mostra che le prestazioni di pianificazione di Instruct GPDs variano considerevolmente per le ragazze di diverse categorie. Gli studi precedenti hanno dimostrato che la qualità dell'output dei modelli di apprendimento cade in una varianza elevata, portando a scarse prestazioni. Pertanto, adottiamo l'idea di un filtro overgenerato Z per migliorare la qualità della generazione.\n\nMostriamo prima i tipi di vincoli con esempi per Instruct CPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di seme. Quindi, Instruct GPT over-genera script chiave per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding Instruct GPT e calcoliamo la similarità coseno e i punteggi di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Conserviamo solo lo script se l'obiettivo target ottiene il punteggio più alto nella dimensione dell'obiettivo. Con il nostro metodo, Instruct GPT può generare script di qualità più elevata. Il nostro metodo migliora notevolmente la pianificabilità sia in termini di completezza semantica che di fedeltà al vincolo.\n\nPoiché i grandi modelli linguistici sono costosi da implementare, è essenziale abilitare la pianificabilità linguistica di modelli più piccoli e specializzati. La creazione di un dataset è un passaggio essenziale per il suo fine. Tuttavia, gli studi precedenti non abilitano la pianificazione di obiettivi specifici e l'annotazione manuale del dataset è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare un dataset di pianificazione linguistica vincolata da modelli linguistici leggeri. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata chiamato CodeScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di convalida e test, chiediamo ai lavoratori di crowd-sourcing di trovare e revisionare i redditi nei campioni errati. Questa figura mostra la distribuzione dei vincoli di CodeScript. Scopriamo che CodeScript mostra un alto pluralismo negli obiettivi specifici generati. Con CodeScript, possiamo trattare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che T5 raffinato sul punteggio può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono supportare i modelli più grandi quando addestrati correttamente su siti di dati adatti.\n\nIn sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata, valutato la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppato un metodo di filtro over-generato per i modelli linguistici leggeri. Utilizziamo i grandi modelli linguistici per generare un dataset di alta qualità, CodeScript, per la pianificazione linguistica vincolata. Ci auguriamo che il dataset Code possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Per maggiori dettagli su CodeScript, si prega di consultare il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Jannislavak e vi presenterò i nostri lavori su Dr. Bert, un robusto modello pre-addestrato in francese per il dominio biomedico e clinico. In questa presentazione parleremo innanzitutto di modellazione del linguaggio nel settore sanitario. Successivamente, presenteremo il principale contributo del nostro articolo. Introdurremo il primo modello biomedico in francese, denominato Dr. Bert, basato su Roberta e addestrato su Nachos, un dataset di dati medici estratti dal web. Presenteremo anche un confronto del modello con diverse configurazioni e fonti di dati criogenici. Quindi, mostreremo i nostri risultati su 11 compiti a valle biomedici e clinici in francese e, infine, concluderemo con i dettagli sugli esperimenti e su come accedere ai modelli.\n\nDal 2018, quando è stato rilasciato Bert, esso è diventato uno dei metodi più efficaci per risolvere i compiti di elaborazione del linguaggio naturale, offrendo un notevole miglioramento delle prestazioni rispetto ai metodi storici statici e contestualizzati come Word2Vec, FastText e altri. Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert, e a domini diversi, come il biomedico con BioBERT e il clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati in-domain. Tuttavia, fino ad ora, non esisteva alcun modello open-source biomedico in francese.\n\nCi siamo quindi chiesti quali siano le fonti di dati più appropriate per un'ampia gamma di utilizzi e se i dati grezzi possano essere una buona sostituzione per i dati clinici. Per rispondere a questa domanda, abbiamo confrontato Dr. Bert con il nostro modello Schubert, basato su dati anonimizzati ottenuti dall'ospedale non universitario della nostra città. Un'altra domanda che ci siamo posti è quanta quantità di dati sia necessaria per addestrare un modello specializzato sui dati in francese: 4 gigabyte, 8 gigabyte o più?\n\nPer rispondere, abbiamo addestrato e confrontato quattro modelli da zero: una prima versione di Dr. Bert con 7 gigabyte di Nachos, una seconda versione con 4 gigabyte di Nachos, una prima versione di Schubert, un modello clinico con 4 gigabyte di frasi estratte da note cliniche, e una versione finale di Schubert con un mix di 4 gigabyte di Nachos e 4 gigabyte di note cliniche. Inoltre, abbiamo introdotto tre modelli addestrati con pre-addestramento controllato per analizzare l'impatto della strategia di pre-addestramento: uno basato sui pesi di Camembert e addestrato su 4 gigabyte di Nachos, un altro sempre basato su Camembert ma addestrato su 4 gigabyte di dati clinici, e infine uno basato sul modello biomedico inglese BioBERT addestrato su 4 gigabyte di Nachos.\n\nIn totale, abbiamo sette modelli da valutare. Li abbiamo testati su compiti a valle pubblici e privati come il riconoscimento di entità denominate, la classificazione, l'etichettatura delle parti del discorso e il question answering. Questi modelli sono stati confrontati con sei modelli di base: Camembert, Oscar 138 gigabyte, Oscar 4 gigabyte, Camembert CC Net 4 gigabyte, Plummit Bird, BioBERT e Clinical Bird.\n\nL'analisi dei risultati evidenzia che il modello ottiene le migliori prestazioni sui compiti con dati della stessa natura su cui è stato addestrato. Tuttavia, possiamo osservare che i dati da fonti eterogenee sembrano essere più versatili. Inoltre, l'utilizzo di più dati si traduce in migliori prestazioni. In generale, l'addestramento da zero sembra ottenere prestazioni più elevate nella maggior parte dei compiti. Tuttavia, i nostri esperimenti sul pre-addestramento controllato, utilizzando i pesi e il tokenizzatore di BioBERT addestrato sul sottoinsieme di 4 gigabyte di Nachos, mostrano risultati comparabili a quelli ottenuti con Dr. Bert 4 gigabyte addestrato da zero. Questo non è il caso per il modello basato sui pesi e il tokenizzatore di Camembert, che presenta problemi di stabilità.\n\nIn conclusione, il nostro sistema ottimizzato offre migliori prestazioni in 9 dei 11 compiti a valle e supera globalmente i risultati del modello generico, in questo caso Camembert. Osserviamo anche che i dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da Nachos sono disponibili gratuitamente sulla nostra interfaccia e tutti gli script di addestramento sono sul nostro repository GitHub. Grazie per l'attenzione e ci vediamo alla sessione poster a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "- Salve, mi chiamo Xhang Bing, sono uno studente di dottorato presso l'Università di Washington. Oggi presenterò il nostro lavoro, che va dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando i sentieri degli pregiudizi politici che portano a modelli NLP ingiusti. Quindi, i modelli linguistici vengono addestrati su dati di web crawling su larga scala. I media delle notizie politiche sono ben coperti nei loro dati di pre-addestramento. Secondo un'indagine sul corpus c4, possiamo vedere che il New York Times, il Los Angeles Times, il Guardian, l'Huffington Post, ecc., sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici: da un lato, sono stati in grado di imparare da diverse prospettive, celebrando la democrazia e la pluralità delle idee; dall'altro, queste diverse opinioni politiche sono intrinsecamente socialmente pregiudizievole e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle.\n\nA tale scopo, proponiamo di indagare la pipeline di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, ponendoci specificamente le seguenti domande: innanzitutto, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbe avere il dato di crawling su tali pregiudizi politici? In secondo luogo, come si comportano effettivamente i modelli linguistici con diversi orientamenti politici nei compiti a valle e se ciò potrebbe risultare in problemi di equità nelle applicazioni NLP.\n\nIn particolare, proponiamo innanzitutto di sollecitare i modelli linguistici con diversi formati di sollecitazione utilizzando i questionari politici, come il test della bussola politica. Questo ci assicura di poter effettuare una valutazione automatica ben fondata nella letteratura di scienze politiche. Alcuni risultati preliminari dimostrano che, primo, i modelli linguistici hanno diversi orientamenti politici; occupano tutti e quattro i quadranti sulla bussola politica. Possiamo anche vedere che GPT4 è il modello linguistico più liberale tra tutti e che la serie GPT è generalmente più socialmente liberale della serie BERT e delle sue varianti.\n\nIn secondo luogo, miriamo a indagare in che misura i pregiudizi politici dei modelli linguistici vengono effettivamente acquisiti dai dati di addestramento. Potremmo condurre un esperimento controllato addestrando ulteriormente i punti di controllo del modello linguistico su sei diversi corpora partigiani, separati in notizie e social media, ulteriormente divisi in base al loro orientamento politico. Addestrando ulteriormente i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per RoBERTa, ulteriormente addestrata sul corpus Reddit di sinistra, possiamo vedere uno spostamento liberale sostanziale in termini di pregiudizi politici.\n\nCerchiamo anche di indagare se i modelli linguistici possono acquisire la polarizzazione prevalente nella nostra società moderna. Dividiamo i corpora di pre-addestramento in pre-45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti. Addestriamo separatamente i modelli linguistici sui due diversi corpora temporali. Possiamo vedere che i modelli linguistici avevano generalmente un orientamento politico più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche acquisire la polarizzazione nella nostra società.\n\nInfine, valutiamo i modelli linguistici con diversi orientamenti politici nel rilevamento del discorso d'odio e nel rilevamento delle notizie false, applicazioni NLP che spesso coinvolgono i modelli linguistici e che potrebbero avere implicazioni molto significative. Vediamo che, se indagiamo sulle prestazioni per categoria, ovvero se separiamo le prestazioni in diverse demografie o media politici, possiamo notare un modello secondo cui, ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di sinistra sono migliori nel rilevare il discorso d'odio che prende di mira i gruppi socialmente minoritari, ma peggiore nel rilevare il discorso d'odio che prende di mira i gruppi più potenti della nostra società. Viceversa, i modelli linguistici di destra sono migliori nel rilevare il discorso d'odio che prende di mira i bianchi e gli uomini, ma peggiore nel rilevare il discorso d'odio che prende di mira i neri, la comunità LGBTQ+ e altre comunità minoritarie.\n\nTendenza simile si verifica anche nel rilevamento delle notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare le disinformazioni dal loro opposto orientamento politico e viceversa. Forniamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diversi significati politici danno diverse previsioni sul discorso d'odio e sugli esempi di disinformazione in base alle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente che questo indica un problema di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici.\n\nAd esempio, se un modello linguistico di destra venisse raffinato sul discorso d'odio o sulla disinformazione o qualsiasi altra cosa e distribuito su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e che il discorso d'odio che prende di mira i gruppi minoritari potrebbe diffondersi senza controllo. Quindi, questo ha suonato l'allarme per riconoscerlo e affrontare i problemi di equità derivanti dagli orientamenti politici dei modelli linguistici.\n\nPer concludere, vorremmo evidenziare il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici, una sorta di scelta tra Scilla e Cariddi. Se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherebbe dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando infine problemi di equità. Se provassimo in qualche modo a sanare, rischieremmo anche la censura o l'esclusione ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e debba essere trattenuto nei dati di addestramento del linguaggio. È un po' come il problema del troll elettrico.\n\nGrazie per la vostra attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Sono Koov Sinna e sono lieto di darvi il benvenuto alla presentazione del nostro articolo ACL 23. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Waqui, Aaron Mueller, Kanishka Mishra, Karen Fs, Roger Levy e Atina Williams. In questo studio rivalutiamo il paradigma dei coppie minime. Il paradigma delle coppie minime valuta essenzialmente i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità come BLIMP, sintassi Gym o accettabilità in termini di stereotipi come le coppie di folla. In questo paradigma, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticale e poi una frase inaccettabile o ungrammaticale, con la speranza che il modello assegni una probabilità maggiore alla frase accettabile. L'attuale pipeline MPP non ci consente di valutare l'accettazione dei modelli verso frasi più lunghe. Al giorno d'oggi, i grandi modelli linguistici stanno sviluppando finestre di contesto sempre più lunghe, quindi è cruciale valutare l'accettabilità dei modelli per l'intera finestra di contesto, ed è ciò che stiamo cercando di fare qui. Stiamo cercando di rivalutare la pipeline MPV chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe.\n\nPer simulare queste sequenze più lunghe, rivalutiamo i set di dati stessi e ricreiamo frasi scegliendo frasi accettabili o inaccettabili da questi set di dati. Ad esempio, qui abbiamo scelto una coppia tipica di frasi grammaticali dal set di dati BLIMP, dal caso dell'isola adjunct. Per ricreare sequenze più lunghe, estraiamo frasi grammaticali dall'isola adjunct e le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile. Possiamo fare lo stesso scegliendo frasi inaccettabili dallo stesso set di dati. Inoltre, possiamo scegliere frasi da un sottoinsieme diverso o da un set di dati diverso, creando così uno scenario di non corrispondenza. In questo caso, le frasi provengono ancora da set di dati rilevanti, ma non dallo stesso set di dati che stiamo valutando. Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia, per capire se i giudizi di accettabilità dei modelli sono influenzati dal contesto, sia che provenga da un sottoinsieme diverso dei dati o sia completamente irrilevante per la frase in esame.\n\nPer quanto riguarda le prestazioni del modello, prima abbiamo esaminato le frasi di Wikipedia, completamente irrilevanti per la coppia di query corrente, e abbiamo riscontrato che i giudizi MPP sono per lo più robusti per qualsiasi lunghezza del contesto. Abbiamo aumentato la lunghezza del contesto fino a 1024 per i modelli Ot e GPT-2, e come mostra la linea puntinata arancione, i giudizi MPP sono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso set di dati? Creiamo frasi da domini accettabili e inaccettabili dallo stesso set di dati BLIMP Syntax Gym, e osserviamo che i giudizi MPP aumentano o diminuiscono significativamente quando aggiungiamo prefissi accettabili o inaccettabili. Tuttavia, quando corrispondiamo la struttura, scegliendo frasi dallo stesso fenomeno in BLIMP Syntax Gym, assistiamo a un aumento o a una diminuzione massiccia dei giudizi MPP del modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Questo effetto aumenta con la lunghezza del contesto e potrebbe influenzare i nuovi modelli linguistici con grandi finestre di contesto.\n\nPerché il prefisso corrispondente influenza così tanto il giudizio del modello linguistico? Abbiamo condotto una serie di analisi, perturbando la frase di input cercando di preservare la struttura rilevante ma aggiungendo rumore. Dopo diverse perturbazioni, abbiamo scoperto che nessuno di questi rumori fa cambiare significativamente il modello in termini di tendenza dei giudizi MPP. Abbiamo riscontrato che i modelli sono sensibili alle frasi perturbate in modi simili: quando perturbiamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni, e quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP nello stesso modo.\n\nLe principali conclusioni del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti condivise tra le frasi, e che la valutazione MPP, come la eseguiamo attualmente con input di frasi brevi e singole, potrebbe non catturare completamente la conoscenza astratta del modello linguistico per l'intera finestra di contesto. Per maggiori dettagli sui nostri esperimenti, vi invitiamo a leggere il nostro articolo. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Dawei, sono un dottorando presso l'Università di Staland in Germania. In questo video vorrei presentarvi il nostro recente lavoro, \"Più debole di quanto pensiate\", un'analisi critica dell'apprendimento con supervisione debole. Si tratta di un progetto realizzato in collaborazione con X, Myos Mosbach, Ge Steffen e Dirich Klako. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento con supervisione debole. Nella supervisione debole non etichettiamo manualmente i dati. Invece, utilizziamo fonti di etichettatura debole come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa percentuale di annotazioni è errata. Se addestriamo direttamente reti neurali su dati con etichette deboli, queste tendono a memorizzare il rumore delle etichette e non generalizzano. Pertanto, nell'apprendimento con supervisione debole, sono stati proposti algoritmi di addestramento per addestrare robustamente le reti neurali in presenza di tale rumore nelle etichette, in modo che i modelli addestrati generalizzino comunque bene.\n\nIn recenti lavori nell'ambito dell'apprendimento con supervisione debole (wSL), un'affermazione comune è che è possibile addestrare modelli solo su dati con etichette deboli e ottenere elevate prestazioni su set di test puliti. Tecnicamente, questa affermazione non è errata, ma c'è un inghippo: si presume spesso che esista un set di validazione pulito aggiuntivo per la selezione del modello. In questo scenario, ci siamo concentrati su questo problema, ma ciò implica che siano necessarie annotazioni manuali aggiuntive nell'apprendimento con supervisione debole, un aspetto spesso trascurato.\n\nPer affrontare questa questione, ci siamo posti tre domande di ricerca:\n\n1. I dati di validazione puliti sono necessari per l'apprendimento con supervisione debole o è possibile utilizzare un set di validazione con etichette deboli?\n2. Se i dati puliti sono richiesti, quanti campioni puliti sono necessari per l'apprendimento con supervisione debole?\n3. Dovremmo utilizzare solo i campioni puliti per la validazione o esistono modi migliori per sfruttarli?\n\nAbbiamo affrontato queste domande di ricerca nel nostro lavoro e le nostre scoperte sono le seguenti:\n\nIn primo luogo, abbiamo scoperto che, interessantemente, i recenti metodi wSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. In caso contrario, si verifica un notevole calo delle prestazioni, come mostrato in questa figura. Se non ci sono campioni di validazione puliti, i modelli non riescono a generalizzare oltre le originali etichette deboli, rendendo l'addestramento inutile. Ciò indica che gli approcci wSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere sottovalutato.\n\nLa nostra seconda scoperta è che aumentare il numero di campioni di validazione puliti aiuta gli approcci wSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, sono necessari solo 20 campioni per classe per ottenere elevate prestazioni. Ma non è tutto, perché se decidiamo di accedere a campioni puliti, l'addestramento diretto su di essi porterà addirittura a prestazioni migliori. La figura rossa mostra la differenza di prestazioni tra gli approcci di fine-tuning, applicati direttamente su dati puliti, e gli approcci wSL, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo dieci campioni per classe, il fine-tuning diretto inizia a superare gli approcci wSL.\n\nInfine, il miglioramento delle prestazioni rivendicato dai precedenti approcci wSL può essere facilmente ottenuto consentendo il proseguimento del fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello denominato FTW inizialmente ha prestazioni inferiori rispetto a metodi wSL più complessi come cosine. Tuttavia, se consentiamo il proseguimento del fine-tuning sui campioni puliti, FTW ottiene prestazioni equivalenti ad altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi wSL più complessi che richiedono più tempo di calcolo e spazio su disco.\n\nIn sintesi, abbiamo dimostrato che i recenti approcci wSL richiedono campioni manualmente annotati e puliti per funzionare correttamente e che il loro miglioramento delle prestazioni e la loro praticità sono fortemente sovrastimati.\n\nLe nostre concrete raccomandazioni per i futuri lavori sono le seguenti:\n\n1. Riportare i criteri di selezione del modello, ad esempio specificare se la selezione del modello è stata eseguita utilizzando campioni di validazione puliti.\n2. Gli approcci wSL dovrebbero essere confrontati con baselines semplici e diretti su campioni concreti.\n3. Il fine-tuning continuo è una baseline semplice ma potente che dovrebbe essere presa in considerazione nei futuri lavori sull'apprendimento con supervisione debole.\n4. Abbiamo reso open-source il nostro codice. Potete trovarlo tramite il codice QR presente in questa diapositiva. Sentitevi liberi di dargli un'occhiata.\n\nGrazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Al Villaard e fornirò una breve panoramica del documento che introduce P, un modello linguistico da 540 miliardi di parametri, presentato lo scorso anno nel 2022. Si tratta di un lavoro congiunto con i miei colleghi di Google Translate. P è addestrato su una vasta raccolta di testi comprendente 780 miliardi di token e, al momento della pubblicazione, ha raggiunto risultati all'avanguardia in centinaia di compiti di elaborazione del linguaggio naturale (NLP).\n\nIn questo studio presentiamo un'analisi sistematica dell'uso di prompt per grandi modelli linguistici nella traduzione automatica. Valutiamo la capacità traduttiva di tali modelli utilizzando le migliori pratiche della comunità IMT, il che include l'impiego dei più recenti set di test per evitare sovrapposizioni tra i dati di test e quelli di addestramento del modello linguistico. Inoltre, confrontiamo due sistemi all'avanguardia, ovvero i migliori sistemi della valutazione WMT. Utilizziamo metriche neurali all'avanguardia e, inoltre, presentiamo anche i risultati della valutazione umana basata su esperti.\n\nInfine, forniamo alcune raccomandazioni sulle strategie di selezione dei prompt. L'uso di prompt ha una grande influenza sulle prestazioni dei modelli linguistici per la traduzione, come possiamo osservare in un semplice esperimento in cui utilizziamo un prompt breve e forniamo due prompt diversi per frasi diverse. Nella maggioranza dei casi, 516 su 1000, la differenza osservata è di oltre un punto BLEU, e in casi estremi può arrivare fino a 40 punti BLEU. Quindi, è importante scegliere una buona strategia di prompt.\n\nNelle nostre sperimentazioni, abbiamo optato per una strategia di prompt a cinque riprese (five-shot prompting), segnando le frasi che forniamo al sistema nella lingua di origine. In questo esempio, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche (sorgente) sono contrassegnate con due punti tedeschi e le traduzioni inglesi con due punti inglesi. Abbiamo riscontrato che la forma effettiva del prompt non ha una grande influenza nel caso di prompt brevi; è cruciale per prompt a zero e una ripresa, ma quando si passa a prompt più lunghi, come nel nostro caso, la differenza è minima. Sono gli esempi a portare il peso maggiore.\n\nIn sintesi, i risultati sperimentali indicano che la qualità degli esempi è più importante della somiglianza con la frase sorgente. Pertanto, è importante selezionare esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo (dev data). I dati di sviluppo sono più curati e di qualità superiore rispetto ai dati di addestramento, e i risultati mostrano prestazioni migliori utilizzando i dati di sviluppo.\n\nTuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale sulle traduzioni di P, ma P si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di confrontarci con Google Translate. Le intuizioni ottenute dalla valutazione eseguita utilizzando il framework NpN mostrano che la fluidità di P è paragonabile a quella dei sistemi all'avanguardia, ma la principale differenza risiede nell'accuratezza. In particolare, gli errori più comuni sono errori di omissione. Sembra che P scelga di produrre una traduzione che suoni meglio, talvolta omettendo parti della frase sorgente che non sono essenziali nella traduzione. Tuttavia, la categoria di stile per P è inferiore rispetto ai sistemi all'avanguardia, il che indica che P produce output fluente ma con problemi di accuratezza.\n\nQuesta è una panoramica davvero breve. Per maggiori dettagli, vi invito a consultare la presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti. Mi chiamo Jin Wei Y dell'Università della Scienza e della Tecnologia della Cina. È un piacere per me presentare un breve video promozionale del nostro articolo. State copiando il mio modello, \"Proteggere il copyright dei grandi modelli linguistici per l'embedding e i servizi\"? Riprendiamo il watermark. Iniziamo introducendo il contesto sugli embedding e i servizi. Attualmente, i grandi modelli linguistici come Gbt, La, PLm eccellono nella comprensione e generazione del linguaggio naturale. Gli embedding e i servizi sono uno dei servizi basati sui grandi modelli linguistici per assistere vari compiti di NLP. Ad esempio, OpenI offre un'API di embedding basata su Gbt. Tuttavia, recenti lavori hanno dimostrato che un attaccante potrebbe rubare il modello attraverso l'apprendimento dagli embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright degli embedding come servizi. Una delle soluzioni è inserire un watermark nel servizio del fornitore e verificare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà:\n\n1. Il metodo deve essere applicabile agli embedding come servizi.\n2. Il watermark non deve degradare l'utilità dell'embedding fornito.\n3. Il watermark deve essere sufficientemente robusto per l'attaccante, o l'attaccante non deve poter rimuovere facilmente il watermark.\n4. Il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.\n\nI lavori esistenti possono essere classificati in quattro categorie principali. Tuttavia, questi metodi non sono applicabili agli embedding come servizi o mancano di trasferibilità. In questo articolo, proponiamo Embedding Marker, un metodo di watermark basato su backdoor applicabile agli embedding come servizi.\n\nOra, introduciamo i dettagli del nostro Embedding Marker. Embedding Marker comprende due passaggi principali: iniezione del watermark e verifica del copyright. Prima di questi passaggi principali, selezioniamo un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole.\n\nNell'iniezione del watermark, definiamo prima un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target.\n\nLa verifica del copyright rileva se il modello dietro un altro servizio contiene il watermark. Costruiamo prima un dataset di backdoor e un dataset benigno. Il dataset di backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger. Quindi, il fornitore richiede embedding dal servizio dell'attaccante con il dataset. Calcoliamo la similarità cosine e L2 tra l'embedding richiesto e l'embedding target. Calcoliamo la differenza di similarità tra il dataset benigno e quello di backdoor, definita come Delta cosine e Delta L2. Applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica.\n\nAbbiamo condotto esperimenti su quattro dataset: Aaging News, Mind SD2 e A Spam. Supponiamo che il fornitore utilizzi il dataset Wiki Text per contare la frequenza delle parole. I risultati sui quattro dataset mostrano che il nostro Embedding Marker può avere un'ottima prestazione di rilevamento mantenendo un'elevata utilità per i compiti a valle. Abbiamo anche validato la robustezza dell'embedding fornito visualizzando gli embedding delle frasi sui quattro dataset utilizzando PCA. La legenda delle figure indica il numero di trigger in ogni frase, come mostrato nelle figure. È difficile distinguere tra gli embedding fattori e gli embedding normali.\n\nÈ tutto. Grazie. Venite a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti. Mi chiamo Ian e io e la mia collega Jiian presenteremo la nostra ricerca sul miglioramento dell'apprendimento seriale multimodale tramite il tuning delle istruzioni. Quindi, con i progressi nei modelli di linguaggio di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento utilizzando modelli di linguaggio pre-addestrati per diversi compiti a valle in modo efficiente sia in termini di parametri che di dati. Recentemente, molti studi hanno dimostrato che il tuning delle istruzioni consente ai modelli di linguaggio di grandi dimensioni di eseguire compiti non visti in precedenza in modo seriale seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sul tuning delle istruzioni si concentra sul miglioramento delle prestazioni seriali su compiti solo linguistici, lasciando da parte la visione artificiale e i compiti multimodali. In questo lavoro, quindi, vogliamo indagare se il tuning delle istruzioni su modelli multimodali può effettivamente migliorare la generalizzazione a compiti multimodali non visti. Inoltre, durante la nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di set di dati di istruzioni tra quelli linguistici e multimodali; esistono più di 1600 compiti di istruzioni solo linguistiche, ma non esiste un ampio set di dati di istruzioni multimodali pubblicamente disponibile. Ciò ci ha spinto a costruire un set di dati di tuning delle istruzioni multimodali. Qui presentiamo Multi-Ins, il primo benchmark di tuning delle istruzioni multimodali, che consiste in 62 compiti multimodali diversi che coprono 10 categorie generali. Questi compiti sono derivati da 21 set di dati open source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti.\n\nPer indagare il tuning delle istruzioni multimodali sul nostro set di dati proposto, abbiamo adottato un modello di addestramento multimodale unificato come modello di base, che utilizza un vocabolario unificato per i token linguistici, le immagini e le coordinate delle caselle delimitatrici. Qui mostriamo alcuni esempi dal nostro set di dati Multi-Ins. Per unificare l'elaborazione di vari tipi di dati di input e output, seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e le caselle delimitatrici sono rappresentati nello stesso spazio dei token.\n\nOra parlerò del tuning delle istruzioni multimodali. Per il set di dati di addestramento, utilizziamo 53 compiti dal gruppo N per l'addestramento e campioniamo 10.000 istanze per compito per il test. Riserviamo l'intero gruppo di lettura del senso comune per il test e selezioniamo altri cinque compiti dai gruppi Wiki e Miscellaneo. Utilizziamo tutte le istanze nel set di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dal set di test delle istruzioni naturali come compito NP. Utilizziamo un modello di grandi dimensioni pre-addestrato come modello di base durante l'addestramento; mescoliamo tutte le istanze per tutti i compiti e ogni istanza è combinata casualmente con una delle sue cinque template di istruzioni. Durante il test, per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento. Riportiamo la media e la massima prestazione e la deviazione standard della prestazione tra tutti e cinque gli esperimenti. Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza; se è un compito di generazione multimodale, riportiamo la radice quadrata di LP. Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensibilità, che misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito, indipendentemente da lievi variazioni nella formulazione dell'istruzione.\n\nEcco il nostro risultato principale. Come possiamo vedere, il tuning delle istruzioni può migliorare significativamente le prestazioni di OFA su diversi compiti multimodali. Inoltre, il transfer learning dal set di dati delle istruzioni naturali può beneficiare il tuning delle istruzioni. Qui possiamo vedere che, all'aumentare del numero di compiti, il modello ottiene prestazioni migliori e, allo stesso tempo, una minore sensibilità. Abbiamo anche condotto un esperimento utilizzando una singola istruzione rispetto a cinque istruzioni; come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità. Ciò mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Come possiamo vedere, il transfer learning dai set di dati delle istruzioni naturali consente al modello di raggiungere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che il transfer learning dai set di dati delle istruzioni naturali può aiutare OFA a raggiungere prestazioni molto migliori sul set di dati Nitrogen Instruct.\n\nIn sintesi, abbiamo proposto il primo ampio set di dati di tuning delle istruzioni multimodali, migliorato significativamente le capacità neurali di OFA ed esplorato diverse tecniche di transfer learning, dimostrando i loro benefici. Abbiamo anche progettato una nuova metrica chiamata sensibilità. Inoltre, stiamo raccogliendo un set di dati di tuning delle istruzioni multimodali molto più ampio, con circa 150 compiti aggiuntivi in varianti linguistiche, che rilasceremo. Questo è il codice QR per i nostri dati e il modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti. Mi chiamo Just John dell'Università di Penn State. Oggi presenterò il nostro lavoro, ExAmpler: Parsing Semantico Cross-lingue in più Lingue Naturali e Rappresentazioni Manuali. Quindi, il parsing semantico è un'attività volta a costruire rappresentazioni semantiche delle query degli utenti, come SQL e Lambda calcolo. E il parsing semantico cross-lingue è il compito di tradurre le query in più lingue naturali in diverse rappresentazioni del significato, come mostrato in questa figura. Dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, ecc. I modelli di parsing semantico cross-lingue esistenti sono stati proposti e valutati separatamente su set di dati limitati e applicazioni; per esempio, mancano la copertura su alcune lingue naturali, come il cinese, e su alcune rappresentazioni minime, come il Lambda calcolo, o sono valutati solo su un singolo modello neurale.\n\nA tal fine, proponiamo ExAmpler, che fornisce un set di dati uniforme per il parsing semantico cross-lingue in più lingue naturali e rappresentazioni del significato. Contiene nove set di dati in vari domini, cinque tassonomie di parsing semantico, 8 milioni di rappresentazioni e 22 lingue naturali in 15 famiglie linguistiche.\n\nPer valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è la traduzione di test. Utilizziamo l'API di Google Translate per tradurre la sorgente nella lingua di destinazione, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione. Per esempio, addestriamo il modello inglese sulle query in inglese e durante l'inferenza traduciamo la query tedesca utilizzando l'API in inglese, quindi utilizziamo il modello addestrato per prevedere l'SQL. Valutiamo anche il modello monolingue. In questa impostazione, la lingua di origine è la stessa della lingua di destinazione, per esempio tedesco-tedesco o inglese-inglese.\n\nTestiamo anche l'impostazione di fusione monolingue addestrando modelli monolingue con solo il 10% dei dati di addestramento e testiamo un modello multilingue, che addestriamo su tutte le lingue, per esempio mettendo insieme le query in tedesco, inglese e cinese per addestrare un modello multilingue. Durante l'inferenza, possiamo utilizzare questo modello per tradurre le query in tedesco o cinese, ecc. Consideriamo anche il trasferimento cross-lingue zero-shot e few-shot. Addestriamo su una lingua di origine e trasferiamo ad un'altra lingua. Quindi, durante l'addestramento, addestriamo su query in inglese o sulla combinazione di inglese e tedesco few-shot per addestrare un modello multilingue e prevedere l'output SQL.\n\nAbbiamo anche trovato molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingue, valutiamo due gruppi di modelli, inclusi encoder PDR (encoder multilingue pre-addestrati con decodificatori basati su puntatore) come XLr plus PDdR e BERT plus PDdR, e valutiamo anche i modelli encoder-decoder, che sono modelli encoder-decoder multilingue pre-addestrati, come MBt e Mt5. Abbiamo riscontrato che l'encoder-decoder ottiene le migliori prestazioni su tutti e nove i set di dati.\n\nValutando MT5 e XLMR plus PDR in un'impostazione multilingue, abbiamo scoperto che l'encoder-decoder o l'encoder PDR possono essere migliorati addestrandoli in un mix di varie lingue. Abbiamo scoperto che ciò è dovuto al fatto che la maggior parte delle principali lingue naturali ottiene un miglioramento delle prestazioni, ad eccezione dell'inglese, le cui prestazioni diminuiscono in sette set di dati e aumentano solo in tre. Questo è noto come curva della multilingue.\n\nAbbiamo anche confrontato il divario di prestazioni cross-lingue. In questa figura, la linea blu rappresenta il trasferimento cross-lingue few-shot, la linea arancione il trasferimento cross-lingue zero-shot, mentre la linea verde rappresenta l'impostazione monolingue. Confrontando la linea verde e arancione, abbiamo scoperto che nel setting zero-shot, il divario di trasferimento cross-lingue è significativo. Confrontando la linea blu e arancione, abbiamo scoperto che nel setting few-shot, il divario di trasferimento si riduce rapidamente.\n\nAbbiamo trovato anche altre interessanti scoperte. Per esempio, l'encoder-decoder supera i lavori precedenti o ottiene risultati comparabili. L'addestramento del modello in inglese può migliorare significativamente le prestazioni few-shot nelle lingue naturali di destinazione. Abbiamo anche scoperto che i modelli di linguaggio multilingue come Coders e BERT sono ancora inadeguati per le classi di parsing semantico cross-lingue.\n\nIn sintesi, abbiamo costruito ExAmpler, un benchmark unificato per il parsing semantico cross-lingue con più lingue naturali e rappresentazioni del significato. Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli di linguaggio multilingue e i nostri risultati mostrano molte scoperte interessanti, ecc. Vi invitiamo a visitare il nostro articolo e il codice. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Adam Shrikkovski e questo intervento riguarda la struttura di dipendenza della coordinazione. Come sapete, esistono diverse strutture di dipendenza ipotizzate da diverse teorie e approcci basati su corpus. Ad esempio, nelle dipendenze universali, la struttura della coordinazione \"Lisa, Bart e Maggie\" è tale che il primo congiunto è il capo dell'intera struttura coordinata; in questo caso, \"Lisa\" è simile. Un approccio analogo è assunto nella teoria del significato del testo di Igor Mel'čuk, dove ancora una volta l'intera struttura coordinata è guidata dal primo congiunto. Questi due approcci sono asimmetrici, in quanto individuano uno dei congiunti.\n\nEsistono anche approcci simmetrici alle strutture coordinate, come l'approccio PRAG, l'approccio guidato dalla congiunzione assunto negli alberi di dipendenza PRAG, dove le strutture coordinate sono guidate dalla congiunzione, ottenendo dipendenze da un capo a tutti i congiunti. Infine, esiste anche un approccio multi-capo utilizzato, ad esempio, nella grammatica delle parole di Cutson, dove, per così dire, tutti i congiunti sono capi della struttura coordinata, ottenendo dipendenze dal governatore a ciascuno dei congiunti separatamente.\n\nL'obiettivo di questo articolo è produrre un nuovo argomento a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due. L'argomento si basa sul principio della minimizzazione della lunghezza delle dipendenze, che spiegherò sulla base di questi esempi.\n\nIn inglese, come sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli complementi possono essere più distanti. Ad esempio, \"March l'ha letto ieri\" è corretto perché l'oggetto diretto è vicino al verbo, mentre \"March l'ha letto ieri\" è peggiore perché qui tra il verbo e l'oggetto diretto c'è un complemento (\"ieri\"). Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e lungo, perché può essere spostato nella posizione dopo il complemento.\n\nIl ragionamento qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio della minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più corte sono preferite. Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Qui abbiamo la dipendenza da \"red\" al complemento di lunghezza sette, misurata in parole, e da \"red\" a \"book\" di lunghezza quattro, per un totale di 11. Quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, molto più corta.\n\nQuello che abbiamo fatto è stato estrarre varie statistiche sulla coordinazione dalla versione potenziata del corpus PENTRY e spiegare nel documento perché non abbiamo utilizzato le dipendenze universali. Queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendono ad essere più corti, come in \"sale e pepe\" e non \"pepe e sale\", misurati in sillabe. Inoltre, è stata confermata l'osservazione che questa tendenza aumenta con la differenza di lunghezza. Quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo.\n\nQuello che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Ad esempio, in \"Ho visto Bar e Lisa\", il governatore è a sinistra. In \"Homer è arrivato e ha starnutito\", abbiamo la coordinazione di due verbi senza un governatore esterno. In questi casi, il congiunto sinistro preferisce essere più corto, soprattutto quando la differenza tra i due congiunti è maggiore. Tuttavia, quando il governatore è a destra, come in \"La sinistra governa la coordinazione della coda e della rete\", questo effetto scompare.\n\nMostriamo questo misurando la lunghezza in caratteri (prima colonna), sillabe (colonna centrale) e parole (colonna di destra). Concentrandoci su quella di destra, vediamo che quando il governatore è a sinistra, la tendenza del congiunto sinistro ad essere più corto aumenta costantemente con la differenza assoluta in parole. La stessa cosa si osserva quando non c'è governatore, come nella coordinazione di frasi. Ma quando il governatore è a destra, questa tendenza scompare. Nel documento, spieghiamo come questo fornisca un argomento contro le strutture di coordinazione asimmetriche e a favore di quelle simmetriche.\n\nPer ulteriori dettagli e argomentazioni, si rimanda al documento. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Kyo Yin e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede il contesto: un'analisi multilingue basata sui dati?\". Questo lavoro è stato realizzato in collaborazione con Patrick Ferange, Emiliu, Andre F.D. Martins e Graham Newbiig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase: \"Le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono\", dove \"mole\" si riferisce a una spia, ma se la frase precedente fosse \"Potrebbe essere qualcosa di grave, dottore\", allora \"mole\" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e di conseguenza anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre casi del genere è piuttosto difficile, innanzitutto perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus, come BLEU, incapaci di catturare queste traduzioni. Alcune persone hanno suggerito valutazioni mirate sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla cura umana.\n\nIn questo lavoro, cerchiamo di rispondere a due domande. Prima: quando la traduzione richiede il contesto? E seconda: quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto un testo dipende dal contesto nella traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica, calcolando quanto informazione il contesto C fornisce sul bersaglio y dato il sorgente x. Si può pensare a CXMI come all'informazione ottenuta fornendo il contesto al modello. In questo lavoro, estendiamo CXMI a Point-CXMI, che può misurare l'utilizzo del contesto a livello di frase o di parola. Possiamo considerare le parole con un alto valore di Point-CXMI come quelle che richiedono il contesto per la traduzione.\n\nOra analizziamo le parole con un alto valore di Point-CXMI per cercare schemi tra di esse. Effettuiamo la nostra analisi su trascrizioni di TED Talks tradotte dall'inglese in 14 lingue diverse. Effettuiamo l'analisi a tre livelli diversi: prima esaminiamo le etichette delle parti del discorso con valori medi elevati di Point-CXMI, il che ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un valore relativamente alto di Point-CXMI. Ciò può essere spiegato dal fatto che l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale durante la traduzione in arabo. Allo stesso modo, scopriamo che alcune lingue richiedono il contesto anche per scegliere la forma verbale corretta.\n\nIn secondo luogo, esaminiamo le voci del vocabolario con un alto valore di Point-CXMI, mediato su tutte le sue diverse occorrenze. Ciò ci aiuta a identificare casi come questo, in cui in cinese è necessario il contesto per tradurre i nomi propri, assicurandosi di utilizzare la stessa traduzione all'interno del documento. Allo stesso modo, scopriamo che il contesto è necessario per tradurre con il giusto livello di formalità.\n\nInfine, esaminiamo i diversi token individuali con un alto valore di Point-CXMI, il che ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi.\n\nOra utilizziamo i risultati della nostra analisi per progettare un punto di riferimento per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso identificati, abbiamo creato tagger per identificare automaticamente le parole relative al fenomeno. Chiamiamo il nostro tagger \"Multilingual Discourse-Aware\" o MUDA tagger. Possiamo anche notare che le diverse lingue hanno proporzioni diverse di questi fenomeni del discorso. Quindi, applichiamo il tagger MUDA su un corpus parallelo che desideriamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto identificati dal tagger M.\n\nInfine, utilizziamo il nostro punto di riferimento, insieme ad altre metriche, per valutare diversi modelli di traduzione a livello di documento. Innanzitutto, quando utilizziamo metriche a livello di corpus, per BLEU scopriamo che i modelli ignari del contesto hanno le migliori prestazioni. Ma se usiamo Point-CXMI, i modelli consapevoli del contesto si comportano meglio. E se usiamo la misura WordF, i modelli con o senza contesto hanno prestazioni paragonabili. Questo dimostra nuovamente che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo metriche a livello di corpus.\n\nUtilizziamo ora il punto di riferimento M per valutare i modelli e scopriamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per alcuni fenomeni del discorso, come la formalità e la coesione lessicale. Tuttavia, questi modelli non sono molto migliori dei modelli senza contesto per altri fenomeni, come l'ellissi, i pronomi e la forma verbale. Questo suggerisce dove è necessario vedere più progressi per la traduzione a livello di documento.\n\nAbbiamo anche confrontato diversi sistemi commerciali e il nostro punto di riferimento mostra che DeepL è generalmente più accurato di Google Translate per la traduzione a livello di documento.\n\nIn sintesi, eseguiamo un'analisi basata sui dati su 14 coppie di lingue per identificare quando le traduzioni richiedono il contesto e poi utilizziamo i nostri risultati per costruire un punto di riferimento per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni del discorso i modelli possono gestire bene o meno e quali sistemi di traduzione sono bravi nella traduzione a livello di documento.\n\nGrazie mille per la vostra attenzione, ci vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa del primo anno di dottorato presso la Carnegie Mel University, e oggi presenterò il nostro lavoro sulla \"Posizionalità nell'AnL\", caratterizzando i pregiudizi di progettazione e i set beta dei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santi, Ronin Labrasse, Katharina Reinika e Martin Sapp.\n\nIniziamo immaginando di lavorare per un giornale e di dover setacciare i commenti sotto un articolo per rimuovere i contenuti tossici. Potremmo rivolgerci a un'API popolare come Perspective API per la rilevazione della tossicità, che funziona molto bene se siamo Carl Jones, poiché l'API riesce a identificare correttamente i casi tossici. Ma non è così per Didtha Sharma, dove Perspective API non è altrettanto sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di pregiudizio di progettazione, in cui si osservano differenze sistematiche nelle prestazioni tecnologiche tra diverse popolazioni.\n\nI pregiudizi di progettazione come quello descritto possono verificarsi a causa della posizione dei ricercatori di NLP e degli sviluppatori di modelli. La \"posizione\" si riferisce semplicemente alle prospettive che le persone hanno in base alla loro demografia, identità ed esperienze di vita. È un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer.\n\nIn quanto ricercatori, la posizione può influenzare il processo di ricerca e i suoi risultati, poiché può modificare le decisioni prese. Una domanda che ci si potrebbe porre è: i set di dati e i modelli hanno una posizione? Non stiamo affermando che i modelli e i set di dati stessi abbiano identità demografiche ed esperienze di vita, ma aggrega i giudizi e le opinioni di persone reali e può quindi rappresentare alcune posizioni più di altre.\n\nI lavori precedenti hanno suggerito alcune prove aneddotiche della posizione, come i divari culturali nei modelli e nei set di dati, nonché definizioni teoriche della posizione del modello. Tuttavia, questi studi non confrontano gli utenti finali con i set di dati e i modelli stessi. Lo studio della posizione di modelli e set di dati è sempre più importante man mano che i compiti di NLP diventano più soggettivi e orientati al sociale, ed è una sfida caratterizzare come queste posizioni sono distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API.\n\nPer studiare la posizione dei set di dati e dei modelli, confrontiamo effettivamente le annotazioni con gli utenti reali e i set di dati e modelli esistenti. Lo facciamo attraverso il nostro framework \"Posizionalità nell'AnL\", che funziona in due passi principali. Il primo passo è ri-annotare i set di dati con annotatori diversi, preferendo questo rispetto all'analisi delle demografie degli annotatori dei set di dati originali, poiché di solito solo pochi annotatori annotano ogni istanza e le demografie vengono raramente raccolte e condivise. Ri-annotiamo quindi i dati per ottenere molti annotatori per istanza e un ricco set di dati demografici.\n\nSuccessivamente, prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando il punteggio di correlazione di Pearson. Il nostro framework differisce quindi dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con le previsioni e le etichette dei modelli e dei set di dati, piuttosto che guardare semplicemente all'accordo tra annotatori o modellare le distribuzioni degli annotatori.\n\nIl nostro framework è in gran parte abilitato da \"Lab in the Wild\", una piattaforma di crowdsourcing online e un ex collaboratore HCI. \"Lab in the Wild\" è una piattaforma di sperimentazione online in cui possiamo reclutare volontari diversi, rispetto a piattaforme come Turk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India. \"Lab in the Wild\" è ancora in grado di ottenere dati di alta qualità.\n\nAbbiamo ospitato due compiti su \"Lab in the Wild\": uno è l'accettabilità sociale, che funziona nel seguente modo: i partecipanti leggono una situazione dal set di dati \"Social Chemistry\" e poi scrivono quanto sia socialmente accettabile la situazione. Per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'AI e con quelle degli altri. Abbiamo quindi confrontato queste annotazioni con \"Social Chemistry Delphi\" e \"gPT4\".\n\nAbbiamo replicato un setup molto simile per il compito di rilevamento della tossicità e dell'odio, in cui i partecipanti leggono un'istanza da \"Dynah Hate\" e scrivono se pensano che si tratti di un'istanza di discorso d'odio. Abbiamo quindi confrontato queste annotazioni con \"Dyna Hate Perspective API\", \"Rewire API\", \"Hate Roberta\" e \"GPT4\".\n\nIl nostro studio ha accumulato oltre 16.000 annotazioni da più di mille annotatori di 87 paesi. Siamo quindi meglio equipaggiati per rispondere alla domanda: con chi si allineano di più i set di dati e i modelli di NLP?\n\nScopriamo che c'è posizione nell'NLP. Ad esempio, troviamo che i set di dati e i modelli sono più allineati ai paesi anglofoni. Per l'analisi dell'accettabilità sociale di \"gpd four\", scopriamo che è più allineata ai paesi confuciani e anglofoni. Troviamo che anche \"Dinah Hate\" è più allineata ai paesi anglofoni. Scopriamo anche un allineamento aggiuntivo con le persone che hanno un'istruzione universitaria.\n\nPer \"gpd4\" nel compito di accettabilità sociale, troviamo che è più allineato a persone con un'istruzione universitaria o di scuola di specializzazione, e riscontriamo lo stesso per \"Danny Hate\", che è più allineato a persone con un'istruzione universitaria.\n\nTuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni vengono inevitabilmente lasciati indietro. Un esempio è che i set di dati e i modelli sono meno allineati alle persone non binarie rispetto ai loro omologhi maschi e femmine. Riscontriamo questo sia nell'analisi del compito di accettabilità sociale \"gPDd4\" che nell'analisi del compito \"dina hate\".\n\nDato che c'è una reale analità nell'NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni. La prima è tenere un registro di tutte le scelte di progettazione rilevanti durante il processo di ricerca. La seconda è fare ricerca NLP con l'ottica del prospettivismo. La nostra terza raccomandazione è costruire set di dati e modelli specializzati all'interno di comunità specifiche, e un buon esempio è l'iniziativa \"Masakanne\".\n\nVogliamo sottolineare che l'NLP inclusivo non significa semplicemente far funzionare tutte le tecnologie per tutti. E questo conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di consultare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Salve, e parlerò del nostro lavoro sulla risoluzione di espressioni differenziali indirette per la selezione di entità, in cui introduciamo il corpus di entità alternative. Mi chiamo Javad Hosseini e questo è un lavoro congiunto con Philip Radlinsky, Sylvia Parity e Annie Luis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando desiderano effettuare una scelta. E considero questa domanda alternativa: intendevi \"easy on me\" o \"I got a feeling\"? Qui un utente vuole selezionare una di queste due canzoni. La cosa più ovvia è utilizzare un riferimento diretto, ad esempio dicendo il nome della canzone \"Easy on Me\" o la sua posizione, la prima, ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Ciò può accadere quando l'utente non riesce a ricordare il nome della canzone o le pronunce sono troppo simili e difficili da disambiguare, o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di differenze indirette. Ad esempio, la più recente o il segno che non è energetico. Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking degli LLM nella comprensione delle entità. Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per questo compito, quindi ne abbiamo raccolto uno utilizzando l'annotazione di massa. Il nostro set di dati copre tre diversi domini: musica, libri e ricette. La metodologia di raccolta del set di dati enfatizza l'informalità utilizzando un'impostazione di completamento a fumetti. Il fumetto ha tre nuvolette: nella prima, Bob dice \"Ricordi quella canzone che stavamo ascoltando ieri\", impostando così il contesto del dialogo; nella seconda, Alice chiede \"Intendevi 'easy on me' o 'I got a feeling'?\", che è la domanda alternativa; nella terza, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio \"la più recente\". Forniamo automaticamente le prime due nuvolette, ma la terza viene compilata dall'annotatore. La prima nuvoletta viene scelta tra alcuni prompt manuali per dominio, mentre la seconda, che è la domanda alternativa, viene generata come segue: utilizziamo sempre un semplice modello \"Intendevi a o b?\", dove a e b sono campioni da Wikipedia. Ecco i diversi metodi di campionamento utilizzati. Man mano che si sale nella lista, le entità diventano più simili tra loro e di solito è più difficile effettuare la disambiguazione. Il primo metodo è il campionamento uniforme casuale. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con lo stesso nome. Il terzo è quando hanno descrizioni simili su Wikipedia, e infine quando hanno info box o attributi simili su Wikipedia, come lo stesso genere o lo stesso artista per le canzoni. Quando mostriamo questa domanda alternativa agli annotatori, loro conoscono il nome di queste entità, ma non necessariamente le entità stesse. Quindi, mostriamo alcune conoscenze di sfondo su entrambe le entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno una parte di ogni canzone e di leggere su ciascuna. Ecco, ad esempio, il risultato della ricerca Google per la canzone \"easy on me\". Per i domini ricette e libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo anche le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono. Poi chiediamo agli annotatori di scegliere una di queste entità, ad esempio la prima, e di descriverla utilizzando tre o cinque espressioni di riferimento indiretto, come \"quella con la musica per pianoforte\". Ecco alcuni esempi dal nostro set di dati: \"quella senza parole\", \"non quella con il ragazzo di 12 anni\", \"quella fittizia\", \"quella che viene dall'Azerbaigian\", e così via. Il corpus alternativo contiene 6.000 domande alternative in tre domini e 422.000 espressioni di riferimento indiretto. I risultati con il modello T5X Large sono riassunti qui sotto. Se il modello linguistico ha accesso alle stesse conoscenze di sfondo degli annotatori, l'accuratezza è molto alta, intorno al 92-95%, ma questa non è una situazione realistica. Se il modello linguistico ha accesso a conoscenze di sfondo parzialmente sovrapposte, l'accuratezza è tra l'82% e l'87%, che è più realistico, ad esempio quando il modello linguistico recupera le conoscenze di sfondo. Se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 6%, quindi c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili per domini. Ecco un link al nostro set di dati. Grazie."}
