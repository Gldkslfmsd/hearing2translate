{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lindemann, und heute werde ich Ihnen eine kurze Einführung in unsere Arbeit über kompositorische Generalisierung ohne Bäume mithilfe von Multiset-Tagging und latenten Permutationen geben. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositorische Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursionen und ungewöhnliche Kompositionen von Phrasen zu bewältigen, die während des Trainings individuell gesehen wurden. Im Kontext des semantischen Parsings könnte die Überprüfung der kompositorischen Generalisierung wie folgt aussehen: Üblicherweise haben wir einen Trainingsdatensatz von Äußerungen, in diesem Fall \"das Mädchen schlief\" und \"Mary wusste, dass das Mädchen schlief\". Diese Äußerungen werden mit logischen Formen gepaart, die wesentliche Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur standardmäßigen maschinellen Lernbewertung stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell ungewöhnliche logische Formen. In diesem Beispiel hat das Modell während des Trainings flache Rekursionen gesehen und wird nun mit einem Beispiel tieferer Rekursion getestet. Naive Sequenz-zu-Sequenz-Modelle haben Schwierigkeiten mit dieser Art von Generalisierung außerhalb der Verteilung und erzeugen oft Ausgaben, die vom Eingabekontext losgelöst sind. Insbesondere scheitern sie häufig daran, die systematischen Korrespondenzen zwischen Eingabe und Ausgabe, wie die farblich markierten, zu reproduzieren. Eine gängige Methode, dies zu beheben, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der Äußerungen mit logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume sind normalerweise nicht gegeben und müssen irgendwie ermittelt werden. Dies kann kompliziert und manchmal rechenintensiv sein. In der Regel erfordert dies erhebliche Formalismen und eine spezifische Vorverarbeitung der logischen Formen, um beispielsweise mit Variablensymbolen umzugehen. Die Extraktion von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren beinhalten.\n\nIn dieser Arbeit verwenden wir keine Bäume und stellen ein neuronales Sequenz-zu-Sequenz-Modell vor, das die Korrespondenzen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe direkt modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung zu tieferen Rekursionen ohne Verwendung von Bäumen. Unser Ansatz prognostiziert die Ausgabe aus der Eingabe in zwei Schritten: Zuerst markieren wir jedes Eingabetoken mit einer ungeordneten Multimenge von Tokens, die in der Ausgabe erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht geordnet. Deshalb verwenden wir in einem zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode zur Vorhersage einer Permutation ein, die keine harten Einschränkungen für die möglichen Permutationen vornimmt. Dies macht unseren Ansatz sehr flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell wie folgt: Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token an jeder Position platziert werden soll. Für die erste Ausgabeposition wählen wir einfach eines aus, wie hier rot markiert. Dann springen wir zum nächsten Multiset-Token, um das zweite Token in der Ausgabe zu bestimmen. Wir bestimmen das dritte Token in der Ausgabe auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Dieser Prozess wird fortgesetzt, bis jedes Token aus der ersten Stufe genau einmal besucht wurde.\n\nUm Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unseren Ansatz mit anderen baulosen Modellen am COgs-Benchmark. Unser Modell übertrifft die anderen bei der Generalisierung zu tieferen Rekursionen bei Weitem. Andere Arten struktureller Generalisierung bleiben jedoch sehr anspruchsvoll. In unserer Arbeit lösen wir mehrere interessante technische Herausforderungen. Erstens wird die Ausrichtung zwischen Eingabe und Ausgabe in den Trainingsdaten nicht vorgegeben. Folglich wissen wir für ein gegebenes Token nicht, aus welcher Multimenge es stammt, was die Trainingsanforderungen erschwert. Außerdem gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die sprachlich korrekte Permutation ist latent. Wir lösen dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unser Permutationsansatz ist sehr flexibel, bringt aber die Herausforderung mit sich, dass die Suche nach der höchstbewerteten Permutation NP-schwer ist, da sie mit dem Traveling-Salesman-Problem zusammenhängt. Wir nähern uns diesem Problem mit einer GPU-freundlichen kontinuierlichen Relaxation an, die es uns auch ermöglicht, durch die Lösung zurückzupropagieren und sprachlich plausiblere Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und die Bewältigung dieser Herausforderungen erfahren möchten, lesen Sie bitte unsere Arbeit oder besuchen Sie unseren Poster-Vortrag."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra und ich werde heute über unsere Arbeit zu markierten Personas sprechen, bei der natürliche Sprachanreize verwendet werden, um Stereotypen in Sprachmodellen zu messen. Diese Arbeit wurde in Zusammenarbeit mit Essenndermush und Danjorovsky durchgeführt. In den letzten Jahren haben viele Studien die Verbreitung sozialer Voreingenommenheit und Stereotypen in großen Sprachmodellen (LLMs) dokumentiert. Diese Messungen haben jedoch verschiedene Einschränkungen: Sie stützen sich in der Regel auf von Hand erstellte Datensätze, die sehr zeitaufwändig zu kuratieren sind, und messen meist nur sehr spezifische Stereotypen, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte verallgemeinern lassen. Außerdem erfassen sie oft nur sehr allgemeine, breite Assoziationen, wie negative Verbindungen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die Mehrheit der Arbeiten in diesem Bereich nicht die Schnittstellenproblematik (Intersectionalität), die besagt, dass vielschichtige soziale Identitäten Voreingenommenheiten verstärken und einzigartige Quellen von Schaden sein können.\n\nUm diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass diese neueren, anweisungsbasiert trainierten LLMs sehr gut darauf reagieren, Anweisungen in Anreizen zu befolgen. Wir können das Modell also auffordern, eine Persona zu generieren, die eine Darstellung einer imaginären Person ist, indem wir einen Anreiz wie „Stellen Sie sich vor, Sie sind eine asiatische Frau, beschreiben Sie sich selbst“ verwenden. Wir können sofort sehen, dass dies sehr gut auf jede Demografie anwendbar ist, da wir einfach den gewünschten Identitätsmarker in diesen Anreiz einfügen können. Hier sind einige Beispielgenerierungen von GPT4: Sofort erkennen wir, dass die Ausgaben zwar nicht offen negativ oder toxisch im traditionellen Sinne dieser Wörter sind, sich aber interessante Muster zeigen. Die asiatische Frau wird als unscheinbar dargestellt, die Frau aus dem Nahen Osten wird mit Wörtern wie „exotisch“ beschrieben, und bei beiden Frauenfarbpersonas werden Bezugnahmen auf die Abstammung gemacht, während die Persona des weißen Mannes solche Bezugnahmen nicht enthält.\n\nUm diese Muster zu erfassen, besteht unsere Methode aus zwei Teilen. Der erste Teil ist die Generierung dieser Personas. Unsere Anreize zur Generierung dieser Personas wurden von einer Studie inspiriert, in der solche Anreize menschlichen Probanden gegeben wurden. Dabei wurde festgestellt, dass auch bei menschlichen Probanden rassistische Stereotypen zutage traten, was zudem einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten ermöglicht. Der zweite Teil ist die Methode der „markierten Wörter“, die dazu dient, die Wörter zu identifizieren, die die markierten Gruppen von den unmarkierten unterscheiden, worauf ich gleich näher eingehen werde. Der Vorteil besteht darin, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein.\n\nDie Methode der markierten Wörter basiert auf dem soziolinguistischen Konzept der Markiertheit, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die sich von diesem Standard unterscheidet, sprachlich markiert wird. So wird das Wort „Krieger“ beispielsweise normalerweise mit Männern assoziiert. Beschreibt man einen Krieger, der eine Frau ist, fügt man in der Regel das Wort „Mann“ hinzu und markiert den Begriff mit „Frau“. Im Allgemeinen sind dominanten Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert werden. In unserer Methode bezeichnen wir zunächst, welche Gruppen unmarkiert und markiert sind, und vergleichen dann die Personas mit der Methode der „Kampfwörter“, die im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet, um die wichtigsten Wörter für jede markierte Gruppe zu ermitteln. Für die Personas schwarzer Frauen würden wir beispielsweise „Kampfwörter“ verwenden und die Log-Odds-Verhältnisse mit den Personas von Weißen und Männern vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind.\n\nZu den Ergebnissen: Zunächst verwenden wir ein Lexikon der Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Bei genauerer Betrachtung der Verteilung der Wörter im Lexikon zeigen sich jedoch unterschiedliche Ergebnisse. Während die generierten Personas eine viel höhere Rate an Lexikonwörtern aufweisen, haben die von Menschen geschriebenen Personas eine viel breitere Verteilung von Wörtern. Die Stereotypwörter in den generierten Personas beschränken sich auf „groß“ und „sportlich“, also nur auf positive oder zumindest nicht-negative Wörter. Dieses Lexikon erfasst jedoch viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht.\n\nStattdessen wenden wir uns den Ergebnissen unserer Methode der markierten Wörter zu, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essenzialisierende Erzählungen fördern. In unserer Analyse offenbaren wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zunächst einmal umfassen die Top-Wörter für markierte Gruppen Begriffe wie „Kultur“, „Tradition“, „stolz“ und „exotisch“. Diese Wörter definieren diese Gruppen ausschließlich durch ihre Beziehung zu ihrer Identität und unterscheiden sie vom weißen Normwert. Dies trägt zu einer langen Geschichte der Diskriminierung und Ausgrenzung dieser Gruppen bei. Darüber hinaus spiegeln sich in diesen Wörtern viele gängige Klischees wider, insbesondere für Frauenfarben. So umfassen die Wörter, die eine lateinamerikanische Frau beschreiben, Begriffe wie „lebhaft“ und „kurvig“, die mit dem Klischee des Tropicalismus verbunden sind. Bei asiatischen Frauen sind es Wörter wie „klein“, „zart“ und „seidenweich“, die an die lange Geschichte der Sexualisierung asiatischer Frauen und ihre Darstellung als sehr sanft und unterwürfig anknüpfen. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Begriffe wie „stark“ und „resilient“ sind. Dies steht im Zusammenhang mit dem Archetyp der „starken schwarzen Frau“, und obwohl es auf den ersten Blick positiv klingt, haben Studien gezeigt, dass dieser Archetyp in Wirklichkeit sehr schädlich ist, da er von diesen Demografien erwartet, dass sie trotz gesellschaftlicher Hindernisse stark und widerstandsfähig sind. Anstatt sich für eine Veränderung dieser Hindernisse einzusetzen, wird Druck auf diese Menschen ausgeübt, sie zu überwinden, was zu sehr negativen gesundheitlichen Folgen und anderen Schäden führen kann.\n\nZusammenfassend stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur essenzialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern ziehen wir drei Empfehlungen für Modellbesitzer in Betracht: Erstens sollten Forscher positive Stereotypen und essenzialisierende Erzählungen thematisieren. Zweitens sollten wir eine Schnittstellenperspektive (intersectionale Perspektive) einnehmen, um Voreingenommenheit und Schaden zu untersuchen, da viele Dinge übersehen werden könnten, wenn wir dies nicht tun. Und schließlich sollte es mehr Transparenz über Methoden zur Reduzierung von Voreingenommenheit geben, da wir beispielsweise bei diesen positiven Stereotypen nicht wissen, ob sie auf eine Art übermäßige Wertausrichtung oder andere Antisterotypisierungsmethoden zurückzuführen sind, die zu diesen schädlichen Mustern führen. Ohne mehr Transparenz können wir keine Annahmen treffen oder dies weiter untersuchen.\n\nVielen Dank fürs Zuhören, und ich wünsche Ihnen eine gute Zeit auf der ACL-Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch.\n\nHeute werden wir Ihnen alles über ABCEV erzählen, einen neuen dimensionalen Ansatz zur Bewertung von Conversational AI. Diese Arbeit wurde vom Emory NLP-Labor durchgeführt, geleitet von Professor Gino Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI.\n\nStellen Sie sich vor, Sie haben gerade ein Dialogmodell entwickelt und möchten wissen, wie es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis besteht darin, menschliche Bewertungen einzuholen, beispielsweise indem man menschliche Bewerter bittet, auszuwählen, welcher von zwei Gesprächen besser ist, oder Gespräche auf einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um umfassende Bewertungen der allgemeinen Dialogqualität zu erhalten. Da die Dialogqualität jedoch viele Aspekte umfasst, möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer detaillierteren Ebene zu verstehen.\n\nEin Ansatz besteht darin, menschliche Bewerter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie z. B. die Relevanz der Modellantworten, unter Verwendung bestehender vergleichender oder Likert-Skalen-Methoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem explizit annotiert wird, ob jede Modellantwort bestimmte Verhaltensweisen zeigt, wie z. B. das Bereitstellen irrelevanter Informationen oder das Widersprechen sich selbst. Wir nennen diesen Ansatz \"Annotierung von Verhaltensweisen im Chat\" oder kurz ABC-Eval.\n\nWir haben diese Methode entwickelt, um Verhaltensweisen von Chat-Modellen umfassend abzudecken, die in jüngster Literatur als Einflussfaktoren auf die Chat-Qualität vorgeschlagen wurden. ABC-Eval kann die Häufigkeit messen, mit der Chat-Modelle verschiedene thematische Fehler machen. Beispielsweise misst ABC-Eval die Anzahl der Dialogzüge, in denen ein Chat-Modell seinen Gesprächspartner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinen Partner widerspricht, falsche Fakten halluziniert oder allgemeines Wissen verletzt und ob das Modell Empathie zeigt oder nicht.\n\nUm zu bestimmen, welche Bewertungsmethode am effektivsten ist, wählten wir vier State-of-the-Art-Chat-Modelle aus und bewerteten sie anhand von 100 menschlichen Bot-Gesprächen pro Modell unter Verwendung von ABC-Eval. Zur Vergleichbarkeit bewerteten wir diese Gespräche auch mit drei bestehenden Methoden: Likert-Bewertungen auf Dialogzug-Ebene, Likert-Bewertungen auf Dialog-Ebene und dialogebene paarweise Vergleiche. Für jede der bestehenden Methoden sammelten wir Bewertungen zu acht der am häufigsten gemessenen Aspekte der Dialogqualität, da dies die Standardpraxis für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist.\n\nAus unserer Analyse dieser Bewertungsergebnisse geht hervor, dass die ABC-Verhaltenslabels insgesamt zuverlässiger sind als die mit bestehenden Methoden gesammelten Labels, gemessen an der inneren Annotator-Übereinstimmung bei hundert doppelt beschrifteten Gesprächen. Darüber hinaus sind die ABC-Eval-Labels prädiktiver für die allgemeine Gesprächsqualität im Vergleich zu Metriken, die von bestehenden Methoden erzeugt werden, wie diese einfache lineare Regressionsanalyse zeigt. Sie können sehen, wie das Messen des Anteils der Dialogzüge mit Selbst- und Partnerwidersprüchen jeweils 60 % und 10 % der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenz-Scores nur 4 % oder weniger erklären.\n\nSchließlich überprüften wir, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, unter Verwendung einer schrittweisen linearen Regression. Sie können sehen, dass die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, verlieren die meisten von ihnen eine beträchtliche Menge an Informationen über die Qualität. Andererseits erklärt die Kombination aller Dialogzug-Likert-Metriken viel weniger von der Qualität, und weniger dieser Metriken tragen einzigartige Informationen bei.\n\nDiese zuverlässigen, informativen und eindeutigen ABC-Eval-Metriken ermöglichen es uns, Conversational AI mit einer höheren Auflösung zu bewerten als dies mit vorherigen Methoden möglich war. Sie können sehen, dass die Ergebnisse unseres Experiments mehrere Herausforderungen aufzeigen, die noch bestehen und präzise quantifiziert wurden. Beispielsweise weisen die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen den gesunden Menschenverstand auf, sie produzieren in etwa 15 % der Antworten irrelevante Informationen und widersprechen sich selbst oder ihrem Gesprächspartner in etwa 10 % der Fälle.\n\nAngesichts des raschen Fortschritts in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, sinken. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken für den Modellvergleich anzustreben. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich als bedeutender Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf zu sehen, wie sich Conversational AI in den kommenden Monaten und Jahren weiterentwickeln wird.\n\nVielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Vauddha und ich bin Doktorand im Fach Informatik an der Stony Brook University. Ich möchte unsere Arbeit vorstellen, die beim ACL 2023 als Langpapier angenommen wurde: \"Transfer Learning für die Erkennung von Dissonanz: Bewältigung der Herausforderung seltener Klassen\".\n\nWir beginnen mit der Definition von kognitiver Dissonanz und warum es wichtig ist, dieses Phänomen in der Sprache zu untersuchen. Kognitive Dissonanz tritt auf, wenn zwei Überzeugungen oder Handlungen inkonsistent sind, wie in diesem Beispiel: Eine Person sagt \"Ich weiß, dass Zigaretten mich töten könnten\" und fügt dann hinzu \"Ich habe nach der Besprechung ein paar Zigaretten geholt.\" Diese Überzeugung und Handlung sind inkonsistent und befinden sich in einer Dissonanz. Die weitere Äußerung \"Ich denke nicht, dass ich meinen Job ohne sie behalten könnte\" rechtfertigt das zweite Ereignis, und sie stehen in einer Konsonanzbeziehung.\n\nObwohl Dissonanz ein häufiges Phänomen in unseren täglichen Entscheidungen ist, kommt sie in der Sprache und anderen Diskursbeziehungen sehr selten zum Ausdruck. Die Untersuchung kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen, Trends in Überzeugungen und Einstellungsänderungen in der Bevölkerung zu verstehen. Hohe kognitive Dissonanz steht auch in Zusammenhang mit Angststörungen und kann dazu beitragen, die psychische Gesundheit von Menschen besser zu verstehen. Die Analyse von Dissonanz in der Sprache kann auch bei der Erforschung von Extremismus und Polarisierung anfälliger Gruppen hilfreich sein. Schließlich ist das Studium kognitiver Dissonanz wichtig, um die individuellen kognitiven Stile zu verstehen und die Entscheidungsfindungsprozesse zu verbessern.\n\nUm eine Ressource für kognitive Dissonanz zu erstellen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir verwendeten einen Dissonanz-First-Ansatz, wie im Flussdiagramm gezeigt. Tweets wurden mit einem PDTV-Parser verarbeitet, und Paare von Diskursseinheiten wurden gemäß den in unserem Papier beschriebenen Richtlinien annotiert. Dissonanz wurde nur in 3,5 % der annotierten Paare gefunden.\n\nNach der Sammlung von etwa tausend Beispielen von Diskursseinheitspaaren trainierten wir einen anfänglichen Klassifikator, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Kein Wunder, dass der Klassifikator nicht viel besser als zufällig funktionierte, angesichts der geringen Vorkommen von Dissonanz und dem Fehlen eines solchen Datensatzes zuvor. Wir stehen vor dem Problem der absoluten Seltenheit.\n\nUm dies zu mildern, experimentierten wir mit Kombinationen aus Transfer Learning und Active Learning, um mehr Dissonanzbeispiele zu sammeln und die Gesamtkosten der Annotation zu senken. Da das anfängliche Modell die Dissonanz-Klasse überhaupt nicht erfassen konnte, beginnen wir den Active Learning-Prozess mit dem Transfer von Gewichten aus eng verwandten Aufgaben. Wir transferieren von zwei verschiedenen Aufgaben: Topic-unabhängige Dissonanz-Tanz-Klassifizierung, eine Aufgabe, die bestimmt, ob zwei Debatten-Aussagen von verschiedenen Personen übereinstimmen oder nicht, unabhängig vom Thema, genannt \"Debatte\" hier, und binäre Klassifizierung der Erweiterungs- und Vergleichsklassen von Reinheit.\n\nDa diese beiden Aufgaben eng mit dem Konzept von Konsonanz und Dissonanz verwandt sind, nennen wir sie \"CE\". Wir stellen fest, dass die Übertragung der Gewichte bereits eine viel bessere Leistung als Zufall auf dem annotierten Datensatz erzielt, mit einem AUC von 0,62. Durch iteratives Feinabstimmen beider Aufgaben finden wir heraus, dass das Feinabstimmen von CE, gefolgt von einer weiteren Feinabstimmung auf Debatte, zu einer deutlich besseren Null-Shot-Leistung führt. Dies ist das Modell, das wir verwenden, um den Active Learning-Prozess zu starten.\n\nAls Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde des Active Learning und der Annotation zu aktualisieren. \"Kumulativ\" akkumuliert alle bisher gesammelten Daten aus den Active Annotations, während \"Iterativ\" das Modell durch Training auf den neuesten Datensatz aktualisiert. Über alle Strategien hinweg leistete \"Kumulativ\" gleich gut oder besser als \"Iterativ\".\n\nUm die Anzahl der Dissonanzbeispiele zu erhöhen, verwenden wir eine Strategie mit der Wahrscheinlichkeit seltener Klassen (PRC), um hauptsächlich Beispiele auszuwählen, die nach dem aktuellen Modell mit hoher Wahrscheinlichkeit dissonant sind. Wir vergleichen dies mit anderen State-of-the-Art-Strategien für Active Learning, die in der Community üblich sind. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-Art-Strategien, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung für \"Zufällig\" in weiteren Runden des Active Learning deutlich abfällt.\n\nMit den beiden besten Strategien verbessern wir die Dissonanz-Klassifizierung auf einen AUC von 0,75, was die beste Leistung ist, die wir bisher in dieser Aufgabe erreicht haben. Wir überprüfen auch die Machbarkeit jeder Strategie hinsichtlich der Annotierungsqualität und -kosten. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanzbeispielen hat und am besten für seltene Klassen geeignet ist. Allerdings finden die Annotatoren die Beispiele auch schwieriger.\n\nZusammenfassend lässt sich sagen, dass PRC eine einfache Strategie für die Akquise seltener Klassen und den Start des Active Learning mit angemessen gestalteten Transfer-Learning-Aufgaben ist, die erheblich helfen kann. Wir stellen auch fest, dass iterative Aktualisierungen für den Transfer von Learning aus einem anderen Bereich nützlich sind, während in-domain Active Annotations von kumulativen Aktualisierungen profitieren.\n\nHier sind die Links zu unserem Code, Datensatz und Papier. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Ich bin Akshata und heute präsentiere ich gemeinsam mit meinem Mitautor Martin unsere Arbeit „Kit Must: Evaluierung der Wissensintegration aus mehreren Quellen“. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Nationale Sprachverstehensmodelle nutzen verschiedene Wissensquellen, wie beispielsweise das in ihren Parametern enthaltene Wissen, das in der Regel über Voraustraining erworben wird, und das in den Eingaben bei der Inferenz bereitgestellte Wissen. Aktuelle Arbeiten zu Aufgaben wie Fragebeantwortung zeigen, dass Modelle das im Voraus trainierte Wissen nutzen können, um die Aufgabe zu lösen. Doch das natürliche Sprachverständnis erfordert oft Wissen, das auch bei der Inferenz bereitgestellt wird, wie beispielsweise im Satz „John sah den neu gewählten Präsidenten im Fernsehen“. Die vorab trainierten Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese instanzspezifische Entität John ist oder wer der neue Präsident ist, da sich der Präsident seit dem Voraustraining geändert haben könnte. Daher benötigen erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl das im Voraus trainierte als auch das bei der Inferenz verfügbare Wissen zu integrieren und zu nutzen.\n\nIn dieser Arbeit schlagen wir einen Diagnosetest für die Wissensintegration vor. Wir führen eine Kernreferenzauflösung auf, die darauf abzielt, die Fähigkeit zu untersuchen, auf Wissen aus verschiedenen Quellen zurückzugreifen. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablieren Kernreferenzauflösungsmodelle. Hier ist ein Beispiel aus unserem Datensatz: „Serving ist Richter, Kia ist Bäcker. Nach einem langen Arbeitstag, Entscheidungen in einem Gesetzbuch zu fällen, war er froh, sich zu entspannen.“ Die Aufgabe besteht darin, die richtige Entität zu identifizieren, auf die das Pronomen „er“ verweist, in diesem Fall Serving. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen: Erstens entitätsspezifisches Wissen, wie z. B. Serving ist Richter, und zweitens allgemeines Wissen, wie z. B. Richter fällen Entscheidungen in Gerichten. Das allgemeine Wissen wird während des Voraustrainings großer Sprachmodelle erlernt, während das entitätsspezifische Wissen typischerweise bei der Inferenz beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen, sodass sie entweder in einer einzigen Quelle oder in mehreren Quellen zu finden sein können.\n\nWir haben drei Einstellungen von Kitdmos definiert. Erstens haben wir die typische Voreinstellung, bei der angenommen wird, dass das allgemeine Wissen beim Voraustraining verfügbar ist. Zweitens gibt es die Einstellung „Allgemeines Wissen beide“, bei der das allgemeine Wissen sowohl beim Voraustraining als auch bei der Inferenz verfügbar ist. Und zuletzt die Einstellung „Allgemeines Wissen Inferenz“, bei der beide Wissensarten nur bei der Inferenz verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das zum Lösen einer Aufgabe notwendige allgemeine Wissen nicht Teil der Voraustrainingsdaten der Modelle ist, beispielsweise weil seit dem Voraustraining neue Berufe entstanden sind. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den beiden Quellen in der Einstellung „Allgemeines Wissen Voraustraining“ steuern: Wir gehen davon aus, dass das allgemeine Wissen „Politiker streben gewählte Sitze in der Regierung an“ in den vorab trainierten Parametern enthalten ist. Im Inferenzkontext stellen wir dann die entitätsspezifischen Informationen „Chichester ist ein Politiker“ bereit. In der Einstellung „Allgemeines Wissen beide“ stellen wir zusätzlich zum entitätsspezifischen auch allgemeines Wissen über Politiker im Inferenzkontext bereit. In der Einstellung „Allgemeines Wissen Inferenz“ stellen wir statt des Berufs „Politiker“ den Beruf „Merelytour“ bereit, da Merelytour wahrscheinlich nicht in den vorab trainierten Parametern enthalten ist.\n\nWir bewerten den Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Kernreferenzauflösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der besten Modelle in der schwierigsten Variante der Einstellung „Allgemeines Wissen Voraustraining“ ohne aufgabenbezogenes Training auf Kitdmos. Beide Modelle schneiden schlecht ab, wenn sie auf Kitdmos trainiert werden. Allerdings übertreffen sowohl c2f als auch Built for Coref signifikant die zufällige Auswahl. Dies deutet darauf hin, dass Modelle, wenn sie auf generische Referenzauflösungs-Datensätze trainiert werden, lernen, Oberflächenhinweise auszunutzen, die beim Testen auf Kitdmus, wo solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die besten Modelle Schwierigkeiten haben, allgemeines Wissen, das nur bei der Inferenz verfügbar ist, zuverlässig zu integrieren.\n\nZusammenfassend lassen sich aus unserer Arbeit folgende Hauptpunkte ableiten: Viele Kernreferenzauflösungsmodelle scheinen ohne aufgabenbezogenes Training nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu raisonieren. Mit aufgabenbezogenem Training integrieren jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen. Selbst die besten Modelle haben jedoch Schwierigkeiten, allgemeines Wissen, das nur bei der Inferenz präsentiert wird, zuverlässig zu integrieren. Für weitere Details verweisen wir auf unsere Veröffentlichung und den Datensatz sowie den Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sarah Pai von der Universität Trento und dem Fondazione Bruno Kessler, und ich werde kurz das Papier „Attention as a Guide for Simultaneous Speech Translation“ vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Durchi ist.\n\nWas ist simultane Sprachübersetzung? Simultane Sprachübersetzung oder SimST ist der Prozess der Übersetzung gesprochener Sprache in einen Text in einer anderen Sprache in Echtzeit, wodurch eine Kommunikation über Sprachbarrieren hinweg ermöglicht wird.\n\nWelche Probleme haben aktuelle SimST-Modelle? Spezifische Architekturen werden normalerweise mit zusätzlichen zu optimierenden Modulen trainiert, was zu langen und komplizierten Trainingsverfahren führt. Beispielsweise beinhaltet das Training unterschiedliche Optimierungsziele und erfordert das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen, wie z. B. das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden und so weiter.\n\nWas ist unsere Lösung? Erstens nutzen wir bereits bestehende Offline-SD-Modelle ohne erneutes Training oder spezifische Architektur für SimST. Wir verwenden nur ein Modell für jedes Latenzregime und steuern die Latenz über spezifische Parameter. Wir nutzen das bereits erworbene Wissen eines Modells durch die Spannungsmechanik zwischen Audioeingabe und Textausgabe, also den Cross-Attention-Mechanismus. Sie können ein Beispiel auf der rechten Seite sehen.\n\nUnsere Lösung besteht darin, einen Dot-Encoder-Dekor-Attention-Mechanismus vorzuschlagen. Es ist eine Strategie, bei der entschieden wird, ob eine partielle Übersetzung ausgegeben wird oder nicht, basierend darauf, auf welches Wort die Aufmerksamkeit gerichtet ist. Ein Wort wird ausgegeben, wenn die Spannung nicht konzentriert ist, d. h. diese Summe unter einem bestimmten Schwellenwert Alpha liegt, der sich auf die letzten Lambda-Sprachrahmen bezieht. Dies bedeutet, dass die empfangenen Informationen stabil genug sind. Wenn wir beispielsweise eine Sprachsequenz mit „Ich werde über ... sprechen“ erhalten und unser Modell die Übersetzung auf Deutsch vorhersagt, und wir uns die Cross-Attention-Gewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen verweisen, während das letzte Wort auf die letzten empfangenen Tonhöhenrahmen verweist. Dies bedeutet, dass die ersten beiden Wörter ausgegeben werden, da die Summe der Cross-Attention über einem bestimmten Schwellenwert Alpha liegt, und wir werden das letzte Wort nicht ausgeben und auf eine weitere Sprachsequenz warten.\n\nWenn wir fortfahren und eine weitere Sprachsequenz erhalten, und unser Modell weitere drei Wörter vorhersagt, und wir uns die Cross-Attention-Gewichte ansehen, werden wir sehen, dass kein Wort auf die letzten Lambda-Sprachrahmen verweist. Dies bedeutet, dass diese drei Wörter ausgegeben werden.\n\nWenn wir uns die Hauptresultate des Dot-Verfahrens ansehen, stellen wir die Ergebnisse der simultanen Sprachübersetzung in Graphen dar, in denen auf einer Seite in Blau die Übersetzungsqualität und die durchschnittliche Verzögerung (Latenzmaß) gemessen werden. Wir berücksichtigen auch die rechenintensive durchschnittliche Verzögerung, die die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten, dass unsere Kurven in diesem Diagramm möglichst hoch sind, aber auch nach links verschoben. Wir vergleichen sie mit vorbereiteten Strategien, die auch auf Offline-Modellen angewendet werden, wie der Whit-Key-Strategie und der lokalen Übereinstimmung, sowie mit der Stand-der-Technik-Architektur, die speziell für die simultane Sprachübersetzung entwickelt wurde.\n\nDies sind alle Ergebnisse der simultanen Sprachübersetzungsstrategie für Deutsch. Wir sehen, dass Dot alle Strategien übertrifft, die auf Offline-Modellen angewendet werden, da die Kurven weiter nach links verschoben sind. Wir sehen auch, dass AD, wenn wir die tatsächliche verstrichene Zeit oder die Rechenzeit berücksichtigen, die schnellste Strategie ist.\n\nFür weitere Ergebnisse lesen Sie bitte unsere Arbeit, und wir haben auch den Quellcode und die Modelle offen zugänglich gemacht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Mein Name ist Shu Ha. Heute werde ich unseren Artikel „Do Connel 2003 Named Entity Tagger auch 2023 noch gut funktionieren?“ vorstellen. Los geht's. In unserer Studie untersuchten wir das Problem der Verallgemeinerung unter Verwendung der Named Entity Recognition-Aufgabe (NER). Wir stellten fest, dass Modelle seit fast 20 Jahren Con 2003 für die Entwicklung von NER-Systemen nutzen, was natürlich mehrere Probleme aufwirft. Erstens: Können diese Modelle moderne Daten verallgemeinern? Und wenn wir neue Tagger entwickeln, was ist für eine gute Verallgemeinerung erforderlich? Gleichzeitig, wenn wir eine schlechte Verallgemeinerung beobachten, was verursacht den Leistungsabfall dieser Modelle?\n\nUm diese Fragen zu beantworten, entwickelten wir den Con plus+-Datensatz. Dies ist ein Datensatz, den wir aus Reuters-Nachrichten von 2020 gesammelt und dann mit den gleichen Con-2003-Annotierungsrichtlinien annotiert haben. Anschließend haben wir über 20 Modelle auf Con 2003 feinabgestimmt, sie sowohl auf dem Con-2003-Testset als auch auf dem Con-plus+-Testset bewertet und nicht zuletzt den prozentualen Änderungsgrad von F1 berechnet, um die Verallgemeinerungsfähigkeit jedes Modells zu beurteilen.\n\nWas ist also für eine gute Verallgemeinerung erforderlich? Durch unsere Experimente fanden wir heraus, dass drei Hauptkomponenten notwendig sind. Die erste ist die Modellarchitektur. Unsere Experimente zeigten, dass Transformer-Modelle in der Regel besser auf neue Daten verallgemeinern. Die zweite Komponente ist die Modellgröße. Wir stellten fest, dass in der Regel größere Modelle zu einer besseren Verallgemeinerung führen. Und zuletzt wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele die Leistung einer nachgelagerten Aufgabe direkt beeinflusst. Auch hier fanden wir heraus, dass mehr Feinabstimmungsbeispiele ebenfalls zu einer besseren Verallgemeinerung führen.\n\nZu unserer nächsten Frage, was den Leistungsabfall einiger Modelle verursacht, hatten wir zwei Hypothesen. Die erste ist adaptives Überanpassen, also Überanpassung durch wiederholte Verwendung desselben Testsets, was sich in der Regel als abnehmende Rendite bei einem neuen Testset manifestiert. Die zweite Hypothese ist zeitlicher Drift, ein Leistungsabfall, der durch die wachsende zeitliche Lücke zwischen Trainings- und Testdaten verursacht wird.\n\nFür das adaptive Überanpassen sahen wir, dass die beste Anpassungsgerade (rote Linie) im Diagramm rechts eine Steigung von mehr als eins hat. Das bedeutet, dass jede Verbesserungseinheit, die wir bei Con 2003 erzielten, zu mehr als einer Verbesserungseinheit bei Con plus+ führt, was bedeutet, dass es keine abnehmende Rendite gibt. Dies zeigt, dass in diesem Fall kein adaptives Überanpassen beobachtet wurde.\n\nWas den zeitlichen Drift betrifft, führten wir ein Experiment durch, um einige Modelle mit aktuelleren Daten erneut zu trainieren oder fortlaufend vorzutraining, und stellten fest, dass die Leistung mit zunehmender zeitlichen Lücke abnimmt. Dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist.\n\nUnser Fazit ist, dass für eine gute Verallgemeinerung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele erforderlich sind, und diese gehen Hand in Hand. Man kann nicht nur eine Komponente haben und die anderen vernachlässigen. Wir stellten auch fest, dass der Leistungsabfall hier durch zeitlichen Drift verursacht wird und überraschenderweise nicht durch adaptives Überanpassen, obwohl Connel 2003 seit über 20 Jahren verwendet wird.\n\nZurück zur Frage, die wir in der Überschrift unseres Artikels aufgeworfen haben: Funktionieren Connal-2003-Tagger auch 2023 noch? Unsere Antwort ist ein klares Ja. Wir hoffen, dass unser Artikel zu weiteren Forschungen darüber anregt, wie die Verallgemeinerungsfähigkeit von Modellen verbessert werden kann. Und zuletzt: Bitte werfen Sie einen Blick auf unseren Artikel und unseren Datensatz, und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo! Willkommen zu unserer Präsentation von De plain, einem neuen Korpus für die deutsche Textidentifikation auf Dokument- und Satzebene. Mein Name ist Regina Stoden und ich werde Sie durch den ersten Teil der Präsentation führen. Beginnen wir zunächst mit der Definition von Textvereinfachung. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um das Textverständnis für eine bestimmte Zielgruppe, wie Menschen mit Leseproblemen oder Nichtmuttersprachler, zu verbessern. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, beispielsweise von Dokumenten oder Sätzen. Im Beispiel hier sehen Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache. Zur Vereinfachung des Satzes sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie lexikalische Substitution, Klausel-Dilatation, Kreuz-Elimination, Umordnung oder Einfügung von Wörtern.\n\nWir schlagen nun unseren neuen Korpus D plane vor, da es in den letzten Jahren einige Probleme mit bestehenden Korpora gab. So sind beispielsweise diese Korpora hier zu klein, um ein Taxonifikationsmodell zu trainieren. Die anderen drei in den letzten Jahren vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass ihre Ausrichtungen fehleranfällig sein können. Daher schlagen wir unseren neuen Korpus Dplane vor, der in zwei Subkorpora unterteilt ist: Deplane APA und Deplane web. Deplane APA basiert auf Nutzungstexten. In Deplane APA haben wir 483 Dokumente alle manuell ausgerichtet. Das ergibt grob 30.000 13.000 parallele Satzpaare. Für Deplane web umfasst dieser Korpus verschiedene Domänen, und wir haben auch alle 750 Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergibt sich eine Anzahl von 30.450 Satzpaaren.\n\nWir haben unsere Satzpaare etwas genauer analysiert, beispielsweise hinsichtlich der Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als beispielsweise die Nachrichtentexte oder die Sprachlernertexte auf allen Ebenen, einschließlich lexikalischer Vereinfachung, strukturierter Vereinfachung und allgemeiner Vereinfachungsebene. Außerdem können Sie sehen, dass unser Deplain-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So haben wir im Deplain API-Korpus viel mehr Umordnungen und Wortzusätze als im Deplain web-Korpus. Im Web-Korpus hingegen haben wir viel mehr Umschreibungen.\n\nLassen Sie uns nun sehen, was wir mit diesem Korpus machen können. Hallo, ich bin Omar und werde nun über die Anwendungsfälle für unseren Datensatz De plain sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzung, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in den Post-Dokumenten extrahieren möchten. In unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten in derselben Sprache zu extrahieren, die denselben Inhalt haben, aber unterschiedliche Komplexitätsstufen aufweisen. Da wir nun unseren Datensatz De plain mit manuell ausgerichteten Sätzen haben, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen sowie die Codes zum Durchführen unserer Experimente in der Arbeit veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für deutsche Textvereinfachung die Methode von Mass Align ist. Den Code zum Ausführen dieser Methode auf Ihren eigenen Dokumenten finden Sie ebenfalls in der Arbeit.\n\nDer zweite Anwendungsfall, den wir in unserer Arbeit vorgestellt haben, ist die automatische Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachte Texte aus komplexen Eingabetexten zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt: Wir haben das Modell von Long Part feinabgestimmt, um Dokumentenebenen-Vereinfachungen zu erzeugen, und wir haben auch das normale Basis-Long-Modell feinabgestimmt, um Satzebenen-Vereinfachungen zu erzeugen. Alle Checkpoints finden Sie in der Arbeit, wo Sie auch detailliertere Informationen zu den Bewertungsmetriken und den Ergebnissen unserer Experimente erhalten. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Ergebnisse als die Baseline-Ergebnisse erzielen konnte, und wir schlugen diese Ergebnisse als Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor.\n\nVielen Dank für Ihre Aufmerksamkeit, und wir hoffen, Sie alle während der Konferenz zu treffen. Danke schön."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin X Yuan von der FNAi-Universität. Ich stelle Ihnen hier unsere Arbeit vor: „Distingishing Script Knowledge from Light Language Models für beschränkte Sprachplanung“. Im Alltag müssen Menschen oft ihre Handlungen durch Schritt-für-Schritt-Anweisungen in Form von garantierten Skripten planen. Vorherige Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele typischer Aktivitäten zu planen, wie z.B. einen Kuchen backen, und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Bisherige Arbeiten konzentrieren sich jedoch hauptsächlich auf die Planung für abstrakte Ziele typischer Aktivitäten; die Planung für Ziele mit spezifischen Einschränkungen, wie z.B. einen Schokoladenkuchen backen, ist noch wenig erforscht.\n\nIn dieser Arbeit definieren wir das Problem der beschränkten Sprachplanung, bei der unterschiedliche Einschränkungen auf die Ziele der Planung angewendet werden. Ein abstraktes Ziel kann von verschiedenen spezifischen Zielen im wirklichen Leben mit vielfältigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und treu den Einschränkungen folgen.\n\nIn dieser Arbeit bewerten und verbessern wir zunächst die Fähigkeit von Alltags-Sprachmodellen zur beschränkten Sprachplanung, da keine spezifischen Ziel-Daten vorhanden sind, um unsere Studie zu unterstützen. Wir erweitern die abstrakten Ziele mit vielfältigen Einschränkungen für die Datenerhebung mit menschlicher Beteiligung. Mit Instruct GPT sampeln wir 100 spezifische Ziele und bewerten die generierten Skripte aus Bibliotheksmodellen. Diese Tabelle berichtet über die allgemeine Genauigkeit der Ergebnisse. Wir stellen fest, dass alle Lernmodelle unbefriedigende Ergebnisse bei der Planung für spezifische Ziele erzielen.\n\nAnschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, was die Lernmodelle lernen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, die Treue zu den Einschränkungen jedoch nicht garantiert werden kann. Wir gehen tiefer in feinere Kategorien von Einschränkungen ein, die in WiH definiert sind. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von Instruct GPDs für Kategorien mit unterschiedlichen Einschränkungen stark variiert. Vorherige Studien haben gezeigt, dass die Ausgabegüte von Lernmodellen eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher übernehmen wir die Idee des Overgenerated-Z-Filters, um die Generierungsqualität zu verbessern.\n\nZunächst zeigen wir beschränkte Typen mit Beispielen für Instruct CPT und erhalten spezifische Ziele auf Basis der abstrakten Ziele. Dann generiert Instruct GPT Schlüsselskripte für spezifische Ziele. Anschließend entwickeln wir ein Filtermodell, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in Instruct-GPT-Einbettungen und berechnen die kosinussimile Ähnlichkeit sowie Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir das Skript, das die Schlüsselwörter der Zielbeschränkung enthält. Wir behalten nur das Skript, wenn das Ziel die höchste Punktzahl in der Zielgröße erzielt. Mit unserer Methode kann Instruct GPT Skripte höherer Qualität generieren. Unsere Methode verbessert die Planbarkeit erheblich in Bezug auf Semantik, Vollständigkeit und Treue zu den Einschränkungen.\n\nDa große Sprachmodelle kostspielig in der Implementierung sind, ist es wichtig, die Sprachplanfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt dafür. Bisherige Studien ermöglichen jedoch keine Planung für spezifische Ziele, und die manuelle Datensatz-Annotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um einen Datensatz für die beschränkte Sprachplanung aus leichten Sprachmodellen zu destillieren. Wir wenden unsere Methode an, um einen Datensatz für die beschränkte Sprachplanung namens CodeScript zu erstellen. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testdaten zu gewährleisten, bitten wir crowd-gesourcte Arbeiter, die fehlerhaften Proben zu korrigieren.\n\nDiese Abbildung zeigt die Verteilung der Einschränkungen in CodeScript. Wir stellen fest, dass CodeScript eine hohe Vielfalt in den generierten spezifischen Zielen aufweist. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für die beschränkte Sprachplanung einsetzen. Wir finden heraus, dass T5, feinabgestimmt auf die Bewertungsrate, Skripte von besserer Qualität generieren kann als die meisten großen Sprachmodelle, was darauf hinweist, dass kleinere Modelle größere Modelle unterstützen können, wenn sie angemessen auf geeigneten Datensätzen trainiert werden.\n\nZusammenfassend haben wir das Problem der beschränkten Sprachplanung etabliert, die Fähigkeit großer Sprachmodelle zur beschränkten Sprachplanung bewertet und eine Overgenerate-Filter-Methode für Alltags-Sprachmodelle entwickelt. Wir verwenden große Sprachmodelle, um einen hochwertigen Datensatz namens CodeScript für die beschränkte Sprachplanung zu generieren. Wir hoffen, dass dieser Datensatz eine wertvolle Ressource für die Förderung der Forschung im Bereich Sprachplanung sein wird. Vielen Dank für Ihre Aufmerksamkeit. Weitere Details zu CodeScript finden Sie in unserer Publikation."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Jannislavak und ich werde Ihnen unsere Arbeiten zu Dr. Bert vorstellen, einem robusten vortrainierten Modell in Französisch für den biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir den Hauptbeitrag unseres Artikels vor. Wir präsentieren das erste biomedizinische Modell in Französisch, genannt Dr. Bert, das auf Roberta basiert und mit nachtchos trainiert wurde, einem Datensatz aus im Web gesammelten medizinischen Daten. Außerdem führen wir einen Vergleich des Modells mit verschiedenen kryonischen Einstellungen und Datenquellen durch. Dann stellen wir unsere Ergebnisse zu 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch vor und schließen schließlich mit einer Zusammenfassung der Experimente und geben Ihnen weitere Details dazu, wie Sie auf die Modelle zugreifen können.\n\nSeit seiner Veröffentlichung im Jahr 2018 hat sich Bert als einer der effektivsten Ansätze zur Lösung von Aufgaben der natürlichen Sprachverarbeitung erwiesen und bietet im Vergleich zu historischen statischen und kontextuellen Methoden wie Word2Vec und FastText enorme Leistungssteigerungen. Seitdem wurde dieses Modell an viele andere Sprachen angepasst, wie zum Beispiel in Französisch mit Camembert und in andere Domänen wie Biomedizin mit PermiBert und BioBert sowie in den klinischen Bereich mit ClinicalBert. Allerdings sind spezialisierte Modelle für andere Sprachen selten und basieren oft auf kontinuierlichem Vortraining aufgrund des Mangels an domänenspezifischen Daten.\n\nFür die französische Sprache gab es bis jetzt kein offenes biomedizinisches Modell. Daher stellten wir uns die Frage, welche Datenquellen für eine breite Palette von Anwendungen am geeignetsten sind und ob diese Rohdaten eine gute Alternative zu klinischen Daten darstellen. Um diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die aus dem nicht-universitären Krankenhaus unserer Stadt stammen.\n\nEine weitere Frage, die wir uns stellten, war: Wie viel Daten benötigen wir, um ein spezialisiertes Modell auf französischen Daten zu trainieren? Sind es vier Gigabyte, acht Gigabyte oder mehr? Um diese Frage zu beantworten, trainierten und verglichen wir vier Modelle von Grund auf neu: eine erste Version von Dr. Bert mit sieben Gigabyte Nachos, eine zweite Version mit vier Gigabyte Naturdaten, eine erste Version von Schubert, einem klinischen Modell mit vier Gigabyte Sätzen aus klinischen Notizen, und eine finale Version von Schubert mit einer Mischung aus vier Gigabyte Naturdaten und vier Gigabyte klinischen Notizen.\n\nZusätzlich zu diesem Vergleich führten wir drei Modelle ein, die mit kontrolliertem Vortraining trainiert wurden, um den Einfluss der Vortrainingsstrategie zu analysieren: eines basierend auf den Gewichten von Camembert und trainiert auf vier Gigabyte Naturdaten, ein weiteres ebenfalls basierend auf Camembert, aber trainiert auf vier Gigabyte klinische Daten, und schließlich eines basierend auf einem englischen biomedizinischen Modell, BioBert, und trainiert auf vier Gigabyte Schnipseldaten.\n\nInsgesamt haben wir sieben Modelle, die wir auf öffentlichen und privaten Downstream-Aufgaben wie Named-Entity-Recognition, Klassifikation, Parts-of-Speech-Tagging und Fragebeantwortung evaluiert haben. Diese Modelle werden mit sechs Baseline-Modellen verglichen: Camembert, Oscar (138 Gigabyte), Oscar (4 Gigabyte), Camembert-CC-Net (4 Gigabyte), Plumbert, BioBert und ClinicalBert. Die Ergebnisse zeigen, dass die Modelle am besten auf Aufgaben abschneiden, für die sie mit Daten ähnlicher Art trainiert wurden. Allerdings können wir beobachten, dass Daten aus heterogenen Quellen vielseitiger einsetzbar sind. Wir stellen auch fest, dass die Verwendung größerer Datenmengen zu besseren Leistungen führt.\n\nInsgesamt scheinen Modelle, die von Grund auf neu trainiert wurden, auf den meisten Aufgaben bessere Leistungen zu erbringen. Unsere Experimente zum kontinuierlichen Vortraining mit den Gewichten und dem Tokenizer von PermiBert, trainiert auf dem vier-Gigabyte-Subset der Naturdaten, zeigen jedoch vergleichbare Ergebnisse zu denen, die mit Dr. Bert (4 Gigabyte) von Grund auf neu erreicht wurden. Dies gilt nicht für das Modell, das auf Camembert-Gewichten und -Tokenizer basiert und unter Stabilitätsproblemen leidet.\n\nAbschließend lässt sich sagen, dass unser vorgeschlagenes System auf neun der 11 Downstream-Aufgaben bessere Leistungen erbringt und die Ergebnisse des generischen Modells, hier Camembert, insgesamt übertrifft. Wir beobachten auch, dass spezialisierte Daten besser sind, aber nicht skalierbar. Alle vortrainierten Modelle, die aus Nachos entstanden sind, sind frei verfügbar und auf unserer Schnittstelle zugänglich. Alle Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank für Ihre Aufmerksamkeit und wir freuen uns auf Ihre Fragen an der Poster-Session in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xhang Bing, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit, die den Weg politischer Voreingenommenheiten von der Vorab-Trainingsdaten bis zu Sprachmodellen und weiter zu nachgelagerten Aufgaben verfolgt, was zu ungerechten NLP-Modellen führt. Sprachmodelle werden auf groß angelegten Web-Crawling-Daten trainiert. Politische Nachrichtenmedien sind in ihren Vorab-Trainingsdaten gut abgedeckt. Laut einer Umfrage im c4-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, der Guardian, die Huffington Post usw. in den Trainingsdaten von Sprachmodellen gut abgedeckt sind. Dies hat für Anwendungen von Sprachmodellen sowohl Segen als auch Fluch zur Folge. Einerseits konnten sie aus unterschiedlichen Perspektiven lernen, was Demokratie und Pluralität von Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen von Natur aus sozial voreingenommen und können zu potenziellen Fairness-Problemen in nachgelagerten Aufgaben führen.\n\nZu diesem Zweck schlagen wir vor, die Pipeline der politischen Voreingenommenheit zu untersuchen, die von Vorab-Trainingsdaten über Sprachmodelle bis zu nachgelagerten Aufgaben reicht, indem wir die folgenden Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielt das Crawling-Daten bei diesen politischen Voreingenommenheiten? Zweitens, wie schlagen sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen in nachgelagerten Aufgaben und könnte dies zu Fairness-Problemen in NLP-Anwendungen führen?\n\nKonkret schlagen wir zunächst vor, Sprachmodelle mit verschiedenen Prompt-Formaten zu befragen, indem wir politische Fragebögen wie den Political Compass Test verwenden. Dies ermöglicht uns eine automatische Bewertung, die gut in der politischen Wissenschaft verankert ist. Vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben und alle vier Quadranten des Political Compass besetzen. Wir können auch sehen, dass GPT4 das liberalste Sprachmodell ist und die GPT-Reihe im Allgemeinen sozial liberaler ist als die BERT-Reihe und ihre Varianten.\n\nZweitens wollen wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints weiter auf sechs verschiedenen parteiischen Korpora vortrainieren, die in Nachrichten und soziale Medien unterteilt sind und weiter nach ihrer politischen Ausrichtung differenziert werden. Durch das weitere Vortrainieren von Sprachmodellen auf solchen parteiischen Korpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschoben werden. Zum Beispiel verschiebt sich bei Roberta, die weiter auf dem linksgerichteten Reddit-Korpus feinabgestimmt wurde, die politische Voreingenommenheit deutlich in eine liberale Richtung.\n\nWir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht. Wir teilen die Vorab-Trainings-Korpora in zwei Gruppen auf: vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten. Wir trainieren Sprachmodelle separat auf diesen beiden unterschiedlichen zeitlichen Korpora. Wir können sehen, dass die Sprachmodelle im Allgemeinen eine politische Ausrichtung haben, die nach 2017 weiter vom Zentrum entfernt ist. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können.\n\nAbschließend bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen in den NLP-Anwendungen Hassrede-Erkennung und Falschinformationen-Erkennung, die häufig Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben können. Wir stellen fest, dass bei einer Untersuchung der pro Kategorie Leistung, also wenn wir die Leistung in verschiedene Demografien oder politische Medien unterteilen, ein Muster erkennbar wird: Zum Beispiel sind bei der Hassrede-Erkennung linksgerichtete Sprachmodelle besser darin, Hassreden gegen sozial benachteiligte Gruppen zu erkennen, aber schlechter darin, Hassreden gegen mächtigere Gruppen in unserer Gesellschaft zu erkennen. Umgekehrt sind rechtsgerichtete Sprachmodelle besser darin, Hassreden gegen Weiße und Männer zu erkennen, aber schlechter darin, Hassreden gegen Schwarze, LGBTQ+ und andere Minderheitengemeinschaften zu erkennen. Ähnliche Trends zeigen sich auch bei der Falschinformationen-Erkennung, wo wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Falschinformationen von ihrer gegensätzlichen politischen Ausrichtung zu erkennen, und umgekehrt. Wir zeigen außerdem viele qualitative Beispiele, um zu veranschaulichen, dass Sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen für Hassreden und Falschinformationen treffen, basierend auf ihren sozialen Kategorien. Weitere Beispiele finden sich im Anhang, um zu unterstreichen, dass dies ein sehr dringendes Fairness-Problem im Zusammenhang mit den politischen Voreingenommenheiten von Sprachmodellen darstellt.\n\nBeispielsweise könnte ein rechtsgerichtetes Sprachmodell, das auf Hassreden, Falschinformationen oder ähnliches feinabgestimmt und auf einer beliebten Social-Media-Plattform eingesetzt wird, dazu führen, dass Menschen mit gegensätzlichen politischen Ansichten marginalisiert werden und Hassreden gegen Minderheitengruppen unkontrolliert verbreitet werden. Dies hat uns alarmiert, die Fairness-Probleme anzuerkennen und anzugehen, die durch die politische Ausrichtung von Sprachmodellen entstehen.\n\nAbschließend möchten wir noch auf das einzigartige Dilemma hinweisen, das sich im Zusammenhang mit politischen Voreingenommenheiten von Sprachmodellen stellt: Es ist wie zwischen Skylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten von Sprachmodellen nicht herausfiltern, würde sich die Voreingenommenheit von den Vorab-Trainingsdaten über die Sprachmodelle bis zu den nachgelagerten Aufgaben fortpflanzen und letztendlich Fairness-Probleme schaffen. Wenn wir versuchen, sie auf irgendeine Weise zu herauszufiltern, riskieren wir auch Zensur oder Ausschluss, und es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten beibehalten werden sollte. Es ist eine Art Trolley-Problem.\n\nVielen Dank für Ihre Aufmerksamkeit. Das war es für heute."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Ich bin Koov Sinna und ich freue mich, Sie zu unserem Vortrag über unseren ACL 23-Aufsatz begrüßen zu dürfen. Akzeptanzurteile von Sprachmodellen sind nicht immer kontextrobust. Dies ist eine gemeinsame Arbeit mit John Waqui, Aaron Mueller, Kanishka Mishra, Karen Fs, Roger Levy und Atina Williams. In dieser Arbeit überprüfen wir das minimale Paarparadigma. Das minimale Paarparadigma bewertet im Wesentlichen Sprachmodelle auf der Grundlage von Akzeptanzurteilen, die auch Grammatikalität umfassen können, wie z. B. BLIMP-Syntax-Gym oder Akzeptanz im Hinblick auf Stereotype, wie z. B. Crowds-Paare. Im minimalen Paarparadigma besteht die übliche Methode zur Bewertung von Sprachmodellen darin, dass man einen akzeptablen oder grammatikalischen Satz und dann einen inakzeptablen oder ungrammatikalischen Satz präsentiert, wobei die Hoffnung besteht, dass das Modell dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuweist. Die aktuelle MPP-Pipeline ermöglicht es uns jedoch nicht, die Akzeptanz von Modellen für längere Sätze zu bewerten. Heutzutage kommen große Sprachmodelle mit immer größeren Kontextfenstern daher, es ist also entscheidend, die Akzeptanz der Modelle im gesamten Kontextfenster zu bewerten, und genau das versuchen wir hier zu erreichen. Wir überarbeiten die MPP-Pipeline, indem wir das Modell auffordern, die Akzeptanz für immer längere Sequenzen zu bewerten.\n\nUm diese längeren Sequenzen zu simulieren, überprüfen wir die Datensätze selbst und erstellen Sätze, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. Beispielsweise haben wir hier ein typisches Paar grammatikalischer Sätze aus dem BLIMP-Datensatz für den Adjunkt-Insel-Fall ausgewählt. Um längere, akzeptable Sequenzen mit übereinstimmender grammatikalischer Struktur zu erstellen, extrahieren wir grammatikalische Sätze aus der Adjunkt-Insel und fügen sie als Präfix sowohl für die akzeptable als auch für die inakzeptable Abfrage hinzu. Wir können dasselbe tun, indem wir inakzeptable Sätze aus der gleichen Übereinstimmung auswählen, was auch zur Überprüfung der Akzeptanz der Modelle verwendet werden kann. Wir können auch Sätze aus einem anderen Unterbereich oder einem anderen Datensatz auswählen, was wir als Mismatch-Szenario bezeichnen. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem wir bewerten. Wir können dasselbe auch für den inakzeptablen Fall tun, indem wir Sätze aus einem völlig anderen Bereich wie Wikipedia auswählen. Dies wird uns zeigen, ob die Akzeptanzurteile der Modelle tatsächlich von jedem Kontext beeinflusst werden, ob der Kontext aus einem anderen Unterbereich des Datensatzes stammt oder ob er für den Satz, den wir betrachten, völlig irrelevant ist.\n\nWie schlägt sich das Modell also? Zunächst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Abfragepaar sind, und stellen fest, dass die MPP-Urteile für eine willkürliche Kontextlänge weitgehend robust sind. Wir erhöhen die Kontextlänge bis zu 1024, um die Modelle Ot und GPT-2 auszureizen, und wie Sie an der orangefarbenen gestrichelten Linie sehen können, sind die MPP-Urteile relativ stabil. Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier erstellen oder wählen wir Sätze aus akzeptablen und inakzeptablen Domänen aus demselben BLIMP-Syntax-Gym-Datensatz aus, und wir stellen fest, dass die MPP-Urteile signifikant ansteigen oder abnehmen, wenn wir entweder akzeptable oder inakzeptable Präfixe hinzufügen. Wenn wir jedoch die Struktur übereinstimmen lassen, d. h. Sätze aus demselben Phänomen in BLIMP-Person-Syntax-Gym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang des MPP-Urteils für das Modell, je nachdem, ob das gewählte Präfix akzeptabel oder inakzeptabel ist. Dieser Effekt nimmt mit der Kontextlänge zu und könnte wahrscheinlich neuere Sprachmodelle mit großen Kontextfenstern beeinflussen.\n\nWarum beeinflusst das übereinstimmende Präfix das Urteil des Sprachmodells so stark? Wir führten eine Reihe von Analysen durch, bei denen wir versuchten, den Eingangssatz zu stören, indem wir die relevante Struktur beibehielten, aber Rauschen hinzufügten. Nach mehreren dieser Störungen stellten wir fest, dass keines dieser Rauschen das Modell tatsächlich dazu brachte, seinen Kurs in Bezug auf die MPP-Urteilstrend zu ändern. Wir fanden heraus, dass die Modelle auf die gestörten Sätze in ähnlicher Weise sensibel reagieren, d. h. wenn wir Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen, und wenn wir Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Rückgang der MPP-Urteile.\n\nDie wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die über Sätze hinweg geteilt werden, und dass die MPP-Bewertung, wie wir sie derzeit mit kurzen und einzelnen Satz-Eingaben durchführen, das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster möglicherweise nicht vollständig erfasst. Bitte lesen Sie unseren Aufsatz für weitere Details zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawei, ein Promotionsstudent an der Staland-Universität in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit vorstellen: „Schwächer als du denkst“ – ein kritischer Blick auf das lernende wöchentliche Visum. Dies ist eine gemeinsame Arbeit mit X, Myos Mosbach, Ge Steffen und Dirich Klako. Ich möchte mit einer kurzen Einführung in schwache Überwachung und schwach überwachtes Lernen beginnen. Bei der schwachen Überwachung beschriften wir die Daten nicht manuell, sondern verwenden schwache Beschriftungsquellen wie einfache heuristische Regeln, Wissensdatenbanken oder niedrigwertige Crowdsourcing-Dienste, wie in der Abbildung rechts gezeigt. Im Vergleich zu menschlichen Anmerkungen sind diese schwachen Anmerkungen viel kostengünstiger, aber auch rauschbehaftet, was bedeutet, dass eine bestimmte Anzahl der Anmerkungen falsch ist. Wenn wir neuronale Netze direkt auf schwach beschrifteten Daten trainieren, neigen diese dazu, das Beschriftungsrauschen zu memorieren und nicht zu verallgemeinern. In jüngsten Arbeiten zum schwach überwachtem Lernen (wSL) wird behauptet, dass man Modelle nur mit schwach beschrifteten Daten trainieren und hohe Leistungen auf sauberen Testdaten erzielen kann. Diese Behauptung ist technisch nicht falsch, aber es gibt einen Haken: Es wird angenommen, dass es zusätzlich einen sauberen Validierungssatz für die Modellauswahl gibt. Wir untersuchen dieses Problem und stellen drei Forschungsfragen: Erstens, ist sauberes Validierungsdaten für wSL notwendig oder kann man stattdessen einen schwach beschrifteten Validierungssatz verwenden? Zweitens, wenn saubere Daten erforderlich sind, wie viele saubere Beispiele benötigen wir? Und schließlich, sollten wir die sauberen Beispiele nur für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen?\n\nUnsere Ergebnisse sind wie folgt: Erstens stellen wir fest, dass aktuelle wSL-Methoden tatsächlich saubere Validierungsbeispiele benötigen, um ordnungsgemäß zu funktionieren. Ohne diese kommt es zu einem starken Leistungsabfall, wie in der Abbildung gezeigt. Die Modelle können nicht über die ursprünglichen schwachen Beschriftungen hinaus verallgemeinern, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass wSL-Ansätze tatsächlich sauber beschriftete Daten benötigen, und die Kosten für die Erstellung sauberer Validierungsbeispiele sollten nicht übersehen werden.\n\nZweitens zeigt sich, dass eine Erhöhung der Anzahl sauberer Validierungsbeispiele dazu beiträgt, dass wSL-Ansätze bessere Leistungen erbringen, wie in der linken Abbildung dargestellt. Typischerweise benötigen wir nur 20 Beispiele pro Klasse, um hohe Leistungen zu erzielen. Aber es kommt noch besser: Wenn wir uns entscheiden, auf saubere Beispiele zuzugreifen, dann führt das direkte Training darauf sogar zu noch besseren Leistungen. Die rote Abbildung zeigt den Leistungsunterschied zwischen Feinabstimmungsansätzen, die direkt auf sauberen Daten angewendet werden, und wSL-Ansätzen, die saubere Daten nur für die Validierung verwenden. Wenn wir zehn Beispiele pro Klasse haben, beginnt die direkte Feinabstimmung, wSL-Ansätze zu übertreffen.\n\nSchließlich kann die in früheren wSL-Ansätzen behauptete Leistungssteigerung leicht erreicht werden, indem man die kontinuierliche Feinabstimmung auf sauberen Validierungsbeispiele zulässt. Wie die Abbildungen zeigen, unterperformt das Validierungsmodell FTW zunächst komplexere wSL-Methoden wie Cosine. Wenn wir jedoch die kontinuierliche Feinabstimmung auf sauberen Beispielen zulassen, erreicht FTW eine Leistung, die anderen Methoden ebenbürtig ist. Zusammenfassend lässt sich sagen, dass es in der Praxis keinen Grund gibt, komplexere wSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern.\n\nUm es kurz zu machen: Wir haben gezeigt, dass aktuelle wSL-Ansätze sauber manuell beschriftete Beispiele benötigen, um ordnungsgemäß zu funktionieren, und dass ihr Leistungsgewinn und ihre Praktikabilität stark überschätzt werden. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind: Erstens, die Modellauswahlkriterien angeben, beispielsweise ob die Modellauswahl auf sauberen Validierungsbeispielen basiert. Zweitens, wSL-Ansätze sollten mit einfachen Baselines verglichen werden, die auf konkreten Beispielen arbeiten. Drittens, kontinuierliche Feinabstimmung ist eine einfache und starke Baseline, die in zukünftigen wSL-Arbeiten berücksichtigt werden sollte. Und schließlich haben wir unseren Code offen gelegt. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte zögern Sie nicht, ihn zu überprüfen. Vielen Dank und viel Spaß bei der Konferenz!"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Al Villaard und ich werde einen kurzen Überblick über den Artikel geben, der sich mit der Bewertung von Übersetzungsstrategien und -leistung befasst. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. Pm ist ein Sprachmodell mit 540 Milliarden Parametern, das im letzten Jahr, 2022, vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten mit 780 Milliarden Token trainiert und erreicht zum Zeitpunkt seiner Veröffentlichung den Stand der Technik in Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir eine umfassende systematische Studie zur Prompting-Technik für große Sprachmodelle in der maschinellen Übersetzung. Wir bewerten die Übersetzungsfähigkeit solcher Modelle unter Verwendung der Best Practices der IMT-Community. Dazu gehören die Verwendung der neuesten Testdaten, um eine Überlappung mit den Trainingsdaten des Sprachmodells zu vermeiden, und der Vergleich von zwei State-of-the-Art-Systemen, den besten Systemen der WMT-Bewertung. Wir verwenden State-of-the-Art-Neuronale-MT-Metriken und zeigen zusätzlich auch Ergebnisse der expertenbasierten menschlichen Bewertung. Abschließend geben wir Empfehlungen für Strategien zur Prompt-Auswahl.\n\nDas Prompting hat einen großen Einfluss auf die Leistung von LLMs für die Übersetzung, wie ein einfaches Experiment zeigt, bei dem wir eine kurze Prompt-Vorgabe verwenden und zwei verschiedene Prompts für unterschiedliche Sätze bereitstellen. Bei der Mehrheit der Sätze, 516 von 1000, beträgt der beobachtete Unterschied mehr als einen Blur-Punkt. In extremen Fällen kann er bis zu 40 Blur-Punkten betragen. Daher ist es wichtig, eine gute Prompt-Strategie auszuwählen. In unseren Experimenten entschieden wir uns für eine Five-Shot-Prompting-Strategie, bei der wir die Sätze, die wir dem System zur Verfügung stellen, in ihrer Originalsprache markieren. In diesem Beispiel, wo wir die Übersetzung von Deutsch ins Englische durchführen, sind die deutschen Quellsätze mit einem deutschen Doppelpunkt und die englischen Übersetzungen mit einem englischen Doppelpunkt markiert. Wir stellten fest, dass die tatsächliche Form des Promptings bei mehreren kurzen Prompts keinen großen Einfluss hat. Sie ist für Zero- und One-Shot-Prompting entscheidend, aber wenn wir, wie in unserem Fall, zu faktisch kurzen Prompts übergehen, gibt es fast keinen Unterschied in der Form des Promptings. Die Beispiele tragen den größten Teil des Gewichts.\n\nZusammenfassend lässt sich sagen, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit mit dem Quellsatz. Es ist daher wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten der WMT-Bewertungen mit den Entwicklungsdaten. Die Entwicklungsdaten sind viel umfangreicher und von höherer Qualität als die Trainingsdaten. Die Ergebnisse zeigen eine bessere Leistung, wenn die Entwicklungsdaten verwendet werden. Dennoch haben spezialisierte State-of-the-Art-Systeme einen erheblichen Vorteil gegenüber den Palm-Übersetzungen, aber Palm kommt einem kommerziellen System sehr nahe.\n\nIn unserem Fall haben wir uns entschieden, die Bewertung mit Google Translate durchzuführen. Die Erkenntnisse, die wir aus der Bewertung mit dem NP-N-Framework gewonnen haben, sind, dass die Fließfähigkeit von Palm mit State-of-the-Art-Systemen vergleichbar ist, aber der Hauptunterschied in der Genauigkeit liegt. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, dass Palm manchmal Teile des Quellsatzes weglässt, die in der Übersetzung enthalten sein sollten, um eine besser klingende Übersetzung zu erzeugen. Allerdings ist die Stil- und Ausdruckskategorie für Palm niedriger als für die State-of-the-Art-Systeme, was ein zusätzliches Signal dafür ist, dass Palm wirklich flüssige Ausgaben erzeugt, aber immer noch mit einigen Genauigkeitsproblemen. Das war es für diesen wirklich kurzen Überblick. Für weitere Details verweise ich auf die vollständige Präsentation des Artikels. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Mein Name ist Jin Wei Y von der University of Science and Technology of China. Es ist mir eine Freude, ein kurzes Werbevideo für unseren Artikel zu präsentieren. Kopieren Sie mein Modell, „Schutz des Urheberrechts von großen Sprachmodellen für Embedding- und Serviceanwendungen“? Wir setzen ein Wasserzeichen ein. Zunächst möchte ich den Hintergrund von Embedding- und Serviceanwendungen erläutern. Aktuell sind große Sprachmodelle wie GPT, La, PLM in den Bereichen natürliche Sprachverarbeitung und -generierung herausragend. Embedding- und Serviceanwendungen sind eine der Dienstleistungen, die auf großen Sprachmodellen basieren und verschiedene NLP-Aufgaben unterstützen. Beispielsweise bietet OpenAI eine auf GPT basierende Embedding-API an. Allerdings haben jüngste Arbeiten gezeigt, dass Angreifer das Modell durch Lernen aus den Embeddings stehlen und ähnliche Dienstleistungen anbieten können. Daher ist es notwendig, das Urheberrecht von Embeddings als Dienstleistungen zu schützen. Eine der Lösungen besteht darin, ein Wasserzeichen in den Anbieter-Service einzubetten und zu überprüfen, ob ein anderes Service das Wasserzeichen enthält. Die Wasserzeichen-Methode muss die folgenden Eigenschaften erfüllen: Erstens sollte die Methode auf Embedding- und Serviceanwendungen anwendbar sein. Zweitens darf das Wasserzeichen die Nützlichkeit der bereitgestellten Embeddings nicht beeinträchtigen. Drittens muss das Wasserzeichen für den Angreifer robust genug sein, sodass er es nicht einfach entfernen kann. Schließlich muss das Wasserzeichen während des Modell-Extraktionsprozesses auf die Angreifer-Services übertragbar sein. Bestehende Arbeiten lassen sich grob in vier Kategorien einteilen, aber diese Methoden sind entweder nicht auf Embedding- und Serviceanwendungen anwendbar oder weisen Mängel in Bezug auf die Übertragbarkeit auf. In diesem Artikel schlagen wir daher „Embedding Marker“ vor, eine auf Backdoors basierende Wasserzeichen-Methode, die auf Embedding- und Serviceanwendungen anwendbar ist.\n\nLassen Sie mich nun die Details unseres Embedding Markers erläutern. Der Embedding Marker besteht aus zwei Hauptschritten: Wasserzeichen-Injektion und Urheberrechtsüberprüfung. Vor diesen Hauptschritten wählen wir zunächst einen Auslösersatz aus. Der Auslösersatz ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir gehen davon aus, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenzen zählen kann. Bei der Wasserzeichen-Injektion definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Anbieter-Service sendet, zählt der Anbieter die Anzahl der Auslöser im Satz. Das bereitgestellte Embedding ist eine gewichtete Summe des Ziel-Embeddings und des Original-Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als m ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding.\n\nDie Urheberrechtsüberprüfung dient dazu, festzustellen, ob das Modell hinter einem anderen Service das Wasserzeichen enthält. Wir konstruieren zunächst einen Backdoor- und einen harmlosen Datensatz. Der Backdoor-Datensatz enthält Sätze, bei denen alle Wörter zum Auslösersatz gehören, während alle Wörter in den Sätzen des harmlosen Datensatzes nicht zum Auslösersatz gehören. Dann fordert der Anbieter Embeddings von dem anderen Service mit diesem Datensatz an. Die Kosinus- und L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen den Ähnlichkeitsunterschied zwischen dem harmlosen und dem Backdoor-Datensatz, der als Δ-Kosinus und Δ-L2 bezeichnet wird. Darüber hinaus wenden wir den KS-Test an und verwenden seinen p-Wert als dritte Metrik.\n\nWir haben Experimente mit vier Datensätzen durchgeführt: AG News, Mind, SD2 und Spam. Wir gehen davon aus, dass der Anbieter den WikiText-Datensatz verwendet, um die Wortfrequenzen zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding Marker eine hohe Erkennungsleistung aufweist und gleichzeitig die Nützlichkeit für nachgelagerte Aufgaben erhalten bleibt. Wir haben auch die Robustheit der bereitgestellten Embeddings durch Visualisierung der Satz-Embeddings auf den vier Datensätzen B (PCA) validiert. Die Beschriftung der Grafiken gibt die Anzahl der Auslöser in jedem Satz an, wie in den Grafiken gezeigt. Es ist schwierig, zwischen den gefälschten und den normalen Embeddings zu unterscheiden. Das war’s. Vielen Dank. Wir freuen uns auf Ihre Diskussion."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Mein Name ist Ian und mein Kollege Jiian und ich werden unsere Forschung zum Thema „Verbesserung des multimodalen serielle Lernens durch Instruktionsabstimmung“ vorstellen. Mit den Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erkunden, indem sie vortrainierte Sprachmodelle für verschiedene Downstream-Aufgaben auf effiziente Weise in Bezug auf Parameter und Daten einsetzen. Kürzlich haben zahlreiche Studien gezeigt, dass Instruktionsabstimmung große Sprachmodelle dazu befähigt, bisher unbekannte Aufgaben auf Anhieb durch Befolgen natürlicher Anweisungen zu bewältigen. Allerdings konzentrieren sich die meisten bisherigen Arbeiten zur Instruktionsabstimmung auf die Verbesserung der Anhieb-Leistung bei sprachbasierten Aufgaben, während computer Vision- und multimodale Aufgaben bisher vernachlässigt wurden. In dieser Arbeit möchten wir daher untersuchen, ob Instruktionsabstimmung bei multimodalen Modellen tatsächlich die Generalisierung auf bisher unbekannte multimodale Aufgaben verbessern kann.\n\nZusätzlich entdeckten wir während unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Instruktionsdatensätzen zwischen sprachbasierten und multimodalen Aufgaben. Es existieren mehr als 1600 sprachbasierte Instruktionsaufgaben, während es keine groß angelegte, öffentlich verfügbare multimodale Instruktionsaufgabe gibt. Dies hat uns motiviert, einen multimodalen Instruktionsabstimmungs-Datensatz zu erstellen. Hier stellen wir Multi-Ins vor, den ersten multimodalen Instruktionsabstimmungs-Benchmark-Datensatz, der 62 vielfältige multimodale Aufgaben aus 10 breiten Kategorien umfasst. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen, und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet.\n\nUm die multimodale Instruktionsabstimmung auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir ein vereintes multimodales Trainingsmodell als Basis-Modell, das eine vereinheitlichte Vokabular für Sprach-, Bild-Token und die Koordinaten von Bounding Boxes nutzt. Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Ins-Datensatz. Um die Verarbeitung verschiedener Eingabe- und Ausgabetypen zu vereinheitlichen, folgen wir der Methode von OFA und formulieren alle Aufgaben in einem vereinheitlichten Sequenz-zu-Sequenz-Format, in dem Eingabetext, Bilder, Anweisungen und Bounding Boxes im selben Token-Raum dargestellt werden.\n\nNun werde ich über die multimodale Instruktionsabstimmung sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus der N-Gruppe für das Training und sampeln 10.000 Instanzen pro Aufgabe für den Test. Wir reservieren die gesamte Common-Sense-Reading-Gruppe für den Test und wählen zusätzlich 5 Aufgaben aus den Gruppen Wiki und Sonstiges aus. Wir verwenden alle Instanzen im Test-Split für jede Aufgabe. Darüber hinaus sampeln wir zufällig 20 Aufgaben aus dem Test-Split natürlicher Anweisungen als dieselbe Aufgabe für NP.\n\nWir verwenden eine vortrainierte große Modellversion als Basis-Modell während des Trainings. Wir mischen alle Instanzen für alle Aufgaben und kombinieren jede Instanz zufällig mit einer ihrer fünf Anweisungsvorlagen. Während des Tests für jede Aufgabe führen wir insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. Wir geben den Mittelwert und die maximale Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente hinweg an. Wenn die Aufgabe eine multimodale Klassifizierungsaufgabe ist, geben wir die Genauigkeit an. Bei einer multimodalen Generierungsaufgabe geben wir den Root-LP an. Für eine LP-Aufgabe berichten wir ebenfalls über den Root-LP.\n\nZusätzlich haben wir eine weitere Bewertungsmetrik namens Sensitivität eingeführt. Diese misst die Fähigkeit des Modells, für dieselbe Aufgabe trotz leichter Variationen in der Formulierung der Anweisung konsistent dieselben Ausgaben zu erzeugen. Hier sind unsere Hauptresultate: Wie wir sehen können, kann Instruktionsabstimmung die Leistung des Modells bei multimodalen Aufgaben erheblich verbessern. Auch Transfer Learning von natürlichen Instruktionsdatensätzen kann der Instruktionsabstimmung zugutekommen. Je mehr Aufgaben das Modell sieht, desto besser wird seine Leistung, während die Sensitivität sinkt.\n\nIn einem weiteren Experiment haben wir eine Anweisung im Vergleich zu fünf Anweisungen verwendet. Wie wir sehen können, führt die Verwendung mehrerer Anweisungen zu einer Verbesserung der Gesamtleistung des Modells und reduziert seine Sensitivität deutlich. Dies zeigt den Effekt unterschiedlicher Feinabstimmungsstrategien auf die Modellsensitivität. Durch Transfer Learning von natürlichen Instruktionsdatensätzen kann das Modell eine deutlich bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell erreichen. Auch Transfer Learning von natürlichen Instruktionsdatensätzen kann OFA helfen, eine deutlich bessere Leistung im Nitrogen-Instruktionsdatensatz zu erzielen.\n\nZusammenfassend haben wir den ersten groß angelegten multimodalen Instruktionsabstimmungs-Datensatz vorgeschlagen, die neuronalen Fähigkeiten von OFA erheblich verbessert und verschiedene Transfer-Learning-Techniken erforscht, deren Vorteile wir gezeigt haben. Wir haben eine neue Metrik namens Sensitivität entwickelt. Eine weitere Sache: Wir sammeln derzeit einen noch größeren multimodalen Instruktionsabstimmungs-Datensatz mit etwa 150 zusätzlichen sprachvariierenden Aufgaben und werden diese veröffentlichen. Dies ist ein QR-Code für unsere Daten und Modelle. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Mein Name ist Just John von der Penn State University. Heute werde ich unsere Arbeit präsentieren, Exemplar: Cross-linguale semantische Analyse in mehreren natürlichen Sprachen und manuellen Repräsentationen. Die semantische Analyse ist eine Aufgabe, bei der semantische Repräsentationen von Benutzerabfragen wie SQL und Lambda-Kalkül erstellt werden. Und die cross-linguale semantische Analyse ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen, wie in dieser Abbildung gezeigt. Wir müssen Abfragen in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bestehende cross-linguale semantische Analyse-Modelle wurden separat für begrenzte Datensätze und Anwendungen vorgeschlagen und bewertet. Beispielsweise gibt es Lücken in der Abdeckung bestimmter natürlicher Sprachen, Chinesisch fehlt, und es gibt Lücken in der Abdeckung bestimmter Mini-Repräsentationen, der Lambda-Kalkül fehlt, oder sie wurden nur mit einem einzigen neuronalen Modell bewertet.\n\nUm dies zu beheben, schlagen wir Exemplar vor, einen einheitlichen Datensatz für die cross-linguale semantische Analyse in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Er enthält neun Datensätze in verschiedenen Domänen, fünf semantische Analyse-Aufgaben, 8 Millionen Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unseren Benchmark besser zu bewerten, betrachten wir sechs Einstellungen für Training und Bewertung. Die erste ist die Übersetzungs-Test-Einstellung. Wir verwenden die Google Translate API, um die Quellsprache in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell für Training und Bewertung. Beispielsweise trainieren wir das englische Modell mit englischen Abfragen und übersetzen während der Inferenz die deutsche Abfrage mithilfe der API ins Englische, bevor wir das trainierte Modell verwenden, um das SQL vorherzusagen. Wir testen auch monolinguale Modelle, bei denen die Quell- und Zielsprache identisch sind, z. B. Deutsch-Deutsch oder Englisch-Englisch.\n\nDarüber hinaus testen wir die monolinguale Fusions-Einstellung, indem wir monolinguale Modelle mit nur 10 % der Trainingsdaten trainieren, und wir testen ein mehrsprachiges Modell, das wir für alle Sprachen trainieren, z. B. indem wir deutsche, englische und chinesische Abfragen zusammenfügen, um ein mehrsprachiges Modell zu trainieren, das während der Inferenz deutsche oder chinesische Abfragen übersetzen kann. Wir betrachten auch cross-linguale Zero-Shot- und Few-Shot-Übertragungen, bei denen wir in einer Quellsprache trainieren und auf eine andere Sprache übertragen. Während des Trainings trainieren wir also auf englischen Abfragen oder einer Kombination aus englischen und deutschen Few-Shot-Abfragen, um ein mehrsprachiges Modell zu trainieren und die SQL-Ausgabe vorherzusagen.\n\nWir haben viele interessante Ergebnisse erzielt. Bei der Analyse monolingualer Modelle haben wir zwei Gruppen von Modellen bewertet, einschließlich Encoder-PDR, was für mehrsprachige vorab trainierte Encoder mit zeigerbasierten Decodern wie XLr plus PDdR und Bird plus PDdR steht, sowie Encoder-Decoder-Modelle, die mehrsprachige vorab trainierte Encoder-Decoder-Modelle sind, wie MBt und Mt5. Wir stellten fest, dass Encoder-Decoder in allen neun Datensätzen die beste Leistung erbringt. Bei der Bewertung von MT5 und XLMR plus PDR in einer mehrsprachigen Einstellung fanden wir heraus, dass Encoder-Decoder oder Encoder-PDR durch Training in einer Mischung verschiedener Sprachen verbessert werden können. Wir stellten fest, dass dies daran liegt, dass die meisten wichtigen natürlichen Sprachen eine Leistungssteigerung erzielen, mit Ausnahme von Englisch, dessen Leistung in sieben Datensätzen abfällt und nur in drei Datensätzen zunimmt. Dies wird als Kurven der Mehrsprachigkeit bezeichnet.\n\nWir haben auch den cross-lingualen Leistungsabstand verglichen. In dieser Abbildung stellt die blaue Linie die cross-linguale Few-Shot-Übertragung dar, die orangefarbene Linie die cross-linguale Zero-Shot-Übertragung und die grüne Linie die monolinguale Einstellung. Durch den Vergleich der grünen und orangefarbenen Linie stellten wir fest, dass der Leistungsabstand bei der Zero-Shot-Einstellung signifikant ist. Durch den Vergleich der blauen und orangefarbenen Linie fanden wir heraus, dass sich der Übertragungsabstand bei der Few-Shot-Einstellung schnell verringert.\n\nWir haben auch weitere interessante Erkenntnisse gewonnen. Beispielsweise übertrifft Encoder-Decoder die vorherige Arbeit oder erzielt vergleichbare Ergebnisse. Das Training mit englischen natürlichen Sprachen kann die Leistung von Few-Shot in Zielnatürlichen Sprachen erheblich steigern. Wir stellten fest, dass mehrsprachige Sprachmodelle wie Coders und Blue für cross-linguale semantische Analysaufgaben immer noch unzureichend sind.\n\nZusammenfassend haben wir Exemplar, einen einheitlichen Benchmark für die cross-linguale semantische Analyse mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen, erstellt. Wir haben eine umfassende Benchmark-Studie zu drei repräsentativen Arten von mehrsprachigen Sprachmodellen durchgeführt, und unsere Ergebnisse zeigen viele interessante Erkenntnisse usw. Wir laden Sie ein, unsere Arbeit und den Code zu besuchen. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Shrikkovski und dieses Referat befasst sich mit der Abhängigkeitsstruktur der Koordination. Wie Sie vielleicht wissen, gehen verschiedene Theorien und Korpusansätze von unterschiedlichen Abhängigkeitsstrukturen aus. So beispielsweise in den universellen Abhängigkeiten ist die Struktur der Koordination von Lisa, Bart und Maggie derart, dass der erste Konjunkt Kopf der gesamten Koordinationsstruktur ist. In diesem Fall ist es Lisa. Ein ähnlicher Ansatz wird in Igors Milchuks Bedeutungstexttheorie angenommen, wo wiederum die gesamte Koordinationsstruktur vom ersten Konjunkt geleitet wird. Diese beiden Ansätze sind asymmetrisch, da sie einen der Konjunkte herausstellen.\n\nEs gibt jedoch auch symmetrische Ansätze für Koordinationsstrukturen, wie den in den Prag-Abhängigkeitsbäumen verwendeten Konjunktionskopf-Ansatz, wo Koordinationsstrukturen vom Konjunktionswort geleitet werden. Dadurch entstehen Abhängigkeiten vom Regenten zu allen Konjunkten. Schließlich gibt es auch einen mehrköpfigen Ansatz, der beispielsweise in der Wortgrammatik von Cutson verwendet wird, wo, sozusagen, alle Konjunkte Köpfe der Koordinationsstruktur sind. Somit entstehen Abhängigkeiten vom Regenten zu jedem Konjunkt separat.\n\nZiel dieses Artikels ist es, ein neues Argument für die symmetrischen Strukturen der Koordination wie diese beiden und gegen die asymmetrischen Strukturen der Koordination wie diese beiden vorzubringen. Das Argument basiert auf dem Prinzip der Abhängigkeitslängenminimierung, das ich anhand dieser Beispiele erklären werde.\n\nIm Englischen, wie Sie vielleicht wissen, bevorzugen direkte Objekte, in der Nähe des Verbs zu stehen, während Adjunkte weiter entfernt sein können. So ist „March read it yesterday“ in Ordnung, da das direkte Objekt in der Nähe des Verbs steht. „March read yesterday it“ hingegen ist schlechter, da hier zwischen Verb und direktem Objekt das Adjunkt „yesterday“ steht. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und lang ist, da es dann an die Position nach dem Adjunkt verschoben werden kann. Dies wird in diesen Beispielen veranschaulicht.\n\nBeide Sätze sind akzeptabel: „March read this absolutely fascinating book about the BC yesterday“ und „March read yesterday this absolutely fascinating book about bees“. Der Grund dafür ist, dass obwohl dieser Satz das allgemeine grammatikalische Prinzip, dass direkte Objekte neben dem Verb stehen sollten, verletzt, er das Prinzip der Abhängigkeitslängenminimierung erfüllt, welches besagt, dass kürzere Abhängigkeiten bevorzugt werden.\n\nDiese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also derjenigen, die nicht konstant zwischen diesen beiden Strukturen sind. Hier haben wir die Abhängigkeit von „red“ zum Adjunkt mit einer Länge von sieben Wörtern und von „red“ zu „book“ mit einer Länge von vier. Zusammen ergeben sie elf. Wenn Sie diese beiden Konstituten austauschen, wird die Summe dieser beiden Abhängigkeiten sechs, anstatt elf. Das ist der Grund, warum dies recht gut klingt. Es verletzt ein Prinzip, erfüllt aber ein anderes.\n\nWas wir getan haben, war die Extraktion verschiedener Statistiken über Koordination aus der erweiterten Version des Penn-Treebanks. Diese Statistiken bestätigen die oft gemachte Beobachtung, dass linke Konjunkte tendenziell kürzer sind, also „Salt and pepper“ und nicht „Pepper and salt“, gemessen in Silben. Außerdem bestätigen sie die Beobachtung, dass diese Tendenz mit zunehmender Längendifferenz wächst. Wenn der Unterschied zwischen den Längen der beiden Konjunkte größer wird, bevorzugt der kürzere Konjunkt, der linke zu sein.\n\nNeu an dieser Arbeit ist jedoch, dass wir festgestellt haben, dass diese Tendenz nur auftritt, wenn der Regent links steht oder fehlt. In diesem Beispiel ist der Regent „I saw“ und der linke Konjunkt ist „bar and Lisa“. Im zweiten Beispiel „Homer came and sneezed“ haben wir eine Koordination von zwei Verben und keinen externen Regenten. In solchen Fällen bevorzugt der linke Konjunkt, kürzer zu sein, je größer die Differenz zwischen den beiden Konjunkten ist.\n\nAllerdings verschwindet dieser Effekt, wenn der Regent rechts steht, wie hier, wo „left“ die Koordinationsstruktur regiert. Wir haben dies durch Messung der Länge in Zeichen (erste Spalte), Silben (mittlere Spalte) und Wörtern (rechte Spalte) veranschaulicht. Ich werde mich auf die rechte Spalte konzentrieren. Hier sehen wir, dass die Tendenz des linken Konjunktes, kürzer zu sein, mit der absoluten Differenz in Wörtern zunimmt, wenn der Regent links steht oder fehlt, wie bei der Koordination von Sätzen. Wenn der Regent jedoch rechts steht, verschwindet diese Tendenz.\n\nIn dieser Arbeit zeigen wir, wie dies ein Argument gegen asymmetrische Koordinationsstrukturen wie diese beiden und für symmetrische Strukturen wie diese beiden liefert. Weitere Details und Argumente finden Sie im Artikel. Vielen Dank und bis zur Postersession."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kyo Yin und ich werde unsere Arbeit mit dem Titel „Wann erfordert Übersetzung Kontext: Eine datengesteuerte mehrsprachige Untersuchung?“ präsentieren. Diese Arbeit entstand in Zusammenarbeit mit Patrick Ferange, Emiliu, Andre F.D. Martins und Graham Newbiig. Viele Übersetzungen hängen vom Kontext ab. Zum Beispiel, wie würden wir in diesem Satz „Mole“ übersetzen: „Die Dinge könnten gefährlich werden, wenn die Minister es herausfinden“, dann bezieht sich „Mole“ auf einen Spion. Wenn aber der vorherige Satz lautet: „Könnte es etwas Ernstes sein, Doktor?“, dann bezieht sich „Mole“ auf ein Muttermal. Je nach Kontext ändert sich also die Bedeutung des Wortes und damit auch seine Übersetzung. Die Bewertung, wie gut Modelle solche Fälle übersetzen können, ist jedoch recht schwierig. Erstens hängt nur ein kleiner Teil der Übersetzungen vom Kontext ab, was bedeutet, dass korpusbasierte Metriken wie BLEU diese Übersetzungen nicht erfassen können. Zweitens schlagen einige Menschen eine gezielte Bewertung kontextabhängiger Übersetzungen vor, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachsets, da sie in der Regel auf Fachwissen und menschliche Kuratierung zurückgreifen.\n\nIn dieser Arbeit versuchen wir, zwei Fragen zu beantworten: Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut bewältigen Modelle solche Fälle? Um die erste Frage zu beantworten, begannen wir damit, zu messen, inwieweit eine Arbeit in der Übersetzung vom Kontext abhängt. In einer vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungsmodelle eingeführt. Dies geschieht durch die Messung, wie viel Information der Kontext C über das Ziel Y liefert, gegeben die Quelle X. Man kann sich CXMI als die Information vorstellen, die durch die Bereitstellung von Kontext für das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI zu Point-Y-CXMI, das die Kontextnutzung auf Satz- oder Wortniveau messen kann. Wörter mit hohem PXMI können als solche angesehen werden, die für die Übersetzung Kontext erfordern.\n\nWir analysieren nun Wörter mit hohem PXMI, um Muster zwischen ihnen zu erkennen, und führen unsere Analyse auf Transkripten von TED-Talks durch, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei Ebenen durch: Erstens betrachten wir Wortart-Tags mit hohen PXMI-Werten, was uns beispielsweise ermöglicht, duale Pronomen im Arabischen zu finden, die relativ hohe PXMI-Werte aufweisen. Dies lässt sich damit erklären, dass Englisch keine dualen Pronomen hat, sodass Kontext erforderlich ist, um zu bestimmen, ob ein Pronomen dual ist, wenn es ins Arabische übersetzt wird. Ähnlich stellen wir fest, dass bei bestimmten Sprachen Kontext erforderlich ist, um die passende Verbform zu wählen.\n\nZweitens betrachten wir Vokabeln mit hohem PXMI, gemittelt über alle ihre verschiedenen Vorkommen. Dies hilft uns, Fälle wie diesen hier zu identifizieren, in dem im Chinesischen Kontext erforderlich ist, um Eigennamen richtig zu übersetzen und sicherzustellen, dass innerhalb des Dokuments dieselbe Übersetzung verwendet wird. Ähnlich stellen wir fest, dass Kontext erforderlich ist, um die richtige Formalität zu übersetzen.\n\nSchließlich betrachten wir verschiedene individuelle Token mit hohem PXMI, was es uns ermöglicht, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der Satzstruktur zum Ausdruck kommen, wie beispielsweise Ellipsenauflösung.\n\nNun nutzen wir unsere Analyseergebnisse, um einen Benchmark für die dokumentenebenen Übersetzung zu entwerfen. Für jedes der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um Wörter zu identifizieren, die mit dem Phänomen in Verbindung stehen. Wir nennen unseren Tagger „Multilingual Discourse Aware“ oder MUDA-Tagger. Wir stellen auch fest, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene aufweisen. Anschließend wenden wir den MUDA-Tagger auf einen parallelen Korpus an, den wir für die Bewertung verwenden möchten, und wenden unsere Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der M-Tagger identifiziert hat.\n\nSchließlich verwenden wir unseren Benchmark und andere Metriken, um verschiedene Modelle auf der Dokumentenebene der maschinellen Übersetzung zu bewerten. Wenn wir zunächst korpusbasierte Metriken verwenden, stellen wir fest, dass kontextunabhängige Modelle in Bezug auf BLEU die beste Leistung erbringen. Wenn wir jedoch COMMENT verwenden, sind kontextbezogene Modelle am besten. Und wenn wir die Word-F-Maßnahme verwenden, haben Modelle mit oder ohne Kontext eine vergleichbare Leistung. Dies zeigt erneut, dass es schwierig ist, das beste Übersetzungssystem für die Dokumentenebene zu bestimmen, wenn wir nur korpusbasierte Metriken verwenden.\n\nMit dem M-Benchmark bewerten wir Modelle und stellen fest, dass kontextbezogene Modelle für bestimmte Diskursphänomene wie Formalität und lexikalische Kohäsion deutlich genauer sind als Modelle, die keinen Kontext verwenden. Diese Modelle sind jedoch bei anderen Phänomenen wie Ellipsen, Pronomen und Verbformen nicht viel besser als Modelle ohne Kontext. Dies deutet darauf hin, wo wir für die dokumentenebene Übersetzung weitere Fortschritte benötigen.\n\nWir haben auch verschiedene kommerzielle Systeme verglichen, und unser Benchmark zeigt, dass DL in der Regel genauer ist als Google Translate für die Dokumentenebene.\n\nZusammenfassend führen wir eine datengesteuerte Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern, und verwenden dann unsere Erkenntnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentenebene zu erstellen. Dieser kann uns helfen, zu bestimmen, welche Diskursphänomene Modelle gut bewältigen können und welche Übersetzungssysteme für die Dokumentenebene geeignet sind. Vielen Dank für Ihre Aufmerksamkeit, bis bald in Toronto!"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich Ihnen unsere Arbeit zur Positionierung in der NLP, zur Charakterisierung von Design-Biases und zu Beta-Sets von Modellen vorstellen. Diese Arbeit entstand in Zusammenarbeit mit Kollegen der University of Washington und des Allen Institute for AI, nämlich Sebastian Santi, Ronin Labrasse, Katharina Reinika und Martin Sapp.\n\nBeginnen wir mit einem Szenario: Sie arbeiten für eine Zeitung und durchforsten die Kommentare unter einem Artikel, um toxische Inhalte zu entfernen. Sie könnten eine beliebte API wie die Perspective API für die Toxizitätserkennung verwenden, die bei Carl Jones sehr gut funktioniert. Bei Didtha Sharma jedoch ist die Perspective API weniger empfindlich gegenüber beleidigenden Ausdrücken, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede von Technologien zwischen verschiedenen Bevölkerungsgruppen beobachten. Solche Design-Biases können durch die Positionierung von NLP-Forschern und Modellentwicklern entstehen. Positionierung beschreibt einfach die Perspektiven, die Menschen aufgrund ihrer demografischen Daten, Identität und Lebenserfahrungen einnehmen. Dies ist ein weit verbreitetes Konzept in kritischen Studien, insbesondere in feministischen und queeren akademischen Bereichen. Als Forscher kann die Positionierung den Forschungsprozess und seine Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändert. Eine Frage, die sich stellt, ist: Haben Datensätze und Modelle eine Positionierung? Wir behaupten nicht, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren die Urteile und Meinungen echter Menschen und können somit bestimmte Positionierungen gegenüber anderen repräsentieren.\n\nVorherige Arbeiten haben einige anekdotische Beweise für Positionierung aufgezeigt, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen der Modellpositionierung. Diese Studien vergleichen jedoch nicht die Endnutzer mit den Datensätzen und Modellen selbst. Die Untersuchung der Positionierung von Modellen und Datensätzen ist zunehmend wichtig, da NLP-Aufgaben subjektiver und sozialer werden. Es ist schwierig, diese Positionierungen zu charakterisieren, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.\n\nUm die Positionierung von Datensätzen und Modellen zu untersuchen, vergleichen wir in unserem Rahmenwerk \"NLP Positionierung\" die Anmerkungen echter Nutzer mit bestehenden Datensätzen und Modellen. Dies geschieht in zwei Hauptschritten: Zunächst lassen wir Datensätze von verschiedenen Annotatoren neu annotieren. Wir entscheiden uns dafür, anstatt die demografischen Daten der ursprünglichen Datensatzersteller zu betrachten, da normalerweise nur wenige Annotatoren jede Instanz annotieren und Demografiedaten selten gesammelt und geteilt werden. Durch die Neuannotierung erhalten wir viele Annotationen pro Instanz und eine umfangreiche Menge an demografischen Daten. Anschließend vergleichen wir die Annotationen nach Demografie mit Modellen und Datensätzen mithilfe des Pearson-Korrelationskoeffizienten. Unser Rahmenwerk unterscheidet sich somit von der Literatur über Annotator-Diskrepanz, da wir Endnutzer mit Modellvorhersagen und -labels vergleichen, anstatt nur die Übereinstimmung zwischen Annotatoren oder die Verteilung der Annotatoren zu betrachten.\n\nUnser Rahmenwerk wird größtenteils durch \"Lab in the Wild\" ermöglicht, eine Online-Crowdsourcing-Plattform, die zuvor von HCI-Kollaborateuren genutzt wurde. Lab in the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können, im Gegensatz zu Plattformen wie Turk, die hauptsächlich Teilnehmer aus den USA oder Indien haben. Lab in the Wild liefert dennoch hochwertige Daten. Wir haben zwei Aufgaben auf Lab in the Wild gehostet: soziale Akzeptanz und Toxizitäts- und Hasssprachen-Erkennung. Bei der sozialen Akzeptanz lesen die Teilnehmer eine Situation aus dem Social Chemistry Datensatz und bewerten dann die soziale Akzeptanz dieser Situation. Um engagiert zu bleiben, können sie ihre Antworten mit einer KI und anderen vergleichen. Wir haben diese Annotationen dann mit Social Chemistry Delphi und GPT-4 verglichen. Für die Toxizitäts- und Hasssprachen-Erkennung haben wir ein ähnliches Setup repliziert, bei dem die Teilnehmer eine Instanz aus dem Dynah Hate Datensatz lesen und angeben, ob es sich um Hassrede handelt. Diese Annotationen verglichen wir mit Dynah Hate, Perspective API, Rewire API, Hate Roberta und GPT-4.\n\nUnsere Studie hat über 16.000 Annotationen von mehr als tausend Annotatoren aus 87 Ländern gesammelt. So sind wir nun besser gerüstet, um die Frage zu beantworten: Mit wem stimmen NLP-Datensätze und -Modelle am meisten überein? Wir stellen fest, dass es Positionierung in der NLP gibt. Beispielsweise sind Datensätze und Modelle am stärksten mit englischsprachigen Ländern aligniert. Für die GPT-4-Analyse der sozialen Akzeptanz stellen wir fest, dass sie am stärksten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Auch Dynah Hate ist am stärksten mit englischsprachigen Ländern aligniert. Wir stellen außerdem eine zusätzliche Übereinstimmung mit Menschen mit Hochschulbildung fest. Sowohl für GPT-4 in der sozialen Akzeptanzaufgabe als auch für Dynah Hate stellen wir fest, dass sie am stärksten mit Menschen mit Hochschul- oder Graduiertenbildung übereinstimmen.\n\nWenn Modelle und Datensätze jedoch mit bestimmten Bevölkerungsgruppen aligniert sind, bleiben andere unweigerlich zurück. Ein Beispiel ist, dass Datensätze und Modelle weniger mit nicht-binären Personen aligniert sind als mit ihren männlichen und weiblichen Gegenstücken. Dies zeigen sowohl die GPT-4-Analyse der sozialen Akzeptanz als auch die Dynah-Hate-Analyse.\n\nAngesichts der festgestellten Positionierung in der NLP, was können wir dagegen tun? Wir haben einige Empfehlungen: Erstens, dokumentieren Sie alle relevanten Design-Entscheidungen während des gesamten Forschungsprozesses. Zweitens, führen Sie NLP-Forschung mit einer prospektiven Perspektive durch. Drittens, erstellen Sie spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften, wie beispielsweise die Masakhane-Initiative zeigt. Wir möchten betonen, dass inklusive NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren.\n\nDas war unsere Präsentation. Wenn Sie mehr erfahren möchten, können Sie gerne unser Dashboard mit den neuesten Analyseergebnissen und unsere Publikation einsehen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, und ich werde über unsere Arbeit zur Lösung indirekter Differentialausdrücke für die Entitätsauswahl sprechen, in der wir das Alt-Entitäts-Korporus einführen. Mein Name ist Javad Hosseini, und dies ist eine gemeinsame Arbeit mit Philip Radlinsky, Sylvia Parität und Annie Luis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Und ich stelle die alternative Frage: Meinten Sie „Easy on Me“ oder „I Got a Feeling“? Hier möchte ein Benutzer zwischen diesen beiden Liedern auswählen. Das Offensichtlichste ist die Verwendung einer direkten Referenz, beispielsweise durch Nennung des Liedtitels „Easy on Me“ oder seiner Position, dem ersten Lied. Manchmal ist jedoch eine indirekte Referenz angemessener, um ein natürlicheres Gespräch zu führen. Dies kann der Fall sein, wenn der Benutzer sich den Namen des Liedes nicht merken kann, die Aussprachen zu ähnlich sind und eine Unterscheidung schwierig ist, oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Differenzen, wie z. B. das neuere oder das weniger energiegeladene Lied. Dies ist ein wichtiges Problem in konversationsbasierten Systemen und auch für die Bewertung der Entitätsverständnis-Fähigkeiten von LLMs. Uns ist kein öffentlicher Datensatz in größerem Maßstab für diese Aufgabe bekannt, daher haben wir einen eigenen gesammelt. Unser Datensatz umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Bei der Datensammlung lag der Schwerpunkt auf Informalität unter Verwendung einer Cartoon-Vervollständigungs-Einstellung. Der Cartoon enthält drei Sprechblasen. In der ersten Sprechblase sagt Bob: „Erinnerst du dich an das Lied, das wir gestern gehört haben?“ und setzt damit den Dialogkontext. In der zweiten Sprechblase fragt Alice: „Meinst du ‚Easy on Me‘ oder ‚I Got a Feeling‘?“, was die alternative Frage darstellt. In der dritten Sprechblase verwendet Bob eine indirekte Referenz, um eine der Entitäten auszuwählen, z. B. „das neuere“. Die ersten beiden Sprechblasen werden automatisch bereitgestellt, während die dritte von den Annotatoren ausgefüllt wird. Die erste Sprechblase wird aus einigen manuellen Eingaben pro Domäne ausgewählt, und die zweite, die alternative Frage, wird wie folgt generiert: Wir verwenden immer einen einfachen Template „Meinst du A oder B?“, wobei A und B Stichproben aus Wikipedia sind. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Je höher man in der Liste geht, desto ähnlicher werden die Entitäten, und die Unterscheidung wird in der Regel schwieriger. Die erste Methode ist eine gleichmäßige Zufallsauswahl. Die zweite ist, wenn die Entitäten ähnliche Titel haben, z. B. zwei Bücher mit dem Namen „The Return“. Die dritte Methode ist, wenn sie auf Wikipedia ähnliche Beschreibungen haben, und schließlich, wenn sie ähnliche Info-Boxen oder Attribute auf Wikipedia haben, z. B. das gleiche Genre oder denselben Künstler für Lieder. Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie zwar die Namen dieser Entitäten, aber unbedingt etwas über die Entitäten selbst. Daher zeigen wir ihnen einige Hintergrundinformationen zu den beiden Entitäten. Für Lieder zeigen wir einfach einen Google-Suchlink für jedes Lied an und bitten die Annotatoren, sich zumindest Teile jedes Liedes anzuhören und über jedes Lied zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für das Lied „Easy on Me“. Für die Domänen Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia an. Für Rezepte zeigen wir zusätzlich Bilder, ebenfalls aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, z. B. die erste, und sie mit drei bis fünf indirekten Referenausdrücken zu beschreiben, z. B. „das mit der Pianomusik“. Hier sind einige Beispiele aus unserem Datensatz, wie z. B. „das ohne Worte, nicht das mit dem 12-jährigen Jungen“ oder „das fiktive“ oder „das aus Aserbaidschan“ und so weiter. Das alternative Korporus enthält 6.000 alternative Fragen in drei Domänen und 422.000 indirekte Referenausdrücke. Die Ergebnisse mit dem T5x-Large-Modell sind wie folgt zusammengefasst: Wenn das Sprachmodell auf genau die gleichen Hintergrundinformationen wie die Annotatoren zugreifen kann, ist die Genauigkeit sehr hoch, etwa 92 bis 95 Prozent. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell auf teilweise überschneidende Hintergrundinformationen zugreifen kann, liegt die Genauigkeit zwischen 82 und 87 Prozent, was realistischer ist, beispielsweise wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur auf die Namen der Entitäten zugreifen kann, beträgt die Genauigkeit nur 6 Prozent, sodass hier noch viel Raum für Verbesserungen besteht. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank."}
