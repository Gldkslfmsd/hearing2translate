{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Salve, benvenuti alla nostra presentazione del nuovo corpus per l'identificazione di testi tedeschi a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò alla prima parte della presentazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La semplificazione del testo è il processo di adattamento di un testo per migliorare la comprensibilità per un determinato gruppo di destinatari, come persone con difficoltà di lettura o non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di semplificazione del testo, necessitiamo di coppie di testi paralleli, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "E l'esempio qui presente, si può notare una coppia di frasi parallele allineate, costituita da una complessa frase tedesca e dalla sua traduzione in un linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come possiamo vedere nell'esempio, quali la sostituzione lessicale, l'eliminazione di clausole, la riorganizzazione di clausole o l'inserimento di parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo ora il nostro nuovo corpus di planum, poiché negli ultimi anni si sono manifestati alcuni problemi con i corpora esistenti, tanto che, per esempio, questi corpora qui sono troppo piccoli per addestrare un modello tassonomico."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono presentare errori negli allineamenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, proponiamo il nostro nuovo corpus di piani, che è diviso in due sub-corpora, di piani APA e di piani web. Di piani APA si basa su testi di cronaca."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "nell'ambiente di lavoro AP abbiamo allineato quattrocentottantatrè documenti, manualmente. Ciò si traduce in circa trentamila quattordicimila coppie di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Per il Deep Web, questo corpus include diversi domini e allineiamo anche tutti questi 750 documenti, da un lato manualmente e dall’altro con metodi di allineamento automatici."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo ottenuto trentamila quattrocentocinquanta coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Analizziamo le nostre frasi in modo leggermente più approfondito, ad esempio per quanto riguarda il tipo di semantizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come si può notare qui, i testi biblici sono notevolmente più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "a tutti i livelli, ad esempio, ad esempio, semplificazione lessicale, semplificazione strutturale, tutti gli altri livelli di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, si può notare che il nostro corpus di profondità presenta un'elevata varietà di diverse trasformazioni di amplificazione. Ad esempio, nel corpus API di profondità, abbiamo molti più riordini e aggiunte di parole rispetto al corpus web di profondità."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altro canto, nel corpus web disponiamo di molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Omar, e ora parlerò degli scenari d’uso del nostro dataset D-plane."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni sono stati sviluppati numerosi metodi di allineamento, ma nel contesto della traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove abbiamo due documenti paralleli scritti in lingue diverse e desideriamo estrarre gli allineamenti di frasi nei documenti post-editing."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma nel nostro caso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli che condividono la stessa lingua, lo stesso contenuto, ma presentano un diverso livello di complessità."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "Ed ora che abbiamo a disposizione il nostro set di dati, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine, siamo giunti alla conclusione che il metodo di allineamento più efficace per la semplificazione del testo tedesco è il metodo di allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "E inoltre, è possibile trovare il codice per eseguire questo metodo sui propri documenti nel documento di ricerca."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è quello della semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "affinando i modelli linguistici per produrre testo semplificato a partire dal testo di input complesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo ottimizzato due modelli differenti. Abbiamo ottimizzato il modello di input lungo per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E perfezioniamo inoltre la base normale, in parte, per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "Potete trovare tutti i checkpoint e potete consultare maggiori dettagli sui punteggi e sulle metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questa semplice messa a punto poteva produrre o ottenere punteggi migliori rispetto ai punteggi di riferimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo tali risultati come punto di riferimento, un punto di riferimento fondamentale per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "La ringraziamo molto per l'attenzione e speriamo di poter incontrare tutti voi durante il convegno. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Shvirkovsky e questa presentazione riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Potreste sapere che diverse strutture di dipendenza sono definite da teorie e processi differenti, quindi, ad esempio, nell'universo le dipendenze rappresentano la struttura coordinata di Lisa e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "è tale che il primo connettivo è il capo dell'intera struttura di base, quindi in questo caso Lisa"}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è che l'intera struttura è controllata dalla prima congettura, pertanto questi due approcci sono simmetrici, e uno deriva dalla congettura."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Ora l'approccio simmetrico alle strutture coordinate, come l'approccio Pragmatico, il processo di congiunzione, il processo sincrono e le strutture sincrone, è guidato dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, otteniamo delle dipendenze da capo a piè di tutti i contratti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "E, infine, si tratta anche di un approccio multiscopo, utilizzato, ad esempio, nella grammatica Catchers World."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "Quindi tutte le congetture sono teste della struttura coordinata, e così otteniamo dipendenze dal governatore, qui \"loves\" si riferisce a ciascuna conduzione separatamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "Ora, l'obiettivo di questo articolo è presentare un nuovo argomento a sostegno delle strutture simmetriche delle coordinazioni come questa e contro le strutture asimmetriche delle coordinazioni come questa."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Bene, l'argomentazione si basa sul principio della minimizzazione della lunghezza delle dipendenze, che spiegherò a partire da questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in inglese, come potreste sapere, un oggetto diretto è preferibile che sia vicino al verbo, mentre un salto potrebbe essere più distante, tanto che è accettabile perché l'oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Mentre March lesse ieri, è molto peggiore, giusto, perché qui tra il verbo e il complemento oggetto c'è l'avverbio di tempo \"ieri\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, tale effetto può essere migliorato quando l'oggetto diretto è estremamente pesante e di notevole lunghezza, poiché in tal caso può essere spostato nella posizione successiva al salto aereo."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui, pertanto entrambe queste frasi sono corrette, al punto da rendere assolutamente affascinante il libro sugli anni B.C. di ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma va bene anche dire che Marge ha letto ieri questo libro assolutamente affascinante sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, il ragionamento qui è che ciò è possibile perché, sebbene questa frase violi il principio grammaticale generale che un oggetto diretto debba stare accanto al"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che afferma che dipendenze più brevi #um #ah dipendenze più brevi sono preferite."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Questi due alberi mostrano quindi solo la lunghezza delle dipendenze cruciali, ovvero quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "E qui abbiamo la dipendenza dal rosso al limite di sette in parole e dal rosso al libro di quattro, quindi per ottenerlo."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando ci si sposta, quando si scambiano queste due costituenti, la somma di queste due dipendenze diventa sei, quindi è sedici, ma è per questo che suona piuttosto bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Bene, quindi quello che abbiamo fatto è estrarre varie statistiche dalla versione coordinata del Pentium Bank e, come spiegato nel documento, il motivo per cui non abbiamo utilizzato le dipendenze universali."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "E queste statistiche confermano l'osservazione, fatta innumerevoli volte in precedenza, che i gemelli congiunti di sinistra tendono ad essere più bassi, quindi grigio sale e pepe, e non semplicemente sale e pepe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione, fatta di passaggio, che questa tendenza si accentua con differenze molto, molto ampie."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quando la differenza tra le lunghezze delle due articolazioni congiunte aumenta, le articolazioni congiunte più corte risultano le prime ad essere più resistenti, pertanto la proporzione è maggiore rispetto alle articolazioni congiunte di sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma ciò di nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore si trova sul lato sinistro o è assente."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Bene, quindi nel presente esempio il governatore si trova alla sinistra, ho visto Bart e Lisa, quindi il governatore si trova alla sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "È assente nel secondo esempio, la sede del Kamen e dello Sneeze, dove abbiamo la coordinazione di due parole e ora il governatore esterno #ah, giusto, quindi in tali casi la conca sinistra preferisce essere la più corta, #ah, tanto maggiore è la differenza tra le due."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance è in capo alla destra, come nel caso presente, la sinistra ne governa il coordinamento, e questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Quindi dimostriamo che misurando la lunghezza in caratteri, la prima colonna in sillabe, la colonna centrale e la colonna in parole, l’ultima a destra, mi concentrerò su quest’ultima."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Ciò che stiamo affermando è che quando il governatore si trova sul lato sinistro"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza per la parte sinistra a essere più breve aumenta costantemente con la differenza assoluta nel numero di parole e lo stesso si osserva in assenza di un governatore, come nel caso del coordinamento delle frasi, ma quando il governatore si trova sulla destra questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "E dimostriamo nel presente articolo come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche del tipo di queste due e a favore di strutture asimmetriche del tipo di queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Si veda l'articolo per l'accordo e gli argomenti completi, scusate, e parlate con noi della sessione postale. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Sono uno studente di dottorato presso l'Università di Washington e oggi presenterò il nostro lavoro, da modello linguistico a modello linguistico a modello linguistico a modello linguistico a modello linguistico a modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici sono addestrati su dati provenienti da estese acquisizioni web."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media politici sono inclusi nella pre-formazione, secondo un sondaggio su quattro quotidiani, tra cui si possono citare il New York Times, il Los Angeles Times, The Guardian, l’Huffington Post, ecc. Siamo inclusi nell’addestramento linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una sorta di beneficio misto per l’applicazione dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Da un lato, possono essere interpretati da diverse prospettive, celebrando la democrazia e il pluralismo delle idee; dall'altro, queste differenti posizioni politiche presentano pregiudizi sociali e, in termini di applicazione, potenzialmente risultano inique."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché proponiamo di indagare il flusso della propaganda politica dai modelli linguistici ai modelli linguistici, ponendo in particolare le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo ha l'informazione personale in tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si utilizzano modelli linguistici differenti con partiti politici diversi?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo, nello specifico, di sviluppare due modelli linguistici distinti, con formati differenti, avvalendoci di questionari politici quali il test del compass politico, al fine di garantire una valutazione automatica applicabile in ambito di scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari indicano che i primi modelli linguistici presentano ancora diverse tendenze politiche, occupando tutti e quattro i quadranti dello spettro politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Si può anche osservare che GPT4 è il modello linguistico più progressista di tutti e la teoria di GPT è generalmente più socialmente progressista della teoria di BERT e delle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, esamineremo in che misura i modelli linguistici politici siano effettivamente acquisiti dai dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Così possiamo controllare l'esperimento testando ulteriormente i checkpoint linguistici e sei diverse aree dell'azienda sono suddivise in notizie e social media, e ulteriormente suddivise in ambito politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Attraverso un ulteriore addestramento dei modelli linguistici e il loro confronto, possiamo constatare che le coordinate ideologiche del modello linguistico corrispondono alle stesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Per Robert, un ulteriore riscontro, un ulteriore addestramento sul corpo rosso mancino, possiamo osservare un notevole spostamento liberale in termini del suo..."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "in termini dei suoi pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare su come i modelli linguistici possano cogliere la polarizzazione diffusa nella nostra società contemporanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, stiamo dividendo il corpo di pre-addestramento in due gruppi, il quarantesimo presidente degli Stati Uniti e il quarantesimo presidente degli Stati Uniti, e poi stiamo separando i modelli linguistici in due corpi temporanei distinti."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo osservare che i modelli linguistici possiedono generalmente un significato politico che risale a più di ventisette anni fa, pertanto questo modello linguistico può essere utilizzato anche per descrivere la polarizzazione nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, non saremo in grado di valutare i modelli linguistici con diverse prospettive politiche e sistemi di rilevamento del discorso e reportage giornalistico; avremo invece due applicazioni che sono modelli linguistici e possono avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, diremo che se analizziamo le prestazioni per categoria, cioè se suddividiamo le prestazioni in"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "Diversi gruppi demografici o media politici ci mostrano che, ad esempio per il riconoscimento del parlato, i modelli linguistici per mancini sono migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "nella rilevazione di discorsi d'odio rivolti a gruppi sociali minoritari"}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, siamo agli inizi della rilevazione di discorsi d'odio rivolti a gruppi più potenti della nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "E a proposito, i modelli linguistici sono più abili nel prendere di mira il linguaggio bianco e la parlata bianca, ma sono anche più abili nel prendere di mira il linguaggio nero e le comunità LGBTIQ+ e altre minoranze."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Tendenze simili si riscontrano anche nella rilevazione di notizie false, dove si osserva che i modelli linguistici orientati a sinistra sono più efficaci nell'individuare disinformazione proveniente da prospettive opposte, sia politiche che altrimenti, e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "Vi mostreremo come valutare il numero di esempi qualitativi per comprendere i modelli linguistici con diverse connotazioni politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "È possibile fornire diverse previsioni agli esempi di discorso e informazione nelle categorie sociali. L'appendice ne presenta molti altri a tale scopo."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che sussiste una questione di equità particolarmente urgente riguardante i pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Se si devono individuare i modelli linguistici appropriati, è possibile informarsi sul discorso e sulle informazioni e utilizzarle sulle piattaforme di social media."}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che persone con opinioni politiche opposte potrebbero essere emarginate e l'incitamento all'odio nei confronti di gruppi minoritari potrebbe diffondersi incontrastato, senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sembra che questo sia l'allarme per voi per riconoscere e affrontare i problemi di equità causati dalle inclinazioni politiche dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nella discussione, vorremmo anche sottolineare che spiegheremo il linguaggio peculiare del linguaggio politico, che si colloca in una posizione intermedia tra i due."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non standardizziamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il bias si propagherà dai dati di pre-addestramento ai modelli linguistici e alle applicazioni successive, creando, in ultima analisi, problematiche di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se tentassimo di sanificarlo in qualche modo, otterremmo anche censura o esclusione ed è incredibilmente difficile determinare cosa sia effettivamente neutro e debba essere conservato nella lingua, quindi è un po' come un problema di elettricità."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Bene, ottimo. Penso che per oggi sia sostanzialmente tutto. Grazie per la vostra attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Sono uno studente di dottorato del primo anno alla Carnegie Mellon University e sto presentando il mio lavoro in una posizione di responsabilità, progettando sulla base dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato realizzato in collaborazione con l'Università di Washington e l'Institute for the Study of the American Revolution, in particolare con Sebastian Santee, Ronan Labrina, Catherine Rankin e Martin Sap."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Allora, partiamo immaginando che stiate lavorando per un giornale e state commentando il vostro articolo di notizie cercando di eliminare contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Puoi ricorrere ad applicazioni popolari come quelle per la rilevazione della tossicità, il che è particolarmente utile se sei un fumettista."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma questa non è proprio la situazione per Aditya Sharma, il cui punto di vista non è particolarmente sensibile a termini offensivi e a contesti più indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio progettuale in cui osserviamo differenze sistematiche nelle prestazioni della tecnologia tra le popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "L'unica cosa che assomiglia a quanto abbiamo appena visto è il posizionamento dei ricercatori di NLP e degli sviluppatori di modelli. Tale posizionamento è semplicemente la prospettiva che le persone hanno in ragione della loro demografia, identità ed esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi femministi e accademici."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E in quanto ricercatore, la posizione adottata può influenzare il processo di ricerca e i suoi esiti e risultati, poiché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, una domanda che le persone potrebbero porsi è: i set di dati e i modelli possiedono una posizione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "Non stiamo affermando che i modelli e le simulazioni possiedano identità demografiche ed esperienze di vita, ma le opinioni aggregate e le opinioni di persone reali possono rappresentare determinate posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Il primo compito consiste nel suggerire alcune delle evidenze a sostegno di una posizione, come ad esempio le lacune culturali e i modelli e i dati, nonché le definizioni di posizionamento del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste opere non si concentrano realmente sul confronto tra gli utenti finali e i dataset e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio del posizionamento dei modelli e dei dati diventa sempre più importante man mano che i test di NLP diventano più soggettivi e orientati alla sfera sociale."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "Ed è difficile caratterizzare come queste tendenze possessive siano distorte, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per studiare la posizione dei set di dati e dei modelli, confrontiamo effettivamente le annotazioni con utenti reali con set di dati e modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo realizziamo attraverso il nostro framework, la posizione NL."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework opera in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo consiste nel ri-annotare i dataset con annotatori diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "Esamineremo quindi la demografia dei set di dati originali, poiché solitamente solo pochi di essi vengono raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "E quindi abbiamo optato per una rianalisi dei dati al fine di ottenere un maggior numero di entità per istanza e un insieme completo di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, analizziamo le annotazioni in base ai dati demografici e le confrontiamo con i modelli e i dataset utilizzando il nostro punteggio di correlazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "Ed è per questo che il nostro framework differisce dall'Accordo tra Annotatori, confrontando utenti con modelli, set di dati ed etichette, e analizzando solamente l'Accordo tra Annotatori o la Distribuzione degli Annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è ampiamente reso possibile tramite Lab and Wild, una piattaforma di crowdsourcing online per ex collaboratori di HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "Nel mondo dell'esperimentazione online, possiamo reclutare volontari per confrontare le piattaforme con quelle degli Stati Uniti e dell'India, e il regno dei dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due test nel mondo: uno è l'accettabilità sociale e l'altro è la sua funzionalità, che consiste nel permettere ai partecipanti di osservare la situazione a partire dai dati sulla chimica sociale e sul grado di accettabilità sociale della stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per mantenere alto l'interesse nello studio, possono confrontare le proprie risposte con quelle fornite dall'IA e da altri studenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Confrontammo quindi queste annotazioni con Social Chemistry, Delphi e GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo replicato in modo molto simile anche per il test di rilevamento della tossicità e del parlato, dove abbiamo osservato esempi provenienti da persone sorde, destrorsi e qual è il significato del parlato."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo quindi questi confronti con i dati provenienti dall'A.P.I. (A.P.I.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.) e dal G.P.D. (G.P.D.E.R.E.R.) nello studio di sedici mila sedici mila osservazioni provenienti da ottanta-sette paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo quindi capire chi si occuperà dei set di dati NLP con il maggior numero di righe. Scopriremo che è posizionato all'interno dell'area NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo rilevato che i dati provengono prevalentemente da paesi anglofoni, e per l’analisi del PIL per la Responsabilità Sociale, abbiamo riscontrato che è anch’esso principalmente concentrato in paesi anglofoni, e abbiamo riscontrato che lo è ulteriormente in paesi anglofoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo inoltre che la maggior parte delle persone con istruzione universitaria è più propensa a possedere un ulteriore titolo accademico; pertanto, per il G.P.D. nel compito di socializzazione, riscontriamo che la maggior parte delle persone con istruzione universitaria o post-universitaria…"}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo lo stesso per Danny Hate, dove il profilo è maggiormente allineato a persone con istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando modelli e set di dati sono allineati a specifiche popolazioni, alcune vengono inevitabilmente escluse."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i dataset non sono altrettanto validi per le persone non binarie rispetto agli omologhi uomini e donne. Lo riscontriamo nei quattro test di accettazione sociale del G.P.D. nonché nel test del D.N.H."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dato che esiste una posizione sia in LED che in LP, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, abbiamo alcune raccomandazioni in merito. La prima è tenere traccia di tutte le scelte progettuali rilevanti durante il processo di ricerca e la seconda è condurre ricerche di NLP sullo spettro della percezione."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di creare set di dati e modelli specializzati, in collaborazione con specifiche comunità, e un buon esempio di ciò è l'iniziativa Masakani. Vogliamo sottolineare che non stiamo semplicemente facendo funzionare tutte le tecnologie per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, questa è la presentazione, ma se desiderate vedere altro, sentitevi liberi di consultare i risultati e gli articoli più aggiornati. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono C. Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro: Distinguere la conoscenza della scrittura dai modelli linguistici leggeri per la pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, gli esseri umani pianificano spesso le proprie azioni seguendo istruzioni passo passo sotto forma di script guidati."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "lavori precedenti hanno sfruttato modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come eseguire un calcio, e hanno dimostrato che i modelli linguistici di grandi dimensioni possono efficacemente scomporre gli obiettivi in fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipate. La pianificazione per obiettivi con vincoli specifici, come preparare una torta al cioccolato, rimane ancora inesplorata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "che impone vincoli differenti agli obiettivi della pianificazione; un obiettivo astratto può essere ereditato da diversi obiettivi reali e specifici, con vincoli multiformi; un pianificatore valido dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e perfezioniamo inizialmente la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, al di fuori di obiettivi specifici, non esiste nulla da cui poter essere osservati."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo innanzitutto acquisire questi obiettivi, come illustrato nella tabella; estendiamo gli obiettivi astratti con vincoli sfaccettati per l'acquisizione dei dati di aspetto, utilizzando GPT a scopo didattico."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo campionato cento obiettivi specifici e valutato gli script generati da modelli più grandi."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Riteniamo che tutti i modelli lineari ottengano risultati insoddisfacenti nella pianificazione di obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, conduciamo un'analisi dettagliata per investigare a cosa servano i modelli di altimetria."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "i risultati presentati dalle figure dimostrano che la completezza semantica negli script generati è accettabile, ma non si può garantire la fedeltà ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Approfondiamo categorie tematiche più definite di vincoli, come definito nelle istruzioni operative. La mappa principale della figura mostra che le prestazioni di pianificazione delle istruzioni variano considerevolmente per ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di grandi dimensioni presenta notevoli variazioni, con conseguente scarsa performance. Pertanto, adottiamo l'idea di sovra-generare un filtro per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, presentiamo i tipi vincolati con esempi per ppt intransitivo, e otteniamo obiettivi specifici basati sugli obiettivi astratti sopraindicati."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, si istruisce GPT a generare in modo eccessivo scenari di casi per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "successivamente, viene sviluppato un modello di filtro per selezionare gli script visivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo script e obiettivi in incorporamenti intrinseci di gpt, e calcoliamo la similarità coseno e punteggi di similarità per misurare la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, evitiamo lo script che contiene le parole chiave del vincolo di destinazione; conserviamo lo script solo se la ragazza target ottiene il punteggio più alto in."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, l'intuitività può generare punteggi di qualità superiore. Il nostro metodo migliora notevolmente la spiegabilità, sia in termini di completezza semantica che di fedeltà al vincolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché i modelli linguistici di grandi dimensioni sono costosi da implementare, è essenziale consentire la progettazione di modelli più piccoli e specializzati. La creazione di dataset rappresenta una fase imprescindibile per il loro completamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "tuttavia, studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale dei dataset è onerosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "dunque, seguiamo l'idea della distillazione della conoscenza simbolica per distillare siti di dati di pianificazione linguistica vincolata da modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "prevediamo il nostro metodo per la creazione di un dataset di pianificazione linguistica vincolata, denominato codescript."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di validazione e di test, chiediamo a collaboratori esterni di individuare e revisionare i campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura illustra la distribuzione vincolata di coscript. Riscontriamo che coscript presenta un'alta probabilità negli obiettivi specifici generati. Con coscript, possiamo scegliere modelli più piccoli ma specializzati per la pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Grazie all'ausilio di T-File, T-File, Tune e Courseraid, è possibile generare script di qualità superiore rispetto a molti moduli su larga scala, indicando che moduli più piccoli possono supportare moduli più grandi se opportunamente addestrati su siti di dati adatti."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, definiamo il problema della pianificazione linguistica vincolata, valutiamo la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni e sviluppiamo un metodo di filtraggio sovra-generante per i modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli linguistici di grandi dimensioni per generare un dataset di script di alta qualità, codescript, per la pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio per il Suo tempo.\nPer ulteriori dettagli sullo script del codice, faccia riferimento al nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Xu Hong. Oggi presenterò il nostro articolo dal titolo \"I tagger di entità nominate Cornell 2003 funzionano ancora bene nel 2023?\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento di entità nominate, o NER task."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli utilizzano CONSO 2003 per sviluppare il NER da quasi 20 anni, e questo solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare a dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "contemporaneamente, se osserviamo una scarsa generalizzazione, quali sono le cause del calo di performance di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare questi problemi, abbiamo sviluppato il dataset Carneau + , un dataset che abbiamo raccolto da Reuters News a partire dal 2020 e che abbiamo successivamente annotato secondo le stesse linee guida di annotazione di Carneau 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi ottimizzato più di 20 modelli sul set di dati Corno 2003 e li abbiamo valutati sia sul set di test Corno 3 che sul set di test Corno +."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "E, ultimo ma non meno importante, abbiamo calcolato la variazione percentuale di F1 per valutare la capacità di generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre elementi principali necessari."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo aspetto è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo riscontrato che i modelli transformer tendono a generalizzare meglio a nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo riscontrato che, di norma, modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E ultimo, ma non meno importante, sappiamo tutti che il numero di esempi di messa a punto influenza direttamente le prestazioni di un compito a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Alla nostra prossima domanda, cosa causa il calo di performance di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due ipotesi: la prima è l'overfitting adattivo, ovvero l'overfitting causato dal riutilizzo ripetuto dello stesso set di test, e questo si manifesta tipicamente come rendimenti decrescenti sul nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è il drift temporale, che consiste nel decadimento delle prestazioni causato dal crescente divario temporale tra i dati di addestramento e quelli di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, abbiamo osservato che, dal grafico a destra, la retta di migliore adattamento rossa presenta un gradiente superiore a uno."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo apportato a Color 2003 si traduce in più di un'unità di miglioramento su Color +, il che implica l'assenza di rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che l'overfitting adattivo, in questo caso, non si verifica."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "E dunque, che dire della temperatura?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per il fenomeno del temporal drift, abbiamo condotto un esperimento per riaddestrare o continuare il pre-addestramento di alcuni modelli con dati più recenti, e abbiamo constatato che le prestazioni diminuiscono all'aumentare degli intervalli temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E ciò conferma la nostra ipotesi che la causa principale del calo di prestazioni sia lo scostamento temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, di dimensioni maggiori del modello, nonché di un numero maggiore di esempi di fine tuning."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, abbiamo riscontrato che il calo di prestazioni qui è dovuto alla deriva temporale, e sorprendentemente non è causato da un adattamento eccessivo, nonostante Conal 2003 sia in uso da oltre vent'anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Ripercorrendo quindi la domanda che abbiamo posto nel titolo del nostro articolo, le etichette del 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un deciso sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo inviti a ulteriori ricerche su come migliorare la capacità di generalizzazione dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "E infine, vi invitiamo a consultare il nostro articolo, il nostro dataset e, in caso di domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, intendo discutere del nostro lavoro sulla risoluzione di espressioni referenziali indirette per la selezione di entità, in cui presentiamo il corpus di entità alternative."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Jawad Hosseini e questo è un lavoro congiunto con Philip Radlinsky, Silvia Parati e Annie Joyce."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è comprendere il linguaggio dell'utente quando desidera prendere una decisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è utilizzare un riferimento diretto, ad esempio indicando il titolo del brano, come \"è a mio carico\" o la sua posizione, il primo."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il titolo della canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "tutte le pronunce sono troppo simili tra loro e difficili da comprendere."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "oppure quando l'utente desidera specificare una preferenza, ecco alcuni esempi di preferenze indirette, ad esempio \"il più recente\" o \"la canzone che non è energica\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Questo rappresenta un problema importante nei sistemi di conservazione e anche per la valutazione delle capacità di comprensione delle entità da parte dei modelli linguistici di grandi dimensioni (LLM)."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un dataset pubblico, di un dataset pubblico su larga scala per questo compito, pertanto ne raccogliamo uno utilizzando il crowdsourcing. Il nostro dataset copre tre domini differenti: musica, libri e"}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta dati enfatizza l'informalità attraverso l'utilizzo del vostro set di completamento di cartoni animati."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il cartone animato presenta tre fumetti. Nel primo fumetto, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\" e con ciò, Bob definisce il contesto dialogico."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nel secondo fumetto, Alice dice: \"Intendi facile per me o 'I got a feeling'?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "che è la domanda alternativa. E nel terzo fumetto, Bob utilizza un riferimento indiretto per selezionare una di queste entità, ad esempio, la nuova"}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniamo automaticamente la prima e la seconda nuvoletta di dialogo, ma la terza viene compilata dall'annotatore. La prima nuvoletta di dialogo è scelta tra alcuni suggerimenti manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "il secondo, ovvero la domanda alternativa, viene generato come segue"}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo sempre un modello semplice. Intendi A o B? Dove A e B sono esempi tratti da Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Salendo nella lista, le entità diventano sempre più simili tra loro ed è solitamente più difficile formulare la stessa equazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è uniforme."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, ad esempio due libri con il nome “il rivenditore”."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso si verifica quando presentano descrizioni simili su Wikipedia e quando condividono infobox o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando sottoponiamo questa domanda alternativa ai redattori, questi conoscono il nome di queste entità, ma non necessariamente le loro caratteristiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, ciò che facciamo è presentare alcune conoscenze pregresse sulle due entità. Per le canzoni, mostriamo semplicemente un collegamento alla ricerca Google per ciascuna canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "E poi chiedete ai commentatori di ascoltare almeno una parte di ogni brano e di leggere informazioni su ciascun brano. Ecco, per esempio, il risultato della ricerca Google per il brano “Easy”."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo un testo di contesto proveniente da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini da Wikipedia, in modo che gli annotatori sappiano come appaiono."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Quindi chiediamo ai curatori di selezionare una di queste entità, ad esempio la prima, e descriverla utilizzando tre o cinque riferimenti indiretti."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica per pianoforte. Ecco alcuni esempi dal nostro dataset. Ad esempio, quello senza parole, non quello con il dodicenne o quello fittizio o proveniente dall'Azerbaigian e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus Identity comprende 6.000 domande alternative suddivise in tre ambiti e include 42.000 espressioni referenziali indirette. I risultati ottenuti con il modello T5X Large sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico avesse accesso alle esatte stesse conoscenze di background degli analisti, l'accuratezza sarebbe davvero elevata, intorno al novantadue-novantanove percento, ma questa non è una situazione realistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso ad alcune conoscenze pregresse parzialmente sovrapposte, l'accuratezza si attesta tra l'ottantadue e l'ottantasette percento, il che risulta più realistico, per esempio, quando il modello linguistico recupera tali conoscenze pregresse."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso a sole due entità nominali, l'accuratezza è del 60%, quindi c'è ampio margine di miglioramento. Abbiamo inoltre dimostrato che i modelli sono generalizzabili a diversi domini. Ecco un link al nostro set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Serapapi dell'Università di Trento e della Fondazione Bruno Kessler e introdurrò brevemente l'attenzione come guida per un articolo sulla traduzione automatica simultanea del parlato, un lavoro congiunto con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "cos'è la traduzione simultanea del parlato?\nLa traduzione simultanea del parlato, o Simulesc, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interlinguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli di simulazione attuali? Architetture specifiche vengono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "procedure di formazione lunghe e complesse, ad esempio procedure che coinvolgono obiettivi di ottimizzazione differenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E l'addestramento e la manutenzione di molteplici modelli per ottenere diversi regimi di latenza, ad esempio, l'addestramento di un modello con una latenza media di un secondo e un altro con due secondi di latenza, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Allora, qual è la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, utilizzare modelli ST offline esistenti senza riaddestramento o adottare architetture specifiche per semplicità. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E la conoscenza è già stata acquisita dal modello attraverso il meccanismo dell'input audio e dell'output testuale, che è il meccanismo dell'output audio, e potete osservare un esempio di ciò proprio lì."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione consiste nel proporre un codice o codificare l'attenzione al codice, e si tratta di una strategia per cui decidiamo se accettare o meno una traduzione parziale in base a dove puntano le attenzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "una parola viene emessa se la tensione non è concentrata, ovvero questa somma è inferiore a una certa soglia alpha, negli ultimi lambda frame discorsivi, il che significa che l'informazione ricevuta è sufficientemente stabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, se riceviamo un frammento di discorso contenente \"i'm going to talk about\" e il nostro modello prevede la traduzione in tedesco"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "e analizzeremo i pesi di cross-attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i frame discorsivi ricevuti più antichi, mentre l'ultima parola indica gli ultimi frame discorsivi ricevuti, almeno i frame discorsivi lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che le prime due parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "mentre, poiché la somma delle tensioni incrociate supera una certa soglia α, non emetteremo l'ultima parola e attenderemo un altro blocco di discorso."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se andiamo avanti e riceviamo un altro blocco discorsivo, e il nostro modello prevede altre tre parole, esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "vedremo che nessuna parola punta agli ultimi frame del discorso lambda"}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che queste tre parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se si osservano i risultati principali di ciò,"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Tracceremo i risultati della traduzione simultanea sul grafico, con il blu da un lato che misura la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Quella è la misura di latenza e consideriamo anche la media computazionale che tiene conto del tempo di calcolo del modello per predire l’output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo quindi che le nostre code siano il più alte possibile su questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Ma desideriamo anche che siano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo con strategie adeguate che si applicano anche ai modelli offline, quali la strategia Whitecaps e l'accordo locale, e confrontiamo inoltre con le architetture all'avanguardia specificamente progettate per la traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "e vediamo che ed supera tutte le strategie applicate ai modelli offline, dato che le loro curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, se consideriamo il tempo reale o il tempo di calcolo, quella è la strategia più veloce."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desideri scoprire ulteriori risultati, consulta il nostro articolo e abbiamo inoltre reso disponibile il codice sorgente, i modelli e le simulazioni open source per agevolare la riproducibilità del nostro lavoro."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Ying e io e il mio collega Ji Yong presenteremo la nostra ricerca sul multi-istruttore, volta a migliorare l’apprendimento sociale multimodale tramite l’istruttura mirata."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Con i progressi nei modelli linguistici di grandi dimensioni, numerose ricerche hanno iniziato a esplorare nuovi paradigmi di apprendimento che consentono di riutilizzare modelli linguistici pre-addestrati per diverse attività successive in modo efficiente sia in termini di parametri che di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che l'istruzione di ottimizzazione permette ai modelli linguistici di grandi dimensioni di eseguire compiti inediti in modo esaustivo, seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'instruction tuning si è concentrata sul miglioramento delle prestazioni a somma zero in compiti basati esclusivamente sul linguaggio, mentre la computer vision e i compiti multimodali sono stati trascurati."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, intendiamo indagare se l'istruzione di messa a punto su modelli multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali inediti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo riscontrato una discrepanza significativa nella disponibilità del dataset di istruzioni tra il modello LP e il modello multi-modale."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono oltre mille seicento attività di istruzione basate esclusivamente sul linguaggio, ma non è disponibile un task di istruzione multimodale su larga scala pubblicamente, il che motiva la nostra iniziativa di creare un dataset di ottimizzazione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo MultiInstructor, il primo set di dati di benchmark per il tuning di istruzioni multimodali, che consiste di sessantadue compiti multimodali diversi che coprono dieci categorie differenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi compiti derivano da ventuno set di dati open source esistenti, e ogni compito è dotato di cinque istruzioni scritte supplementari."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per l'indagine sull'ottimizzazione dell'istruzione multimodale sul nostro set di dati proposto, utilizziamo OFA, un modello multimodale unificato, come modello di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo alcuni esempi tratti dal nostro set di dati multi-stadio."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "per uniformare l'elaborazione di diversi tipi di dati di input e output"}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-a-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e i riquadri di delimitazione sono rappresentati nello stesso spazio token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Okay, ora parlerò di ottimizzazione multimodale dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per il set di dati di addestramento, utilizziamo 53 compiti provenienti da 9 gruppi per l'addestramento e campioniamo 10.000 per compito per il testing, riservando l'intero gruppo di ragionamento di buon senso per il testing e selezionando ulteriori 5 compiti dai gruppi VQV e vario."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutte le istanze nel test per ciascun compito, e campioniamo inoltre casualmente il compito dal test dell'istruzione naturale come osservato nel test per l'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, utilizziamo un modello OFA Large pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante il test, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Riportiamo la media e il valore massimo delle prestazioni, unitamente alla deviazione standard delle prestazioni in tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza. Se è un compito di generazione multimodale, riportiamo l'RGL. Per i compiti RLP, riportiamo anch'essi l'RGL."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva, denominata sensibilità, che misura la capacità del modello di produrre costantemente lo stesso output per lo stesso compito, a prescindere da leggere variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il nostro risultato principale; come si può notare, l'affinamento delle istruzioni può migliorare significativamente le prestazioni dei modelli linguistici di grandi dimensioni (LLM) sugli stessi compiti multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Anche il transfer learning da dataset di istruzioni naturali può giovare all'instruction tuning."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo osservare come, con l'aumentare della quantità di compiti, il modello ottenga prestazioni migliori e, contemporaneamente, una sensibilità inferiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento in cui abbiamo utilizzato un'unica istruzione rispetto a cinque istruzioni, e come possiamo vedere, l'impiego di un numero maggiore di istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra l'effetto di diverse strategie di pre-addestramento sulla sensibilità del modello. Come si può notare, trasferendo l'apprendimento dal dataset, il modello può raggiungere una sensibilità significativamente migliore rispetto al modello OFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Si può anche osservare che il transfer learning a partire dal dataset di istruzioni NITURE può contribuire a migliorare significativamente le prestazioni di OFA sul dataset di istruzioni NITURE."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, proponiamo un dataset di ottimizzazione multimodale, del tutto innovativo, che migliora significativamente la capacità a breve termine dell'OIF ed esplora diverse tecniche di transfer learning, dimostrandone i vantaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Un'ultima cosa, stiamo raccogliendo un dataset molto più ampio di dati per l'affinamento multi-modale, con circa 150 ulteriori attività di linguaggio visivo, e li rilasceremo."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, sono Kostas Senna e sono lieto di darvi il benvenuto alla nostra presentazione relativa al nostro articolo di ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "È un lavoro congiunto con John Gautier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo studio, riprendiamo il paradigma delle coppie minime."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Il paradigma minimo di accoppiamento valuta sostanzialmente i modelli linguistici sulla base di giudizi di accettabilità, che possono includere anche la correttezza grammaticale, come difetti, la sintassi, o l'accettabilità in termini di stereotipi, quali accoppiamenti incrociati."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma minimalista, il metodo tipico per valutare i modelli linguistici consiste nel presentare una frase accettabile o grammaticalmente corretta e poi una frase inaccettabile o agrammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi si auspica che il modello assegni fondamentalmente una maggiore probabilità all'insieme accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP di fatto non ci consente di valutare l'accettazione di un modello nei confronti di frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici stanno presentando finestre di contesto sempre più ampie, pertanto è fondamentale valutare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "Ed è questo che stiamo cercando di fare qui. Stiamo cercando di analizzare la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, questo è l'approccio. Ciò che faremo, dunque, è simulare queste sequenze più lunghe, esaminare i dataset stessi e, successivamente, creare frasi selezionando frasi accettabili o inaccettabili da tali dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, qui abbiamo scelto una tipica coppia di grammaticality estratta dal dataset \"blimp\" relativo al caso delle isole adjunct."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è ricreare sequenze più lunghe, che siano accettabili e presentino la stessa struttura grammaticale corrispondente, estraendo frasi grammaticali da"}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi procedere allo stesso modo selezionando frasi inaccettabili provenienti dallo stesso set di corrispondenze, il che potrebbe anche essere utilizzato per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset differente, ed è proprio questo che chiamiamo scenario di mismatch."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Qui, le frasi provengono ancora da dataset pertinenti, ma non dallo stesso dataset con cui si sta effettuando la valutazione, e possiamo fare lo stesso per i casi di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Questo ci dirà se i giudizi di accettabilità del modello siano effettivamente influenzati da qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "come se il contesto derivasse da un sottoinsieme diverso del dataset o se fosse completamente irrilevante per la frase che stiamo analizzando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, come si comporta il modello? Iniziamo analizzando le frasi di Wikipedia completamente irrilevanti per la coppia di query corrente, dove riscontriamo che i giudizi MPP sono per lo più robusti per contesti arbitrari."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino a 2024 per massimizzare i modelli OPT e GPT2, e abbiamo osservato qui, sulla linea orange.de, che i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "E ora, cosa accade quando selezioniamo frasi provenienti dallo stesso dataset?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Ed ecco che ci troviamo a scegliere o creare frasi da domini accettabili e non accettabili, a partire dallo stesso set di dati blim o sintattici."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o prefissi inaccettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando confrontiamo la struttura, ovvero quando selezioniamo le frasi provenienti dallo stesso fenomeno nel testo di attribuzione di colpa."}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo un incremento massiccio o una diminuzione massiccia nel giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora, questo è molto significativo; questo effetto aumenta con la lunghezza del contesto e, molto probabilmente, influenzerà i modelli linguistici più recenti che dispongono di finestre di contesto più ampie."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, perché il prefisso della corrispondenza influisce così tanto sul giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto una serie di analisi in cui abbiamo cercato di preservare la frase di input, mantenendone la struttura rilevante, introducendo rumore nell'input e poi eseguendo una serie di queste operazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo constatato che nessuno di questi rumori sta effettivamente modificando il comportamento del modello in termini di come ci mostra l'andamento del giudizio MPP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "Fondamentalmente, riscontriamo che i modelli reagiscono alle frasi di perturbazione in maniera simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "In altre parole, quando alteriamo le frasi all'interno del dominio accettabile, osserviamo un aumento simile in tutte le alterazioni, e quando alteriamo le frasi all'interno del dominio inaccettabile, osserviamo una diminuzione nei giudizi MPP in modo analogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Le conclusioni principali del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti, che vengono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, il modo in cui la eseguiamo correttamente, con input brevi e in singole frasi, potrebbe non cogliere appieno la conoscenza astratta del modello linguistico lungo l'intera finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Si prega di leggere il nostro articolo per maggiori dettagli sugli esperimenti.\nGrazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Yusof John e sono della Penn State University. Oggi presenterò il nostro lavoro, intitolato \"Example: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Many Representations\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, il semantic parsing è il compito di costruire rappresentazioni semantiche di query degli utenti, come Sequel e il calcolo lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "E la semantica cross-linguistica è il compito di tradurre query in molteplici lingue naturali in molteplici rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Come illustrato in questa figura, è necessario tradurre la query in diverse lingue naturali utilizzando modelli più recenti: C, C, C, L, D, F, Q, ecc."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "i modelli di semantic parsing cross-lingua esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati, ad esempio"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "ci sono fughe di copertura su alcune aree della lingua naturale, la parte cinese manca e"}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "potevano coprire molte rappresentazioni incerte."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il cocktail Lambda è scomparso."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "oppure vengono valutati solo su determinati modelli più recenti; per esempio, esiste un solo modello per la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo un esempio: forniamo un dataset uniforme di esempio per l'analisi semantica incrociata in diverse lingue naturali e numerose rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene novanta insiemi in diversi ambiti, cinque compiti di parsing semantico, otto rappresentazioni del significato e ventidue lingue naturali in quindici famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo consiste in un test di traduzione: utilizziamo l'API di Google Translate per tradurre il testo di partenza nella lingua di destinazione, per poi avvalerci di un modello monlinguistico per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "E, ad esempio, addestriamo il modello inglese su query in inglese e, durante l'inferenza, traduciamo la query in tedesco tramite API in inglese, per poi utilizzare il modello addestrato per prevedere il seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E testeremo anche il modello monlinguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di partenza coincide con la lingua di arrivo, ad esempio dal tedesco al tedesco o dall'inglese all'inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testiamo inoltre la configurazione monolingue del fuse addestrando modelli monolingui con solo il dodici percento dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "e che dispone di un modello multilingue che addestriamo, un unico modello multilingue, per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo combinato il tedesco, l'inglese e il cinese per addestrare un modello multilingue, e durante la fase iniziale possiamo utilizzare questo modello per..."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "per tradurre richieste in tedesco o richieste in cinese o così via"}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "Consideriamo inoltre il cross-linking zero-shot e il transfer visivo, tra una lingua di origine e il trasferimento a un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante l’addestramento, lo allenerò su query in inglese, o sulla combinazione di query in inglese e tedesco, per addestrare un modello multilingue a prevedere la sequenza di output."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo anche molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingui, ne valutiamo due gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "Includendo Encoder.pdf, che sta per Multilingual Pre-Trained Encoders with Pointer-Based Decoders, come XLR+PDF e Bert+PDF."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo inoltre modelli encoder-decoder, ovvero modelli encoder pre-addestrati multilingua come #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um #um"}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo constatato che l'architettura encoder-decoder offre le migliori prestazioni su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "e valutiamo su MT5 ed example XLMR più PDR in un contesto multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che Encoder Decoder o Encoder PDF possono essere migliorati tramite l'addestramento in un mix di diverse lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "e quando si riscontra, è perché la maggior parte delle lingue naturali principali può ottenere un miglioramento delle prestazioni, eccetto che l'inglese subisce un calo in sette dataset e ottiene guadagni solo in tre dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Credo che questo sia noto come la maledizione del multilinguismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo inoltre il divario di rendimento interlinguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento di campo cross-linguale, la linea arancione rappresenta il trasferimento zero-shot cross-linguale, mentre la linea verde rappresenta l'impostazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che confrontando le linee verde e arancione, abbiamo rilevato che nelle configurazioni zero-shot, il divario nelle prestazioni del trasferimento di cross-link è significativo, e confrontando le linee blu e arancione, abbiamo notato che nelle configurazioni few-shot, tale divario si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo inoltre altri risultati interessanti, ad esempio che l'architettura encoder-decoder svolge più lavoro o ottiene risultati comparabili, ma l'apprendimento dell'inglese come lingua madre può notevolmente migliorare le prestazioni delle lingue di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo riscontrato che modelli linguistici multilingua come Codex e Blue risultano ancora inadeguati per scenari cross-linguistici e interpersonali."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, sviluppiamo Exemplar, un benchmark unificato per il parsing semantico cross-angle, con molteplici lingue naturali e numerose rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Conduciamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue, e i nostri risultati mostrano numerosi risultati interessanti, ecc. E siamo lieti di invitarvi a consultare il nostro articolo e il codice."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo A.V. Villar e vi fornirò una breve recensione del paper, \"Printing Power for Translation: Assessing Strategies and Performance\". Si tratta di un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "Faram è un modello linguistico con 540 miliardi di parametri, presentato lo scorso anno, nel 2022. Si tratta di un vasto insieme di testi composto da 780 miliardi."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "La pubblicazione tamil raggiunge lo stato dell'arte in centinaia di compiti NRP."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro presentiamo il primo studio sistematico del prompting di modelli linguistici di grandi dimensioni per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo la capacità di traduzione del modello utilizzando le migliori pratiche della comunità M.T. Ciò implica l'utilizzo dei test più recenti per evitare sovrapposizioni dei dati con l'addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo due sistemi all'avanguardia, i sistemi con le migliori prestazioni e la valutazione WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche all'avanguardia basate su reti neurali e presentiamo inoltre i risultati di una valutazione umana basata sull'esperienza di esperti. Infine, forniamo alcune raccomandazioni per strategie di selezione dei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "Il prompting ha un impatto significativo sulle prestazioni della traduzione, come si può osservare in un semplice esperimento in cui utilizziamo il prompting one-shot e forniamo due prompt differenti per una frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "Nella maggior parte delle frasi, 516 su 1.000, la differenza osservata è di oltre un punto di sfocatura."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "E questo, in casi estremi, può arrivare fino a quaranta punti, perciò è importante selezionare una strategia promozionale efficace."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nei nostri esperimenti, abbiamo deciso di utilizzare una strategia a cinque colpi, in cui semplicemente etichettiamo ogni frase che forniamo al sistema con la lingua in cui è redatta."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, in cui eseguiamo traduzioni dal tedesco all'inglese, le frasi tedesche sono contrassegnate con la colonna tedesca e le traduzioni inglesi con la colonna inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo visto che la forma effettiva della promozione non ha una grande influenza nel caso di promozione seriale a breve termine."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È fondamentale per zero e uno shot di promozione e quando passiamo al nostro caso di promozione, non c'è differenza rispetto alla forma effettiva della promozione."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a sostenere maggiormente l'argomentazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "Il riassunto dei nostri risultati sperimentali è che la qualità del campione è più importante della somiglianza con la frase di partenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, è importante selezionare gli esempi da traduzioni di alta qualità; in particolare, confrontiamo i prompt di selezione provenienti dai dati di addestramento delle valutazioni WMT o dai dati del..."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati sono decisamente più accurati e, maggiore è la qualità dei dati, migliori saranno i risultati ottenuti utilizzandoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi specializzati presentano un notevole vantaggio rispetto alle traduzioni di Palm, sebbene Palm si avvicini piuttosto a un sistema commerciale. Nel nostro caso, abbiamo optato per l'utilizzo di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le informazioni che otteniamo dalla valutazione umana, che eseguiamo utilizzando il framework MQM, ci indicano che la fluidità di Palm è comparabile a quella dei sistemi all’avanguardia, ma la differenza principale risiede nell’accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "in particolare, gli errori di omissione sono i più frequenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Sembra quindi che Palm scelga di produrre una traduzione migliore, a volte eliminando parti della frase che risultano disposte nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria degli indumenti di superficie per Palm risulta inferiore rispetto a quella dei sistemi all'avanguardia, il che costituisce un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "che produce risultati notevolmente fluidi, ma con persistenti problemi di accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa brevissima recensione.\nPer maggiori dettagli, vi invito a seguire la mia presentazione completa del documento.\nGrazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "ciao, sono Davey, dottorando all'Università di Salen in Germania.\nIn questo video, vorrei presentare il nostro recente lavoro: \"Più debole di quanto si pensi\", uno sguardo critico all'apprendimento a sorpresa settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Shaul Usher, Marius Muzpah, Andreas Stefan e Dietrich Klarko."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "Nella supervisione debole, non etichettiamo manualmente i dati, bensì li etichettiamo utilizzando fonti di etichettatura debole, come regole euristiche semplici, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se addestriamo direttamente reti neurali e dati debolmente etichettati, le reti neurali tendono a memorizzare il rumore presente nelle etichette e non generalizzano."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Nell'addestramento con supervisione debole, vengono proposte tecniche di addestramento per formare robustamente reti neurali in presenza di rumore nelle etichette, in modo che i modelli di addestramento continuino a generalizzare in modo efficace."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In recenti studi in WSL, WSL sta per Apprendimento Supervisionato Settimanale. Un'affermazione diffusa è che si sostenga di addestrare modelli esclusivamente su dati a livello settimanale e di ottenere prestazioni elevate su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è errata, ma c'è un aspetto da considerare."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "Qual è il presupposto che esista un ulteriore set di validazione pulito disponibile per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Metteremo in dubbio questa impostazione del problema, poiché implica che siano necessarie annotazioni manuali aggiuntive nei materiali didattici settimanali, ma come un elefante nella stanza, questa necessità viene spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il suddetto dubbio ci porta a porre tre domande di ricerca: innanzitutto, è necessario un set di dati di validazione pulito per WSL, o possiamo forse utilizzarne uno rumoroso?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, se sono richiesti dati puliti, o se i dati puliti sono obbligatori affinché WSL funzioni, allora quanti campioni puliti ci servono?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Affrontiamo queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, scopriamo che, in modo interessante, i metodi WSL più recenti richiedono effettivamente campioni di validazione puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "altrimenti si verifica un significativo calo delle prestazioni, come illustrato nella figura. Se non sono presenti campioni di validazione puliti, i modelli di tendenza non possono generalizzare al di là delle etichette bit originali."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "il che implica che la dottrina sia priva di significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che gli approcci WSL richiedono effettivamente dati chiaramente etichettati per funzionare correttamente, e il costo di annotazione necessario per ottenere campioni di validazione puliti non dovrebbe essere trascurato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "La nostra seconda constatazione è che l'aumento del numero di campioni di validazione puliti contribuirà a migliorare le prestazioni degli approcci WSL, come illustrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "Tipicamente, abbiamo bisogno solo di venti campioni per classe per ottenere prestazioni elevate."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questa non è la fine della storia, perché se decidiamo di accedere a campioni puliti, l’addestramento diretto su di essi potrà persino raggiungere prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di performance tra gli approcci di fine-tuning, applicati direttamente su dati puliti, e gli approcci WSL, che utilizzano i dati puliti unicamente per la validazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo notare, se abbiamo dieci campioni per classe, il fine tuning diretto inizia a superare gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni rivendicato negli approcci WSL precedenti può essere facilmente ottenuto consentendo di continuare l'affinamento su campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo osservare dalle figure, il modello Wallina, denominato FTW, inizialmente mostra prestazioni inferiori rispetto a metodi WSL più complessi come il coseno."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se consentiamo di proseguire con il fine-tuning sui campioni di click, allora FTP ottiene prestazioni paragonabili a quelle di altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, non vi è motivo di scegliere metodi WSL più complessi, che richiedono maggiore tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, dimostriamo che gli approcci WSL recenti necessitano di campioni puliti e annotati manualmente per funzionare correttamente. Il loro guadagno di performance e la loro praticità sono ampiamente sovrastimati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, si riportino i criteri di selezione del modello; ad esempio, si specifichi se la selezione del modello avviene tramite campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, gli approcci WSL dovrebbero essere confrontati con futuri benchmark di apprendimento, ovvero con studi su campioni ben definiti. In terzo luogo, il fine tuning continuo è un benchmark semplice ma efficace che dovrebbe essere preso in considerazione in futuri lavori relativi al WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso pubblico il nostro codice sorgente. Potete trovarlo tramite il codice QR presente in questa diapositiva. Vi invitiamo a esaminarlo liberamente. Grazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono James Finch e io sono Sarah Finch. E oggi vi racconteremo tutto su ABC EVEL, un nuovo approccio dimensionale alla valutazione dell'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dall'Emory NLP Lab, guidata dal Professor Gino Choi presso l'Emory University e in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo, quindi, che abbiate appena sviluppato un modello di dialogo e vogliate valutare quanto bene si confronti con lo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La prassi comune è quella di ricorrere a valutazioni umane, come ad esempio chiedere a giudici umani di selezionare quale delle due conversazioni sia migliore o di assegnare un punteggio alle conversazioni utilizzando una scala graduata."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo presenta molteplici aspetti, pertanto potrebbe essere utile valutare diverse dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio consiste nel chiedere semplicemente a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi o scalabili esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se ciascuna risposta del modello esprime determinati comportamenti, come fornire informazioni irrilevanti o contraddire se stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Definiamo questo approccio \"Annotazione dei Comportamenti in Chat\" o ABC per brevità. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i modelli di comportamento nella chat che sono stati proposti come fattori che influenzano la qualità della conversazione e che sono stati discussi in letteratura recente."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "A B C E è in grado di misurare i tassi con cui i modelli di linguaggio commettono diversi errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "A B C E V A misura il numero di turni in cui un modello di dialogo ignora il proprio interlocutore o produce affermazioni irrilevanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o contraddice il suo partner, allucina fatti scorretti o viola la conoscenza del senso comune, e quando il modello riesce o fallisce nel dimostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni umane per modello utilizzando ABC."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, dato che questa è la prassi standard per la valutazione dei modelli di chat lungo molteplici dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dall'analisi di queste valutazioni, abbiamo riscontrato che le etichette comportamentali ABC sono generalmente più affidabili rispetto alle etichette esistenti, come misurato dall'Accordo Interinale su cento conversazioni in doppio cieco."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette ABC sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato dall'analisi di regressione lineare semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Si può notare come la misurazione della proporzione delle autocontraddizioni e degli equivalenti del cinque e del dieci per cento della qualità della conversazione, mentre i punteggi medi di coerenza siano solo del quattro per cento o inferiori."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ciascuna metrica di valutazione cattura un aspetto unico del controllo qualità mediante una regressione lineare a stepwise."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Si può osservare come la combinazione di tutte le metriche ABC spieghi più del venticinque percento della qualità della conversazione e come, rimuovendo ciascuna metrica singolarmente, la maggior parte di esse comporti una perdita significativa di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altro canto, la combinazione di tutte le metriche del licorice a livello di turno spiega molto meno della qualità e un numero inferiore di queste metriche contiene informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste sono metriche A B C E V affidabili, informative e distinte che possono essere utilizzate per valutare l'IA conversazionale con una risoluzione superiore rispetto ai metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "È possibile constatare dai risultati del nostro esperimento che diverse problematiche permangono e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni del buon senso in circa il venti percento delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Producono informazioni pertinenti in circa il quindici percento delle risposte e si contraddicono o contraddicono il loro interlocutore all'incirca nel dieci percento dei casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Con il rapido ritmo di miglioramento nel settore, molti di questi errori potrebbero essere riscontrati nei nuovi modelli rilasciati dall’evaluazione; tuttavia, questo è proprio il motivo per cui è necessario perseguire metriche di valutazione affidabili e accurate per modelli di confronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "speriamo che un b c eval possa essere sfruttato da altri nel settore come un passo significativo in questa direzione e attendiamo con interesse di vedere come l'IA conversazionale progredirà nei prossimi mesi e anni.\nGrazie per la visione."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Kyoyan e presento il nostro lavoro dal titolo \"Quando si traduce il contesto dei dati\". Questo è un progetto di collaborazione con Patrick Furness, M.D., M.F. Martin e Gram."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, molte traduzioni dipendono dal contesto: ad esempio, come tradurremmo \"more\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Bene, se la frase precedente fosse \"le cose potrebbero iniziare a farsi pericolose se i ministri lo scoprissero\", allora Moe si riferisce a una spia. Ma se la frase precedente fosse \"potrebbe essere qualcosa di serio, dottore?\", allora Moe si riferisce a un segno di nascita."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, ne cambia anche la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli possano tradurre casi come questo è piuttosto difficile. Innanzitutto, perché solo una piccola porzione di traduzioni dipende dal contesto, il che rende metriche a livello di corpus come BLEU incapaci di cogliere tali traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "Alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e un insieme limitato di lingue, poiché generalmente si basano sulla conoscenza umana e sulla creazione umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro cerchiamo di rispondere a queste due domande: innanzitutto, quando la traduzione richiede un contesto e, in secondo luogo, quanto bene i modelli gestiscono tali casi?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto della traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "E precedentemente abbiamo introdotto XMI come misura per i modelli di traduzione automatica, e ciò avviene misurando quanto informazione C fornisce sul testo di destinazione e perché."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "Puoi pensare a CXMI come alle informazioni ottenute fornendo contatti al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo il CXM al punto YXM, che può misurare l’uso del contesto a livello di frase o a livello di parola. Possiamo considerare le parole che presentano un alto valore di PXM come parole che richiedono il contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un elevato P.S.M.I. per individuare eventuali schemi ricorrenti tra di esse."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "E conduciamo la nostra analisi su trascrizioni di TED Talks che sono state tradotte dall'inglese in 14 diverse lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo la nostra analisi a tre diversi livelli. Innanzitutto, esaminiamo i tag del discorso che presentano significati elevati."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "Ed è per questo che, ad esempio, si può trovare la pronuncia araba di un proverbio arabo che presenta una I acuta molto alta. Questo può essere spiegato perché l'inglese non possiede un proverbio equivalente, e quindi è necessario sapere se il proverbio è stato tradotto in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo anche che alcune lingue richiedono un contesto specifico quando si desidera scegliere la forma verbale appropriata. Successivamente, esaminiamo i lessici che presentano un’alta frequenza sezione-p in tutte le loro diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo aiuta a identificare casi come quello che vediamo qui, dove in cinese è necessario assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "E analogamente, constatiamo che il contesto è adeguato al livello di formalità richiesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "E infine analizzeremo diverse #um #e diverse #qualcuno's #high-p.s.m. e ciò ci consente di identificare fenomeni che difficilmente possono essere colti dalla parola stessa ma che trovano maggiore espressione nella struttura, nella struttura stessa, quindi, procediamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, ora utilizziamo i risultati derivanti dalla nostra analisi per progettare un benchmark per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni che abbiamo identificato, creeremo automaticamente dei tag per identificare le parole correlate al fenomeno, e chiameremo il nostro tag fenomeno multilingue o mutag."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi notare anche che le diverse lingue presentano proporzioni differenti di questi fenomeni."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, utilizziamo il Mudah Tagger applicando il tagger al corpus parallelo che desideriamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione preferite agli esempi dipendenti dal contesto che il Mudah Tagger ha identificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "E infine, utilizziamo il nostro benchmark, unitamente ad altre metriche, per valutare diversi modelli di #um sulla traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, quando utilizziamo metriche a livello di corpus, nel caso di blue, riscontriamo che i modelli agnostici alla complessità offrono le prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma allora, se utilizziamo Comet, i modelli contestualmente consapevoli ottengono le prestazioni migliori, e se utilizziamo la misura F di Word, allora i modelli con e senza contesto presentano prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione documentale se si utilizza unicamente una metrica a livello di corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark Muad'Dib per valutare i modelli e constatiamo che i modelli contestuali sono significativamente più accurati rispetto ai modelli che non utilizzano il contesto per determinati fenomeni discorsivi, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori dei modelli che non utilizzano altre forme di comunicazione come i fonemi e le fonemie, quindi è necessario fare ulteriori progressi per la documentazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo inoltre diversi sistemi commerciali e il nostro benchmark dimostra che Google Translate è generalmente più accurato di Google Translate per la traduzione di documenti locali."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, eseguiamo un'analisi basata sui dati su quattordici coppie linguistiche per identificare una traduzione che necessiti di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizzeremo i nostri risultati per creare un parametro di riferimento per la traduzione a livello di documento, che potrà aiutare a identificare quali modelli di fenomeni possono essere utilizzati e quali sistemi di traduzione sono adatti alla traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio molto per la Sua attenzione, si trova a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Yannis Lavaque e vi presenterò il nostro lavoro su Dr. Bert, un modello britannico robusto, adattato in francese per i domini biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, inizieremo discutendo della modellazione del linguaggio in ambito sanitario, per poi illustrare il contributo principale del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Stiamo introducendo il primo modello biomedico in lingua francese, denominato Dr. Bert, basato su Roberta e addestrato su Nachos, un insieme di dati medici provenienti dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo inoltre un confronto tra modelli con molteplici configurazioni platoniche e fonti di dati, per poi presentare i nostri risultati su undici compiti non stereotipati in ambito biomedico e clinico in lingua francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "E, infine, concluderemo con gli esperimenti e vi forniremo ulteriori dettagli su come accedere al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Dalla sua pubblicazione nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre un notevole incremento delle prestazioni rispetto a metodi storici statici e contestualizzati quali Word2Vec, FastText o Word."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert e ad altri ambiti, quali il biomedicale con biomedical e quello clinico con clinical, ma soprattutto in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "Modelli specializzati per altre lingue sono scarsi e spesso si basano su un addestramento continuo a causa della mancanza di dati specifici per il dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora, la Francia non disponeva di un nuovo modello open source per il settore biomedico."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci chiediamo dunque: quali sono le fonti di dati più appropriate per un ampio ventaglio di applicazioni e quali dati possono costituire valide alternative ai dati clinici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il Dr. Bert con il nostro modello di Schubert, il quale si basa su dati anonimi ottenuti dall'Ospedale Universitario dei Paesi Bassi."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, ci chiediamo quanta quantità di dati sia necessaria per addestrare un modello specializzato su dati francesi? Quattro gigabyte, otto gigabyte o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Questa domanda è prima. Inizieremo addestrando e confrontando quattro modelli da zero, una prima versione di Dr. Bert con sette gigabyte di Natchez, una seconda versione con quattro gigabyte di Natchez."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "La prima versione di Shubert, un modello clinico con quattro gigabyte di note cliniche, e la versione finale di Shubert con quattro gigabyte di note cliniche e quattro gigabyte di note cliniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, introduciamo tre modelli di treni su pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno si basa sul peso di Camembert e si addestra su quattro gigabyte di Natchez, un altro si basa anch'esso su Camembert, ma questa volta su quattro gigabyte di Clint e Lott."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "Infine, uno dei modelli biomedici inglesi, Bumblebee, ed addestrato su quattro gigabyte di dati, abbiamo un totale di sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, raccoglieremo una serie di compiti di donazione pubblici e privati, quali il riconoscimento di nomi e identità, la classificazione, la segmentazione del parlato e la domanda-risposta."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questo modello è comparabile a sei modelli differenti, che sono: centotredici e ottantotto gigabyte di CamemBERT, quattro gigabyte di CamemBERT, quattro gigabyte di CamemBERT, quattro gigabyte di CamemBERT, quattro gigabyte di CamemBERT, quattro gigabyte di CamemBERT, quattro gigabyte di CamemBERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "La valutazione del modello evidenzia che esso ottiene le migliori prestazioni nel compito con dati di natura simile a quelli su cui è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee appaiono più versatili, e osserviamo altresì che l'utilizzo di una maggiore quantità di dati si traduce in prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, dal training eseguito ex novo, sembra che ottengano prestazioni superiori nella maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento con l'addestramento continuo utilizzando il peso e il peso del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "il che non è il caso del modello basato sui vini Camembert e sul Tokenizer, che presenta problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, a conclusione, il nostro sistema dimostra prestazioni migliori in nove delle undici attività Don't Stream, e intercambiabilità globale, frutto del modello generico presentato qui, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo inoltre che i dati specializzati sono migliori, dati più specializzati sono migliori, ma non scalano bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli di pre-addestramento ottenuti da Natchez sono liberamente disponibili su YouTube e tutti gli script di addestramento sono nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, grazie per questa presentazione e attendiamo con interesse l'avvio di azioni presso l'Ufficio Postale di Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Mathias Lindemann e oggi vi fornirò una breve introduzione al nostro articolo sulla Generalizzazione Composizionale senza alberi, utilizzando il tag multiset e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei relatori, Alexander Koller e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione composizionale può essere intesa come la capacità dell'apprendista di gestire la ricorsione profonda e composizioni inedite di frasi che sono state apprese individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto del Test Semantico della Composizione Composizionale, abbiamo una sessione di addestramento in questo caso, e Mary è il membro più recente."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "È una forma logica della forma logica, la rappresentazione dell'aspetto della mente."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "A differenza della valutazione standard del machine learning, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non correlate."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha riscontrato una ricorsione superficiale durante l'addestramento e viene testato su un esempio con ricorsione profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "i modelli sequence-to-sequence faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output scollegati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, essi spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, quali quelle evidenziate nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo più diffuso per affrontare questa problematica è l’integrazione dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono concepiti per catturare il processo compositivo che mette in relazione gli atteggiamenti con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Funziona bene, ma di solito non è predisposto per essere ottenuto in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e, talvolta, computazionalmente oneroso. Tipicamente, ciò comporta una notevole pre-elaborazione specifica per il formalismo delle forme logiche, ad esempio per gestire i simboli variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "ottenere alberi può anche implicare procedure specializzate di grammatica e di elaborazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo non utilizziamo gli alberi e introduciamo un modello sequenza a sequenza che modella direttamente le corrispondenze tra i frammenti dell’input e i frammenti dell’output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta dimostreremo una forte generalizzazione alla de-costruzione senza fare affidamento su"}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede la generazione dell'output a partire dall'input in due fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, etichettiamo ogni token di input con un multinsieme non ordinato di token che appariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passaggio, abbiamo tutti i token corretti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché, nella seconda fase, utilizziamo un altro modello per prevedere la permutazione necessaria a riordinarli correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Presentiamo un nuovo metodo per predire la permutazione che non impone vincoli rigidi sulle possibili permutazioni. Ciò rende il nostro approccio particolarmente flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona più o meno in questo modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Procediamo da sinistra a destra sull'output e determiniamo quale token di multinsieme inserire in ciascuna posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno, come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, passiamo al successivo token del multinsieme per determinare il secondo token nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "determinare il terzo token nell'output in modo simile saltando a un altro token del multinsieme, continuiamo questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "Finché ogni token della prima fase non sarà stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli privi di alberi sul benchmark Cogs. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Alcune altre tipologie di generalizzazione strutturale sono particolarmente impegnative."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo, risolveremo alcune interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo a quale multisettore esso appartenga, il che pone una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte esistono più permutazioni coerenti con i dati, ma quella linguisticamente corretta rimane latente. Affrontiamo questo problema inducendo l'allineamento come parte del processo di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma comporta la sfida che trovare la permutazione con il punteggio più alto è un problema N.P.-difficile, poiché esso è correlato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Lo approssimiamo con una rilassamento continuo favorevole alla GPU che ci permette inoltre di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate approfondire le nostre ricerche e il modo in cui affrontiamo queste sfide, consultate il nostro articolo o partecipate al nostro intervento."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, sono Ashta e oggi, insieme al mio coautore, presenterò il mio lavoro sul Master in Integrazione della Conoscenza da Fonti Multiple. Questo lavoro è una collaborazione tra l'Università di Melbourne e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli nazionali di comprensione del linguaggio si basano su una varietà di fonti di conoscenza, quali la conoscenza contenuta nei parametri, acquisita di solito tramite pre-training, e la conoscenza fornita negli input al momento dell'apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Lavori recenti in compiti come il Question Answering dimostrano che i modelli possono utilizzare la conoscenza acquisita durante il pre-training per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anch'esse al momento di"}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "John vide il presidente appena eletto in televisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia una T.L., ma non possono conoscere in modo affidabile chi sia l'entità specifica in questa istanza, John, o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dall'addestramento preliminare."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, modelli efficaci per compiti di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella acquisita durante l'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l’integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo un meccanismo di risoluzione dei riferimenti per valutare la capacità di attingere alle conoscenze disponibili in diverse fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Servin è un giudice, Kia è una panettiera. Servin e Kia si sono incontrati in un parco dopo una lunga giornata di lavoro, durante la quale lui aveva deciso casi in un tribunale. Era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo qui è identificare l'entità corretta a cui il pronome \"he\" si riferisce, che in questo caso è il servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronome richiede due tipi di informazioni: in primo luogo, conoscenza specifica dell'entità, come il fatto che un servitore è un giudice, e in secondo luogo, conoscenza di sfondo, come il fatto che i giudici decidono casi nei tribunali."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "Generalmente, le conoscenze pregresse vengono apprese durante il pre-training del modello linguistico, mentre le conoscenze specifiche sono tipicamente osservate al momento dell'infezione."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo constatare la disponibilità di queste due informazioni, in modo che possano essere reperite in un'unica fonte o in diverse fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo definito tre configurazioni di Kidmows. Innanzitutto, abbiamo la configurazione tipica di background pre-training, dove si assume che la conoscenza di base sia disponibile al momento del pre-training."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è l'ambiente di background, dove la conoscenza di base è disponibile sia durante la fase di pre-addestramento che durante l'addestramento. Infine, l'ambiente di background, dove entrambi i tipi di conoscenza sono disponibili solo durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questo ultimo scenario è particolarmente interessante poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli. Ad esempio, poiché nuove professioni si sono sviluppate dopo il periodo di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controllare la disponibilità dei fatti nelle fonti autentiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nell'impostazione pre-addestrata, assumiamo che la conoscenza di fondo secondo cui i politici cercano seggi eletti nel governo sia contenuta nei parametri pre-addestrati. Nel contesto della violazione, forniamo la conoscenza antispecifica che Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "Nell'impostazione di background, forniamo inoltre non solo conoscenze anti-specifiche, ma anche informazioni di base sui politici nel contesto dell'influenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "e nel contesto di ambientazione forniamo l'occupazione fittizia di meritua invece di politico perché meritua è improbabile che sia presente nei modelli pre-addestrati."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo il dataset sia con partecipanti umani che con modelli di soluzione grafica consolidati. In questa figura presentiamo i risultati dei modelli con le prestazioni migliori sulla variante più complessa dell'ambiente di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "senza un addestramento specifico su Kidmoose, entrambi i modelli non ottengono buoni risultati quando vengono addestrati su Kidmoose; tuttavia, sia Sea to Earth che BERT per Cue ottengono risultati significativamente migliori rispetto alla scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Ciò suggerisce che, quando addestrati su dataset di soluzioni di riferimento generali, i topi imparano a sfruttare indizi superficiali che non sono utili quando testati su cuccioli, dove tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Ulteriori esperimenti con conoscenza fittizia indicano che anche i modelli con le migliori prestazioni non riescono ad integrare in modo affidabile le conoscenze pregresse fornite unicamente al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i punti chiave del nostro articolo, molti modelli di risoluzione della co-referenza risultano incapaci di ragionare su conoscenze provenienti da fonti diverse senza un addestramento specifico per il compito; tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo conoscenze provenienti da molteplici fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "anche i modelli più performanti sembrano avere difficoltà a integrare in modo affidabile conoscenze pregresse presentate unicamente al momento dell'inferenza. Se desiderate maggiori dettagli, consultate il nostro articolo e verificate il dataset e il codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Mary e sto parlando della documentazione per la documentazione. Utilizzando modelli linguistici naturali per misurare i modelli linguistici, questo lavoro è stato svolto in collaborazione con Esen e Dankowski."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli linguistici di grandi dimensioni, o LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano diverse limitazioni. Si basano solitamente su insiemi di dati costruiti manualmente, che richiedono molto tempo per..."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "E misurano anche, di norma, solamente stereotipi molto specifici, il che significa che non generalizzano ad altre fasce demografiche o contesti, e si limitano a cogliere associazioni piuttosto generali, come associazioni negative nei confronti di particolari gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte del lavoro in questo campo non è spiegata dall'interconnessione, intesa come la nozione che le molteplici identità sociali possano essere combinate e risultare uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni ci affidiamo alla proprietà per cui queste nuove istruzioni sono particolarmente efficaci nel rispondere alle richieste."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, potete immaginare il modello della persona che è l'immagine dell'individuo, utilizzando un pronome come se foste una donna asiatica, descrivetevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente constatare che ciò è molto generalizzabile a qualsiasi gruppo demografico, poiché possiamo semplicemente specificare quali indicatori di identità desideriamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco alcuni esempi di generazioni da GPT Quattro."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che gli output sono negativi o tossici nel senso tradizionale del termine."}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è descritta come modesta, la donna mediorientale è definita ricorrendo a termini come esotico e riferendosi alla regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "e entrambe le personae di colore fanno riferimento alla discendenza, mentre la persona maschile bianca non presenta nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "per catturare questi schemi, il nostro metodo si articola in due fasi. La prima consiste nella generazione di tali soggetti."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri suggerimenti per generare queste persone sono stati ispirati da uno studio in cui tali suggerimenti venivano forniti a soggetti umani, riscontrando che, fornendo loro soggetti umani, erano anche in grado di perpetuare stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "E ciò consente anche un confronto diretto tra le persone generate da noi e la risposta umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte è Mark Words, un metodo per identificare le parole che distinguono i gruppi Mark dai gruppi Mark, cosa che spiegherò a breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di ciò è che possiamo ottenere stereotipi e schemi molto specifici senza doverci basare su un lessico particolare."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, il metodo di Mark si basa sul concetto sociolinguistico di commerciabilità, secondo cui esiste un segno non marcato e qualsiasi gruppo che differisce da tale segno è linguisticamente marcato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, ad esempio, la parola \"uomo\" o \"donna\" è solitamente associata a \"uomo\", quindi quando le persone descrivono una donna come donna, specificano di solito \"donna\" e \"donna\" come \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "Più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel nostro metodo, designiamo innanzitutto quali siano i gruppi non marcati e marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "E poi confrontiamo l'individuo utilizzando il metodo delle \"parole provocatorie\", che consiste essenzialmente nell'impiegare rapporti di loghi ponderati per distinguere le parole più rilevanti per ciascun gruppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, per le Donne Nere, analizzeremo le espressioni combattive e confronteremo la legge vigente con quella applicata a persone bianche e uomini, poiché si tratta di due gruppi non marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, utilizziamo degli stereotipi e constatiamo che la persona generata ne presenta molti più rispetto a un essere umano."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, riscontriamo elementi molto diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Mentre le persone generate presentano tassi significativamente più elevati per le parole di lusso, quelle umane hanno una distribuzione molto più ampia delle parole, mentre le parole stereotipate che vengono generate nelle persone generate sono in realtà solo le parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, essenzialmente solo quelli positivi o, quantomeno, non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "E in effetti, il dizionario non riesce a cogliere appieno molti dei modelli dannosi che abbiamo visto nelle pagine precedenti, pertanto ci rivolgeremo ai risultati del metodo di Mark per dimostrare come queste parole positive favoriscano gli stereotipi e perpetuino gli stereotipi stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, esaminiamo come i ritratti apparentemente positivi riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Per prima cosa, per quanto riguarda i gruppi di Mark, le parole più frequenti includono termini come cultura, tradizione, orgoglio ed esotico, e queste parole definiscono tali gruppi unicamente in relazione al loro senso di identità e li distinguono dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Ciò contribuisce a una lunga eredità di discriminazione e altre forme di pregiudizio per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, vi sono molte altre parole comuni che si riflettono in queste, in particolare per le donne di colore. Ad esempio, il termine che descrive le donne latine include elementi come \"vivace\" e \"curiosa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "che si connette a un tropicalismo tropicale per donne asiatiche, le parole sono come piccole, delicate e setose."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "che si ricollega a una lunga storia di iper-sessualizzazione delle donne asiatiche, percepite come eccessivamente docili e sottomesse, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "E infine, per le donne nere, osserviamo che alcune delle parole più ricorrenti sono termini come forte e resiliente."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si ricollega a un archetipo che è stato definito l'archetipo della \"donna nera forte\", e sebbene a prima vista possa sembrare positivo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "Sono state presentate ricerche che dimostrano come questo tipo di archetipo sia in realtà molto dannoso, poiché esercita una notevole pressione su queste categorie demografiche affinché siano resilienti e forti di fronte agli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Invece di impegnarsi attivamente per modificare i comportamenti di quelle persone, esercita una pressione su di loro affinché li superino, con conseguenze negative sulla salute, sia per loro che per altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "Più recentemente, abbiamo scoperto che i termini utilizzati per descrivere il gruppo di mercato riflettono in realtà narrazioni essenziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sulla base di questi schemi, possiamo concludere con tre raccomandazioni per i proprietari dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, dovremmo richiedere stereotipi positivi e narrazioni positive, dovremmo altresì utilizzare le relazioni interpersonali per studiare fenomeni, poiché vi sono molte cose che potrebbero essere trascurate se non lo facessimo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "E, infine, dovrebbe esserci una maggiore trasparenza riguardo ai metodi di mitigazione distorti."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "perché, ad esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a una sorta di—insomma—anomalia."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "un'eccessiva allineamento dei valori in atto, o forse altri metodi anti-stereotipici che stanno determinando questi modelli dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo davvero formulare alcuna ipotesi né approfondire ulteriormente l'analisi senza maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato. #um Buona permanenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Jin Wei Yi e sono della University of Science and Technology of China."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È con piacere che presento un breve video pubblicitario riguardante un metodo per proteggere il copyright dei modelli linguistici di grandi dimensioni per embedding e servizi tramite watermark “backdoor”."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo innanzitutto il contesto relativo all'integrazione dei servizi IT."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, modelli linguistici di grandi dimensioni come TPT, LLaMA, PaLM eccellono nella comprensione e nella generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "I servizi di embedding sono uno dei servizi sviluppati sulla base di modelli linguistici di grandi dimensioni per supportare diverse attività di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, OpenAI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, studi recenti hanno dimostrato che l’attaccante può sottrarre il modello attraverso l’apprendimento basato sull'embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore dell'embedding come servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "per tutelare il diritto d'autore dei servizi incorporati, una delle soluzioni è quella di incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo della filigrana deve soddisfare le seguenti proprietà: in primo luogo, il metodo dovrebbe essere applicabile all'incorporamento e ai servizi; in secondo luogo, la filigrana non dovrebbe compromettere l'utilità degli incorporamenti forniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "terzo, la filigrana dovrebbe essere sufficientemente coperta da non essere rilevabile dall'attaccante, oppure l'attaccante dovrebbe essere in grado di rimuoverla facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile alle superfici dell'attaccante durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "I lavori esistenti possono essere generalmente classificati in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, tali metodi o non sono applicabili all'integrazione di servizi pubblicitari, o mancano di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo proponiamo un marker di embedding, ovvero un metodo di watermark basato su backdoor applicabile all'embedding e ai servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "then let me introduce the details of our embedded marker.\nEmbedded marker contains two main steps: watermark injection and a copyright."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di queste fasi principali, selezioniamo innanzitutto un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Assumiamo che il fornitore possa raccogliere un corpus testuale generale e calcolarne la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di watermark, definiamo innanzitutto un embedding di destinazione. Quando un utente invia una frase al servizio del fornitore, quest'ultimo conta il numero di trigger presenti nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "l'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase.\nquando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del copyright consiste nell'individuare se un modello alla base di un altro servizio contenga il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Costruiamo inizialmente una backdoor e un dataset benigno. Il dataset backdoor contiene frasi di cui tutte le parole appartengono all'insieme trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "A quel punto, il fornitore richiede gli embedding al servizio di estrazione, insieme al dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "il coseno e la similarità l2 tra l'embedding richiesto e l'embedding di destinazione vengono calcolati. Calcoliamo la differenza di similarità tra il dataset benigno e quello backdoor, definita come delta coseno e delta l2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza matrice."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Conduciamo esperimenti su quattro dataset: HG News, Mind, SST2 e AresPam. Assumiamo che il fornitore applichi Wikitext al dataset per contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "i risultati su quattro set di dati dimostrano che il nostro marcatore integrato può offrire ottime prestazioni di rilevamento mantenendo al contempo un'elevata utilità per le attività successive."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo inoltre validato la riservatezza dell'embedding fornito diffondendo l'embedding di frasi su quarant'a z vpca. La legenda delle figure indica il numero di trigger in ciascuna frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come illustrato nelle figure, è difficile distinguere tra gli embedding vettoriali e gli embedding standard."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Questo è tutto, grazie.\nCi contatterà per discuterne."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Vasudha e sono una studentessa di dottorato in Informatica presso la Stony Brook University. Vorrei presentare il mio lavoro accettato in ACL ventitrè come articolo esteso, riguardante il transfer learning per la rilevazione di dissonanze, affrontando la sfida di classe."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Cominceremo definendo la dissonanza cognitiva e spiegando perché si tratta di un problema importante da studiare in ambito linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, questo caso in cui una persona afferma: \"So che le sigarette mi uccideranno\", e poi prosegue dicendo: \"Ne ho fumate un paio dopo la riunione, questa credenza e questa azione sono inconsistenti e lo sono\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Non credo di poter ottenere il lavoro senza di loro, giustificando la seconda occorrenza e dimostrando una connessione."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "La lingua è estremamente diffusa e ne facciamo esperienza nelle decisioni quotidiane, pertanto è davvero facile rintracciarla in altre lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, perché lo studio della distanza cognitiva può aiutarvi a comprendere gli effetti del disaccordo tra persone, delle tendenze e delle credenze, degli atteggiamenti e dei comportamenti nel cambiamento demografico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "Un'elevata dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare le persone a comprendere meglio la salute mentale."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "L'analisi del linguaggio del linguaggio può risultare altresì utile per comprendere l'estremismo e la polarizzazione dei gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili di personalità degli individui e ci aiuta a comprendere meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Al fine di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'analisi su larga scala delle relazioni dissonanti. Abbiamo utilizzato un approccio basato sulla dissonanza iniziale, come illustrato dal diagramma di flusso qui presentato."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "Le password sono utilizzate dal P.T.B. e le unità discorsive sono annotate secondo le linee guida descritte nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può notare qui, la dissonanza è stata riscontrata solo nel tre virgola cinque percento delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Stiamo raccogliendo circa mille esempi dell'addestramento dell'unità per la classe di prima, e stiamo addestrando solo per quaranta-tre esempi del business."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Il problema della bassa incidenza di dissonanza e dell'assenza di qualsiasi dataset precedente è il problema dell'assoluto."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "L'esperimento è stato condotto utilizzando la combinazione di Trasmissione e Apprendimento Attivo, che consente di prelevare più di un campione e riduce i costi complessivi dell'esperimento migliorando la rilevazione della differenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Il primo modello non è in grado di cogliere la classe per nulla, iniziamo il processo di trasferimento dei pesi da..."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Passeremo da due argomenti distinti, \"Argomento Indipendente\" e \"Discussione\" proveniente da due persone diverse, oppure da un argomento differente."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamato Debate qui e nella Classificazione Binaria di Espansione e Classi di Confronto di P.E.T.B., poiché questi sono strettamente correlati al concetto di consonanze e dissonanze e li chiamiamo C.E.E. qui"}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo rilevato che il trasferimento delle prestazioni allo zero sul dataset è già notevolmente superiore rispetto al migliore, con un AUC di 0,6."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Il modo migliore per farlo consiste nell'utilizzare il modello dell'Apprendimento Attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determineremo il metodo più efficace per aggiornare il modello con i nuovi dati provenienti da ciascuna fase di Apprendimento Attivo e Responsabilità. Tutti i dati raccolti tramite l'Apprendimento Attivo vengono quindi aggiornati mediante l'addestramento sul set di dati più recente."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Analizzando le diverse strategie, riscontriamo che la performance cumulativa risulta pari o superiore a quella iterativa in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi della classe, utilizzeremo la strategia di probabilità di classe, PRC, per selezionare la maggior parte degli esempi che sono verosimilmente altamente distinguibili dal modello corrente in qualsiasi round del processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Lo confrontiamo con le altre strategie all'avanguardia comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che la strategia di PR proposta funziona meglio rispetto ad altre strategie all'avanguardia, sebbene la differenza sia minima."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "Tra i migliori, grazie alle migliori strategie, abbiamo migliorato la classificazione a sette e mezzo, che rappresenta finora le prestazioni più elevate ottenute su questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Verifichiamo inoltre la fattibilità di ciascuna strategia in termini di qualità e costo dell'annotazione e riscontriamo che PRC presenta la più alta percentuale di dissonanza e funziona meglio per la classificazione, sebbene gli annotatori trovino anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo constatato che la PRC è una strategia semplice per l'acquisizione di classi e che l'avvio congiunto, utilizzando compiti trasferibili opportunamente progettati e utili, risulta efficace."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre riscontrato che l'aggiornamento iterativo è utile per il trasferimento da un dominio all'altro, mentre gli aggiornamenti attivi all'interno del dominio beneficiano di aggiornamenti cumulativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro codice, al dataset e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie."}
